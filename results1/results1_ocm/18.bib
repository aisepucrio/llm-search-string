@inproceedings{10.1145/3646548.3676599,
author = {Gomez-Vazquez, Marcos and Cabot, Jordi},
title = {Exploring the Use of Software Product Lines for the Combination of Machine Learning Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676599},
doi = {10.1145/3646548.3676599},
abstract = {The size of Large Language Models (LLMs), and Machine Learning (ML) models in general, is a key factor of their capacity and quality of their responses. But it comes with a high cost, both during the training and the model execution phase. Recently, various model merging techniques and Mixture of Experts (MoE) architectures are gaining popularity as they enable the creation of large models by combining other existing ones (the "experts" in the MoE approach). Creating these combinations remains a deep technical task with many possible configurations to consider. In this sense, this paper aims to democratize the creation of combined ML models by presenting a product line approach to the specification and training of this type of ML architectures from an initial feature model that helps users define, among other aspects, the type of models they want to combine, the combination strategy and even, for the MoE approach, the tasks that should be associated to each expert.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {26–29},
numpages = {4},
keywords = {Feature Model, Large Language Model, Machine Learning, Mixture of Experts, Model Merging, Software Product Line},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579028.3609018,
author = {Nienaber, S\"{o}ren and Soorati, Mohammad D. and Ghasemzadeh, Arash and Ghofrani, Javad},
title = {Software Product Lines for Development of Evolutionary Robots},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609018},
doi = {10.1145/3579028.3609018},
abstract = {Evolutionary Robotics utilizes evolutionary algorithms for training robot controllers (e.g., neural networks) and adapting robot morphologies for different environments in design and runtime. One of the main challenges in robotics is the lack of reusability as AI-based robot controllers have to be trained from scratch for any change in the environment or a new task specification that a robot should adapt to. Training Artificial Neural Networks can be computationally heavy, time-consuming, and hard to reuse due to their monolithic black-box nature. The building blocks of emerging behaviors from Artificial Neural Networks cannot be fully separated or reused. We address the issue of reusability and propose an incremental approach for applying the reusability of behaviors. We implemented an Evolutionary Robotics framework to form a product family of robots. This product family is used to show the feasibility of our method for handling variability in a domain. Our results can be used to demonstrate a sample binding between the software product lines and machine learning domains.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {77–84},
numpages = {8},
keywords = {Software Product Lines, Primitive Behaviors, Mobile Robots, Evolutionary Robotics, Configuration},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3546991,
author = {Tavassoli, Shaghayegh and Damasceno, Carlos Diego N. and Khosravi, Ramtin and Mousavi, Mohammad Reza},
title = {Adaptive behavioral model learning for software product lines},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546991},
doi = {10.1145/3546932.3546991},
abstract = {Behavioral models enable the analysis of the functionality of software product lines (SPL), e.g., model checking and model-based testing. Model learning aims to construct behavioral models. Due to the commonalities among the products of an SPL, it is possible to reuse the previously-learned models during the model learning process. In this paper, an adaptive approach, called PL*, for learning the product models of an SPL is presented based on the well-known L* algorithm. In this method, after learning each product, the sequences in the final observation table are stored in a repository which is used to initialize the observation table of the remaining products. The proposed algorithm is evaluated on two open-source SPLs and the learning cost is measured in terms of the number of rounds, resets, and input symbols. The results show that for complex SPLs, the total learning cost of PL* is significantly lower than that of the non-adaptive method in terms of all three metrics. Furthermore, it is observed that the order of learning products affects the efficiency of PL*. We introduce a heuristic to determine an ordering which reduces the total cost of adaptive learning.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {142–153},
numpages = {12},
keywords = {software product lines, finite state machines, automata learning, adaptive model learning},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3425898.3426959,
author = {Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Correctness-by-construction for feature-oriented software product lines},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426959},
doi = {10.1145/3425898.3426959},
abstract = {Software product lines are increasingly used to handle the growing demand of custom-tailored software variants. They provide systematic reuse of software paired with variability mechanisms in the code to implement whole product families rather than single software products. A common domain of application for product lines are safety-critical systems, which require behavioral correctness to avoid dangerous situations in-field. While most approaches concentrate on post-hoc verification for product lines, we argue that a stepwise approach to create correct programs may be beneficial for developers to manage the growing variability. Correctness-by-construction is such a stepwise approach to create programs using a set of small, tractable refinement rules that guarantee the correctness of the program with regard to its specification. In this paper, we propose the first approach to develop correct-by-construction software product lines using feature-oriented programming. First, we extend correctness-by-construction by two refinement rules for variation points in the code. Second, we give a proof for the soundness of the proposed rules. Third, we implement our technique in a tool called VarCorC and show the applicability of the tool by conducting two case studies.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {22–34},
numpages = {13},
keywords = {software product lines, formal verification, feature-oriented programming, correctness-by-construction},
location = {Virtual, USA},
series = {GPCE 2020}
}

@inproceedings{10.1145/3546932.3546998,
author = {Trasobares, Jose Ignacio and Domingo, \'{A}frica and Arcega, Lorena and Cetina, Carlos},
title = {Evaluating the benefits of software product lines in game software engineering},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546998},
doi = {10.1145/3546932.3546998},
abstract = {Video game development is one of the fastest-growing industries in the world. The use of software product lines (SPLs) has proven to be effective in developing different types of software at a lower cost, in less time, and with higher quality. There are recent research efforts that propose to apply SPLs in the domain of video games. Video games present characteristics that differentiate their development from the development of classic software; for example, game developers perceive more difficulties than other non-game developers when reusing code. In this paper, we evaluate if the adoption of an SPL in game software engineering (GSE) can generate the same benefits as in classic software engineering (CSE) considering the case study of Kromaia. As in other disciplines dealing with human behaviour, empirical research allows for building a reliable knowledge base in software engineering. We present an experiment comparing two development approaches, Clone and Own (CaO) and an SPL in terms of correctness, efficiency, and satisfaction when subjects develop elements of a commercial video game. The results indicate that the elements developed using the SPL are more correct than those developed with CaO but do not indicate significant improvement in efficiency or satisfaction. Our findings suggest that SPLs in GSE may play a different role than the one they have played for decades in CSE. Specifically, SPLs can be relevant to generating new video game content or to balancing video game difficulty.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {120–130},
numpages = {11},
keywords = {software product line engineering, game software engineering, empirical comparison},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609016,
author = {Acher, Mathieu and Martinez, Jabier},
title = {Generative AI for Reengineering Variants into Software Product Lines: An Experience Report},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609016},
doi = {10.1145/3579028.3609016},
abstract = {The migration and reengineering of existing variants into a software product line (SPL) is an error-prone and time-consuming activity. Many extractive approaches have been proposed, spanning different activities from feature identification and naming to the synthesis of reusable artefacts. In this paper, we explore how large language model (LLM)-based assistants can support domain analysts and developers. We revisit four illustrative cases of the literature where the challenge is to migrate variants written in different formalism (UML class diagrams, Java, GraphML, statecharts). We systematically report on our experience with ChatGPT-4, describing our strategy to prompt LLMs and documenting positive aspects but also failures. We compare the use of LLMs with state-of-the-art approach, BUT4Reuse. While LLMs offer potential in assisting domain analysts and developers in transitioning software variants into SPLs, their intrinsic stochastic nature and restricted ability to manage large variants or complex structures necessitate a semiautomatic approach, complete with careful review, to counteract inaccuracies.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {57–66},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1007/s00165-021-00554-3,
author = {de Lara, Juan and Guerra, Esther},
title = {Language Family Engineering with Product Lines of Multi-level Models},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00554-3},
doi = {10.1007/s00165-021-00554-3},
abstract = {Modelling is an essential activity in software engineering. It
typically involves two meta-levels: one includes meta-models that
describe modelling languages, and the other contains models built by
instantiating those meta-models.  Multi-level modelling generalizes this approach by allowing models to span an arbitrary
number of meta-levels. A scenario that profits from multi-level
modelling is the definition of language families that can be
specialized (e.g., for different domains) by successive refinements
at subsequent meta-levels, hence promoting language reuse. This
enables an  open set of variability options given by all
possible specializations of the language family. However,
multi-level modelling lacks the ability to express closed variability regarding the availability of language primitives or the
possibility to opt between alternative primitive realizations. This
limits the reuse opportunities of a language family. To improve this
situation, we propose a novel combination of product lines with
multi-level modelling to cover both open and closed variability. Our
proposal is backed by a formal theory that guarantees correctness,
enables top-down and bottom-up language variability design, and is
implemented atop the MetaDepth multi-level modelling tool.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1173–1208},
numpages = {36},
keywords = {MetaDepth, Software language engineering, Domain-specific languages, Product lines, Multi-level modelling, Meta-modelling}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00022,
author = {Azanza, Maider and Irastorza, Arantza and Medeiros, Raul and D\'{\i}az, Oscar},
title = {Onboarding in software product lines: concept maps as welcome guides},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00022},
doi = {10.1109/ICSE-SEET52601.2021.00022},
abstract = {With a volatile labour and technological market, onboarding is becoming increasingly important. The process of incorporating a new developer, a.k.a. the newcomer, into a software development team is reckoned to be lengthy, frustrating and expensive. Newcomers face personal, interpersonal, process and technical barriers during their incorporation, which, in turn, affects the overall productivity of the whole team. This problem exacerbates for Software Product Lines (SPLs), where their size and variability combine to make onboarding even more challenging, even more so for developers that are transferred from the Application Engineering team into the Domain Engineering team, who will be our target newcomers. This work presents concept maps on the role of sensemaking scaffolds to help to introduce these newcomers into the SPL domain. Concept maps, used as knowledge visualisation tools, have been proven to be helpful for meaningful learning. Our main insight is to capture concepts of the SPL domain and their interrelationships in a concept map, and then, present them incrementally, helping newcomers grasp the SPL and aiding them in exploring it in a guided manner while avoiding information overload. This work's contributions are four-fold. First, concept maps are proposed as a representation to introduce newcomers into the SPL domain. Second, concept maps are presented as the means for a guided exploration of the SPL core assets. Third, a feature-driven concept map construction process is introduced. Last, the usefulness of concept maps as guides for SPL onboarding is tested through a formative evaluation.Link to the online demo: https://rebrand.ly/wacline-cmap},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {122–133},
numpages = {12},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {software product lines, machine learning, configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {systematic literature review, software engineering, machine learning, Tertiary study}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {performance-influence models, machine learning, dynamic software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3510466.3510470,
author = {Greiner, Sandra and Nieke, Michael and Seidl, Christoph},
title = {Towards Trace-Based Synchronization of Variability Annotations in Evolving Model-Driven Product Lines},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510470},
doi = {10.1145/3510466.3510470},
abstract = {Annotative model-driven product lines allow to derive individual variants from a multi-variant model by exploiting annotations. Those declare the presence of each model element in a specific set of variants via a logical expression over features and may change during evolution. This provokes the risk of introducing conflicts causing logically cohesive elements of different models to appear in diverging sets of variants, which threatens the consistency of the product line. Existing work on propagating annotations across models employs the comparatively simple strategy of either overwriting or manually protecting any changed annotation in the target model but does not consider a backward propagation nor any form of synchronization. Therefore, we contribute a sophisticated method for synchronizing annotations which detects corresponding elements based on model transformation traces and resolves conflicting annotations by preserving syntactically different but semantically equal annotations according to the feature model. We demonstrate challenges and our solution method in a scenario of synchronizing two corresponding evolving multi-variant models.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {10},
keywords = {Software Evolution, Model-driven Software Product Line Engineering, Model Transformation},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1109/ASE56229.2023.00026,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Bridging the Gap between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00026},
doi = {10.1109/ASE56229.2023.00026},
abstract = {This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world --- Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1098–1110},
numpages = {13},
keywords = {machine learning, software defect prediction, nokia 5G, industry introduction, experience paper},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@article{10.1145/3467477,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3467477},
doi = {10.1145/3467477},
abstract = {Evolutionary Computation (EC) approaches are inspired by nature and solve optimization problems in a stochastic manner. They can offer a reliable and effective approach to address complex problems in real-world applications. EC algorithms have recently been used to improve the performance of Machine Learning (ML) models and the quality of their results. Evolutionary approaches can be used in all three parts of ML: preprocessing (e.g., feature selection and resampling), learning (e.g., parameter setting, membership functions, and neural network topology), and postprocessing (e.g., rule optimization, decision tree/support vectors pruning, and ensemble learning). This article investigates the role of EC algorithms in solving different ML challenges. We do not provide a comprehensive review of evolutionary ML approaches here; instead, we discuss how EC algorithms can contribute to ML by addressing conventional challenges of the artificial intelligence and ML communities. We look at the contributions of EC to ML in nine sub-fields: feature selection, resampling, classifiers, neural networks, reinforcement learning, clustering, association rule mining, and ensemble methods. For each category, we discuss evolutionary machine learning in terms of three aspects: problem formulation, search mechanisms, and fitness value computation. We also consider open issues and challenges that should be addressed in future work.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {161},
numpages = {35},
keywords = {swarm intelligence, learning optimization, Evolutionary computation}
}

@inproceedings{10.1145/3302333.3302345,
author = {Ali, Shaukat and Arcaini, Paolo and Hasuo, Ichiro and Ishikawa, Fuyuki and Lee, Nian-Ze},
title = {Towards a Framework for the Analysis of Multi-Product Lines in the Automotive Domain},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302345},
doi = {10.1145/3302333.3302345},
abstract = {Safety analyses in the automotive domain (in particular automated driving) present unprecedented challenges due to its complexity and tight integration with the physical environment. Given the diversity in the types of cars, potentially unlimited number of possible environmental and driving conditions, it is crucial to devise a systematic way of managing variability in hazards, driving and environmental conditions in individual cars, families of cars, and families of families of cars to facilitate analyses efficiently. To this end, we present our ongoing work in a research project that focuses on devising a model-based reasoning framework for systematically managing hazards in the automotive domain and supporting safety analyses (e.g., falsification).},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {12},
numpages = {6},
keywords = {Simulink, Product Lines, Hazard analysis, Falsification, Automotive domain},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3511805,
author = {Ram\'{\i}rez, Aurora and Feldt, Robert and Romero, Jos\'{e} Ra\'{u}l},
title = {A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511805},
doi = {10.1145/3511805},
abstract = {Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {21},
numpages = {42},
keywords = {industry, test case prioritisation, machine learning, taxonomy, Regression testing}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {feedback loops, Self-adaptation, MAPE-K}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Product Line Architecture, Machine Learning, Human-computer interaction},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {software product lines, model learning, family model, 150% model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {variability models, software product lines, metrics, implementation, feature models, SPL},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3338906.3342484,
author = {Moghadam, Mahshid Helali},
title = {Machine learning-assisted performance testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342484},
doi = {10.1145/3338906.3342484},
abstract = {Automated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1187–1189},
numpages = {3},
keywords = {Test case generation, Stress testing, Reinforcement learning, Performance testing, Autonomous testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3307650.3322267,
author = {Tarsa, Stephen J. and Chowdhury, Rangeen Basu Roy and Sebot, Julien and Chinya, Gautham and Gaur, Jayesh and Sankaranarayanan, Karthik and Lin, Chit-Kwan and Chappell, Robert and Singhal, Ronak and Wang, Hong},
title = {Post-silicon CPU adaptation made practical using machine learning},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322267},
doi = {10.1145/3307650.3322267},
abstract = {Processors that adapt architecture to workloads at runtime promise compelling performance per watt (PPW) gains, offering one way to mitigate diminishing returns from pipeline scaling. State-of-the-art adaptive CPUs deploy machine learning (ML) models on-chip to optimize hardware by recognizing workload patterns in event counter data. However, despite breakthrough PPW gains, such designs are not yet widely adopted due to the potential for systematic adaptation errors in the field.This paper presents an adaptive CPU based on Intel SkyLake that (1) closes the loop to deployment, and (2) provides a novel mechanism for post-silicon customization. Our CPU performs predictive cluster gating, dynamically setting the issue width of a clustered architecture while clock-gating unused resources. Gating decisions are driven by ML adaptation models that execute on an existing microcontroller, minimizing design complexity and allowing performance characteristics to be adjusted with the ease of a firmware update. Crucially, we show that although adaptation models can suffer from statistical blindspots that risk degrading performance on new workloads, these can be reduced to minimal impact with careful design and training.Our adaptive CPU improves PPW by 31.4% over a comparable non-adaptive CPU on SPEC2017, and exhibits two orders of magnitude fewer Service Level Agreement (SLA) violations than the state-of-the-art. We show how to optimize PPW using models trained to different SLAs or to specific applications, e.g. to improve datacenter hardware in situ. The resulting CPU meets real world deployment criteria for the first time and provides a new means to tailor hardware to individual customers, even as their needs change.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {adaptive hardware, clustered architectures, machine learning, runtime optimization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/2658761.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient testing of software product lines via centralization (short paper)},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658768},
doi = {10.1145/2658761.2658768},
abstract = {Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {49–52},
numpages = {4},
keywords = {random testing, automatic test generation, Software Product Lines},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.5555/3201607.3201711,
author = {Dyck, Jeff},
title = {Machine learning for engineering},
year = {2018},
publisher = {IEEE Press},
abstract = {Applying machine learning techniques to solve production problems within electronic design automation is complex. This is because production engineering applications have accuracy, scalability, complexity, verifiability, and usability requirements that are not met by traditional machine learning approaches. These additional challenges are often not well understood or adequately solved in practice, which causes production machine learning approaches to fail. This invited paper examines these engineering-specific challenges and presents some effective solutions based on Solido's experience developing a suite of successful applied machine learning solutions for EDA over the past twelve years.},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
pages = {422–427},
numpages = {6},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {software product lines, services, health watcher system, cloud platforms, cloud computing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {software testing, feature models, GCC},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3410352.3410730,
author = {BenIdris, Mrwan and Ammar, Hany and Dzielski, Dale and Benamer, Wisam H.},
title = {Prioritizing Software Components Risk: Towards a Machine Learning-based Approach},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410730},
doi = {10.1145/3410352.3410730},
abstract = {Technical Debt (TD) can be detected using different methods. TD is a metaphor that refers to short-term solutions in software development, which may affect the cost of the software development life-cycle. Several tools have been developed to detect, estimate, or manage TD. TD can be indicated through smells, code comments, and software metrics. Machine learning Techniques (MLTs) are used in many software engineering topics such as fault-proneness, bug severity, and code smell. In this paper we use four internal structure metrics to identify and classify Architecture Technical Debt (ATD) risk by using MLTs. We show that MLTs can identify and classify the risk of ATD on software components to help the decision-makers to prioritizing the refactoring decisions based on the level of the risk.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {1},
numpages = {11},
keywords = {Architecture Smells, Architecture Technical Debt, Machine Learning, Software Risk},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/3646548.3676543,
author = {Stummer, Alexander and Hager, Anna-Lena and Rabiser, Rick},
title = {Towards a Flexible Approach for Variability Mining},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676543},
doi = {10.1145/3646548.3676543},
abstract = {Software Product Line (SPL) Engineering relies on explicitly documenting and managing variability information, typically in variability models such as feature models, and relating them with reusable artifacts, e.g., software components. Many companies still rely on clone-and-own reuse approaches and could benefit from adopting an SPL approach instead. However, extracting variability information from existing systems is often challenging due to their size and complexity. Also, manually creating and maintaining variability models and relating them with reusable artifacts is very expensive and requires expert knowledge. To address this problem, SPL reverse-engineering approaches try to automatically extract variability information from existing systems to populate variability models with it. Unfortunately, many existing approaches are limited to a single artifact type and only a few more widely applicable methods have been proposed. In this short paper, we present our vision of a flexible, extensible and artifact-independent approach, which provides a framework for users to mine the variability of their existing system variants and automatically reverse-engineer SPLs. We discuss challenges and give an overview of a possible architecture as well as the steps necessary to realize our approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {75–81},
numpages = {7},
keywords = {Reverse Engineering, Software Product Lines, Variability Mining},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/1806799.1806819,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and K\"{a}stner, Christian and Schulze, Michael},
title = {An analysis of the variability in forty preprocessor-based software product lines},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806819},
doi = {10.1145/1806799.1806819},
abstract = {Over 30 years ago, the preprocessor cpp was developed to extend the programming language C by lightweight metaprogramming capabilities. Despite its error-proneness and low abstraction level, the preprocessor is still widely used in present-day software projects to implement variable software. However, not much is known about how cpp is employed to implement variability. To address this issue, we have analyzed forty open-source software projects written in C. Specifically, we answer the following questions: How does program size influence variability? How complex are extensions made via cpp's variability mechanisms? At which level of granularity are extensions applied? Which types of extension occur? These questions revive earlier discussions on program comprehension and refactoring in the context of the preprocessor. To provide answers, we introduce several metrics measuring the variability, complexity, granularity, and types of extension applied by preprocessor directives. Based on the collected data, we suggest alternative implementation techniques. Our data set is a rich source for rethinking language design and tool support.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {105–114},
numpages = {10},
keywords = {software product lines, empirical study, C preprocessor},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3634713.3634732,
author = {Acher, Mathieu},
title = {A Demonstration of End-User Code Customization Using Generative AI},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634732},
doi = {10.1145/3634713.3634732},
abstract = {Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configurators. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {139–145},
numpages = {7},
keywords = {LLM, code synthesis, customization, end-user programming, generative AI, generator, software product lines, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2837060.2837066,
author = {Han, Ji-Hyeong and Kim, Rockwon and Chi, Su-Young},
title = {Applications of Machine Learning Algorithms to Predictive Manufacturing: Trends and Application of Tool Wear Compensation Parameter Recommendation},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837066},
doi = {10.1145/2837060.2837066},
abstract = {The manufacturing industry has become more competitive because of globalization and fast change in the industry. To survive from the global market, manufacturing enterprises should reduce the product cost and increase the productivity. The most promising way is applying the information communication technology especially machine learning algorithms to the traditional manufacturing system. This paper presents recent trends of applying machine learning techniques to manufacturing system and briefly explains each kind of applications. As a representative application of machine learning algorithms to manufacturing system, a generalized tool wear compensation parameter recommendation framework using regression algorithms and preliminary results using real data gathered from local and small manufacturing are also presented.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {51–57},
numpages = {7},
keywords = {tool wear compensation parameter recommendation, machine learning, Predictive manufacturing},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {variability, tool, simulation, product line engineering, dynamic product line models, constraints},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3646548.3672597,
author = {Purandare, Salil and Cohen, Myra B.},
title = {Exploration of Failures in an sUAS Controller Software Product Line},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672597},
doi = {10.1145/3646548.3672597},
abstract = {Small uncrewed aerial systems (sUAS) are growing in their use for commercial, scientific, recreational, and emergency management purposes. A critical part of a successful flight is a correctly tuned controller which manages the physics of the vehicle. If improperly configured, it can lead to flight instability, deviation, or crashes. These types of misconfigurations are often within the valid ranges specified in the documentation; hence, they are hard to identify. Recent research has used fuzzing or explored only a small part of the parameter space, providing little understanding of the configuration landscape itself. In this work we leverage software product line engineering to model a subset of the parameter space of a widely used flight control software, using it to guide a systematic exploration of the controller space. Via simulation, we test over 20,000 configurations from a feature model with 50 features and 8.88 \texttimes{} 1034 products, covering all single parameter value changes and all pairs of changes from their default values. Our results show that only a small number of single configuration changes fail (15%), however almost 40% fail when we evaluate changes to two-parameters at a time. We explore the interactions between parameters in more detail, finding what appear to be many dependencies and interactions between parameters which are not well documented. We then explore a smaller, exhaustive product line model, with eight of the most important features (and 6,561 configurations) and uncover a complex set of interactions; over 48% of all configurations fail.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {125–135},
numpages = {11},
keywords = {Configurability, Software Product Lines, sUAS},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {variability modeling, software product lines, design patterns},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609007,
author = {Fortz, Sophie},
title = {Variability-aware Behavioural Learning},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609007},
doi = {10.1145/3579028.3609007},
abstract = {Addressing variability proactively during software engineering activities means shifting from reasoning on individual systems to reasoning on families of systems. Adopting appropriate variability management techniques can yield important economies of scale and quality improvements. Conversely, variability can also be a curse, especially for Quality Assurance (QA), i.e., verification and testing of such systems, due to the combinatorial explosion of the number of software variants. Featured Transition Systems (FTSs) were introduced as a way to represent and reason about the behaviour of Variaility-intensive Systems (VISs). By labelling a transition system with feature expressions, FTSs capture multiple variants of a system in a single model, enabling reasoning at the family level. They have shown significant improvements in automated QA activities such as model-checking and model-based testing, as well as guiding design exploration activities. Yet, as most model-based approaches, FTS modelling requires both strong human expertise and significant effort that would be unaffordable in many cases, in particular for large legacy systems with outdated specifications and/or systems that evolve continuously.Therefore, this PhD project aims to automatically learn FTSs from existing artefacts, to ease the burden of modelling FTS and support continuous QA activities. To answer this research challenge, we propose a two-phase approach. First, we rely on deep learning techniques to locate variability from execution traces. For this purpose, we implemented a tool called VaryMinions. Then, we use these annotated traces to learn an FTS. In this second part, we adapt the seminal L algorithm to learn behavioural variability. Both frameworks are open-source and we evaluated them separately on several datasets of different sizes and origins (e.g., software product lines and configurable business processes).},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {11–15},
numpages = {5},
keywords = {Variability Mining, Software Product Lines, Reverse Engineering, Featured Transition Systems, Active Automata Learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3547008,
author = {Amraoui, Yassine El and Blay-Fornarino, Mireille and Collet, Philippe and Precioso, Fr\'{e}d\'{e}ric and Muller, Julien},
title = {Evolvable SPL management with partial knowledge: an application to anomaly detection in time series},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547008},
doi = {10.1145/3546932.3547008},
abstract = {In Machine Learning (ML), the resolution of anomaly detection problems in time series presents a great diversity of practices as it can correspond to many different contexts. These practices cover both grasping the business problem and designing the solution itself. By practice, we designate explicit and implicit steps toward resolving a problem, while a solution corresponds to a combination of algorithms selected for their performance on a given problem. Two related issues arise. The first one is that the practices are individual and not explicitly mutualized. The second one is that choosing one solution over another is all the more difficult to justify because the space of solutions and the evaluation criteria are vast and evolve rapidly with the advances in ML. To solve these issues and tame the evolving diversity in ML, a Software Product Line (SPL) approach can be envisaged to represent the variable set of solutions. However, this requires characterizing an ML business problem through an explicit set of criteria and justifying one ML solution over all others. The resolution of anomaly detection problems is thus different from finding the best configuration workflow from past configurations but lies more in guiding the configuration towards a solution that may never have been studied before. This paper proposes an SPL approach that capitalizes on past practices by exploiting a variability-aware representation to detect new criteria and constraints when practices adopt different solutions to seemingly similar problems. We report on the evaluation of our approach using a set of applications from the literature and an ML software company. We show how the analysis of practices makes it possible to consolidate the knowledge contained in the SPL.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {222–233},
numpages = {12},
keywords = {software product line, metrics, machine learning, evolution},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3571788.3571804,
author = {Temple, Paul and Perrouin, Gilles},
title = {Explicit or Implicit? On Feature Engineering for ML-based Variability-intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571804},
doi = {10.1145/3571788.3571804},
abstract = {Software variability engineering benefits from Machine Learning (ML) to learn e.g., variability-aware performance models, explore variants of interest and minimize their energy impact. As the number of applications of combining variability with ML grows, we would like to reflect on what is the core to the configuration process in software variability and inference in ML: feature engineering. These disciplines previously managed features explicitly, easing graceful combinations. Now, deep learning techniques derive automatically obscure but efficient features from data. Shall we give up explicit feature management in variability-intensive systems to embrace machine learning advances?},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {91–93},
numpages = {3},
keywords = {software variability, machine learning, feature},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3622748.3622750,
author = {Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline},
title = {Feature selection in an interactive search-based PLA design approach},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622750},
doi = {10.1145/3622748.3622750},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers’ (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Machine Learning, Interactive search-based Software Engineering, Feature Selection},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Software Product Lines, Requirement Documents, Feature Terms Identification, Feature Extraction},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {software product lines, performance prediction, machine learning, configurable systems},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3646548.3672590,
author = {Sundermann, Chico and Brancaccio, Vincenzo Francesco and Kuiter, Elias and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas},
title = {Collecting Feature Models from the Literature: A Comprehensive Dataset for Benchmarking},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672590},
doi = {10.1145/3646548.3672590},
abstract = {Feature models are widely used for specifying the valid configurations of product lines. Many automated analyses on feature models have been considered, but they often depend on computationally complex algorithms (e.g., solving satisfiability problems). To identify and develop efficient reasoning engines, it is necessary to compare their performance on practically relevant feature models. However, empirical evaluations on feature-model analysis often suffer from the limitations of available feature-model datasets in terms of transferability. A major problem is the accessibility of relevant feature models as they are scattered over numerous publications. In this work, we perform a literature survey on empirical evaluations that target the performance of feature-model analyses to examine common evaluation practices and collect feature models for future evaluations. Furthermore, we examine the suitability of the derived collection for benchmarking performance. To improve accessibility, we provide a repository including all 2,518 identified feature models from 13 application domains, such as system software.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {54–65},
numpages = {12},
keywords = {benchmark, evaluation, feature model, product line, survey},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3691620.3695594,
author = {Sundermann, Chico and Loth, Jacob and Th\"{u}m, Thomas},
title = {Efficient Slicing of Feature Models via Projected d-DNNF Compilation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695594},
doi = {10.1145/3691620.3695594},
abstract = {Configurable systems often contain components from different fields or disciplines that are relevant for distinct stakeholders. For instance, tests or analyses targeting interactions of the software of a cyber-physical system may be only applicable for software components. However, managing such components in isolation is not trivial due, for instance, interdependencies between features. Feature models are a common formalism to specify such dependencies. Feature-model slicing corresponds to creating a subset of the feature model (e.g., with only components relevant to a particular stakeholder) that still preserves transitive dependencies from discarded features. However, slicing is computationally expensive and subsequent analyses often depend on complex computations, such as SAT or #SAT. With knowledge compilation, the original feature model can be translated to a beneficial format (e.g., d-DNNF or BDD) with an initial effort that accelerates subsequent analyses. Consequentially, acquiring a sliced target format depends on two expensive subsequent algorithms. In this work, we merge both steps by proposing projected d-DNNF compilation; a novel way to slice feature models that coincidently performs knowledge compilation to d-DNNF. Our empirical evaluation on real-world feature models shows that our tool pd4 often reduces runtimes substantially compared to existing techniques and scales to more input instances.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1332–1344},
numpages = {13},
keywords = {feature models, configurable systems, product lines, d-DNNF, knowledge compilation, slicing, projection},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {software product lines, reverse engineering, requirement documents, feature extraction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {software reuse, similarity, SPL adoption},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {mCRL2, Software Product Lines, Family-based model checking},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Variability Extraction, Systematic Literature Review, Software Product Lines, Reverse Engineering, Natural Language Documents, Feature Identification},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3520304.3529041,
author = {Rosa, Cl\'{a}udia Tupan and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards an interactive ranking operator for NSGA-II},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3529041},
doi = {10.1145/3520304.3529041},
abstract = {Knowing the Decision Makers (DM) preferences during the search process may help the algorithms to find solutions adequate to the DM profile. For this purpose, some interactive approaches allow the DM to evaluate solutions during the search process, rating them with scores. These scores represent the adequacy level of the solutions concerning the DM preferences and should influence the evolution of the search algorithm. Nevertheless, in these interactive approaches, this prioritization is partial and/or specific for a problem domain. In this context, this work aims to propose an initial version of an interactive ranking operator for NSGA-II, whose goal is to support the ranking of solutions considering both the DM preferences and the fitness of the solutions, for any software engineering domain. For validating the proposal, we incorporate this operator in an interactive approach for Product Line Architecture design. The results pointed out that the new ranking operator can properly deal with the DM preferences, giving a greater chance of surviving to those solutions with higher scores without compromising the diversity of solutions.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {794–797},
numpages = {4},
keywords = {human-computer interaction, machine learning, product line architecture},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3546932.3547014,
author = {Tavassoli, Shaghayegh and Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Khosravi, Ramtin},
title = {A benchmark for active learning of variability-intensive systems},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547014},
doi = {10.1145/3546932.3547014},
abstract = {Behavioral models are the key enablers for behavioral analysis of Software Product Lines (SPL), including testing and model checking. Active model learning comes to the rescue when family behavioral models are non-existent or outdated. A key challenge on active model learning is to detect commonalities and variability efficiently and combine them into concise family models. Benchmarks and their associated metrics will play a key role in shaping the research agenda in this promising field and provide an effective means for comparing and identifying relative strengths and weaknesses in the forthcoming techniques. In this challenge, we seek benchmarks to evaluate the efficiency (e.g., learning time and memory footprint) and effectiveness (e.g., conciseness and accuracy of family models) of active model learning methods in the software product line context. These benchmark sets must contain the structural and behavioral variability models of at least one SPL. Each SPL in a benchmark must contain products that requires more than one round of model learning with respect to the basic active learning L* algorithm. Alternatively, tools supporting the synthesis of artificial benchmark models are also welcome.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {245–249},
numpages = {5},
keywords = {model learning, featured finite state machines, benchmarking, behavioral variability},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3238147.3238201,
author = {Mukelabai, Mukelabai and Ne\v{s}i\'{c}, Damir and Maro, Salome and Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp},
title = {Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238201},
doi = {10.1145/3238147.3238201},
abstract = {Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {155–166},
numpages = {12},
keywords = {Product Lines, Highly Configurable Systems, Analysis},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3646548.3672593,
author = {Greiner, Sandra and Schulthei\ss{}, Alexander and Bittner, Paul Maximilian and Th\"{u}m, Thomas and Kehrer, Timo},
title = {Give an Inch and Take a Mile? Effects of Adding Reliable Knowledge to Heuristic Feature Tracing},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672593},
doi = {10.1145/3646548.3672593},
abstract = {Tracing features to software artifacts is a crucial yet challenging activity for developers of variability-intensive software projects. Developers can provide feature traces either proactively in a manual and rarely semi-automated way or recover them retroactively where automated approaches mainly rely on heuristics. While proactive tracing promises high reliability as developers know which features they realize when working on them, the task is cumbersome and without immediate benefit. Conversely, automated retroactive tracing offers high automation by employing heuristics but remains unreliable and dependent on the quality of the heuristic. To exploit the benefits of proactive and retroactive tracing while mitigating their drawbacks, this paper examines how providing a minimal seed of accurate feature traces proactively (give an inch) can boost the accuracy of automated, heuristic-based retroactive tracing (take&nbsp;a&nbsp;mile). We examine how comparison-based feature location, as one representative of retroactive feature tracing, can benefit from increasing amounts of proactively provided feature mappings. For retroactive comparison-based feature tracing, we find not only that increasing amounts of proactive information can boost the overall accuracy of the tracing but also that the number of variants available for comparison affects the effectiveness of the combined tracing. As a result, our work lays the foundations to optimize the accuracy of retroactive feature tracing techniques with pinpointed proactive knowledge exploitation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {84–95},
numpages = {12},
keywords = {software evolution, software product lines, software variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1145/3611663,
author = {Oh, Jeho and Batory, Don and Heradio, Rub\'{e}n},
title = {Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611663},
doi = {10.1145/3611663},
abstract = {A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {7},
numpages = {36},
keywords = {Software product lines, configuration optimization, product spaces, machine learning, uniform random sampling, random search, order statistics}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608989,
author = {K\"{o}nig, Christoph and Rosiak, Kamil and Cleophas, Loek and Schaefer, Ina},
title = {True Variability Shining Through Taxonomy Mining},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608989},
doi = {10.1145/3579027.3608989},
abstract = {Software variants of a Software Product Line (SPL) consist of a set of artifacts specified by features. Variability models document the valid relationships between features and their mapping to artifacts. However, research has shown inconsistencies between the variability of variants in features and artifacts, with negative effects on system safety and development effort. To analyze this mismatch in variability, the causal relationships between features, artifacts, and variants must be uncovered, which has only been addressed to a limited extent. In this paper, we propose taxonomy graphs as novel variability models that reflect the composition of variants from artifacts and features, making mismatches in variability explicit. Our evaluation with two SPL case studies demonstrates the usefulness of our variability model and shows that mismatches in variability can vary significantly in detail and severity.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {182–193},
numpages = {12},
keywords = {Variability Modeling, Taxonomy, Software Product Lines},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3652620.3687798,
author = {Sousa, Tiago and Ries, Beno\^{\i}t and Guelfi, Nicolas},
title = {Model-Driven Software Product Line Engineering of AI-Based Applications for Achieving Sustainable Development Goals: Vision Paper},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687798},
doi = {10.1145/3652620.3687798},
abstract = {Achieving the Sustainable Development Goals (SDGs) set by the United Nations requires innovative solutions to address the related complex and interconnected challenges. The application of AI has demonstrated the potential to significantly contribute to these efforts by providing advanced analytics and decision-making capabilities. However, integrating AI into sustainability initiatives faces several challenges, including the need for flexible and reusable solutions that can be adapted to diverse and evolving SDG contexts, as well as the challenge of making these technologies accessible to nonexpert stakeholders. This paper proposes an integrated approach that combines Model-Driven Engineering (MDE) with Software Product Line Engineering (SPLE) to address these challenges. The proposed process includes key activities such as domain analysis, metamodel-driven requirements specification, product derivation, and AI model training. This approach aims to automate the derivation of flexible and reusable AI architectures tailored to specific SDG contexts, thus reducing the development time of AI-based software solutions for sustainability efforts.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {523–527},
numpages = {5},
keywords = {model-driven engineering, software product line, artificial intelligence, sustainable development goals, vision paper},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1109/JCDL57899.2023.00018,
author = {Akella, Akhil Pandey and Koop, David and Alhoori, Hamed},
title = {Laying Foundations to Quantify the "Effort of Reproducibility"},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL57899.2023.00018},
doi = {10.1109/JCDL57899.2023.00018},
abstract = {Why are some research studies easy to reproduce while others are difficult? Casting doubt on the accuracy of scientific work is not fruitful, especially when an individual researcher cannot reproduce the claims made in the paper. There could be many subjective reasons behind the inability to reproduce a scientific paper. The field of Machine Learning (ML) faces a reproducibility crisis, and surveying a portion of published articles has resulted in a group realization that although sharing code repositories would be appreciable, code bases are not the end all be all for determining the reproducibility of an article. Various parties involved in the publication process have come forward to address the reproducibility crisis and solutions such as badging articles as reproducible, reproducibility checklists at conferences (NeurIPS, ICML, ICLR, etc.), and sharing artifacts on OpenReview come across as promising solutions to the core problem. The breadth of literature on reproducibility focuses on measures required to avoid ir-reproducibility, and there is not much research into the effort behind reproducing these articles. In this paper, we investigate the factors that contribute to the easiness and difficulty of reproducing previously published studies and report on the foundational framework to quantify effort of reproducibility.},
booktitle = {Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
pages = {56–60},
numpages = {5},
keywords = {effort of reproducibility, reproducibility, replicability, computational reproducibility, scholarly communication, science of science},
location = {Santa Fe, New Mexico, USA},
series = {JCDL '23}
}

@article{10.1145/3708554,
author = {Haigh, Thomas},
title = {Artificial Intelligence Then and Now},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3708554},
doi = {10.1145/3708554},
abstract = {From engines of logic to engines of bullshit?},
journal = {Commun. ACM},
month = jan,
pages = {24–29},
numpages = {6}
}

@inproceedings{10.1145/3593013.3594100,
author = {Kwegyir-Aggrey, Kweku and Gerchick, Marissa and Mohan, Malika and Horowitz, Aaron and Venkatasubramanian, Suresh},
title = {The Misuse of AUC: What High Impact Risk Assessment Gets Wrong},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594100},
doi = {10.1145/3593013.3594100},
abstract = {When determining which machine learning model best performs some high impact risk assessment task, practitioners commonly use the Area under the Curve (AUC) to defend and validate their model choices. In this paper, we argue that the current use and understanding of AUC as a model performance metric misunderstands the way the metric was intended to be used. To this end, we characterize the misuse of AUC and illustrate how this misuse negatively manifests in the real world across several risk assessment domains. We locate this disconnect in the way the original interpretation of AUC has shifted over time to the point where issues pertaining to decision thresholds, class balance, statistical uncertainty, and protected groups remain unaddressed by AUC-based model comparisons, and where model choices that should be the purview of policymakers are hidden behind the veil of mathematical rigor. We conclude that current model validation practices involving AUC are not robust, and often invalid.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1570–1583},
numpages = {14},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3639856.3639913,
author = {Rameshan, Renu M and Pandya, Meet and Goel, Somya},
title = {DepScan: 3D Under Vehicle Scanning System},
year = {2024},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639856.3639913},
doi = {10.1145/3639856.3639913},
abstract = {One of our earliest product lines is the Under Vehicle Scanning System (UVSS). The utility of this product is to scan the under side of a vehicle and detect a threat object which could be implanted on the under side. Our first version of the product was based on image level comparisons, with the system having only one camera. In the current version, using a stereo camera we do a depth based comparison which leads to better accuracy in object detection as well as aids in eliminating false positives. We have used learning based solutions for tackling some of the problems that arise in the whole pipeline. In this demo, we will be showing the software pipeline starting from image capture to the threat object detection.},
booktitle = {Proceedings of the Third International Conference on AI-ML Systems},
articleno = {55},
numpages = {3},
keywords = {3D object detection, RGBD saliency detection, classification, domain adaptation, registration, segmentation},
location = {Bangalore, India},
series = {AIMLSystems '23}
}

@inproceedings{10.1145/3637528.3671604,
author = {Mukerji, Abhimanyu and More, Sushant and Kannan, Ashwin Viswanathan and Ravi, Lakshmi and Chen, Hua and Kohli, Naman and Khawand, Chris and Mandalapu, Dinesh},
title = {Valuing an Engagement Surface using a Large Scale Dynamic Causal Model},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671604},
doi = {10.1145/3637528.3671604},
abstract = {With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5556–5565},
numpages = {10},
keywords = {causal inference, causal modeling, dynamic causal model, engagement surface, investment decisions, large-scale modeling, observational causal model, program valuation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3579027.3608985,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Greiner, Sandra and Moosherr, Benjamin and Krieter, Sebastian and Tinnes, Christof and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Views on Edits to Variational Software},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608985},
doi = {10.1145/3579027.3608985},
abstract = {Software systems are subject to frequent changes, for example to fix bugs or meet new customer requirements. In variational software systems, developers are confronted with the complexity of evolution and configurability on a daily basis; essentially handling changes to many distinct software variants simultaneously. To reduce the complexity of configurability for developers, filtered or projectional editing was introduced: By providing a partial or complete configuration, developers can interact with a simpler view of the variational system that shows only artifacts belonging to that configuration. Yet, such views are available for individual revisions only but not for edits performed across revisions. To reduce the complexity of evolution in variational software for developers, we extend the concept of views to edits. We formulate a correctness criterion for views on edits and introduce two correct operators for view generation, one operator suitable for formal reasoning, and a runtime optimized operator. In an empirical study, we demonstrate the feasibility of our operators by applying them to the change histories of 44 open-source software systems.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {141–152},
numpages = {12},
keywords = {variation control, software variability, software product lines, software evolution, projectional editing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608972,
author = {Acher, Mathieu and Duarte, Jos\'{e} Galindo and J\'{e}z\'{e}quel, Jean-Marc},
title = {On Programming Variability with Large Language Model-based Assistant},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608972},
doi = {10.1145/3579027.3608972},
abstract = {Programming variability is central to the design and implementation of software systems that can adapt to a variety of contexts and requirements, providing increased flexibility and customization. Managing the complexity that arises from having multiple features, variations, and possible configurations is known to be highly challenging for software developers. In this paper, we explore how large language model (LLM)-based assistants can support the programming of variability.We report on new approaches made possible with LLM-based assistants, like: features and variations can be implemented as prompts; augmentation of variability out of LLM-based domain knowledge; seamless implementation of variability in different kinds of artefacts, programming languages, and frameworks, at different binding times (compile-time or run-time). We are sharing our data (prompts, sessions, generated code, etc.) to support the assessment of the effectiveness and robustness of LLMs for variability-related tasks.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {8–14},
numpages = {7},
keywords = {variability, software product lines, programming, large language model, generative AI},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3547019,
author = {Kr\"{u}ger, Jacob and Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2022)},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547019},
doi = {10.1145/3546932.3547019},
abstract = {Implementing variability in a software system allows developers to deal with different customer needs and requirements---establishing a family of related system variants. Variability can be managed through opportunistic (e.g., clone-and-own) or systematic strategies (e.g., a software product line). In the product-line community, variability management has been researched for systems in numerous domains, such as defense, avionics, or finance, and for various platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities; especially those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems; are less aware of the respective state-of-the-art in variability management. Consequently, these communities face similar problems and start to redeveloped similar solutions as the product-line community already did. With the International Workshop on Variability Management for Modern Technologies, we intend to foster and strengthen synergies between the communities regarding variability management for modern technologies. We aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, problem descriptions, or solutions that are connected to reuse and variability management for modern technologies. By inviting different communities and initiating collaborations between them, we hope that VM4ModernTech raises the interest of researchers outside the product-line community for variability management, and thus avoid expensive redevelopments.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {266},
numpages = {1},
keywords = {variability management, software architecture, modern technologies},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {variability extraction, reverse engineering, relational concept analysis, complex software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@proceedings{10.1145/3639856,
title = {AIMLSystems '23: Proceedings of the Third International Conference on AI-ML Systems},
year = {2023},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/3646548.3672596,
author = {Chueca, Jorge and Blasco, Daniel and Cetina, Carlos and Font, Jaime},
title = {Leveraging Phylogenetics in Software Product Families: The Case of Latent Content Generation in Video Games},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672596},
doi = {10.1145/3646548.3672596},
abstract = {A family of software products comprises similar products within a defined scope that share common characteristics, often due to reuse techniques applied during development. This paper introduces an approach that applies biological insights to map the landscape of a software product family, identifying potential gaps within its scope. Phylogenetics studies the gene similarity among groups of organisms to understand ancestry among species. Leveraging Phylogenetics in software, our approach offers a structured view of a product family, aiding in the discovery of unexplored areas fitting the scope of the family. Our approach creates a phylogenetic tree that enables to easily identify latent products (ancestors) that did not exist in the original family. Those ancestors can then be reconstructed from existing products (descendants). The product family evaluated is a set of industry-scale video game non-playable characters. We assess this approach through video game simulations and scope metrics to determine how closely the reconstructed products align with the family’s scope. The results confirm that the content generated with phylogenetics aligns better with the family scope than the state-of-the-art procedural content generation techniques using evolutionary algorithms. Phylogenetics enhances content generation by providing a framework to understand and expand the product family with new content.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {113–124},
numpages = {12},
keywords = {Game Software Engineering, Phylogenetics, Procedural Content Generation, Software Product Families},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608971,
author = {Eichhorn, Domenik and Pett, Tobias and Osborne, Tobias and Schaefer, Ina},
title = {Quantum Computing for Feature Model Analysis: Potentials and Challenges},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608971},
doi = {10.1145/3579027.3608971},
abstract = {Feature modeling is a technique to model the variability of configurable systems. When working with feature models, it is possible to analyze them, for instance, by counting the number of valid configurations, searching feature model anomalies, or creating samples of configurations for testing. Classical feature model analysis techniques are based on solving algorithmic problems such as boolean satisfiability, satisfiability modulo theories, or integer linear programming. Existing analysis approaches provide satisfactory solutions for small and medium-sized problem instances, but scaling issues are observed for large-sized feature models. Quantum computers provide up to superpolynomial speedups for specific algorithmic problems and have the potential to solve those scaling issues. This paper analyzes the algorithmic techniques used in classical product line analysis and identifies potentials and challenges for quantum speedups. Our findings show that quantum algorithms like QAOA and Grover have the potential to speed up SAT and ILP-based feature model analysis techniques, but only after additional improvements in quantum hardware have been made.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {1–7},
numpages = {7},
keywords = {quantum computing, quantum algorithms, feature model analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2695664.2695907,
author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han\^{e}ne},
title = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695907},
doi = {10.1145/2695664.2695907},
abstract = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1602–1609},
numpages = {8},
keywords = {software product lines, measurement, feature model, evaluation},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3550356.3561575,
author = {Shi, Yechuan and Kienzle, J\"{o}rg and Guo, Jin L. C.},
title = {Feature-oriented modularization of deep learning APIs},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561575},
doi = {10.1145/3550356.3561575},
abstract = {Deep learning libraries provide vast APIs because of the multitude of supported input data types, pre-processing operations, and neural network types and configuration options. However, developers working on one concrete application typically use only a small subset of the API at any one given time. Newcomers hence have to read through tutorials and API documentation, gathering scattered information, trying to find the API that fits their needs. This is time consuming and error prone. To remedy this, we show how we modularized the API of a popular Java DL framework Deeplearning4j (DL4J) according to features. Beginner developers can interactively select desired high level features, and our tool generates the subset of the DL library API that corresponds to the selected features. We evaluate our modularization on DL4J code samples, demonstrating an average recall of 98.9% for API classes and 98.0% for API methods. The respective precision is 19.3% and 13.8%, which represents an improvement of two orders of magnitude compared to the complete DL4J API.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {367–374},
numpages = {8},
keywords = {feature interaction, concern-oriented reuse, DL4J, API generation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3472674.3473980,
author = {Fortz, Sophie and Temple, Paul and Devroey, Xavier and Heymans, Patrick and Perrouin, Gilles},
title = {VaryMinions: leveraging RNNs to identify variants in event logs},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473980},
doi = {10.1145/3472674.3473980},
abstract = {Business processes have to manage variability in their execution, e.g., to deliver the correct building permit in different municipalities. This variability is visible in event logs, where sequences of events are shared by the core process (building permit authorisation) but may also be specific to each municipality. To rationalise resources (e.g., derive a configurable business process capturing all municipalities’ permit variants) or to debug anomalous behaviour, it is mandatory to identify to which variant a given trace belongs. This paper supports this task by training Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) algorithms on two datasets: a configurable municipality and a travel expenses workflow. We demonstrate that variability can be identified accurately (&gt;87%) and discuss the challenges of learning highly entangled variants.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {13–18},
numpages = {6},
keywords = {Variability Mining, Recurrent Neural Networks, Configurable processes},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/3503229.3547050,
author = {Le, Viet-Man and Tran, Thi Ngoc Trang and Felfernig, Alexander},
title = {Consistency-based integration of multi-stakeholder recommender systems with feature model configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547050},
doi = {10.1145/3503229.3547050},
abstract = {Feature models are used to represent variability properties of complex items. In most of the cases, the assumption in feature model configuration is that single users/stakeholders are interacting with the underlying configurator. However, there are many scenarios where multiple stakeholders need to jointly complete a configuration task, for example, when selecting the features to be included in a company-wide software service or when deciding about the software features to be included in upcoming releases. In such cases, decisions have to be taken jointly where the constraints and preferences of individual stakeholders have to be taken into account. In this paper, we show how multi-stakeholder recommender systems can be integrated in feature model configuration scenarios.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {178–182},
numpages = {5},
keywords = {feature model configuration, feature models, group decision making, multi-stakeholder recommendation},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {variability management, software architecture},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1145/3524015,
author = {Edwards, Chris},
title = {Neural networks learn to speed up simulations},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524015},
doi = {10.1145/3524015},
abstract = {Physics-informed machine learning is gaining attention, but suffers from training issues.},
journal = {Commun. ACM},
month = apr,
pages = {27–29},
numpages = {3}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Software Product Line, Requirements Engineering, Natural Language Processing, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Product Line Architecture, Multi-Objective Optimization, Human-computer interaction},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00037,
author = {Yan, Ming and Chen, Junjie and Mao, Hangyu and Jiang, Jiajun and Hao, Jianye and Li, Xingjian and Tian, Zhao and Chen, Zhichao and Li, Dong and Xian, Zhangkong and Guo, Yanwei and Liu, Wulong and Wang, Bin and Sun, Yuefeng and Cui, Yongshun},
title = {Achieving Last-Mile Functional Coverage in Testing Chip Design Software Implementations},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00037},
doi = {10.1109/ICSE-SEIP58684.2023.00037},
abstract = {Defective chips may cause huge losses (even disasters), and thus ensuring the reliability of chips is fundamentally important. To ensure the functional correctness of chips, adequate testing is essential on the chip design implementation (CDI), which is the software implementation of the chip under design in hardware description languages, before putting on fabrication. Over the years, while some techniques targeting CDI functional testing have been proposed, there are still a number of hard-to-cover functionality points due to huge input space and complex constraints among variables in a test input. We call the coverage of these points last-mile functional coverage.Here, we propose the first technique targeting the significant challenge of improving last-mile functional coverage in CDI functional testing, called LMT, which does not rely on domain knowledge and CDI internal information. LMT first identifies the relevant variables in test inputs to the coverage of last-mile functionality points inspired by the idea of feature selection in machine learning, so as to largely reduce the search space. It then incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs (that satisfy complex constraints among variables) with a larger possibility. We conducted a practical study on two industrial CDIs in Huawei to evaluate LMT. The results show that LMT achieves at least 49.27% and 75.09% higher last-mile functional coverage than the state-of-the-art CDI test input generation techniques under the same number of test inputs, and saves at least 94.24% and 84.45% testing time to achieve the same functional coverage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {343–354},
numpages = {12},
keywords = {machine learning, functional coverage, test generation, chip design testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {symmetries, product line, knowledge compilation, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3555228.3555232,
author = {Freire, Willian and Rosa, Cl\'{a}udia and Amaral, Aline and Colanzi, Thelma},
title = {Validating an Interactive Ranking Operator for NSGA-II to Support the Optimization of Software Engineering Problems},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555228.3555232},
doi = {10.1145/3555228.3555232},
abstract = {Search-Based Software Engineering (SBSE) has been beneficial for optimizing the solution of several Software Engineering (SE) problems. The incorporation of Decision Makers (DM) preferences during the search process may help the algorithms to find more adequate solutions for their profiles. Some interactive approaches allow the DM to evaluate solutions, rating them with scores during the search process. These scores represent the adequacy level of the solutions in relation to the DM preferences and should influence the evolution of the search algorithm. In previous work, we proposed an interactive ranking operator for NSGA-II to support the complete prioritization of solutions for any SE problem domain. Although this operator worked satisfactorily in an application example, its validation is required so that it can be used in real application contexts. In this sense, we instantiated the interactive ranking operator for NSGA-II presented in previous work, and we conducted an exploratory study with a twofold goal: (i) validate the impact in the ranking of solutions, and (ii) check the diversity of them. To accomplish such goals, we made statistical tests such as correlation and regression analysis using quality metrics for Product Line Architecture (PLA) Design. The results pointed out that the interactive ranking operator can properly deal with the DM preferences, giving a greater chance of surviving to those solutions with higher scores, without compromising their diversity.},
booktitle = {Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
pages = {337–346},
numpages = {10},
keywords = {Optimization for Software Engineering problems, NSGA-II ranking operator., Interactive SBSE},
location = {Virtual Event, Brazil},
series = {SBES '22}
}

@inproceedings{10.1145/3534678.3542634,
author = {Chua, Watson W.K. and Li, Lu and Goh, Alvina},
title = {Classifying Multimodal Data Using Transformers},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542634},
doi = {10.1145/3534678.3542634},
abstract = {The increasing prevalence of multimodal data in our society has led to the increased need for machines to make sense of such data holistically. However, data scientists and machine learning engineers aspiring to work on such data face challenges fusing the knowledge from existing tutorials which often deal with each mode separately. Drawing on our experience in classifying multimodal municipal issue feedback in the Singapore government, we conduct a hands-on tutorial to help flatten the learning curve for practitioners who want to apply machine learning to multimodal data.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4780–4781},
numpages = {2},
keywords = {computer vision, deep learning, multimodal learning, natural language processing, transformers, vision-language representation},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3579027.3608987,
author = {Jacobs, Jef and Nicolay, Jens and De Meuter, Wolfgang},
title = {VariMod: A Structured Approach to Variability in 3D Modelling},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608987},
doi = {10.1145/3579027.3608987},
abstract = {Today's manufacturing industry is confronted with an increasing demand for product variability that stems from product customisation needs and the engineering process. Different customer demands and the mass-customisation of physical products require designing multiple variants of products, and additional requirements may be introduced when the product reaches subsequent stages (simulation, manufacturing, assembly...) in its engineering process.The state-of-the-art 3D modelling software deals with variability in a mostly ad-hoc fashion. Designing products typically involves creating digital 3D models using Computer-Aided Design (CAD) software, and implementing variability requires duplication of entire models or parts thereof that then require changes without any identification of or distinction between the different requirements that caused them. Parametric CAD approaches do enable designing 3D models that contain modifiable parameters, but designers must still ensure that the 3D model with updated parameter values satisfies all requirements. It is therefore difficult or impossible with current approaches and tools to design variants of products in a structured and efficient manner.In this work, we present VariMod, a 3D modelling approach that distinguishes between invariant requirements that each variant of a 3D model must satisfy, and variant-specific requirements that individual variants must satisfy. Hereby, VariMod enables the specification of 'generic' 3D models that satisfy invariant requirements, of which the parameter values can be optimised so that they also satisfy variant-specific requirements. To this end, VariMod represents both types of requirements as bidirectional constraints that are solved to find optimal parameter values that satisfy all constraints. VariMod features a constraint-solving process that aims to minimise the modifications made to parameter values when optimising a 3D model, thereby preventing unexpected modifications to the 3D model. We use PrintTalk, a programmatic CAD language for parametric 3D modelling, as a vehicle for implementing and validating VariMod by demonstrating how it can be used for designing variants of 3D models in a structured and efficient manner.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {160–169},
numpages = {10},
keywords = {Variational Design, PrintTalk, Parametric CAD, Non-Functional Requirements, DFX, Constraints, 3D Modelling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {variability model, system evolution, fault, automatic repair},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {recommender systems, feature models, evaluation, configuration},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/3535850.3536123,
author = {Delcourt, Kevin},
title = {Towards Multi-Agent Interactive Reinforcement Learning for Opportunistic Software Composition in Ambient Environments},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In order to manage the ever-growing number of devices present in modern and future ambient environments, as well as their dynamics and openness, we aim to propose a distributed multi-agent system that learns, in interaction with a human user, what would be their preferred applications given the services available.The goal of this Ph.D. thesis is to focus on the interaction between a reinforcement learning system and the human user, to improve the system's learning capabilities as well as the user's ease with the system, and ultimately build a working prototype, usable by end-users.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1839–1840},
numpages = {2},
keywords = {multi-agent system, machine learning, human-in-the-loop, human-AI interaction, emergence, ambient intelligence},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3550355.3552397,
author = {G\"{o}ttmann, Hendrik and Caesar, Birte and Beers, Lasse and Lochau, Malte and Sch\"{u}rr, Andy and Fay, Alexander},
title = {Precomputing reconfiguration strategies based on stochastic timed game automata},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552397},
doi = {10.1145/3550355.3552397},
abstract = {Many modern software systems continuously reconfigure themselves to (self-)adapt to ever-changing environmental contexts. Selecting presumably best-fitting next configurations is, however, very challenging, depending on functional and non-functional criteria like real-time constraints as well as inherently uncertain future contexts which makes greedy one-step decision heuristics ineffective. In addition, the computational overhead caused by reconfiguration planning at run-time should not outweigh its benefits. On the other hand, completely pre-planning reconfiguration decisions at design time is also infeasible due to the lack of knowledge about the context behavior. In this paper, we propose a game-theoretic setting for precomputing reconfiguration decisions under partially uncertain real-time behavior. We employ stochastic timed game automata as reconfiguration model to derive winning strategies which enable the first player (the system) to make fast look-ups for presumably best-fitting reconfiguration decisions satisfying the second player (the context). To cope with the high computational complexity of finding winning strategies, our tool implementation1 utilizes the statistical model-checker Uppaal Stratego to approximate near-optimal solutions. In our evaluation, we investigate efficiency/effectiveness trade-offs by considering a real-world example consisting of a reconfigurable robot support system for the construction of aircraft fuselages.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {31–42},
numpages = {12},
keywords = {strategy synthesis, stochastic timed game automata, statistical model-checking, proactive self-adaptation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Software Product Lines, Software Evolution, Delta Modeling},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3600006.3613175,
author = {Jayaram Subramanya, Suhas and Arfeen, Daiyaan and Lin, Shouxu and Qiao, Aurick and Jia, Zhihao and Ganger, Gregory R.},
title = {Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613175},
doi = {10.1145/3600006.3613175},
abstract = {The Sia scheduler efficiently assigns heterogeneous deep learning (DL) cluster resources to elastic resource-adaptive jobs. Although some recent schedulers address one aspect or another (e.g., heterogeneity or resource-adaptivity), none addresses all and most scale poorly to large clusters and/or heavy workloads even without the full complexity of the combined scheduling problem. Sia introduces a new scheduling formulation that can scale to the search-space sizes and intentionally match jobs and their configurations to GPU types and counts, while adapting to changes in cluster load and job mix over time. Sia also introduces a low-profiling-overhead approach to bootstrapping (for each new job) throughput models used to evaluate possible resource assignments, and it is the first cluster scheduler to support elastic scaling of hybrid parallel jobs.Extensive evaluations show that Sia outperforms state-of-the-art schedulers. For example, even on relatively small 44- to 64-GPU clusters with a mix of three GPU types, Sia reduces average job completion time (JCT) by 30--93%, 99th percentile JCT and makespan by 28--95%, and GPU hours used by 12--55% for workloads derived from 3 real-world environments. Additional experiments demonstrate that Sia scales to at least 2000-GPU clusters, provides improved fairness, and is not over-sensitive to scheduler parameter settings.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {642–657},
numpages = {16},
keywords = {deep learning training, resource allocation, cluster scheduling},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings{10.1145/3385412.3386016,
author = {He, Jingxuan and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
title = {Learning fast and precise numerical analysis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386016},
doi = {10.1145/3385412.3386016},
abstract = {Numerical abstract domains are a key component of modern static analyzers. Despite recent advances, precise analysis with highly expressive domains remains too costly for many real-world programs. To address this challenge, we introduce a new data-driven method, called LAIT, that produces a faster and more scalable numerical analysis without significant loss of precision. Our approach is based on the key insight that sequences of abstract elements produced by the analyzer contain redundancy which can be exploited to increase performance without compromising precision significantly. Concretely, we present an iterative learning algorithm that learns a neural policy that identifies and removes redundant constraints at various points in the sequence. We believe that our method is generic and can be applied to various numerical domains.  We instantiate LAIT for the widely used Polyhedra and Octagon domains. Our evaluation of LAIT on a range of real-world applications with both domains shows that while the approach is designed to be generic, it is orders of magnitude faster on the most costly benchmarks than a state-of-the-art numerical library while maintaining close-to-original analysis precision. Further, LAIT outperforms hand-crafted heuristics and a domain-specific learning approach in terms of both precision and speed.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1112–1127},
numpages = {16},
keywords = {Abstract interpretation, Machine learning, Numerical domains, Performance optimization},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@proceedings{10.1145/3564121,
title = {AIMLSystems '22: Proceedings of the Second International Conference on AI-ML Systems},
year = {2022},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {similarity metrics, product lines, feature-based representation, commonality and variability, case-based reasoning},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {software product line, requirements engineering, natural language processing, feature model extraction, NLTK},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {IoT, time series forecasting, software architecture evaluation, Continuous evaluation}
}

@inproceedings{10.1145/3555776.3578611,
author = {Limaylla-Lunarejo, Maria-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
title = {Towards a FAIR Dataset for non-functional requirements},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578611},
doi = {10.1145/3555776.3578611},
abstract = {In the last years, the application of supervised Machine Learning (ML) algorithms in Requirements Engineering (RE) has allowed increasing the performance (e.g. accuracy, precision) and scalability of automatic requirements classification. However, the lack of publicly labeled datasets is still one concern when conducting ML experiments. Few publicly labeled datasets for non-functional requirements classification are available, and even less in the Spanish language. Moreover, most of the available datasets present some limitations, such as imbalanced classes (e.g. PROMISE NFR). This study aims to generate a FAIR dataset of non-functional requirements in the Spanish language for facilitating reuse in ML classification experiments. 109 non-functional requirements were collected from final degree projects from the University of A Coru\~{n}a. We conducted a pilot quasi-experiment for non-functional requirements labeling in the categories and subcategories of the ISO/IEC 25010 quality model. The labeling process was accomplished by 7 annotators. The inter-annotator agreement using a Fleiss' Kappa test obtained a substantial agreement in the category level (0.78) and a moderate agreement (0.48) when the classification is per subcategory.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1414–1421},
numpages = {8},
keywords = {FAIR principles, spanish dataset, non-functional requirements, data labeling},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/2365324.2365326,
author = {Shepperd, Martin},
title = {The scientific basis for prediction research},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365326},
doi = {10.1145/2365324.2365326},
abstract = {In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {defect prediction, empirical research, machine learning, software metrics},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {vulnerable configuration, vulnerabilities, testing, reasoning, pentesting, feature model, cybersecurity},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2639490.2639504,
author = {Russo, Barbara},
title = {A proposed method to evaluate and compare fault predictions across studies},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639504},
doi = {10.1145/2639490.2639504},
abstract = {Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {confusion matrix, fault, machine learning, model comparison},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1145/3546932.3547007,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Quality-aware analysis and optimisation of virtual network functions},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547007},
doi = {10.1145/3546932.3547007},
abstract = {The softwarisation and virtualisation of network functionality is the last milestone in the networking industry. Software-Defined Networks (SDN) and Network Function Virtualization (NFV) offer the possibility of using software to manage computer and mobile networks and build novel Virtual Network Functions (VNFs) deployed in heterogeneous devices. To reason about the variability of network functions and especially about the quality of a software product defined as a set of VNFs instantiated as part of a service (i.e., Service Function Chaining), a variability model along with a quality model is required.However, this domain imposes certain challenges to quality-aware reasoning of service function chains, such as numerical features or configuration-level Quality Attributes (QAs) (e.g., energy consumption). Incorporating numerical reasoning with quality data into SPL analyses is challenging and tool support is rare. In this work, we present 3 groups of operations: model report, aggregate functions to dynamically convert QAs at the feature-level into the configuration-level, and quality-aware optimisation. Our objective is to test the most complete reasoning tools to exploit the extended variability with quality attributes needed for VNFs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210–221},
numpages = {12},
keywords = {virtual network function, variability, reasoning, quality attribute, optimization, numerical feature},
location = {Graz, Austria},
series = {SPLC '22}
}

@article{10.1145/3503509,
author = {Yang, Yanming and Xia, Xin and Lo, David and Bi, Tingting and Grundy, John and Yang, Xiaohu},
title = {Predictive Models in Software Engineering: Challenges and Opportunities},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3503509},
doi = {10.1145/3503509},
abstract = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {56},
numpages = {72},
keywords = {survey, software engineering, deep learning, machine learning, Predictive models}
}

@inproceedings{10.1145/3522664.3528603,
author = {Husom, Erik Johannes and Tverdal, Simeon and Goknil, Arda and Sen, Sagar},
title = {UDAVA: an unsupervised learning pipeline for sensor data validation in manufacturing},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528603},
doi = {10.1145/3522664.3528603},
abstract = {Manufacturing has enabled the mechanized mass production of the same (or similar) products by replacing craftsmen with assembly lines of machines. The quality of each product in an assembly line greatly hinges on continual observation and error compensation during machining using sensors that measure quantities such as position and torque of a cutting tool and vibrations due to possible imperfections in the cutting tool and raw material. Patterns observed in sensor data from a (near-)optimal production cycle should ideally recur in subsequent production cycles with minimal deviation. Manually labeling and comparing such patterns is an insurmountable task due to the massive amount of streaming data that can be generated from a production process. We present UDAVA, an unsupervised machine learning pipeline that automatically discovers process behavior patterns in sensor data for a reference production cycle. UDAVA performs clustering of reduced dimensionality summary statistics of raw sensor data to enable high-speed clustering of dense time-series data. It deploys the model as a service to verify batch data from subsequent production cycles to detect recurring behavior patterns and quantify deviation from the reference behavior. We have evaluated UDAVA from an AI Engineering perspective using two industrial case studies.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {159–169},
numpages = {11},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/2851613.2852013,
author = {Hussain, Shahid},
title = {Threshold analysis of design metrics to detect design flaws: student research abstract},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2852013},
doi = {10.1145/2851613.2852013},
abstract = {Detection of design flaws at different granularity levels of software can help the software engineer to reduce the testing efforts and maintenance cost. In the context of metric-based analysis, current state of art for the quality assurance tools is to extract the metrics from the source code and analyzed the design complexity. But in case of legacy systems, a software engineer needs to pass through the re-engineering process. In this study, I propose a methodology to investigate the threshold effect of software design metrics in order to detect design flaws and its effect over the granularity level of software. Moreover, I will use some statistical methods and machine learning techniques to derive and validate the effect of thresholds over the NASA and open source datasets retrieve from the PROMISE repository.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1584–1585},
numpages = {2},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line engineering, product derivation, product configurator, product baselines, product audit, hierarchical product lines, feature profiles, feature modeling, bill-of-features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/3652037.3652067,
author = {Islam, Samia and Owen, Charles and Mukherjee, Ranjan and Woodring, Ira},
title = {Wrinkle Detection and Cloth Flattening through Deep Learning and Image Analysis as Assistive Technologies for Sewing},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3652067},
doi = {10.1145/3652037.3652067},
abstract = {Robotic manipulation of fabric has potential as an enabling accessibility technology for individuals with disabilities, opening up a range of employment opportunities and helping to decrease the underemployment of this population. This research seeks to reliably characterize wrinkles and facilitate robotic removal of the wrinkles, with the focus on managing the outfeed of a sewing process, facilitating employment for individuals unable to reach behind the machine while performing sewing tasks. Outfeed management is critical in sewing to prevent bunching and maintain sewing productivity. To smooth out a fabric and eliminate wrinkles, the wrinkles need to be located and characterized, and points identified where a robotic arm can apply force on the fabric to smooth the fabric. For this purpose, we employ a deep learning technique to detect wrinkles and use corner detection of the fabric to determine an effective point for wrinkle removal.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {233–242},
numpages = {10},
keywords = {Accessibility, Cloth Flattening, Cloth manipulation, Fabric Wrinkle Detection, Sewing},
location = {Crete, Greece},
series = {PETRA '24}
}

@article{10.1145/3514232,
author = {Bertolotti, Francesco and Cazzola, Walter},
title = {Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3514232},
doi = {10.1145/3514232},
abstract = {We introduce a novel approach to source code representation to be used in combination with neural networks. Such a representation is designed to permit the production of a continuous vector for each code statement. In particular, we present how the representation is produced in the case of Java source code. We test our representation for three tasks: code summarization, statement separation, and code search. We compare with the state-of-the-art non-autoregressive and end-to-end models for these tasks. We conclude that all tasks benefit from the proposed representation to boost their performance in terms of F1-score, accuracy, and mean reciprocal rank, respectively. Moreover, we show how models trained on code summarization and models trained on statement separation can be combined to address methods with tangled responsibilities, meaning that these models can be used to detect code misconduct.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {6},
numpages = {31},
keywords = {intent identification, method name suggestion, learning representations, Big code}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {software product lines, reuse, reliability, failures, change},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/3338906.3342508,
author = {Radavelli, Marco},
title = {Using software testing to repair models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342508},
doi = {10.1145/3338906.3342508},
abstract = {Software testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1253–1255},
numpages = {3},
keywords = {timed automata, software testing, software product lines, search-based software engineering, mutation, model repair, CIT},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {software product line, reusability, performance, knowledge, interoperability, industrial analytics, extensibility, asset health},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Software Engineering, Search-Based Software Engineering, Product Line Testing, Highly-Configurable Systems, Fault Detection, Cyber-Physical Systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/MODELS-C.2019.00046,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {Inferring metamodel relaxations based on structural patterns to support model families},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00046},
doi = {10.1109/MODELS-C.2019.00046},
abstract = {A model family is a set of related models in a given language that results from the evolution of models over time and/or variations over the space (product) dimension. To enable a more efficient analysis of family members, all at once, we have already proposed union models to capture the union of all elements in all family members, in a compact and exact manner. However, despite having each model in a model family conforming to the same metamodel, there is still no guarantee that their union model will conform to the original metamodel of the family members. This paper aims to support the representation of union models (as valid instances of a metamodel) by inferring, from the structure of the original metamodel, a relaxed metamodel to which a union model conforms. In particular, instead of relaxing all metamodel constraints, the paper contributes a heuristic method that relaxes particular constraints (related only to multiplicities of attributes and association ends) by inferring where such relaxations are needed in the metamodel. To infer relaxation points, structural patterns are first identified in metamodels, then an evidence-based or an anticipation-based approach is applied to get the actual inference. The purpose behind inferring particular metamodel relaxation points is to be able to adapt the existing tools and analysis techniques once and minimally for all potential model families of a given modeling language.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {294–303},
numpages = {10},
keywords = {metamodel, metamodel relaxation, model, model family, relaxation point, structural pattern, union model},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579028.3609017,
author = {Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {On the Reuse of Existing Configurations for Testing Evolving Feature Models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609017},
doi = {10.1145/3579028.3609017},
abstract = {Software Product Lines (SPLs) are used for representing a variety of highly configurable systems or families of systems. They are commonly represented by feature models (FMs). Starting from FMs, configurations, used as test cases, can be generated to identify the products of interest for further activities. As the other types of software, SPLs and their FMs may evolve due to changing requirements or bug-fixing. However, no guidance is usually given on what to do with derived configurations when an FM evolves. The common approach is based on generating all configurations from scratch, which is not optimal since a greater effort is required for concretizing the new tests, and some of the old ones may be still applicable.In this paper, we present the use of a technique for generating combinatorial tests for evolving feature models: this technique incrementally builds the new combinatorial configuration set starting from the one generated from the previous model. Furthermore, we present a novel definition of dissimilarity among configuration sets that can be used to evaluate how much an evolved test suite differs from the previous one and thus allows evaluating the effort required for adapting old test cases to the new ones.Our experiments confirm that using the proposed technique, in general, leads to lower dissimilarity and test suite size w.r.t. the generation of tests from scratch.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {67–76},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1145/3704905,
author = {Cai, Yufan and Hou, Zhe and Sanan, David and Luan, Xiaokun and Lin, Yun and Sun, Jun and Dong, Jin Song},
title = {Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704905},
doi = {10.1145/3704905},
abstract = {Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To initiate this vision, we propose Refine4LLM, an approach that aims to:
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Formally refine the specifications,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(2) Automatically prompt and guide the LLM using refinement calculus,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(3) Interact with the LLM to generate the code,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(5) Learn and build more advanced refinement laws to extend the refinement calculus.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {69},
numpages = {33},
keywords = {Large Language Model, Program Refinement, Program Synthesis}
}

@article{10.5555/1953048.2021053,
author = {Bubeck, S\'{e}bastien and Munos, R\'{e}mi and Stoltz, Gilles and Szepesv\'{a}ri, Csaba},
title = {X-Armed Bandits},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by √n, that is, the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {1655–1695},
numpages = {41}
}

@inproceedings{10.1145/1449913.1449921,
author = {Batory, Don},
title = {Using modern mathematics as an FOSD modeling language},
year = {2008},
isbn = {9781605582672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449913.1449921},
doi = {10.1145/1449913.1449921},
abstract = {Modeling languages are a fundamental part of automated software development. MDD, for example, uses UML class diagrams and state machines as languages to define applications. In this paper, we explore how Feature Oriented Software Development (FOSD) uses modern mathematics as a modeling language to express the design and synthesis of programs in software product lines, but demands little mathematical sophistication from its users. Doing so has three practical benefits: (1) it offers a simple and principled mathematical description of how FOSD transforms, derives, and relates program artifacts, (2) it exposes previously unrecognized commuting relationships among tool chains, thereby providing new ways to debug tools, and (3) it reveals new ways to optimize software synthesis.},
booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
pages = {35–44},
numpages = {10},
keywords = {software product lines, model driven design, geodesics, features, commuting diagrams},
location = {Nashville, TN, USA},
series = {GPCE '08}
}

@article{10.1145/3630252,
author = {Lustosa, Andre and Menzies, Tim},
title = {Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630252},
doi = {10.1145/3630252},
abstract = {When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK’s 12-month prediction errors are {I=0%, R=33%&nbsp;C=47%}, whereas other methods have far larger errors of {I=61%,R=119%&nbsp;C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {58},
numpages = {22},
keywords = {Hyperparameter tuning, software health, indepedent variable clustering}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3377024.3377035,
author = {Kirchhof, J\"{o}rg Christian and Rumpe, Bernhard and Schmalzing, David and Wortmann, Andreas},
title = {Structurally evolving component-port-connector architectures of centrally controlled systems},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377035},
doi = {10.1145/3377024.3377035},
abstract = {The increasing complexity of software variants demands for variation management techniques suitable for industrial practice. As "clone-and-own" with subsequent manual evolution still is a popular method to create software variants, this leads to product lines that are difficult to maintain and evolve and can produce conflicts when changes occur in both, product line and variant. Where general approaches to differencing and merging handcrafted changes to products perform suboptimally, respecting assumptions on the structure of the architecture can reduce the differencing search space and yield better results. We present a novel method for differencing and merging hierarchical component-port-connector architectures based on the Focus calculus. It leverages assumptions on the distribution of components to facilitate calculating differences between architecture versions and deriving delta bundles to update software products to changes in the underlying product line. Through a (preliminary) survey with 27 participants, we compared the results of our method with the results of manually differencing. The survey showed that this method yields deltas and merge results that are considered correct by the participants. Overall, we found that including assumptions about the architectural style and that grouping related deltas into compact bundles can greatly facilitate merging manually created and evolved products back into their underlying product lines.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {23},
numpages = {9},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3715111,
author = {Abrah\~{a}o, Silvia and Grundy, John and Pezz\`{e}, Mauro and Storey, Margaret-Anne and Andrew Tamburri, Damian},
title = {Software Engineering by and for Humans in an AI Era},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715111},
doi = {10.1145/3715111},
abstract = {The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb
}

@article{10.1145/3609333,
author = {Li, Zhong and Zhu, Yuxuan and Van Leeuwen, Matthijs},
title = {A Survey on Explainable Anomaly Detection},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3609333},
doi = {10.1145/3609333},
abstract = {In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {23},
numpages = {54},
keywords = {explainable artificial intelligence, explainable machine learning, outlier detection, anomaly detection, anomaly explanation, interpretable anomaly detection, Explainable anomaly detection}
}

@inproceedings{10.1145/3038912.3052704,
author = {Wu, Chao-Yuan and Ahmed, Amr and Kumar, Gowtham Ramani and Datta, Ritendra},
title = {Predicting Latent Structured Intents from Shopping Queries},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052704},
doi = {10.1145/3038912.3052704},
abstract = {In online shopping, users usually express their intent through search queries. However, these queries are often ambiguous. For example, it is more likely (and easier) for users to write a query like "high-end bike" than "21 speed carbon frames jamis or giant road bike". It is challenging to interpret these ambiguous queries and thus search result accuracy suffers. A user oftentimes needs to go through the frustrating process of refining search queries or self-teaching from possibly unstructured information. However, shopping is indeed a structured domain, that is composed of category hierarchy, brands, product lines, features, etc. It would be much better if a shopping site could understand users' intent through this structure, present organized information, and then find the items with the right categories, brands or features.In this paper we study the problem of inferring the latent intent from unstructured queries and mapping them to structured attributes. We present a novel framework that jointly learns this knowledge from user consumption behaviors and product metadata. We present a hybrid Long Short-term Memory (LSTM) joint model that is accurate and robust, even though user queries are noisy and product catalog is rapidly growing. Our study is conducted on a large-scale dataset from Google Shopping, that is composed of millions of items and user queries along with their click responses. Extensive qualitative and quantitative evaluation shows that the proposed model is more accurate, concise, and robust than multiple possible alternatives. In terms of information retrieval (IR) performance, our model is able to improve the quality of current Google Shopping production system, which is a very strong baseline.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1133–1141},
numpages = {9},
keywords = {shopping, recurrent neural networks, query understanding, entity relationship modeling, autoencoder},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/1862372.1862375,
author = {Di Penta, Massimiliano},
title = {Empirical studies on software evolution: should we (try to) claim causation?},
year = {2010},
isbn = {9781450301282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1862372.1862375},
doi = {10.1145/1862372.1862375},
abstract = {In recent and past years, there have been hundreds of studies aimed at characterizing the evolution of a software system. Many of these studies analyze the behavior of a variable over a given period of observation. How does the size of a software system evolve? What about its complexity? Does the number of defects increase over time or does it remain stable?In some cases, studies also attempt to correlate variables, and, possibly, to build predictors upon them. This is to say, one could estimate the likelihood that a fault occurs in a class, based on some metrics the class exhibits, on the kinds of changes the class underwent. Similarly, change couplings can be inferred by observing how artifacts tend to co-change. Although in many cases we are able to obtain models ensuring good prediction performances, we are not able to claim any causal-effect relationship between our independent and dependent variables. We could easily correlate the presence of some design constructs with the change-proneness of a software component, however the same correlation could be found with the amount of good Belgian beer our developers drink. As a matter of fact, the component could undergo changes for other, external reasons.Recent software evolution studies rely on fine-grained information mined by integrating several kinds of repositories, such as versioning systems, bug tracking systems, or mailing lists. Nowadays, many other precious sources of information, ranging from code search repositories, vulnerability databases, informal communications, and legal documents are also being considered. This would possibly aid to capture the rationale of some events occurring in a software project, and link them to statistical relations we observed.The road towards shifting from solid empirical models towards "principles of software evolution" will likely be long and difficult, therefore we should prepare ourselves to traverse it and go as far as possible with limited damages. To do this, we need to carefully prepare our traveling equipment by paying attention at: (i) combining quantitative studies with qualitative studies, surveys, and informal interviews, (ii) relating social relations among developers with variables observed on the project, (iii) using proper statistical and machine learning techniques able to capture the temporal relation among different events, and (iv) making a massive use of natural language processing and text mining among the various sources of information available.},
booktitle = {Proceedings of the Joint ERCIM Workshop on Software Evolution (EVOL) and International Workshop on Principles of Software Evolution (IWPSE)},
pages = {2},
numpages = {1},
keywords = {software evolution, qualitative research, empirical studies},
location = {Antwerp, Belgium},
series = {IWPSE-EVOL '10}
}

@article{10.1145/3464305,
author = {Sobhy, Dalia and Bahsoon, Rami and Minku, Leandro and Kazman, Rick},
title = {Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464305},
doi = {10.1145/3464305},
abstract = {Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {51},
numpages = {50},
keywords = {uncertainty, run-time software architecture evaluation, design-time software architecture evaluation, Continuous software architecture evaluation}
}

@inproceedings{10.1145/2695664.2696059,
author = {Hozano, Mario and Ferreira, Henrique and Silva, Italo and Fonseca, Baldoino and Costa, Evandro},
title = {Using developers' feedback to improve code smell detection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2696059},
doi = {10.1145/2695664.2696059},
abstract = {Several studies are focused on the study of code smells and many detection techniques have been proposed. In this scenario, the use of rules involving software-metrics has been widely used in refactoring tools as a mechanism to detect code smells automatically. However, actual approaches present two unsatisfactory aspects: they present a low agreement in its results and, they do not consider the developers' feedback. In this way, these approaches detect smells that are not relevant to the developers. In order to solve the above mentioned unsatisfactory aspects in the state-of the-art of code smells detection, we propose the Smell Platform able to recognize code smells more relevant to developers by using its feedback. In this paper we present how such platform is able to detect four well known code smells. Finally, we evaluate the Smell Platform comparing its results with traditional detection techniques.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1661–1663},
numpages = {3},
keywords = {refactoring, developer's feedback, code smell detection},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.5555/2627435.2670332,
author = {Dhurandhar, Amit and Petrik, Marek},
title = {Efficient and accurate methods for updating generalized linear models with multiple feature additions},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we propose an approach for learning regression models efficiently in an environment where multiple features and data-points are added incrementally in a multistep process. At each step, any finite number of features maybe added and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares, weighted least squares, generalized least squares and ridge regression, but also more generally for generalized linear models and lasso regression that use iterated re-weighted least squares for maximum likelihood estimation. Our approach instantiated to linear settings has close relations to the partitioned matrix inversion mechanism based on Schur's complement. For arbitrary regression methods, even a relaxation of the approach is no worse than using the model from the previous step or using a model that learns on the additional features and optimizes the residual of the model at the previous step. Such problems are commonplace in complex manufacturing operations consisting of hundreds of steps, where multiple measurements are taken at each step to monitor the quality of the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate steps can be extremely useful in enhancing productivity. We further validate our claims through experiments on synthetic and real industrial data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2607–2627},
numpages = {21},
keywords = {manufacturing, logistic regressions, linear regression, lasso, group lasso, feature selection}
}

@article{10.1145/3092691,
author = {Filho, Roberto Rodrigues and Porter, Barry},
title = {Defining Emergent Software Using Continuous Self-Assembly, Perception, and Learning},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3092691},
doi = {10.1145/3092691},
abstract = {Architectural self-organisation, in which different configurations of software modules are dynamically assembled based on the current context, has been shown to be an effective way for software to self-optimise over time. Current approaches to this rely heavily on human-led definitions: models, policies, and processes to control how self-organisation works. We present the case for a paradigm shift to fully emergent computer software that places the burden of understanding entirely into the hands of software itself. These systems are autonomously assembled at runtime from discovered constituent parts and their internal health and external deployment environment continually monitored. An online, unsupervised learning system then uses runtime adaptation to continuously explore alternative system assemblies and locate optimal solutions. Based on our experience over the past 3 years, we define the problem space of emergent software and present a working case study of an emergent web server as a concrete example of the paradigm. Our results demonstrate two main aspects of the problem space for this case study: that different assemblies of behaviour are optimal in different deployment environment conditions and that these assemblies can be autonomously learned from generalised perception data while the system is online.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
articleno = {16},
numpages = {25},
keywords = {modularity, machine learning, adaptive systems, Emergent software}
}

@article{10.1145/1571629.1571630,
author = {Tan, Hee Beng Kuan and Zhao, Yuan and Zhang, Hongyu},
title = {Conceptual data model-based software size estimation for information systems},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1571629.1571630},
doi = {10.1145/1571629.1571630},
abstract = {Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. Some information required by existing software sizing methods is difficult to predict in the early stage of software development. A conceptual data model is widely used in the early stage of requirements analysis for information systems. Lines of code (LOC) is a commonly used software size measure. This article proposes a novel LOC estimation method for information systems from their conceptual data models through using a multiple linear regression model. We have validated the proposed method using samples from both the software industry and open-source systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {4},
numpages = {37},
keywords = {multiple linear regression model, line of code (LOC), conceptual data model, Software sizing}
}

@inproceedings{10.5555/3466184.3466402,
author = {Ratusny, Marco and Ay, Alican and Ponsignon, Thomas},
title = {Characterizing customer ordering behaviors in semiconductor supply chains with convolutional neural networks},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Advancements in the semiconductor industry have resulted in the need for extracting vital information from vast amounts of data. In the operational processes of demand planning and order management, it is important to understand customer demand data due to its potential to provide insights for managing supply chains. For this purpose, customer ordering behaviors are visualized in the form of two-dimensional heat maps. The goal is to classify the customers into predefined ordering patterns on the example of a semiconductor manufacturing, namely Infineon Technologies. Therefore, a convolutional neural network is used. By classifying the customers into preselected ordering patterns, a better understanding on how the customer demand develops over time is achieved. The results show that customers have a certain ordering pattern, but their behavior can be meaningfully classified only to a certain extend due to unidentified behaviors in the data. Further research could identify additional ordering patterns.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1931–1942},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3663529.3663832,
author = {Tao, Zhu and Gao, Yongqiang and Qi, Jiayi and Peng, Chao and Wu, Qinyun and Chen, Xiang and Yang, Ping},
title = {Neat: Mobile App Layout Similarity Comparison Based on Graph Convolutional Networks},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663832},
doi = {10.1145/3663529.3663832},
abstract = {A wide variety of device models, screen resolutions and operating systems have emerged with recent advances in mobile devices. As a result, the graphical user interface (GUI) layout in mobile apps has become increasingly complex due to this market fragmentation, with rapid iterations being the norm. Testing page layout issues under these circumstances hence becomes a resource-intensive task, requiring significant manpower and effort due to the vast number of device models and screen resolution adaptations. One of the most challenging issues to cover manually is multi-model and cross-version layout verification for the same GUI page. To address this issue, we propose Neat, a non-intrusive end-to-end mobile app layout similarity measurement tool that utilizes computer vision techniques for GUI element detection, layout feature extraction, and similarity metrics. Our empirical evaluation and industrial application have demonstrated that our approach is effective in improving the efficiency of layout assertion testing and ensuring application quality.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {104–114},
numpages = {11},
keywords = {CNN, GCN, Graphical User Interface, Mobile App, OCR, YOLOX},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/1370788.1370801,
author = {Menzies, Tim and Turhan, Burak and Bener, Ayse and Gay, Gregory and Cukic, Bojan and Jiang, Yue},
title = {Implications of ceiling effects in defect predictors},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370801},
doi = {10.1145/1370788.1370801},
abstract = {Context: There are many methods that input static code features and output a predictor for faulty code modules. These data mining methods have hit a "performance ceiling"; i.e., some inherent upper bound on the amount of information offered by, say, static code features when identifying modules which contain faults. Objective: We seek an explanation for this ceiling effect. Perhaps static code features have "limited information content"; i.e. their information can be quickly and completely discovered by even simple learners. Method:An initial literature review documents the ceiling effect in other work. Next, using three sub-sampling techniques (under-, over-, and micro-sampling), we look for the lower useful bound on the number of training instances. Results: Using micro-sampling, we find that as few as 50 instances yield as much information as larger training sets. Conclusions: We have found much evidence for the limited information hypothesis. Further progress in learning defect predictors may not come from better algorithms. Rather, we need to be improving the information content of the training data, perhaps with case-based reasoning methods.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {47–54},
numpages = {8},
keywords = {defect prediction, naive bayes, over-sampling, under-sampling},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.1145/3397271.3401041,
author = {Chen, Fanglin and Liu, Xiao and Proserpio, Davide and Troncoso, Isamar and Xiong, Feiyu},
title = {Studying Product Competition Using Representation Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401041},
doi = {10.1145/3397271.3401041},
abstract = {Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1261–1268},
numpages = {8},
keywords = {representation learning, product2vec, product competition},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3579028.3609008,
author = {Galindo, Jos\'{e} A. and Horcas, Jose-Miguel and Felferning, Alexander and Fernandez-Amoros, David and Benavides, David},
title = {FLAMA: A collaborative effort to build a new framework for the automated analysis of feature models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609008},
doi = {10.1145/3579028.3609008},
abstract = {Nowadays, feature models are the de facto standard when representing commonalities and variability, with modern examples spanning up to 7000 features. Manual analysis of such models is challenging and error-prone due to sheer size. To help in this task, automated analysis of feature models (AAFM) has emerged over the past three decades. However, the diversity of these tools and their supported languages presents a significant challenge that motivated the MOD-EVAR community to initiate a project for a new tool that supports the UVL language. Despite the rise of machine learning and data science, along with robust Python-based libraries, most AAFM tools have been implemented in Java, creating a collaboration gap. This paper introduces Flama, an innovative framework that automates the analysis of variability models. It focuses on UVL model analysis and aims for easy integration and extensibility to bridge this gap and foster better community and cross-community collaboration.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {16–19},
numpages = {4},
keywords = {visualization design process, variability, software product line, graphs and tables, effective communication, data visualization},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3316782.3321549,
author = {Gollasch, David and Engel, Christin and Branig, Meinhardt and Weber, Gerhard},
title = {Applying software variability methods to design adaptive assistance robots},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3321549},
doi = {10.1145/3316782.3321549},
abstract = {We believe that assistance and service robots are a major technological trend that could lead to a spread similar to smartphones today. Thus, it could be normal to have such a robot at home to support people during their daily life. The main challenge with today's developments in this segment is a lack of features and a lack of adaptivity. Most service robots are specialised systems (vacuum cleaners) or do not consider the user's needs and preferences when fulfilling their tasks. Our goal is to cope with these challenges and propose a solution by applying principles of software variability engineering to create an adaptive and accessible robot that can be extended and re-configured to fit the user's needs better. Therefore, we modified feature and variability modelling known from product lines and software ecosystems and developed an approach to configure the robot based on user requirements (needs and preferences).},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {313–314},
numpages = {2},
keywords = {software variability, software ecosystems, personal robot, needs and preferences, human-robot interaction, human-computer interaction, assistive technology, adaptive technology, accessibility},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@article{10.1145/3640335,
author = {Neelofar, Neelofar and Aleti, Aldeida},
title = {Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640335},
doi = {10.1145/3640335},
abstract = {Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road’s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {94},
numpages = {32},
keywords = {Testing autonomous vehicles, feature-impact analysis, instance space analysis, search-based software testing}
}

@inproceedings{10.1145/1993478.1993495,
author = {Singer, Jeremy and Kovoor, George and Brown, Gavin and Luj\'{a}n, Mikel},
title = {Garbage collection auto-tuning for Java mapreduce on multi-cores},
year = {2011},
isbn = {9781450302630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993478.1993495},
doi = {10.1145/1993478.1993495},
abstract = {MapReduce has been widely accepted as a simple programming pattern that can form the basis for efficient, large-scale, distributed data processing. The success of the MapReduce pattern has led to a variety of implementations for different computational scenarios. In this paper we present MRJ, a MapReduce Java framework for multi-core architectures. We evaluate its scalability on a four-core, hyperthreaded Intel Core i7 processor, using a set of standard MapReduce benchmarks. We investigate the significant impact that Java runtime garbage collection has on the performance and scalability of MRJ. We propose the use of memory management auto-tuning techniques based on machine learning. With our auto-tuning approach, we are able to achieve MRJ performance within 10% of optimal on 75% of our benchmark tests.},
booktitle = {Proceedings of the International Symposium on Memory Management},
pages = {109–118},
numpages = {10},
keywords = {mapreduce, machine learning, java, garbage collection},
location = {San Jose, California, USA},
series = {ISMM '11}
}

@article{10.1145/3464939,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat},
title = {Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464939},
doi = {10.1145/3464939},
abstract = {Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {53},
numpages = {36},
keywords = {testing, multi-objective search, mined rules, interacting products, configuration recommendation, Product line}
}

@inproceedings{10.1145/1173706.1173738,
author = {Czarnecki, Krzysztof and Pietroszek, Krzysztof},
title = {Verifying feature-based model templates against well-formedness OCL constraints},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173738},
doi = {10.1145/1173706.1173738},
abstract = {Feature-based model templates have been recently proposed as a approach for modeling software product lines. Unfortunately, templates are notoriously prone to errors that may go unnoticed for long time. This is because such an error is usually exhibited for some configurations only, and testing all configurations is typically not feasible in practice. In this paper, we present an automated verification procedure for ensuring that no ill-structured template instance will be generated from a correct configuration. We present the formal underpinnings of our proposed approach, analyze its complexity, and demonstrate its practical feasibility through a prototype implementation.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {211–220},
numpages = {10},
keywords = {software-product lines, model-driven development, model templates, metaprogramming, formal verification, feature modeling, feature interaction, configuration, UML, OCL},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@article{10.1145/3649319,
author = {Quattrocchi, Giovanni and Heuvel, Willem-Jan van den and Tamburri, Damian Andrew},
title = {The Data Product-service Composition Frontier: A Hybrid Learning Approach},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3649319},
doi = {10.1145/3649319},
abstract = {The service dominant logic is a base concept behind modern economies and software products, with service composition being a well-known practice for companies to gain a competitive edge over others by joining differentiated services together, typically assembled according to a number of features. At the other end of the spectrum, product compositions are a marketing device to sell products together in bundles that often augment the value for the customer, e.g., with suggested product interactions, sharing, and so on. Unfortunately, currently each of these two streams—namely, product and service composition—are carried out and delivered individually in splendid isolation: anything is being offered as a product and as a service, disjointly. We argue that the next wave of services computing features more and more service fusion with physical counterparts as well as data around them. Therefore a need emerges to investigate the interactive engagement of both (data) products and services. This manuscript offers a real-life implementation in support of this argument, using (1) genetic algorithms (GA) to shape product-service clusters, (2) end-user feedback to make the GAs interactive with a data-driven fashion, and (3) a hybridized approach which factors into our solution an ensemble machine-learning method considering additional features. All this research was conducted in an industrial environment. With such a cross-fertilized, data-driven, and multi-disciplinary approach, practitioners from both fields may benefit from their mutual state of the art as well as learn new strategies for product, service, and data product-service placement for increased value to the customer as well as the service provider. Results show promise but also highlight plenty of avenues for further research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {22},
keywords = {Anomaly detection, time series, unsupervised, literature review}
}

@inproceedings{10.1145/3426020.3426039,
author = {Nguyen, Huy Toan and Shin, Nu-ri and Yu, Gwang-Hyun and Kwon, Gyeong-Ju and Kwak, Woo-Young and Kim, Jin-Young},
title = {Deep learning-based defective product classification system for smart factory},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426039},
doi = {10.1145/3426020.3426039},
abstract = {In this paper, the defective product classification based on deep learning for a smart factory is introduced. The proposed system contains PLC (Programmable Logic Controller), Artificial Intelligence (AI) embedded board and cloud service. The AI embedded board is connected and communicated to receive and send commands to PLC via SPI (Serial Peripheral Interface) protocol. The pre-trained defective product classification model is uploaded, saved on a cloud server and downloaded to AI Embedded board for each particular product. The core technique of the system is the AI-based embedded board. Due to the limitation of label data, we use transfer learning method to retrain deep neural networks (DNN). We implement and compare the classification results on different deep neural network including ResNet, DenseNet, and GoogLeNet. We trained these networks by GPU server on casting product classification data. After that, the pre-trained models are optimized and applied on practical embedded board. The experimental results show that our system is able to classify defective products with high accuracy and fast speed.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {80–85},
numpages = {6},
keywords = {defective product classification, deep learning, Smart factory},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@article{10.1145/3612918,
author = {Sun, Danfeng and Hu, Junjie and Wu, Huifeng and Wu, Jia and Yang, Jian and Sheng, Quan Z. and Dustdar, Schahram},
title = {A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3612918},
doi = {10.1145/3612918},
abstract = {The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {50},
numpages = {37}
}

@article{10.1145/3590961,
author = {Zhang, Zhicheng},
title = {Study on Logistic Service Management of Colleges and Universities Based on Data Mining Algorithms},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3590961},
doi = {10.1145/3590961},
abstract = {Construction of a large logistics service (LS) that can adapt to the new situation is necessary for improving the self-development capability of university logistics in the reform process of socialization, and the measures are as follows: with support from the government sector, to create an external environment; with resource integration as a goal, to create an organizational structure; with market mechanism as a promoter, to the Independent college is a significant innovation of the higher education system, whose method of operation achieves the partnership between resources and social forces in higher education. There are several references in the text for further logistic reform in universities via data mining (DM) algorithms concerning logistic entities and autonomous colleges, which examine the market features and interaction between them. The logistics service data mining (LS-DM) approach plays a critical role in advancing logistic management science while boosting the economy's overall benefits when used with other measures. As a result of the rapid popularization of higher education, new features and models place an even greater demand on logistics management in colleges and universities. Refined management must be advocated and implemented in the new scenario. To apply refined management, you must alter your management philosophy, fine-tune your rules and regulations, enhance performance capabilities, and put mechanisms for monitoring and assessing progress. As a result, logistics management can be continuously improved, students and teachers receive better and more gratifying services, and the scientific growth of colleges and universities may be laid solidly.. The proposed LS-DM system with logistics service, data mining, and machine learning model demonstrates simulation outcomes with an accuracy of 89.7% and a precision of 87.8%, which is greater than the accuracy and precision exhibited by the existing models.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
keywords = {Environment, Organization, IoT, Management, Logistics, University, College}
}

@inproceedings{10.1145/2254756.2254791,
author = {Yoo, Wucherl and Larson, Kevin and Baugh, Lee and Kim, Sangkyum and Campbell, Roy H.},
title = {ADP: automated diagnosis of performance pathologies using hardware events},
year = {2012},
isbn = {9781450310970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254756.2254791},
doi = {10.1145/2254756.2254791},
abstract = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge.},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {283–294},
numpages = {12},
keywords = {resource bottleneck, performance analysis, micro-benchmark, machine learning, hardware event, fingerprint},
location = {London, England, UK},
series = {SIGMETRICS '12}
}

@inproceedings{10.1145/2365324.2365332,
author = {Azhar, Damir and Mendes, Emilia and Riddle, Patricia},
title = {A systematic review of web resource estimation},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365332},
doi = {10.1145/2365324.2365332},
abstract = {Background: Web development plays an important role in today's industry, so an in depth view into Web resource estimation would be valuable. However a systematic review (SR) on Web resource estimation in its entirety has not been done.Aim: The aim of this paper is to present a SR of Web resource estimation in order to define the current state of the art, and to identify any research gaps that may be present.Method: Research questions that would address the current state of the art in Web resource estimation were first identified. A comprehensive literature search was then executed resulting in the retrieval of 84 empirical studies that investigated any aspect of Web resource estimation. Data extraction and synthesis was performed on these studies with these research questions in mind.Results: We have found that there are no guidelines with regards to what resource estimation technique should be used in a particular estimation scenario, how it should be implemented, and how its effectiveness should be evaluated. Accuracy results vary widely and are dependent on numerous factors. Research has focused on development effort/cost estimation, neglecting other facets of resource estimation like quality and maintenance. Size measures have been used in all but one study as a resource predictor.Conclusions: Our results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {49–58},
numpages = {10},
keywords = {systematic review, web resource estimation},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/2372251.2372285,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
title = {Method-level bug prediction},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372285},
doi = {10.1145/2372251.2372285},
abstract = {Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments---performed on 21 Java open-source (sub-)systems---show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {171–180},
numpages = {10},
keywords = {method-level bug prediction, fine-grained source code changes, code metrics},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {vector space model, model comparison, clustering, Model-driven engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3192366.3192416,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {A data-driven CHC solver},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192416},
doi = {10.1145/3192366.3192416},
abstract = {We present a data-driven technique to solve Constrained Horn Clauses (CHCs) that encode verification conditions of programs containing unconstrained loops and recursions. Our CHC solver neither constrains the search space from which a predicate's components are inferred (e.g., by constraining the number of variables or the values of coefficients used to specify an invariant), nor fixes the shape of the predicate itself (e.g., by bounding the number and kind of logical connectives). Instead, our approach is based on a novel machine learning-inspired tool chain that synthesizes CHC solutions in terms of arbitrary Boolean combinations of unrestricted atomic predicates. A CEGAR-based verification loop inside the solver progressively samples representative positive and negative data from recursive CHCs, which is fed to the machine learning tool chain. Our solver is implemented as an LLVM pass in the SeaHorn verification framework and has been used to successfully verify a large number of nontrivial and challenging C programs from the literature and well-known benchmark suites (e.g., SV-COMP).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {707–721},
numpages = {15},
keywords = {Program Verification, Invariant Inference, Data-Driven Analysis, Constrained Horn Clauses (CHCs)},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@inproceedings{10.1145/3485832.3485892,
author = {Zhang, Zhaohe (John) and Yang, Edwin and Fang, Song},
title = {CommanderGabble: A Universal Attack Against ASR Systems Leveraging Fast Speech},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485892},
doi = {10.1145/3485832.3485892},
abstract = {Automatic Speech Recognition (ASR) systems are widely used in various online transcription services and personal digital assistants. Emerging lines of research have demonstrated that ASR systems are vulnerable to hidden voice commands, i.e., audio that can be recognized by ASRs but not by humans. Such attacks, however, often either highly depend on white-box knowledge of a specific machine learning model or require special hardware to construct the adversarial audio. This paper proposes a new model-agnostic and easily-constructed attack, called CommanderGabble, which uses fast speech to camouflage voice commands. Both humans and ASR systems often misinterpret fast speech, and such misinterpretation can be exploited to launch hidden voice command attacks. Specifically, by carefully manipulating the phonetic structure of a target voice command, ASRs can be caused to derive a hidden meaning from the manipulated, high-speed version. We implement the discovered attacks both over-the-wire and over-the-air, and conduct a suite of experiments to demonstrate their efficacy against 7 practical ASR systems. Our experimental results show that the over-the-wire attacks can disguise as many as 96 out of 100 tested voice commands into adversarial ones, and that the over-the-air attacks are consistently successful for all 18 chosen commands in multiple real-world scenarios.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {720–731},
numpages = {12},
keywords = {ASR misinterpretation, adversarial audio, syllabification},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/2180921.2180923,
author = {Anwikar, Vallabh and Naik, Ravindra and Contractor, Adnan and Makkapati, Hemanth},
title = {Domain-driven technique for functionality identification in source code},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180923},
doi = {10.1145/2180921.2180923},
abstract = {While migrating existing software systems to Software Product Lines, finding out the functionalities in the software is critical. For maintenance activities like deleting or changing existing features, or adding new similar features, identifying and extracting functionalities from the software is significant. This paper describes a technique for creating mapping between the source code and functionalities implemented by it while exploiting the domain knowledge. The technique is based on the notion of function variables that are used by developers for expressing functionality in the source code. By tracking the known values of the function variables and evaluating the conditions that use them, the mapping is identified. Our technique makes use of static data ow analysis and partial evaluation, and is designed with automation perspective. After applying to few samples representing real-life code structure and programming practices, the technique identified precise mapping of the detailed program elements to functions},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–8},
numpages = {8},
keywords = {partial evaluation, functionality identification, function variables}
}

@inproceedings{10.1145/2020390.2020395,
author = {Zeller, Andreas and Zimmermann, Thomas and Bird, Christian},
title = {Failure is a four-letter word: a parody in empirical research},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020395},
doi = {10.1145/2020390.2020395},
abstract = {Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though.Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion.Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies.Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice.Conclusions: With the abundance of software development data, even the simplest methods can produce "actionable" results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {5},
numpages = {7},
keywords = {empirical research, parody},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1145/3450445,
author = {Roy, Soumyadeep and Sural, Shamik and Chhaya, Niyati and Natarajan, Anandhavelu and Ganguly, Niloy},
title = {An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis, and Recommendation},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3450445},
doi = {10.1145/3450445},
abstract = {A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.},
journal = {ACM Trans. Web},
month = may,
articleno = {9},
numpages = {25},
keywords = {text classification, sentence ranking, online reputation management, Brand personality}
}

@inproceedings{10.1145/2025113.2025156,
author = {Lee, Taek and Nam, Jaechang and Han, DongGyun and Kim, Sunghun and In, Hoh Peter},
title = {Micro interaction metrics for defect prediction},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025156},
doi = {10.1145/2025113.2025156},
abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {311–321},
numpages = {11},
keywords = {mylyn, micro interaction metrics, defect prediction},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.5555/3172795.3172805,
author = {Di Prospero, Adam and Norouzi, Nojan and Fokaefs, Marios and Litoiu, Marin},
title = {Chatbots as assistants: an architectural framework},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Automated text-based or speech-based personal assistants, also known as chatbots, have been prevalent in several domains including marketing and technical support. Through mainstream applications, such as Siri or Alexa, their popularity has increased and we now see them being used in even more domains. Although the purpose of chatbots varies among domains, there are common elements that all chatbots share. By identifying these elements, it is possible to streamline the development of chatbots en masse and in a structured manner. Additionally, there can be common challenges in the development of such applications, for example, how to treat novice versus expert users or how to establish memory of the conversation. In this work, we propose a reference architecture for chatbots using concepts from Software Product Lines and Feature Models, where we outline the common elements as well as the common challenges. Using Watson and Bluemix as the basic platforms, we also present the creation of two chatbots, for different purposes, based on this reference architecture to highlight these commonalities.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {76–86},
numpages = {11},
keywords = {software component architectures, cognitive assistants, chatbots},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/1653662.1653717,
author = {Meneely, Andrew and Williams, Laurie},
title = {Secure open source collaboration: an empirical study of linus' law},
year = {2009},
isbn = {9781605588940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1653662.1653717},
doi = {10.1145/1653662.1653717},
abstract = {Open source software is often considered to be secure. One factor in this confidence in the security of open source software lies in leveraging large developer communities to find vulnerabilities in the code. Eric Raymond declares Linus' Law "Given enough eyeballs, all bugs are shallow." Does Linus' Law hold up ad infinitum? Or, can the multitude of developers become "too many cooks in the kitchen", causing the system's security to suffer as a result? In this study, we examine the security of an open source project in the context of developer collaboration. By analyzing version control logs, we quantified notions of Linus' Law as well as the "too many cooks in the kitchen" viewpoint into developer activity metrics. We performed an empirical case study by examining correlations between the known security vulnerabilities in the open source Red Hat Enterprise Linux 4 kernel and developer activity metrics. Files developed by otherwise-independent developer groups were more likely to have a vulnerability, supporting Linus' Law. However, files with changes from nine or more developers were 16 times more likely to have a vulnerability than files changed by fewer than nine developers, indicating that many developers changing code may have a detrimental effect on the system's security.},
booktitle = {Proceedings of the 16th ACM Conference on Computer and Communications Security},
pages = {453–462},
numpages = {10},
keywords = {vulnerability, metric, linus' law, developer network, contribution network},
location = {Chicago, Illinois, USA},
series = {CCS '09}
}

@inproceedings{10.1145/1595696.1595716,
author = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
title = {Fair and balanced? bias in bug-fix datasets},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595716},
doi = {10.1145/1595696.1595716},
abstract = {Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical data from several software projects, and find strong evidence of systematic bias. We then investigate the potential effects of "unfair, imbalanced" datasets on the performance of prediction techniques. We draw the lesson that bias is a critical problem that threatens both the effectiveness of processes that rely on biased datasets to build prediction models and the generalizability of hypotheses tested on biased data.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {121–130},
numpages = {10},
keywords = {bias},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/3558481.3591077,
author = {Haqi, Alireza and Zarrabi-Zadeh, Hamid},
title = {Almost Optimal Massively Parallel Algorithms for k-Center Clustering and Diversity Maximization},
year = {2023},
isbn = {9781450395458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558481.3591077},
doi = {10.1145/3558481.3591077},
abstract = {Clustering and diversification are two central problems with various applications in machine learning, data mining, and information retrieval. The k-center clustering and k-diversity maximization are two of the most well-studied and widely-used problems in this area. Both problems admit sequential algorithms with optimal approximation factors of 2 in any metric space. However, finding distributed algorithms matching the same optimal approximation ratios has been open for more than a decade, with the best current algorithms having factors at least twice the optimal. In this paper, we settle this open problem by presenting constant-round distributed algorithms for k-center clustering and k-diversity maximization in the massively parallel computation (MPC) model, achieving an approximation factor of 2 + ε in any metric space for any constant ε &gt; 0, which is essentially the best possible considering the lower bound of 2 on the approximability of both these problems. Our algorithms are based on a novel technique for approximating vertex degrees and finding a so-called k-bounded maximal independent set in threshold graphs, using only a constant number of MPC rounds. Other applications of our general technique is also implied, including an almost optimal (3 + ε)-approximation algorithm for the k-supplier problem in any metric space in the MPC model.},
booktitle = {Proceedings of the 35th ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {239–247},
numpages = {9},
keywords = {$k$-center clustering, diversity maximization, massively parallel algorithms, maximal independent set},
location = {Orlando, FL, USA},
series = {SPAA '23}
}

@inproceedings{10.1145/1985793.1985859,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
title = {Dealing with noise in defect prediction},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985859},
doi = {10.1145/1985793.1985859},
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises.This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {481–490},
numpages = {10},
keywords = {noise resistance, defect prediction, data quality, buggy files, buggy changes},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2372251.2372287,
author = {Wang, Jue and Zhang, Hongyu},
title = {Predicting defect numbers based on defect state transition models},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372287},
doi = {10.1145/2372251.2372287},
abstract = {During software maintenance, a large number of defects could be discovered and reported. A defect can enter many states during its lifecycle, such as NEW, ASSIGNED, and RESOLVED. The ability to predict the number of defects at each state can help project teams better evaluate and plan maintenance activities. In this paper, we present BugStates, a method for predicting defect numbers at each state based on defect state transition models. In our method, we first construct defect state transition models using historical data. We then derive a stability metric from the transition models to measure a project's defect-fixing performance. For projects with stable defect-fixing performance, we show that we can apply Markovian method to predict the number of defects at each state in future based on the state transition model. We evaluate the effectiveness of BugStates using six open source projects and the results are promising. For example, when predicting defect numbers at each state in December 2010 using data from July 2009 to June 2010, the absolute errors for all projects are less than 28. In general, BugStates also outperforms other related methods.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {191–200},
numpages = {10},
keywords = {markov models, defect-fixing performance, defect state transitions, defect prediction, defect numbers},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/3564121.3564133,
author = {Dutta, Jeet and Dey, Swarnava and Mukherjee, Arijit and Pal, Arpan},
title = {Acceleration-aware, Retraining-free Evolutionary Pruning for Automated Fitment of Deep Learning Models on Edge Devices},
year = {2023},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564121.3564133},
doi = {10.1145/3564121.3564133},
abstract = {Deep Learning architectures used in computer vision, natural language and speech processing, unsupervised clustering, etc. have become highly complex and application-specific in recent times. Despite existing automated feature engineering techniques, building such complex models still requires extensive domain knowledge or a huge infrastructure for employing techniques such as Neural Architecture Search (NAS). Further, many industrial applications need in-premises decision-making close to sensors, thus making deployment of deep learning models on edge devices a desirable and often necessary option. Instead of freshly designing application-specific Deep Learning models, the transformation of already built models can achieve faster time to market and cost reduction. In this work, we present an efficient re-training-free model compression method that searches for the best hyper-parameters to reduce the model size and latency without losing any accuracy. Moreover, our proposed method takes into account any drop in accuracy due to hardware acceleration, when a Deep Neural Network is executed on accelerator hardware.},
booktitle = {Proceedings of the Second International Conference on AI-ML Systems},
articleno = {10},
numpages = {10},
keywords = {pruning, neural networks, nas, edge, deep learning},
location = {Bangalore, India},
series = {AIMLSystems '22}
}

@inproceedings{10.1145/1143997.1144313,
author = {Bouktif, Salah and Sahraoui, Houari and Antoniol, Giuliano},
title = {Simulated annealing for improving software quality prediction},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144313},
doi = {10.1145/1143997.1144313},
abstract = {In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions.We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available expert and it compares favorably with the expert build via a customized genetic algorithm.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1893–1900},
numpages = {8},
keywords = {software quality, simulated annealing, predictive models, expertise reuse, Bayesian classifiers},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1145/1370750.1370772,
author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
title = {An extension of fault-prone filtering using precise training and a dynamic threshold},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370772},
doi = {10.1145/1370750.1370772},
abstract = {Fault-prone module detection in source code is important for assurance of software quality. Most previous fault-prone detection approaches have been based on software metrics. Such approaches, however, have difficulties in collecting the metrics and in constructing mathematical models based on the metrics. To mitigate such difficulties, we have proposed a novel approach for detecting fault-prone modules using a spam-filtering technique, named Fault-Prone Filtering. In our approach, fault-prone modules are detected in such a way that the source code modules are considered as text files and are applied to the spam filter directly. In practice, we use the training only errors procedure and apply this procedure to fault-prone. Since no pre-training is required, this procedure can be applied to an actual development field immediately. This paper describes an extension of the training only errors procedures. We introduce a precise unit of training, "modified lines of code," instead of methods. In addition, we introduce the dynamic threshold for classification. The result of the experiment shows that our extension leads to twice the precision with about the same recall, and improves 15% on the best F1 measurement.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {89–98},
numpages = {10},
keywords = {text mining, spam filter, fault-prone modules},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1109/ASE.2019.00041,
author = {Zheng, Wujie and Lu, Haochuan and Zhou, Yangfan and Liang, Jianming and Zheng, Haibing and Deng, Yuetang},
title = {iFeedback: exploiting user feedback for real-time issue detection in large-scale online service systems},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00041},
doi = {10.1109/ASE.2019.00041},
abstract = {Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {352–363},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {software product line testing, search-based software engineering, preference-based multi-objective algorithms},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@proceedings{10.1145/3568834,
title = {ICIBE '22: Proceedings of the 8th International Conference on Industrial and Business Engineering},
year = {2022},
isbn = {9781450397582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@inproceedings{10.1145/1101908.1101941,
author = {Langelier, Guillaume and Sahraoui, Houari and Poulin, Pierre},
title = {Visualization-based analysis of quality for large-scale software systems},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101941},
doi = {10.1145/1101908.1101941},
abstract = {We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {214–223},
numpages = {10},
keywords = {software visualization, quality assessment, metrics},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@inproceedings{10.1145/2460999.2461002,
author = {Matos, Olavo and Fortaleza, Luiz and Conte, Tayana and Mendes, Emilia},
title = {Realising web effort estimation: a qualitative investigation},
year = {2013},
isbn = {9781450318488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460999.2461002},
doi = {10.1145/2460999.2461002},
abstract = {Context: Reliable effort estimation is essential for better management of Web projects, hence the need to identify what are the key factors that affect effort estimates for new Web projects and how they are inter-related. Objective: This paper improves our understanding of Web effort estimation using as basis the knowledge of Web effort estimation experts. Method: We employed qualitative research with the participation of four different Web development companies in Manaus (Brazil) using semi-structured interviews for data collection; our data analysis was carried out using Grounded Theory-based procedures to identify and combine factors affecting the estimation effort of Web projects. Results: We identified four main groupings (categories) of factors - Web project, Web development complexity, Web development team, and Clients. Each of these groupings contains a set of factors that impact upon Web effort estimation. Conclusions: This is the first time that qualitative research is employed in the field of Web effort estimation to further understand and help improve this process. In addition, some of the factors found had never been identified in any of the previous studies in this field, thus suggesting that the use of Grounded Theory-based procedures may provide a way to enrich our understanding of the phenomenon under investigation via the identification of factors that overlap and also complement those from previous studies.},
booktitle = {Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {web project management, web effort predictors, web effort estimation, qualitative studies, grounded-theory},
location = {Porto de Galinhas, Brazil},
series = {EASE '13}
}

@inproceedings{10.1145/1342211.1342232,
author = {Shukla, Ruchi and Misra, Arun Kumar},
title = {Estimating software maintenance effort: a neural network approach},
year = {2008},
isbn = {9781595939173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1342211.1342232},
doi = {10.1145/1342211.1342232},
abstract = {Software maintenance forms an essential component of software development. Its planning includes estimation of maintenance effort, duration, personnel and costs. Adequate information regarding size, complexity and maintainability is however often unavailable. In the present work, a Neural Network (NN) based effort estimator is developed using Matlab. A feed forward back- propagation NN employing Bayesian regularization training is selected and trained for one dataset. Various categories of software maintenance cost drivers and their effect on maintenance effort have been analyzed using different combinations of number of hidden layers and hidden neurons etc. The NN is able to successfully model the maintenance effort as the obtained results are well within the previously published error limits},
booktitle = {Proceedings of the 1st India Software Engineering Conference},
pages = {107–112},
numpages = {6},
keywords = {software maintenance, neural network, effort estimation},
location = {Hyderabad, India},
series = {ISEC '08}
}

@proceedings{10.1145/3643690,
title = {IWSiB '24: Proceedings of the 7th ACM/IEEE International Workshop on Software-intensive Business},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The workshop brings together research communities working on softwareintensive business and software engineering. It aims to bridge the gap between the research in these areas. This year's theme, "Software Business in the Era of Generative Artificial Intelligence," reflects our focus on exploring how generative artificial intelligence (GenAI) and the related large language models (LLMs) impact the established practices of software engineering and software business.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1109/WI-IAT.2009.118,
author = {Ziegler, Cai-Nicolas and Viermetz, Maximilian},
title = {Discovery of Technology Synergies through Collective Wisdom},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.118},
doi = {10.1109/WI-IAT.2009.118},
abstract = {Spotting and quantifying technological synergies across organizational levels is of utter importance for corporate strategy departments. These efforts aim at saving resources by consolidating scattered expertise and by reusing technologies across multiple product lines. In the past, this task has been done in a manual process by domain experts. While feasible, the major drawback lies in the enormous cost of time: For a structured and complete analysis every combination of any two technologies has to be assessed. We present an approach that discovers those synergies in an automated fashion, using collective wisdom from the Web. Our method has been deployed for the synergy evaluation process within Siemens. We have also conducted evaluations based on randomly selected technology pairs so as to benchmark the accuracy of our approach, as compared to a group of general computer science technologists as well as a control group of domain experts.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {701–706},
numpages = {6},
keywords = {text mining, synergy, similarity, semantics},
series = {WI-IAT '09}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {structural typing, software product line engineering, delta-oriented programming},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/2872518.2891070,
author = {Lakshminarayan, Choudur and Kosuru, Ram and Hsu, Meichun},
title = {Modeling Complex Clickstream Data by Stochastic Models: Theory and Methods},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891070},
doi = {10.1145/2872518.2891070},
abstract = {As the website is a primary customer touch-point, millions are spent to gather web data about customer visits. Sadly, the trove of data and corresponding analytics have not lived up to the promise. Current marketing practice relies on ambiguous summary statistics or small-sample usability studies. Idiosyncratic browsing and low conversion (browser-to-buyer) make modeling hard. In this paper, we model browsing patterns (sequence of clicks) via Markov chain theory to predict users' propensity to buy within a session. We focus on model complexity, imputing missing values, data augmentation, and other attendant issues that impact performance. The paper addresses the following aspects; (1) Determine appropriate order of the Markov chain (assess the influence of prior history in prediction), (2) Impute missing transitions by exploiting the inherent link structure in the page sequences, (3) predict the likelihood of a purchase based on variable-length page sequences, and (4) Augment the training set of buyers (which is typically very small: 2% by viewing the page transitions as a graph and exploiting its link structure to improve performance. The cocktail of solutions address important issues in practical digital marketing. Extensive analysis of data applied to a large commercial web-site shows that Markov chain based classifiers are useful predictors of user intent.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {879–884},
numpages = {6},
keywords = {prediction, markov chains, link analysis, imputation, click streams},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {software metrics, defect prediction, cost-sensitive classification},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/1571941.1572035,
author = {Ziegler, Cai-Nicolas and Jung, Stefan},
title = {Leveraging sources of collective wisdom on the web for discovering technology synergies},
year = {2009},
isbn = {9781605584836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1571941.1572035},
doi = {10.1145/1571941.1572035},
abstract = {One of the central tasks of R&amp;D strategy and portfolio management at large technology companies and research institutions refers to the identification of technological synergies throughout the organization. These efforts are geared towards saving resources by consolidating scattered expertise, sharing best practices, and reusing available technologies across multiple product lines. In the past, this task has been done in a manual evaluation process by technical domain experts. While feasible, the major drawback of this approach is the enormous effort in terms of availability and time: For a structured and complete analysis every combination of any two technologies has to be rated explicitly. We present a novel approach that recommends technological synergies in an automated fashion, making use of abundant collective wisdom from the Web, both in pure textual form as well as classification ontologies. Our method has been deployed for practical support of the synergy evaluation process within our company. We have also conducted empirical evaluations based on randomly selected technology pairs so as to benchmark the accuracy of our approach, as compared to a group of general computer science technologists as well as a control group of domain experts.},
booktitle = {Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {548–555},
numpages = {8},
keywords = {web 2.0, text mining, semantic similarity, collective wisdom},
location = {Boston, MA, USA},
series = {SIGIR '09}
}

@article{10.1007/s00165-019-00479-y,
author = {Dimovski, Aleksandar S. and Brabrand, Claus and W\k{a}sowski, Andrzej},
title = {Finding suitable variability abstractions for lifted analysis},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-019-00479-y},
doi = {10.1007/s00165-019-00479-y},
abstract = {Many software systems are today variational: they are built as program families or Software Product Lines. They can produce a potentially huge number of related programs, known as products or variants, by selecting suitable configuration options (features) at compile time. Many such program families are safety critical, yet the appropriate tools only rarely are able to analyze them effeciently. Researchers have addressed this problem by designing specialized variability-aware static (dataflow) analyses, which allow analyzing all variants of the family, simultaneously, in a single run without generating any of the variants explicitly. They are also known as lifted or family-based analyses. They take as input the common code base, which encodes all variants of a program family, and produce precise analysis results corresponding to all variants. These analyses scale much better than “brute force” approach, where all individual variants are analyzed in isolation, one-by-one, using off-the-shelf single-program analyzers. Nevertheless, the computational cost of lifted analyses still greatly depends on the number of features and variants (which is often huge). For families with a large number of features and variants, the lifted analyses may be too costly or even infeasible. In order to speed up lifted analyses and make them computationally cheaper, variability abstractions which simplify variability away from program families and lifted analyses have been introduced. However, the space of possible variability abstractions is still intractably large to search naively, with most abstractions being either too imprecise or too costly. We introduce here a method to efficiently find suitable variability abstractions from a large space of possible abstractions for a lifted static analysis. The main idea is to use a pre-analysis to estimate the impact of variability-specific parts of the program family on the analysis’s precision. The pre-analysis is fully variability-aware while it aggressively abstracts the other semantics aspects. Then we use the pre-analysis results to find out when and where the subsequent abstract lifted analysis should turn off or on its variability-awareness. The abstraction constructed in this way is effective in discarding variability-specific program details that are irrelevant for showing the analysis’s ultimate goal. We formalize this approach and we illustrate its effectiveness on several Java case studies. The evaluation shows that our approach which consists of running a pre-analysis followed by a subsequent abstract lifted analysis achieves competitive the precision-speed tradeoff compared to the standard lifted analysis.},
journal = {Form. Asp. Comput.},
month = apr,
pages = {231–259},
numpages = {29},
keywords = {Abstract interpretation, Variability abstractions, Lifted static analysis, Program families}
}

@inproceedings{10.1145/2528265.2528270,
author = {Erwig, Martin and Walkingshaw, Eric and Chen, Sheng},
title = {An abstract representation of variational graphs},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528270},
doi = {10.1145/2528265.2528270},
abstract = {In the context of software product lines, there is often a need to represent graphs containing variability. For example, extending traditional modeling techniques or program analyses to variational software requires a corresponding notion of variational graphs. In this paper, we introduce a general model of variational graphs and a theoretical framework for discussing variational graph algorithms. Specifically, we present an abstract syntax based on tagging for succinctly representing variational graphs and other data types relevant to variational graph algorithms, such as variational sets and paths. We demonstrate how (non-variational) graph algorithms can be generalized to operate on variational graphs, to accept variational inputs, and produce variational outputs. Finally, we discuss a filtering operation on variational graphs and how this interacts with variational graph algorithms.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {25–32},
numpages = {8},
keywords = {variational data structures, variational algorithms, choice calculus},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@inproceedings{10.1145/1181775.1181781,
author = {Kim, Sunghun and Pan, Kai and Whitehead, E. E. James},
title = {Memories of bug fixes},
year = {2006},
isbn = {1595934685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1181775.1181781},
doi = {10.1145/1181775.1181781},
abstract = {The change history of a software project contains a rich collection of code changes that record previous development experience. Changes that fix bugs are especially interesting, since they record both the old buggy code and the new fixed code. This paper presents a bug finding algorithm using bug fix memories: a project-specific bug and fix knowledge base developed by analyzing the history of bug fixes. A bug finding tool, BugMem, implements the algorithm. The approach is different from bug finding tools based on theorem proving or static model checking such as Bandera, ESC/Java, FindBugs, JLint, and PMD. Since these tools use pre-defined common bug patterns to find bugs, they do not aim to identify project-specific bugs. Bug fix memories use a learning process, so the bug patterns are project-specific, and project-specific bugs can be detected. The algorithm and tool are assessed by evaluating if real bugs and fixes in project histories can be found in the bug fix memories. Analysis of five open source projects shows that, for these projects, 19.3%-40.3% of bugs appear repeatedly in the memories, and 7.9%-15.5% of bug and fix pairs are found in memories. The results demonstrate that project-specific bug fix patterns occur frequently enough to be useful as a bug detection technique. Furthermore, for the bug and fix pairs, it is possible to both detect the bug and provide a strong suggestion for the fix. However, there is also a high false positive rate, with 20.8%-32.5% of non-bug containing changes also having patterns found in the memories. A comparison of BugMem with a bug finding tool, PMD, shows that the bug sets identified by both tools are mostly exclusive, indicating that BugMem complements other bug finding tools.},
booktitle = {Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {35–45},
numpages = {11},
keywords = {prediction, patterns, fix, fault, bug finding tool, bug},
location = {Portland, Oregon, USA},
series = {SIGSOFT '06/FSE-14}
}

@inproceedings{10.1145/1150402.1150520,
author = {Forman, George and Kirshenbaum, Evan and Suermondt, Jaap},
title = {Pragmatic text mining: minimizing human effort to quantify many issues in call logs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150520},
doi = {10.1145/1150402.1150520},
abstract = {We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs. The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type, to appropriately target engineering resources, and to provide the best diagnosis, support and documentation for most common issues. We present a new set of techniques for doing this efficiently on an industrial scale, without requiring manual coding of calls in the call center. Our approach involves (1) a new text clustering method to identify common and emerging issues; (2) a method to rapidly train large numbers of categorizers in a practical, interactive manner; and (3) a method to accurately quantify categories, even in the face of inaccurate classifications and training sets that necessarily cannot match the class distribution of each new month's data. We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {852–861},
numpages = {10},
keywords = {text mining, text classification, supervised machine learning, quantification, log processing, applications},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/3368089.3409693,
author = {P\^{a}rundefinedachi, Profir-Petru and Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T.},
title = {Flexeme: untangling commits using lexical flows},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409693},
doi = {10.1145/3368089.3409693},
abstract = {Today, most developers bundle changes into commits that they submit to a shared code repository. Tangled commits intermix distinct concerns, such as a bug fix and a new feature. They cause issues for developers, reviewers, and researchers alike: they restrict the usability of tools such as git bisect, make patch comprehension more difficult, and force researchers who mine software repositories to contend with noise. We present a novel data structure, the 𝛿-NFG, a multiversion Program Dependency Graph augmented with name flows. A 𝛿-NFG directly and simultaneously encodes different program versions, thereby capturing commits, and annotates data flow edges with the names/lexemes that flow across them. Our technique, Flexeme, builds a 𝛿-NFG from commits, then applies Agglomerative Clustering using Graph Similarity to that 𝛿-NFG to untangle its commits. At the untangling task on a C# corpus, our implementation, Heddle, improves the state-of-the-art on accuracy by 0.14, achieving 0.81, in a fraction of the time: Heddle is 32 times faster than the previous state-of-the-art.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {63–74},
numpages = {12},
keywords = {graph kernels, commint untangling, clustering},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3641399.3641439,
author = {Barat, Souvik and Ramamohan, Varun},
title = {Workshop Report on Reimagining Future Enterprises and Society using Digital Twin},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641439},
doi = {10.1145/3641399.3641439},
abstract = {In the dynamic landscape of technological advancements, the Digital Twin is emerging as a powerful aid with the potential to transform traditional decision-making approaches for diverse complex systems. Technically, it offers a paradigm shift in our understanding and resolution of intricate problems by effectively mirroring real-world entities. This enables us to comprehend their behaviors through simulation, anticipate anomalies for outlier environmental conditions, and derive evidence-driven solutions. While proven effective in physical and cyber-physical systems, its untapped potential lies in the realm of techno-socio-economic systems that operate in dynamic and uncertain environments. The use of such technology in informed decision-making is significant for enterprises and society, as modern businesses and societal structures demand innovative solutions that transcend conventional approaches. With the growing need, the global digital twin market is expected to expand from $11.51 billion in 2023 to $137.67 billion by 2030, impacting domains such as manufacturing, healthcare, sustainability, smart cities, and more. However, realizing this vast business potential hinges on advancing technological capabilities and fostering innovation. This workshop aims to discuss the existing landscape of digital twins and explore uncharted territory by harnessing the latent potential of Digital Twins within techno-socio-economic systems.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {22},
numpages = {2},
keywords = {Complex Systems, Decision Making, Enterprise Digital Twins, Modelling and Simulation, Optimization},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/1150402.1150423,
author = {Forman, George},
title = {Quantifying trends accurately despite classifier error and class imbalance},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150423},
doi = {10.1145/1150402.1150423},
abstract = {This paper promotes a new task for supervised machine learning research: quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set, with no concern for predictions on individual cases. A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. These tasks cover a large and important family of applications that measure trends over time.The paper establishes a research methodology, and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications. In empirical tests, Median Sweep methods show outstanding ability to estimate the class distribution, despite wide disparity in testing and training conditions. The paper addresses shifting class priors and costs, but not concept drift in general.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {157–166},
numpages = {10},
keywords = {text mining, quantification, cost quantification, classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@inproceedings{10.1145/3652620.3688199,
author = {F\"{o}ldi\'{a}k, M\'{a}t\'{e}},
title = {Probabilistic Graph Queries for Design Space Exploration Under Uncertainty},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688199},
doi = {10.1145/3652620.3688199},
abstract = {Critical cyber-physical systems have an increasingly significant role in the world, and ensuring their safety is a high priority objective. State of the art approaches and engineering tools can support the development process from very early stages, with high-level system modeling, analysis capabilities, and exploration of alternatives. However, these approaches are limited when it comes evaluation of complex extra-functional characteristics over designs with uncertainties, typical to early system designs. In my thesis project, I intend to introduce probabilistic graph queries for high level, scalable probabilistic analysis, for analysing system models with design uncertainty and applicable in design space exploration. The approach will be evaluated on external case studies, focusing on key performance metrics related to applicability in the target context, such as runtime, precision and formal guarantees.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {142–148},
numpages = {7},
keywords = {cyber-physical systems, probabilistic analysis, graph queries, design space exploration, design uncertainty, lifting, safety},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3627673.3679072,
author = {Deshmukh, Abhishek Sudhakar and Dutta, Arnab},
title = {A Supervised BERT Model for Identifying Core-Intent Bearing Phrases in e-Commerce Queries},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679072},
doi = {10.1145/3627673.3679072},
abstract = {In the realm of e-Commerce, a fundamental problem is accurate interpretation of users' core intent. The intent is often subtly expressed implicitly or stated explicitly with the usage of verbose tokens or key phrases in a user query. In this work, we focus on the later class of problems where we identify a subset of query tokens which are the primary intent bearing phrases that convey explicit intents. We did not solve this as an intent detection problem but rather an immutable component detection problem because we believe that discovering the immutable phrases in a query entails that those are the intent bearing phrases. Furthermore, identifying a certain set of query tokens as immutable ensures better downstream processing in terms of unprecedented token handling, query category detection or query rewrites. We have developed a BERT based supervised learned model which can identify core-intent tokens, thereby improving F1 score over the baseline by over 35%. Furthermore, we integrated our proposed approach for a query recovery strategy which produces approximately 11.9% improvement in offline relevance scores compared to the production model.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5558–5559},
numpages = {2},
keywords = {bert, query intent, query reformulations, query tokens weighing},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/1414004.1414049,
author = {Vivanco, Rodrigo and Jin, Dean},
title = {Enhancing predictive models using principal component analysis and search based metric selection: a comparative study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414049},
doi = {10.1145/1414004.1414049},
abstract = {Predictive models are used for the detection of potentially problematic component that decrease product quality. Source code metrics can be used as input features in predictive models; however, there exist numerous structural measures that capture different aspects of size, coupling, cohesion, inheritance and complexity. An important question to answer is which metrics should be used with a predictor. A comparative analysis of metric selection strategies (principal component analysis, a genetic algorithm and the CK metrics set) has been carried out. Initial results indicate that search-based metric selection gives the best predictive performance in identifying Java classes with high cognitive complexity that degrades product maintenance.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {273–275},
numpages = {3},
keywords = {predictive models, pca, metric selection, genetic algorithm},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1145/3708527,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Marchezan, Luciano and Arkoh, Lawrence and Egyed, Alexander and Ramler, Rudolf},
title = {Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708527},
doi = {10.1145/3708527},
abstract = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Evolution, Software Migration, Re-designing, Re-engineering}
}

@inproceedings{10.1145/1540438.1540448,
author = {Mende, Thilo and Koschke, Rainer},
title = {Revisiting the evaluation of defect prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540448},
doi = {10.1145/1540438.1540448},
abstract = {Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show.An important aspect often ignored during evaluation is the effort reduction gained by using such models. Models are usually evaluated per module by performance measures used in information retrieval, such as recall, precision, or the area under the ROC curve (AUC). These measures assume that the costs associated with additional quality assurance activities are the same for each module, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of a module.In this paper, we investigate this discrepancy using optimal and trivial models. We describe a trivial model that takes only the module size measured in lines of code into account, and compare it to five classification methods. The trivial model performs surprisingly well when evaluated using AUC. However, when an effort-sensitive performance measure is used, it becomes apparent that the trivial model is in fact the worst.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {cost-sensitive performance measures, defect prediction},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/2430502.2430531,
author = {Wulf-Hadash, Ora and Reinhartz-Berger, Iris},
title = {Cross product line analysis},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430531},
doi = {10.1145/2430502.2430531},
abstract = {Due to increase in market competition and merger and acquisition of companies, different software product lines (SPLs) may exist under the same roof. These SPLs may be developed applying different domain analysis processes, but are likely not disjoint. Cross product line analysis aims to examine the common and variable aspects of different SPLs for improving maintenance and future development of related SPLs. Currently different SPL artifacts, or more accurately feature models, are compared, matched, and merged for supporting scalability, increasing modularity and reuse, synchronizing feature model versions, and modeling multiple SPLs for software supply chains. However, in all these cases the focus is on creating valid merged models from the input feature models. Furthermore, the terminology used in all the input feature models is assumed to be the same, namely similar features are named the same. As a result these methods cannot be simply applied to feature models that represent different SPLs. In this work we offer adapting similarity metrics and text clustering techniques in order to enable cross product line analysis. This way analysis of feature models that use different terminologies in the same domain can be done in order to improve the management of the involved SPLs. Preliminary results reveal that the suggested method helps systematically analyze the commonality and variability between related SPLs, potentially suggesting improvements to existing SPLs and to the maintenance of sets of SPLs.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {8},
keywords = {feature similarity, feature diagram merging, feature diagram matching, feature clustering, empirical evaluation},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/3640457.3688174,
author = {Borgersen, Karl Audun Kagnes and Goodwin, Morten and Grundetjern, Morten and Sharma, Jivitesh},
title = {A Dataset for Adapting Recommender Systems to the Fashion Rental Economy},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688174},
doi = {10.1145/3640457.3688174},
abstract = {In response to the escalating ecological challenges that threaten global sustainability, there’s a need to investigate alternative methods of commerce, such as rental economies. Like most online commerce, rental or otherwise, a functioning recommender system is crucial for their success. Yet the domain has, until this point, been largely neglected by the recommender system research community. Our dataset, derived from our collaboration with the leading Norwegian fashion rental company Vibrent, encompasses 77.1k transactions, rental histories from 7.4k anonymized users, and 15.6k unique outfits in which each physical item’s attributes and rental history is meticulously tracked. All outfits are listed as individual items or their corresponding item groups, referring to shared designs between the individual items. This notation underlines the novel challenges of rental as compared to more traditional recommender system problems where items are generally interchangeable. For example, an RS for rental items requires tracking each physical item to ensure it isn’t rented for the same time period to several different customers, as compared to retail, in which tracking or recommending individual items is largely unnecessary. Each outfit is accompanied by a set of tags describing some of their attributes. We also provide a total of 50.1k images displaying across all items, along with a set of precomputed zero-shot embeddings. We apply a myriad of common recommender system methods to the dataset to provide a performance baseline. This baseline is calculated for both the traditional fashion recommender system problem of recommending outfit groups and the novel problem of predicting individual item rental. To our knowledge, this is the first published article to directly discuss fashion rental recommender systems, as well as the first published dataset intended for this purpose. We hope that the publication of this dataset will serve as a catalyst for a new branch of research for specialized fashion rental recommender systems. The dataset has been made freely available at https://www.kaggle.com/datasets/kaborg15/vibrent-clothes-rental-dataset All code associated with the project have been made available at:https://github.com/cair/Vibrent_Clothes_Rental_Dataset_Collection},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {945–950},
numpages = {6},
keywords = {Dataset, Fashion, Recommender Systems, Rental},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3643655.3643876,
author = {Rossi, Maria Teresa and Tundo, Alessandro and Mariani, Leonardo},
title = {Towards Model-Driven Dashboard Generation for Systems-of-Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643876},
doi = {10.1145/3643655.3643876},
abstract = {Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {9–12},
numpages = {4},
keywords = {automatic dashboard generation, model-driven engineering, model-based dashboard, systems of systems, monitoring dashboard},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3540250.3549151,
author = {Dong, Liming and Zhang, He and Liu, Wei and Weng, Zhiluo and Kuang, Hongyu},
title = {Semi-supervised pre-processing for learning-based traceability framework on real-world software projects},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549151},
doi = {10.1145/3540250.3549151},
abstract = {The traceability of software artifacts has been recognized as an important factor to support various activities in software development processes. However, traceability can be difficult and time-consuming to create and maintain manually, thereby automated approaches have gained much attention. Unfortunately, existing automated approaches for traceability suffer from practical issues. This paper aims to gain an understanding of the potential challenges for the underperforming of the state-of-the-art, ML-based trace link classifiers applied in real-world projects. By investigating different industrial datasets, we found that two critical (and classic) challenges, i.e. data imbalance and sparse problems, lie in real-world projects’ traceability automation. To overcome these challenges, we developed a framework called SPLINT to incorporate hybrid textual similarity measures and semi-supervised learning strategies as enhancements to the learning-based traceability approaches. We carried out experiments with six open-source platforms and ten industry datasets. The results confirm that SPLINT is able to operate at higher performance on two communities’ datasets. Specifically, the industrial datasets, which significantly suffer from data imbalance and sparsity problems, show an increase in F2-score over 14% and AUC over 8% on average. The adjusted class-balancing and self-training policies used in SPLINT (CBST-Adjust) also work effectively for the selection of pseudo-labels on minor classes from unlabeled trace sets, demonstrating SPLINT’s practicability.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {570–582},
numpages = {13},
keywords = {Data Imbalance, Data Sparsity, Industry Practice, Learning-based Model, Semi-supervised Learning, Software Traceability},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1109/SC41406.2024.00028,
author = {Yang, Zeyu and Adamek, Karel and Armour, Wesley},
title = {Accurate and Convenient Energy Measurements for GPUs: A Detailed Study of NVIDIA GPU's Built-In Power Sensor},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00028},
doi = {10.1109/SC41406.2024.00028},
abstract = {GPU has emerged as the go-to accelerator for HPC workloads, however its power consumption has become a major limiting factor for further scaling HPC systems. An accurate understanding of GPU power consumption is essential for further improving its energy efficiency, and consequently reducing the associated carbon footprint. Despite the limited documentation and lack of understanding, NVIDIA GPUs' built-in power sensor is widely used in energy-efficient computing research. Our study seeks to elucidate the internal mechanisms of the power readings provided by nvidia-smi and assess the accuracy of the measurements. We evaluated over 70 different GPUs across 12 architectural generations, and identified several unforeseen problems that can lead to drastic under/overestimation of energy consumed, for example on the A100 and H100 GPUs only 25% of the runtime is sampled. We proposed several mitigations that could reduce the energy measurement error by an average of 35% in the test cases we present.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {22},
numpages = {17},
keywords = {Energy consumption, Energy measurement, Green computing, High performance computing, Power measurement},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@book{10.1145/3640508,
editor = {Townsend, Gloria Childress},
title = {Rendering History: The Women of ACM-W},
year = {2024},
isbn = {9798400717741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {58},
abstract = {The Association for Computing Machinery (ACM) has more than 100,000 members circling the globe, including trailblazing women who created ACM-W (ACM’s Committee on Women in Computing) in 1993. This book, published in celebration of ACM-W’s 30th birthday, divides the history of ACM-W into three parts.The first section provides a traditional history that details the evolution of ACM-W’s projects. In the next section, Rendering History allows the women of ACM-W to tell their own stories. What motivated them to trade personal time and energy for work that would change the face of computing for women and young girls? Among many others, Sue Black relates a story that spans her escape from two abusive homes to recognition for her computing accomplishments by both the late Queen of England and the current King. Kathy Kleiman describes her contributions to the field, including helping to rescue the wireless spectrum (now used by WiFi) from the (US) Federal Communications Commission’s plan to sell it. Bhavani Thuraisingham writes about her birth in Sri Lanka, an arranged marriage to a man eight years her senior, and cutting-edge research in the integration of cyber security and machine learning. The final section of the book provides an annotated bibliography of the research that launched ACM-W and continued to inform its projects over the next 30 years.ACM-W advocates internationally for the full engagement of women in all aspects of the computing field, providing a wide range of programs and services to ACM members and working in the larger community to advance the contributions of technical women. The main theme of ACM-W’s 30-year history as detailed in this book is the organization’s maturation from a US-centric organization to a global leader in supporting the advancement of women in computer science.“This is, quite simply, a book you won’t want to put down! What a wonderful collection of stories – blending personal and professional reflections – of many of the women who have been pioneers in computing and its roles in society, and in ACM-W. Gloria Childress Townsend is one of those pioneers and we owe her great thanks for putting together this captivating collection of stories, and for telling the very impressive and influential history of ACM-W.” - Bobby Schnabel, University of Colorado Boulder – co-founder of the National Center for Women &amp; Information Technology}
}

@inproceedings{10.1145/3652620.3687815,
author = {Raeisdanaei, Ali and Murphy, Logan and Di Sandro, Alessio and Askarpour, Mehrnoosh and Viger, Torin and Chechik, Marsha},
title = {Evaluation of Automotive OTA Updates Using Assurance Cases},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687815},
doi = {10.1145/3652620.3687815},
abstract = {Software-intensive vehicles require regular over-the-air (OTA) updates. To ensure that OTA updates do not compromise system safety, such updates should be assured. Automotive safety engineers need to efficiently estimate the effort it would take to assure these updates for the entire fleet. To address this challenge, we propose a process for supporting assurance of OTA updates. Our process proposes to model the fleet of vehicles as a software product line (SPL), assured with a variability-aware assurance case. It then measures the difficulty of assuring the proposed update using this variability-aware AC model via a set of heuristics.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {720–724},
numpages = {5},
keywords = {assurance, over-the-air updates, impact assessment},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/2370216.2370449,
author = {Yang, Rayoung and Newman, Mark W.},
title = {Living with an intelligent thermostat: advanced control for heating and cooling systems},
year = {2012},
isbn = {9781450312240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370216.2370449},
doi = {10.1145/2370216.2370449},
abstract = {In order to better understand the opportunities and challenges of an intelligent system in the home, we studied the lived experience of a thermostat, the Nest. The Nest utilizes machine learning, sensing, and networking technology, as well as eco-feedback features. To date, we have conducted six interviews and one diary study. Our findings show that improved interfaces through web and mobile applications changed the interactions between users and their home system. Intelligibility and accuracy of the machine learning and sensing technology influenced the way participants perceive and adapt to the system. The convenient control over the system combined with limitations of the technology may have prevented the desired energy savings. These findings assert that thoughtful, continuous involvement from users is critical to the desired system performance and the success of interventions to promote sustainable choices. We suggest that an intelligent system in the home requires improved intelligibility and a better way in which users can provide deliberate input to the system.},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
pages = {1102–1107},
numpages = {6},
keywords = {digital home, energy saving, thermostat, user control},
location = {Pittsburgh, Pennsylvania},
series = {UbiComp '12}
}

@inproceedings{10.1109/ASE.2015.58,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Configuration-aware change impact analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.58},
doi = {10.1109/ASE.2015.58},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {change impact analysis, configuration, maintenance, program analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1109/PROMISE.2007.9,
author = {Koru, A. Gunes and Zhang, Dongsong and Liu, Hongfang},
title = {Modeling the Effect of Size on Defect Proneness for Open-Source Software},
year = {2007},
isbn = {0769529542},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PROMISE.2007.9},
doi = {10.1109/PROMISE.2007.9},
abstract = {Quality is becoming increasingly important with the continuous adoption of open-source software. Previous research has found that there is generally a positive relationship between module size and defect proneness. Therefore, in open-source software development, it is important to monitor module size and understand its impact on defect proneness. However, traditional approaches to quality modeling, which measure specific system snapshots and obtain future defect counts, are not well suited because open-source modules usually evolve and their size changes over time. In this study, we used Cox proportional hazards modeling with recurrent events to study the effect of class size on defect-proneness in the Mozilla product. We found that the effect of size was significant, and we quantified this effect on defect proneness.},
booktitle = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
pages = {10},
series = {PROMISE '07}
}

@proceedings{10.1145/3640310,
title = {MODELS '24: Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Linz, Austria}
}

@inproceedings{10.1145/3674213.3674219,
author = {Osaki, Atsuya and Poisson, Manuel and Makino, Seiki and Shiiba, Ryusei and Fukuda, Kensuke and Okoshi, Tadashi and Nakazawa, Jin},
title = {Dynamic Fixed-point Values in eBPF: a Case for Fully In-kernel Anomaly Detection},
year = {2024},
isbn = {9798400709852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674213.3674219},
doi = {10.1145/3674213.3674219},
abstract = {eBPF and XDP are promising technologies that are capable of accelerating packet processing inside the Linux kernel. Despite these benefits, eBPF is constrained by a number of rigorous restrictions that are imposed to protect the kernel. One such restriction is the lack of support for floating-point values, which was introduced to achieve faster execution and avoid non-deterministic behavior. However, this has become a significant obstacle to expanding the functionality of eBPF programs with advanced algorithms. In this paper, we propose dynamic fixed-point as a solution to overcome this challenge within the restrictions of eBPF. Dynamic fixed-point values are an expansion from traditional fixed-point values, with the bit allocation adjusted dynamically. Benefit of dynamic fixed-point is that the accuracy of calculations are improved, which is one of the critical shortcomings of fixed-point. To demonstrate the effectiveness of our approach, we have designed and implemented a prototype of an entropy-based traffic anomaly detection framework and have reported on its throughput and the detection accuracy. Our prototype, which employs dynamic fixed-point, has achieved an 18% improvement in throughput while also matching the detection accuracy of a similar system that employs floating-point values in user space.},
booktitle = {Proceedings of the Asian Internet Engineering Conference 2024},
pages = {46–54},
numpages = {9},
keywords = {Anomaly Detection, DDoS, Dynamic Fixed-point, XDP, eBPF},
location = {Sydney, NSW, Australia},
series = {AINTEC '24}
}

@article{10.1145/3589227,
author = {Weyns, Danny and Gerostathopoulos, Ilias and Abbas, Nadeem and Andersson, Jesper and Biffl, Stefan and Brada, Premek and Bures, Tomas and Di Salle, Amleto and Galster, Matthias and Lago, Patricia and Lewis, Grace and Litoiu, Marin and Musil, Angelika and Musil, Juergen and Patros, Panos and Pelliccione, Patrizio},
title = {Self-Adaptation in Industry: A Survey},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3589227},
doi = {10.1145/3589227},
abstract = {Computing systems form the backbone of many areas in our society, from manufacturing to traffic control, healthcare, and financial systems. When software plays a vital role in the design, construction, and operation, these systems are referred to as software-intensive systems. Self-adaptation equips a software-intensive system with a feedback loop that either automates tasks that otherwise need to be performed by human operators or deals with uncertain conditions. Such feedback loops have found their way to a variety of practical applications; typical examples are an elastic cloud to adapt computing resources and automated server management to respond quickly to business needs. To gain insight into the motivations for applying self-adaptation in practice, the problems solved using self-adaptation and how these problems are solved, and the difficulties and risks that industry faces in adopting self-adaptation, we performed a large-scale survey. We received 184 valid responses from practitioners spread over 21 countries. Based on the analysis of the survey data, we provide an empirically grounded overview the of state of the practice in the application of self-adaptation. From that, we derive insights for researchers to check their current research with industrial needs, and for practitioners to compare their current practice in applying self-adaptation. These insights also provide opportunities for applying self-adaptation in practice and pave the way for future industry-research collaborations.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {5},
numpages = {44},
keywords = {Self-adaptation, industry, survey}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {UML state machine, aspect-oriented modeling, behavioral variability, model-based testing, product line engineering},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1145/3670795,
author = {Broy, Manfred and Brucker, Achim D. and Fantechi, Alessandro and Gleirscher, Mario and Havelund, Klaus and Kuppe, Markus Alexander and Mendes, Alexandra and Platzer, Andr\'{e} and Ringert, Jan Oliver and Sullivan, Allison},
title = {Does Every Computer Scientist Need to Know Formal Methods?},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1145/3670795},
doi = {10.1145/3670795},
abstract = {We focus on the integration of Formal Methods as mandatory theme in any Computer Science University curriculum. In particular, when considering the ACM Curriculum for Computer Science, the inclusion of Formal Methods as a mandatory Knowledge Area needs arguing for why and how does every computer science graduate benefit from such knowledge. We do not agree with the sentence “While there is a belief that formal methods are important and they are growing in importance, we cannot state that every computer science graduate will need to use formal methods in their career.” We argue that formal methods are and have to be an integral part of every computer science curriculum. Just as not all graduates will need to know how to work with databases either, it is still important for students to have a basic understanding of how data is stored and managed efficiently. The same way, students have to understand why and how formal methods work, what their formal background is, and how they are justified. No engineer should be ignorant of the foundations of their subject and the formal methods based on these.In this article, we aim at highlighting why every computer scientist needs to be familiar with formal methods. We argue that education in formal methods plays a key role by shaping students' programming mindset, fostering an appreciation for underlying principles, and encouraging the practice of thoughtful program design and justification, rather than simply writing programs without reflection and deeper understanding. Since integrating formal methods into the computer science curriculum is not a straightforward process, we explore the additional question: what are the tradeoffs between one dedicated knowledge area of formal methods in a computer science curriculum versus having formal methods scattered across all knowledge areas? Solving problems while designing software and software-intensive systems demands an understanding of what is required, followed by a specification and formalizing a solution in a programming language. How to do this systematically and correctly on solid grounds is exactly supported by formal methods.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {6},
numpages = {17},
keywords = {Formal methods, software and systems engineering, computer science university curriculum}
}

@inproceedings{10.1145/3109859.3109907,
author = {Qazi, Maleeha and Fung, Glenn M. and Meissner, Katie J. and Fontes, Eduardo R.},
title = {An Insurance Recommendation System Using Bayesian Networks},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109907},
doi = {10.1145/3109859.3109907},
abstract = {In this paper we describe a deployed recommender system to predict insurance products for new and existing customers. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they were adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our system. Experimental results show advantages of using probabilistic graphical models over the widely used low-rank matrix factorization model for the insurance domain.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {274–278},
numpages = {5},
keywords = {bayesian networks, deployed system, insurance domain, recommender systems, structure learning},
location = {Como, Italy},
series = {RecSys '17}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3673226,
author = {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle, Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
title = {Context, Composition, Automation, and Communication: The C2AC Roadmap for Modeling and Simulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3673226},
doi = {10.1145/3673226},
abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {23},
numpages = {51},
keywords = {Modeling, simulation, state of the art, open challenges, reuse, composition, communication, reproducibility, automation, intelligent modeling and simulation lifecycle}
}

@article{10.1145/3712004,
author = {Casadei, Roberto and Aguzzi, Gianluca and Audrito, Giorgio and Damiani, Ferruccio and Pianini, Danilo and Scarso, Giordano and Torta, Gianluca and Viroli, Mirko},
title = {Software Engineering for Collective Cyber-Physical Ecosystems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712004},
doi = {10.1145/3712004},
abstract = {Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {cyber-physical ecosystems, collective adaptive systems, swarm intelligence, macro-programming, edge-cloud continuum, multi-agent systems, distributed artificial intelligence}
}

@article{10.1145/3685936,
author = {Wardzi\'{n}ski, Andrzej and Jarzundefinedbowicz, Aleksander},
title = {Automated Generation of Modular Assurance Cases with the System Assurance Reference Model},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3685936},
doi = {10.1145/3685936},
abstract = {Assurance cases are structured arguments used to demonstrate specific system properties such as safety or security. They are used in many industrial sectors including automotive, aviation and medical devices. Assurance cases are usually divided into modules which address goals allocated to specific system properties, components, functions, modes of operation or environmental conditions. Depending on the system and assurance process characteristics, assurance case modules may follow shared argument templates. The templates refer to the system, process or environment attributes, described collectively as an assurance case context and stored in external context models. Our goal is to manage all contextual relations at the level of assurance case templates and instantiated arguments with the use of a generic System Assurance Reference Model (SARM). We describe its structure and demonstrate how it can be used to automatically generate assurance case modules, based on templates and context models. The article also presents a prototype tool, SARMER, which implements the SARM model and enables automatic data flow between models and assurance cases. The use of SARM and the SARMER tool is illustrated with an example of a component-based system and a modular assurance case to demonstrate that allocated contracts are satisfied for each component.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {23},
numpages = {29},
keywords = {Assurance case, safety case, argument templates, context models, contract-based design}
}

@inproceedings{10.1145/3579028.3609019,
author = {Bazin, Alexandre and Huchard, Marianne and Martin, Pierre},
title = {Towards Analyzing Variability in Space and Time of Products from a Product Line using Triadic Concept Analysis},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609019},
doi = {10.1145/3579028.3609019},
abstract = {In this paper, we report an ongoing work on exploring the ability of Triadic Concept Analysis to provide a framework for analyzing products evolution in time and space, and highlight possible usages in the lifecycle of a product line.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {85–89},
numpages = {5},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3603166.3632143,
author = {St\"{o}tzner, Miles and Klinaku, Floriment and Pesl, Robin Dominic and Becker, Steffen},
title = {Enhancing Deployment Variability Management by Pruning Elements in Deployment Models},
year = {2024},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603166.3632143},
doi = {10.1145/3603166.3632143},
abstract = {Since applications often need to be deployed in different variants, deployment technologies, such as Ansible and Terraform, support modeling variability. Unfortunately, applications typically need the combination of multiple deployment technologies, which have proprietary and non-interoperable variability concepts. Therefore, variable deployment models have been introduced to model deployment variability across different technologies by assigning variability conditions to elements to specify their presence. However, the manual modeling of these conditions is repetitive, error-prone, and time-consuming. In this paper, we propose to reduce the modeling effort by a pruning concept, i.e., the automated removal of elements due to consistency issues and semantic aspects. To validate the practical feasibility, we implemented a prototype based on Open-TOSCA Vintner. Moreover, we present a case study that shows that the number of conditions to be modeled is significantly decreased.},
booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
articleno = {18},
numpages = {11},
keywords = {pruning, deployment models, variability management, TOSCA},
location = {Taormina (Messina), Italy},
series = {UCC '23}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3643660.3643949,
author = {Garlan, David and Schmerl, Bradley and Wohlrab, Rebekka and C\'{a}mara, Javier},
title = {Challenges in Creating Effective Automated Design Environments: An experience report from the domain of generative manufacturing},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643949},
doi = {10.1145/3643660.3643949},
abstract = {The emergence of powerful automated design tools in many domains is changing the nature of design, as human-intensive activities can be increasingly off-loaded to those tools. Rather than having a human consider only handful of options, as has been done historically, such tools now enable the generation of a large space of potential designs, exhibiting different tradeoffs among competing qualities of merit, and supporting systematic exploration of the design space. At the same time, this paradigm raises new challenges centered on enabling humans to effectively navigate that generated space in order to select a design that best meets their requirements. In this paper we describe our experience in the domain of generative manufacturing, in which we developed a novel design environment for airplane parts manufacturing that incorporates a number of sophisticated design tools and attempts to tackle the emergent problems of design space exploration that are faced by designers of those parts. We use this experience to highlight the challenges that we faced and reflect on their applicability more generally to tool-assisted software design environments.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {15–20},
numpages = {6},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/2723742.2723759,
author = {Shobe, Joseph F. and Karim, Md Yasser and Kagdi, Huzefa},
title = {How Often does a Source Code Unit Change within a Release Window?},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723759},
doi = {10.1145/2723742.2723759},
abstract = {To form a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed. The traceability between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. Using this traceability, an empirical study is reported on the frequency distribution of file changes for different release windows. In Chrome, the majority (50%) of the committed files change only once between a pair of consecutive releases. This trend is reversed after expanding the window size to at least 10. That is, the majority (50%) of the files change multiple times when commits constituting 10 or greater releases are considered. These results suggest that a training set of at least 10 releases is needed to provide a prediction coverage for majority of the files.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {166–175},
numpages = {10},
keywords = {Commit History, Empirical Studies, Mining Software Repositories, Software Releases},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/3686081.3686137,
author = {Kong, Xiao},
title = {Research on the Construction and Application of Knowledge Graph in Manufacturing Enterprises},
year = {2024},
isbn = {9798400718151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686081.3686137},
doi = {10.1145/3686081.3686137},
abstract = {This study investigates the construction and application of knowledge graphs within manufacturing enterprises, highlighting their benefits for enhancing production processes, equipment maintenance, fault diagnosis, and quality control. The discussion begins with a foundational review of knowledge graph theories and key technologies, followed by detailed case studies that illustrate the process from data collection and entity relationship extraction to knowledge integration and storage. These KPIs could then be contrasted before and after the implementation of a knowledge graph to be able to show how knowledge graphs have substantial positive impacts on improving production efficiency, reducing cost, and increasing product quality. The paper also focuses on the knowledge graphs that help companies gain improved insights, thus do much better in response to market demand and technological advancement.},
booktitle = {Proceedings of the International Conference on Decision Science &amp; Management},
pages = {332–338},
numpages = {7},
keywords = {Data integration, Knowledge graph, Manufacturing enterprise, Production efficiency},
location = {
},
series = {ICDSM '24}
}

@inproceedings{10.5555/3712729.3712879,
author = {Surman, Richard and Nehl, Matt and Evanson, Cole and Low, Soo Leen and Chan, Kern Chern and Liu, Hui Sian and Gan, Boon Ping},
title = {Capacity Planning Accuracy and the Effect of Dyanmic Dedication Changes for a Single Wafer Lot Semiconductor Factory},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In the context of a single wafer lot semiconductor factory characterized by high levels of Research and Development (RD) work-in-progress (WIP), low levels of product-based lots and lengthy cycle times, this paper investigates the accuracy of capacity planning given the impact of dynamic changes in dedication. We delve into several critical aspects related to dedication planning, drawing insights from historical data and dispatch logic used. The experimental results show the improvement in model accuracy with the incorporation of dedication changes as distribution functions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1809–1817},
numpages = {9},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.5555/2818754.2818821,
author = {Jia, Yue and Cohen, Myra B. and Harman, Mark and Petke, Justyna},
title = {Learning combinatorial interaction test generation strategies using hyperheuristic search},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our algorithm's strong generic performance results from its unsupervised learning. Hyperheuristic search is thus a promising way to relocate CIT design intelligence from human to machine.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {540–550},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@proceedings{10.1145/3689930,
title = {RICSS '24: Proceedings of the 2024 Workshop on Re-design Industrial Control Systems with Security},
year = {2024},
isbn = {9798400712265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Re-design Industrial Control Systems with Security (RICSS).As ICS software and systems are foundational to the critical infrastructure relied upon by industry, academia, and government, addressing their security challenges is more crucial than ever. Historically, these systems were not designed with security in mind, and the rapid expansion of interconnectivity has exposed significant vulnerabilities, leaving many practitioners reliant on a patchwork of security measures. While some proprietary ICS software providers have begun incorporating security features, the security of free and open-source ICS software remains underdeveloped and underappreciated. This workshop aims to change that.},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3634713.3634717,
author = {May, Richard and Denecke, Kerstin},
title = {Conversational Agents in Healthcare: A Variability Perspective},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634717},
doi = {10.1145/3634713.3634717},
abstract = {Conversational agents in healthcare are gaining popularity, for example, in the context of eliciting medical histories. Furthermore, due to the growing diversity of use cases and stakeholders, they are becoming increasingly configurable and are often based on variability mechanisms. In this paper, we present a high-level perspective on typical variability aspects and describe common challenges based on our research and practical experience in developing and evaluating conversational agents in the healthcare domain. We introduce variability aspects that are classified into technology-related (e.g., intelligence framework, input/output mode) and user-related aspects (e.g., careflow integration, health literacy). Moreover, these aspects are described in a case study on the Digital Medical Interview Assistant (DMIA) for radiology. We highlight main challenges that arise in the context of evolution, verification, input processing, privacy and security compliance, as well as ethical considerations. Our findings are intended to help developers, researchers, and healthcare professionals understand the importance and impact of configurability and to spur further discussions on variability aspects of conversational agents.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {123–128},
numpages = {6},
keywords = {chatbot, configuration, conversational agent, healthcare, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/568760.568824,
author = {Denaro, Giovanni and Morasca, Sandro and Pezz\`{e}, Mauro},
title = {Deriving models of software fault-proneness},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568824},
doi = {10.1145/568760.568824},
abstract = {The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality.By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of homogeneous software products. It also proposes the use of cross-validation for selecting valid models even for small data sets.The early results show that it is possible to build statistical models based on historical data for estimating fault-proneness of software modules before testing, and thus better planning and monitoring the testing activities.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {361–368},
numpages = {8},
keywords = {cross-validation, fault-proneness models, logistic regression, software faultiness, software metrics, software testing process},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1145/1868328.1868350,
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
title = {On the value of learning from defect dense components for software defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868350},
doi = {10.1145/1868328.1868350},
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment.RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4--5 times more often than any other method, and also lost the least amount of times.CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {14},
numpages = {9},
keywords = {ceiling effect, defect dense components, defect prediction, sampling},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1145/3702231,
author = {ter Beek, Maurice and Broy, Manfred and Dongol, Brijesh},
title = {The Role of Formal Methods in Computer Science Education},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3702231},
doi = {10.1145/3702231},
journal = {ACM Inroads},
month = nov,
pages = {58–66},
numpages = {9}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3430984.3431017,
author = {Azad, Amar Prakash and Garg, Dinesh and Agrawal, Priyanka and Kumar, Arun},
title = {Deep Domain Adaptation under Label Scarcity},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431017},
doi = {10.1145/3430984.3431017},
abstract = {The goal behind Domain Adaptation (DA) is to leverage the labeled examples from a source domain to infer an accurate model for a target domain where labels are not available or in scarce at the best. Recently, there has been a surge in adversarial learning based deep-net approaches for DA problem – a prominent example being DANN approach&nbsp;[9]. These methods require a large number of source labeled examples to infer a good model for the target domain; but start performing poorly with reduced labels. In this paper, we study the behavior of such approaches (especially DANN) under such scarce label scenarios. Further, we propose an architecture, namely TRAVERS, that amalgamates TRAnsductive learning principles with adVERSarial learning so as to provide a cushion to the performance of these approaches under label scarcity. Experimental results (both on text and images) show a significant boost in the performance of TRAVERS over approaches such as DANN under scarce label scenarios.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {101–109},
numpages = {9},
keywords = {adversarial learning, cross domain representation, domain adaptation},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@proceedings{10.1145/3643665,
title = {FinanSE '24: Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software development has an integral role in every financial organisation; indeed, almost every service provided by a bank utilizes some form of software solution. While SE research has led to solutions and innovations for many popular SE problems, there remain unresolved challenges, particularly, those challenges faced in software development in financial firms. An example of such a challenge is defect prediction, where defects are not equal as some may lead to larger reputational and financial damage than others. Consequently, testing and verification is burdened with a further set of restraints for finance-based SE teams. Financial firms began automating processes as early as the 1960s, and as such, must maintain large legacy systems which may host critical operations. This problem is further exacerbated by the numerous mergers and acquisitions common in the financial sector, which leaves firms with a set of heterogeneous legacy systems that need to communicate with one another effectively and efficiently. Therefore, maintaining these systems while modernizing them leads to intriguing challenges, spanning from model extraction and process optimisation to code translation. Moreover, highly regulated institutions like financial firms require a high degree of transparency and accountability. This requirement facilitates the need for model fairness and explainability for any SE solution, in particular those that rely on AI.The 1st International Workshop on Software Engineering Challenges in Financial Firms (FinanSE 2024) is a forum to bring together academia and industry to share new ideas and results in tackling these challenges.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3477314.3507004,
author = {Ribeiro, Quelita A. D. S. and Ribeiro, Moniky and Castro, Jaelson},
title = {Requirements engineering for autonomous vehicles: a systematic literature review},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507004},
doi = {10.1145/3477314.3507004},
abstract = {Context: Autonomous Vehicles (AVs) will transform the way we live and work. Several benefits are envisaged, including: reduction in traffic deaths, drop in harmful emissions, improvement in fuel economy, reduction in travel time, and consumer savings. Indeed, the trend in the automobile industry is the development of AVs in preparation for the introduction and mass implementation of driverless vehicles. However, given the complexity and increasing connectivity of the AV, the challenges for effective and efficient development are immense. Many problems are related to misconceptions in the requirements engineering phase for AVs. Hence, a Requirements Engineering (RE) process is crucial in the development of AVs. Objective: The purpose of this work is to identify and analyse the current RE approaches used for AVs development. The analysis is based on answers related to the type of RE problems addressed by the study, the RE phases covered by the approach, requirements modelling styles used, the type of requirements described in the study, the specific AVs considered in the study, and the open problems reported. Method: We conducted a Systematic Literature Review (SLR) as the basis for our work. Results: Our SLR draws on 31 studies in which we identified the RE problems addressed by the studies and the phases of the RE processes considered. We also uncovered the languages and description styles used to describe the requirementes of the AVs. Special attention was paid to the non-functional requirements of interest. Different types of AVs were identified. Last but not least, several challenges were revealed. Conclusions: This paper reports the current state of the art of RE for AVs and identifies some open issues that deserve further investigation.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1299–1308},
numpages = {10},
keywords = {autonomous vehicles, requirements engineering, systematic literature review},
location = {Virtual Event},
series = {SAC '22}
}

@article{10.1145/3617946.3617955,
author = {Bucchiarone, Antonio and Cooper, Kendra M. L. and Lin, Dayi and Smith, Adam and Wanick, Vanissa},
title = {Fostering Collaboration and Advancing Research in Software Engineering and Game Development for Serious Contexts},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617955},
doi = {10.1145/3617946.3617955},
abstract = {The potential benefits of using the engaging and interactive nature of games to achieve specific objectives have been recognized by researchers and professionals from numerous domains. Serious games have been developed to impart knowledge, skills, and awareness in areas such as education, healthcare and the environment, while gamification has been applied to enhance the engagement, motivation, and participation of users in non-game activities such as sustainability and learning. As a result, the fields of game engineering, software engineering, and user experience are increasingly converging to create innovative solutions that blend the strengths of games with real-world applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {46–50},
numpages = {5}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@article{10.1145/3559736.3559743,
author = {Purser, David},
title = {SIGLOG monthly 227},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3559736.3559743},
doi = {10.1145/3559736.3559743},
abstract = {This event will be conducted in hybrid mode: in person in Paris (organizers' preferred choice) and virtually. Registration is mandatory for both modes (please find the corresponding links here: https://learnaut22.github.io/registration.html)It is our pleasure to inform you about LearnAut 2022, the fourth edition of the workshop, this time co-located with ICALP.},
journal = {ACM SIGLOG News},
month = aug,
pages = {63},
numpages = {1}
}

@proceedings{10.1145/3603287,
title = {ACMSE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@inproceedings{10.1145/3524459.3527353,
author = {Harman, Mark},
title = {Scaling genetic improvement and automated program repair},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527353},
doi = {10.1145/3524459.3527353},
abstract = {This paper outlines techniques and research directions for scaling genetic improvement and automated program repair, highlighting possible directions for future work and open challenges.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {1–7},
numpages = {7},
keywords = {automated program repair, genetic improvement, search based software engineering (SBSE)},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3579371.3589349,
author = {Loh, Gabriel H. and Schulte, Michael J. and Ignatowski, Mike and Adhinarayanan, Vignesh and Aga, Shaizeen and Aguren, Derrick and Agrawal, Varun and Aji, Ashwin M. and Alsop, Johnathan and Bauman, Paul and Beckmann, Bradford M. and Beigi, Majed Valad and Blagodurov, Sergey and Boraten, Travis and Boyer, Michael and Brantley, William C. and Chalmers, Noel and Chen, Shaoming and Cheng, Kevin and Chu, Michael L. and Cownie, David and Curtis, Nicholas and Del Pino, Joris and Duong, Nam and Duundefinedu, Alexandru and Eckert, Yasuko and Erb, Christopher and Freitag, Chip and Greathouse, Joseph L. and Gurumurthi, Sudhanva and Gutierrez, Anthony and Hamidouche, Khaled and Hossamani, Sachin and Huang, Wei and Islam, Mahzabeen and Jayasena, Nuwan and Kalamatianos, John and Kayiran, Onur and Kotra, Jagadish and Lee, Alan and Lowell, Daniel and Madan, Niti and Majumdar, Abhinandan and Malaya, Nicholas and Manne, Srilatha and Mashimo, Susumu and McDougall, Damon and Mednick, Elliot and Mishkin, Michael and Nutter, Mark and Paul, Indrani and Poremba, Matthew and Potter, Brandon and Punniyamurthy, Kishore and Puthoor, Sooraj and Raasch, Steven E. and Rao, Karthik and Rodgers, Gregory and Scrbak, Marko and Seyedzadeh, Mohammad and Slice, John and Sridharan, Vilas and van Oostrum, Ren\'{e} and van Tassell, Eric and Vishnu, Abhinav and Wasmundt, Samuel and Wilkening, Mark and Wolfe, Noah and Wyse, Mark and Yalavarti, Adithya and Yudanov, Dmitri},
title = {A Research Retrospective on AMD's Exascale Computing Journey},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589349},
doi = {10.1145/3579371.3589349},
abstract = {The pace of advancement of the top-end supercomputers historically followed an exponential curve similar to (and driven in part by) Moore's Law. Shortly after hitting the petaflop mark, the community started looking ahead to the next milestone: Exascale. However, many obstacles were already looming on the horizon, such as the slowing of Moore's Law, and others like the end of Dennard Scaling had already arrived. Anticipating significant challenges for the overall high-performance computing (HPC) community to achieve the next 1000x improvement, the U.S. Department of Energy (DOE) launched the Exascale Computing Program to enable and accelerate fundamental research across the many technologies needed to achieve exascale computing.AMD had the opportunity to contribute to the so-called "*Forward" programs from the DOE, which were a series of public-private partnerships focused on research and co-design activities covering compute architectures, interconnects, memory systems, chiplets and packaging, software stacks, applications, and more. Some of the research from these programs can now be found in the world's first exascale supercomputer, some were a little ahead of their time and may have an impact in the coming years, and others simply did not pan out. In this paper, we provide a retrospective of AMD's nearly decade-long research journey covering how we tried to predict the architecture of a supercomputer a decade into the future, what we got right, what we got wrong, and some of the insights and learnings that we discovered along the way.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {81},
numpages = {14},
keywords = {exascale, HPC, high-performance computing, supercomputing, frontier, memory, chiplets, heterogeneous compute, accelerated processing unit, research},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3512290.3528697,
author = {Arrieta, Aitor},
title = {Multi-objective metamorphic follow-up test case selection for deep learning systems},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528697},
doi = {10.1145/3512290.3528697},
abstract = {Deep Learning (DL) components are increasing their presence in safety and mission-critical software systems. To ensure a high dependability of DL systems, robust verification methods are required, for which automation is highly beneficial (e.g., more test cases can be executed). Metamorphic Testing (MT) is a technique that has shown to alleviate the test oracle problem when testing DL systems, and therefore, increasing test automation. However, a drawback of this technique lies into the need of multiple test executions to obtain the test verdict (named as the source and the follow-up test cases), requiring additional testing cost. In this paper we propose an approach based on multi-objective search to select follow-up test cases. Our approach makes use of source test cases to measure the uncertainty provoked by such test inputs in the DL model, and based on that, select failure-revealing follow-up test cases. We integrate our approach with the NSGA-II algorithm. An empirical evaluation on three DL models tackling the image classification problem, along with five different metamorphic relations demonstrates that our approach outperformed the baseline algorithm between 17.09 to 59.20% on average when considering the revisited Hypervolume quality indicator.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1327–1335},
numpages = {9},
keywords = {deep learning systems, metamorphic testing, multi-objective search, test case selection},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/2499393.2499396,
author = {Minku, Leandro L. and Yao, Xin},
title = {An analysis of multi-objective evolutionary algorithms for training ensemble models based on different performance measures in software effort estimation},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499396},
doi = {10.1145/2499393.2499396},
abstract = {Background: Previous work showed that Multi-objective Evolutionary Algorithms (MOEAs) can be used for training ensembles of learning machines for Software Effort Estimation (SEE) by optimising different performance measures concurrently. Optimisation based on three measures (LSD, MMRE and PRED(25)) was analysed and led to promising results in terms of performance on these and other measures.Aims: (a) It is not known how well ensembles trained on other measures would behave for SEE, and whether training on certain measures would improve performance particularly on these measures. (b) It is also not known whether it is best to include all SEE models created by the MOEA into the ensemble, or solely the models with the best training performance in terms of each measure being optimised. Investigating (a) and (b) is the aim of this work.Method: MOEAs were used to train ensembles by optimising four different sets of performance measures, involving a total of nine different measures. The performance of all ensembles was then compared based on all these nine performance measures. Ensembles composed of different sets of models generated by the MOEAs were also compared.Results: (a) Ensembles trained on LSD, MMRE and PRED (25) obtained the best results in terms of most performance measures, being considered more successful than the others. Optimising certain performance measures did not necessarily lead to the best test performance on these particular measures probably due to overfitting. (b) There was no inherent advantage in using ensembles composed of all the SEE models generated by the MOEA in comparison to using solely the best SEE model according to each measure separately.Conclusions: Care must be taken to prevent overfitting on the performance measures being optimised. Our results suggest that concurrently optimising LSD, MMRE and PRED (25) promoted more ensemble diversity than other combinations of measures, and hence performed best. Low diversity is more likely to lead to overfitting.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {8},
numpages = {10},
keywords = {ensembles of learning machines, multi-objective evolutionary algorithms, software effort estimation},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/3442391.3442398,
author = {Bressan, Lucas and de Oliveira, Andr\'{e} Luiz and Campos, Fernanda and Capilla, Rafael},
title = {A variability modeling and transformation approach for safety-critical systems},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442398},
doi = {10.1145/3442391.3442398},
abstract = {Safety-critical autonomous systems are becoming highly variant-intensive with thousands of variations points within a single product. Modeling these systems requires the specification of safety properties, but the diversity of these properties makes hard to configure these systems manually to prevent emerging hazards and fault behaviors. Because existing software variability techniques provide rudimentary mechanisms for mapping variability constructs to functional safety models, we describe in this paper an experience report showing how a novel annotative modeling approach and tool can be used to derive system models enriched with functional safety information. We validate our approach using a case study from the automotive domain and we estimate the effort reduction in the tasks comparing our approach with two similar tools.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {7},
keywords = {Functional safety, Reuse, Safety critical systems, Variability modeling},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3575879.3575976,
author = {Eleftherakis, George and Baxhaku, Fesal and Vasilescu, Anca},
title = {Bio-inspired Adaptive Architecture for Wireless Sensor Networks},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575976},
doi = {10.1145/3575879.3575976},
abstract = {Wireless Sensor Networks (WSN) are expected to revolutionize daily life by connecting everyday objects with sensing capabilities, offering numerous opportunities for a wide range of applications. To facilitate the integration and enable all these opportunities to be realized, it becomes a necessity for middleware architectures that: (a) perform well in non-well-defined infrastructures, (b) are able to deal with the large number of users and heterogeneous devices integrated into it (ultra scalable), and (c) enable autonomy of the system overall. This work introduces a bio-inspired middleware optimized for wireless sensor networks proposing a refinement, the regional network, in a work published earlier as a bio-inspired self-adaptive architecture for the Internet of Things, while providing a comparison of other similar middleware approaches and a discussion on the motivating health monitoring scenario.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {116–122},
numpages = {7},
keywords = {Bio-inspired Adaptive Systems, IoT, Middleware, Wireless Sensor Networks, eHealth},
location = {Athens, Greece},
series = {PCI '22}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {Taormina (Messina), Italy}
}

@inproceedings{10.1145/3377024.3377028,
author = {Schlingloff, Holger and Kruse, Peter M. and Saadatmand, Mehrdad},
title = {Excellence in variant testing},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377028},
doi = {10.1145/3377024.3377028},
abstract = {In this short paper, we report on the motivation, background and ambition of the ITEA3 project XIVT - e&lt;u&gt;x&lt;/u&gt;cellence &lt;u&gt;i&lt;/u&gt;n &lt;u&gt;v&lt;/u&gt;ariant &lt;u&gt;t&lt;/u&gt;esting. We describe a work flow and tool chain for testing of configurable and highly-variant embedded systems in various domains.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {12},
numpages = {2},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3644033.3644371,
author = {Zeyen, Olivier and Cordy, Maxime and Perrouin, Gilles and Acher, Mathieu},
title = {Preprocessing is What You Need: Understanding and Predicting the Complexity of SAT-based Uniform Random Sampling},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644371},
doi = {10.1145/3644033.3644371},
abstract = {Despite its NP-completeness, the Boolean satisfiability problem gave birth to highly efficient tools that are able to find solutions to a Boolean formula and compute their number. Boolean formulae compactly encode huge, constrained search spaces for variability-intensive systems, e.g., the possible configurations of the Linux kernel. These search spaces are generally too big to explore exhaustively, leading most testing approaches to sample a few solutions before analysing them. A desirable property of such samples is uniformity: each solution should get the same selection probability. This property motivated the design of uniform random samplers, relying on SAT solvers and counters and achieving different tradeoffs between uniformity and scalability. Though we can observe their performance in practice, understanding the complexity these tools face and accurately predicting it is an under-explored problem. Indeed, structural metrics such as the number of variables and clauses involved in a formula poorly predict the sampling complexity. More elaborated ones, such as minimal independent support (MIS), are intractable to compute on large formulae. We provide an efficient parallel algorithm to compute a related metric, the number of equivalence classes, and demonstrate that this metric is highly correlated to time and memory usage of uniform random sampling and model counting tools. We explore the role of formula preprocessing on various metrics and show its positive influence on correlations. Relying on these correlations, we train an efficient classifier (F1-score 0.97) to predict whether uniformly sampling a given formula will exceed a specified budget. Our results allow us to characterise the similarities and differences between (uniform) sampling, solving and counting.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {23–32},
numpages = {10},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1145/3563835.3568737,
author = {Gazzillo, Paul and Cohen, Myra B.},
title = {Bringing Together Configuration Research: Towards a Common Ground},
year = {2022},
isbn = {9781450399098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563835.3568737},
doi = {10.1145/3563835.3568737},
abstract = {Configurable software makes up most of the software in use today. Configurability, i.e., the ability of software to be customized without additional programming, is pervasive, and due to the criticality of problems caused by misconfiguration, it has been an active topic researched by investigators in multiple, diverse areas. This broad reach of configurability means that much of the literature and latest results are dispersed, and researchers may not be collaborating or be aware of similar problems and solutions in other domains. We argue that this lack of a common ground leads to a missed opportunity for synergy between research domains and the synthesis of efforts to tackle configurability problems. In short, configurability cuts across software as a whole and needs to be treated as a first class programming element. To provide a foundation for addressing these concerns we make suggestions on how to bring the communities together and propose a common model of configurability and a platform, ACCORD, to facilitate collaboration among researchers and practitioners.},
booktitle = {Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {259–269},
numpages = {11},
keywords = {community building, configurability},
location = {Auckland, New Zealand},
series = {Onward! 2022}
}

@article{10.1145/3686903,
author = {Valentine, Melissa A. and Pratt, Amanda L. and Hinds, Rebecca and Bernstein, Michael S.},
title = {The Algorithm and the Org Chart: How Algorithms Can Conflict with Organizational Structures},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686903},
doi = {10.1145/3686903},
abstract = {Algorithms are introducing changes to individuals? jobs, but do algorithms also lead to changes in the structures of organizations themselves? Organizational structures, as often formalized into organization (org) charts, are meant to facilitate coordinated decision-making. Yet our 10-month ethnographic study of a large online retail company reveals why the organizational structures that facilitate effective decision-making by humans may be in tension with the organizational structures that facilitate effective decision-making using algorithms. Our findings show that the human decision-makers needed small, divided-up sets of decisions, and they had previously accomplished this through how they structured individuals' roles and teams in the org chart. In contrast, when data scientists developed a new algorithm and first deployed it within organizational structures meant to support human decision-making, they realized that these small divided-up decision spaces were arbitrarily constraining the algorithm's search space. When not constrained in this manner, the algorithm could identify and recommend better solutions, but those optimal solutions did not always align with the structure of roles and teams in the org chart. This study suggests that as algorithms are integrated into the workplace, organization designs may begin to more explicitly reflect the contours of those algorithms' behaviors.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {364},
numpages = {31},
keywords = {algorithms, automation, ethnography, hierarchy, organizational structure, planning}
}

@proceedings{10.1145/3607947,
title = {IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Noida, India}
}

@book{10.1145/3664191,
author = {Kumar, Amruth N. and Raj, Rajendra K. and Aly, Sherif G. and Anderson, Monica D. and Becker, Brett A. and Blumenthal, Richard L. and Eaton, Eric and Epstein, Susan L. and Goldweber, Michael and Jalote, Pankaj and Lea, Douglas and Oudshoorn, Michael and Pias, Marcelo and Reiser, Susan and Servin, Christian and Simha, Rahul and Winters, Titus and Xiang, Qiao},
title = {Computer Science Curricula 2023},
year = {2024},
isbn = {9798400710339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/3452383.3452387,
author = {Mishra, Siba and Sharma, Arpit},
title = {A Generalized Semantic Filter for Glossary Term Extraction from Large-Sized Software Requirements},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452387},
doi = {10.1145/3452383.3452387},
abstract = {A glossary is an essential component of every software requirements document. Extracting glossary terms manually from a large requirements document is expensive in terms of both time and cost required to do so. Additionally, this is also an error-prone task. To overcome these issues, we propose a generalized semantic filter which can automatically extract key technical terms present in a large body of software requirements. Our semantic filter is based on a word embeddings model which can identify domain-specific terms. To achieve this goal, a domain-neutral reference corpus is created containing data of news headlines published over a period of 17 years by Australian Broadcasting Corp news website. We use this domain-neutral corpus to calculate the similarity scores of potential glossary terms extracted using text chunking and coverage filtering on the requirements document. The key idea is that if the context of a candidate term in the requirements document is different from the context in which it was used in the domain-neutral corpus, then the term is labeled as domain-specific. Since our semantic filter is domain-neutral, it can potentially be applied to requirements documents of any application domain. Our proposed technique has been applied to the CrowdRE document which is a large-sized document with roughly 3000 user stories for smart home application domain. Results show that our approach is very effective for glossary extraction from enormous documents containing software requirements.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {4},
numpages = {9},
keywords = {CrowdRE., Domain, FastText, Glossary, Requirements},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@inproceedings{10.1109/ICSE48619.2023.00080,
author = {Rong, Guoping and Gu, Shenghui and Shen, Haifeng and Zhang, He and Kuang, Hongyu},
title = {How Do Developers' Profiles and Experiences Influence their Logging Practices? An Empirical Study of Industrial Practitioners},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00080},
doi = {10.1109/ICSE48619.2023.00080},
abstract = {Logs record the behavioral data of running programs and are typically generated by executing log statements. Software developers generally carry out logging practices with clear intentions and associated concerns (I&amp;Cs). However, I&amp;Cs may not be properly fulfilled in source code as log placement --- specifically determination of a log statement's context and content--- is often susceptible to an individual's profile and experience. Some industrial studies have been conducted to discern developers' main logging I&amp;Cs and the way I&amp;Cs are fulfilled. However, the findings are only based on the developers from a single company in each individual study and hence have limited generalizability. More importantly, there lacks a comprehensive and deep understanding of the relationships between developers' profiles and experiences and their logging practices from a wider perspective. To fill this significant gap, we conducted an empirical study using mixed methods comprising questionnaire surveys, semi-structured interviews, and code analyses with practitioners from a wide range of companies across a variety of industrial domains. Results reveal that while developers share common logging I&amp;Cs and conduct logging practices mainly in the coding stage, their profiles and experiences profoundly influence their logging I&amp;Cs and the way the I&amp;Cs are fulfilled. These findings pave the way to facilitate the acceptance of important logging I&amp;Cs and the adoption of good logging practices by developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {855–867},
numpages = {13},
keywords = {logging practice, intention, concern, fulfill},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3579851,
author = {Greca, Renan and Miranda, Breno and Bertolino, Antonia},
title = {State of Practical Applicability of Regression Testing Research: A Live Systematic Literature Review},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3579851},
doi = {10.1145/3579851},
abstract = {Context: Software regression testing refers to rerunning test cases after the system under test is modified, ascertaining that the changes have not (re-)introduced failures. Not all researchers’ approaches consider applicability and scalability concerns, and not many have produced an impact in practice. Objective: One goal is to investigate industrial relevance and applicability of proposed approaches. Another is providing a live review, open to continuous updates by the community. Method: A systematic review of regression testing studies that are clearly motivated by or validated against industrial relevance and applicability is conducted. It is complemented by follow-up surveys with authors of the selected papers and 23 practitioners. Results: A set of 79 primary studies published between 2016–2022 is collected and classified according to approaches and metrics. Aspects relative to their relevance and impact are discussed, also based on their authors’ feedback. All the data are made available from the live repository that accompanies the study. Conclusions: While widely motivated by industrial relevance and applicability, not many approaches are evaluated in industrial or large-scale open-source systems, and even fewer approaches have been adopted in practice. Some challenges hindering the implementation of relevant approaches are synthesized, also based on the practitioners’ feedback.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {274},
numpages = {36},
keywords = {Regression Testing, test case selection, test case prioritization, test suite reduction, test suite amplification, systematic literature review}
}

@proceedings{10.1145/3718491,
title = {AIBDF '24: Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
year = {2024},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@proceedings{10.1145/3652037,
title = {PETRA '24: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Crete, Greece}
}

@inproceedings{10.1145/3540250.3558950,
author = {Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu},
title = {Understanding automated code review process and developer experience in industry},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558950},
doi = {10.1145/3540250.3558950},
abstract = {Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1398–1407},
numpages = {10},
keywords = {code review, code review automation, review bot, static analysis},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/3708359,
title = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3477282.3477283,
author = {Kelesakis, Dimitrios and Vavliakis, Konstantinos N. and Symeonidis, Andreas L.},
title = {Personalized Dynamic Pricing with RFM Modeling},
year = {2021},
isbn = {9781450376846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477282.3477283},
doi = {10.1145/3477282.3477283},
abstract = {Dynamic pricing primitives from the airline and hotel industry have lately shifted to the wider electronic retail industry, however there is still a lack of ready to use frameworks for applying or testing dynamic pricing policies in online e-commerce stores. This has practically generated limitations in the way dynamic pricing can be applied in real-life. This paper introduces a new dynamic pricing model that uses an extended version of the RFM model to calculate a personal price for each product sold online. Moreover, our work introduces an open-source simulation framework that allows testing and validation or different dynamic pricing policies. According to our evaluation, the proposed methodology achieved 54.33% increase in net profits when compared with nine other merchants following a fixed pricing policy and 16.13% increase when compared with the derivative-following pricing strategy.},
booktitle = {Proceedings of the 7th International Conference on E-Society, e-Learning and e-Technologies},
pages = {35–41},
numpages = {7},
keywords = {Dynamic Pricing, RFM, e-commerce, neural network},
location = {Portsmouth, United Kingdom},
series = {ICSLT '21}
}

@proceedings{10.1145/3660515,
title = {EICS '24 Companion: Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3567512.3567529,
author = {Fr\"{o}lich, Damian and van Binsbergen, L. Thomas},
title = {iCoLa: A Compositional Meta-language with Support for Incremental Language Development},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567512.3567529},
doi = {10.1145/3567512.3567529},
abstract = {Programming languages providing high-level abstractions can increase programmers’ productivity and program safety. Language-oriented programming is a paradigm in which domain-specific languages are developed to solve problems within specific domains with (high-level) abstractions relevant to those domains. However, language development involves complex design and engineering processes. These processes can be simplified by reusing (parts of) existing languages and by offering language-parametric tooling. In this paper we present iCoLa, a meta-language supporting incremental (meta-)programming based on reusable components. In our implementation of iCoLa, languages are first-class citizens, providing the full power of the host-language (Haskell) to compose and manipulate languages. We demonstrate iCoLa through the construction of the Imp, SIMPLE, and MiniJava languages via the composition and restriction of language fragments and demonstrate the variability of our approach through the construction of several languages using a fixed-set of operators.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {202–215},
numpages = {14},
keywords = {Domain-specific languages, Language Composition, Prototyping, funcons, meta-language},
location = {Auckland, New Zealand},
series = {SLE 2022}
}

@inproceedings{10.1145/3487552.3487813,
author = {Ma, Zane and Austgen, James and Mason, Joshua and Durumeric, Zakir and Bailey, Michael},
title = {Tracing your roots: exploring the TLS trust anchor ecosystem},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487813},
doi = {10.1145/3487552.3487813},
abstract = {Secure TLS server authentication depends on reliable trust anchors. The fault intolerant design of today's system---where a single compromised trust anchor can impersonate nearly all web entities---necessitates the careful assessment of each trust anchor found in a root store. In this work, we present a first look at the root store ecosystem that underlies the accelerating deployment of TLS. Our broad collection of TLS user agents, libraries, and operating systems reveals a surprisingly condensed root store ecosystem, with nearly all user agents ultimately deriving their roots from one of three root programs: Apple, Microsoft, and NSS. This inverted pyramid structure further magnifies the importance of judicious root store management by these foundational root programs.Our analysis of root store management presents evidence of NSS's relative operational agility, transparency, and rigorous inclusion policies. Unsurprisingly, all derivative root stores in our dataset (e.g., Linuxes, Android, NodeJS) draw their roots from NSS. Despite this solid footing, derivative root stores display lax update routines and often customize their root stores in questionable ways. By scrutinizing these practices, we highlight two fundamental obstacles to existing NSS-derived root stores: rigid on-or-off trust and multi-purpose root stores. Taken together, our study highlights the concentration of root store trust in TLS server authentication, exposes questionable root management practices, and proposes improvements for future TLS root stores.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {179–194},
numpages = {16},
location = {Virtual Event},
series = {IMC '21}
}

@proceedings{10.1145/3615834,
title = {iWOAR '23: Proceedings of the 8th international Workshop on Sensor-Based Activity Recognition and Artificial Intelligence},
year = {2023},
isbn = {9798400708169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {L\"{u}beck, Germany}
}

@proceedings{10.1145/3624007,
title = {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’23). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming languages research communities.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3674805.3686690,
author = {Zamorano, Mar and Domingo, \'{A}frica and Cetina, Carlos and Sarro, Federica},
title = {Game Software Engineering: A Controlled Experiment Comparing Automated Content Generation Techniques},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686690},
doi = {10.1145/3674805.3686690},
abstract = {Background Video games are complex projects that involve a seamless integration of art and software during the development process to compose the final product. In the creation of a video game, software is fundamental as it governs the behavior and attributes that shape the player’s experience within the game. When assessing the quality of a video game, one needs to consider specific quality aspects, namely ‘design’, ‘difficulty’, ‘fun’, and ‘immersiveness’, which are not considered for traditional software. On the other hand, there are not well-established best practices for the empirical assessment of video games as there are for the empirical evaluation of more traditional software. Aims Our goal is to carry out a rigorous empirical evaluation of the latest proposals to automatically generate content for video games following best practices established in software engineering research. Specifically, we compare Procedural Content Generation (PCG) and Reuse-based Content Generation (RCG). Our study also considers the perception of players and professional developers on the generated content. Method We conducted a controlled experiment where human subjects had to play with content that was automatically generated for a commercial video game by the two techniques (PCG and RCG), and evaluate it according to specific quality aspects of video games. A total of 44 subjects including professional developers and players participated in our experiment. Results The results suggest that participants perceive that RCG generates content is of higher quality than PCG. Conclusions The results can turn the tide for content generation. So far, RCG has been neglected as a viable option: typically, reuse is frowned upon by the developers, who aim to avoid repetition in their video games as much as possible. However, our study uncovered that RCG unlocks latent content that is actually favoured by players and developers alike. This revelation poses an opportunity towards opening new horizons for content generation research.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {302–313},
numpages = {12},
keywords = {Empirical Study, Game Software Engineering, Video Game},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@inproceedings{10.1145/3490099.3511119,
author = {Sun, Jiao and Liao, Q. Vera and Muller, Michael and Agarwal, Mayank and Houde, Stephanie and Talamadupula, Kartik and Weisz, Justin D.},
title = {Investigating Explainability of Generative AI for Code through Scenario-based Design},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511119},
doi = {10.1145/3490099.3511119},
abstract = {What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {212–228},
numpages = {17},
keywords = {explainable AI, generative AI, human-centered AI, scenario based design, software engineering tooling},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3542945,
author = {Araujo, Hugo and Mousavi, Mohammad Reza and Varshosaz, Mahsa},
title = {Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542945},
doi = {10.1145/3542945},
abstract = {We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {51},
numpages = {61},
keywords = {Verification and validation, robotics, autonomous systems, testing, literature survey}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3708036,
title = {ICCSMT '24: Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
year = {2024},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3589335.3648338,
author = {Chen, Yaoqi and Zheng, Ruicheng and Chen, Qi and Xu, Shuotao and Zhang, Qianxi and Wu, Xue and Han, Weihao and Yuan, Hua and Li, Mingqin and Wang, Yujing and Li, Jason and Yang, Fan and Sun, Hao and Deng, Weiwei and Sun, Feng and Zhang, Qi and Yang, Mao},
title = {OneSparse: A Unified System for Multi-index Vector Search},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3648338},
doi = {10.1145/3589335.3648338},
abstract = {Multi-index vector search has become the cornerstone for many applications, such as recommendation systems. Efficient search in such a multi-modal hybrid vector space is challenging since no single index design performs well for all kinds of vector data. Existing approaches to processing multi-index hybrid queries either suffer from algorithmic limitations or processing inefficiency. In this paper, we propose OneSparse, a unified multi-vector index query system that incorporates multiple posting-based vector indices, which enables highly efficient retrieval of multi-modal data-sets. OneSparse introduces a novel multi-index query engine design of inter-index intersection push-down. It also optimizes the vector posting format to expedite multi-index queries. Our experiments show OneSparse achieves more than 6x search performance improvement while maintaining comparable accuracy. OneSparse has already been integrated into Microsoft online web search and advertising systems with 5x+ latency gain for Bing web search and 2.0% Revenue Per Mille (RPM) gain for Bing sponsored search.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {393–402},
numpages = {10},
keywords = {approximate nearest neighbor search, multi-index search, retrieval system, sparse and dense search},
location = {Singapore, Singapore},
series = {WWW '24}
}

@proceedings{10.1145/3705677,
title = {CITCE '24: Proceedings of the 4th International Conference on Computer, Internet of Things and Control Engineering},
year = {2024},
isbn = {9798400711848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3470482.3479628,
author = {Silvestre, Vit\'{o}ria R. N. and Gomes, Francisco A. A. and C\^{a}ndido, Adriano L. and Fernandes, Filipe and Rocha, Lincoln S. and Trinta, Fernando A. M.},
title = {DOP-MS: A Microservice-based Data Offloading Service with Support for Data Anonymisation},
year = {2021},
isbn = {9781450386098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470482.3479628},
doi = {10.1145/3470482.3479628},
abstract = {Due to mobile devices' growing presence in our daily routine, mobile applications are becoming increasingly complex. They require more powerful processing capability and more extensive data storage, which characterizes a challenge when computational constraints of these devices are taken into account. Several software infrastructures proposed to help the development of mobile applications with data offloading features. However, they lack essential features for data offloading, such as configurable data synchronization policy models, privacy mechanisms for the offloaded data, and scalability and performance analyses. This work presents DOP, a solution to assist the development of mobile applications that use data migration, including contextual data, from mobile devices to a remote environment, based on a microservice architecture. The data offloading technique enables data migration into a remote environment, allowing (i) storage savings on the mobile device and (ii) sharing data among users. The experiments on DOP showed benefits in storage savings on mobile devices and new possibilities for inferring situations based on shared data from multiple users.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {145–152},
numpages = {8},
keywords = {Data Offloading, Microservices, Mobile Cloud Computing},
location = {Belo Horizonte, Minas Gerais, Brazil},
series = {WebMedia '21}
}

@proceedings{10.1145/3716895,
title = {ICAICE '24: Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering},
year = {2024},
isbn = {9798400718007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3689484,
title = {GPCE '24: Proceedings of the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2024},
isbn = {9798400712111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’24). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming language and software engineering research communities.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@proceedings{10.1145/3640457,
title = {RecSys '24: Proceedings of the 18th ACM Conference on Recommender Systems},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bari, Italy}
}

@proceedings{10.1145/3661904,
title = {ICETT '24: Proceedings of the 2024 10th International Conference on Education and Training Technologies},
year = {2024},
isbn = {9798400717895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3510003.3510208,
author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
title = {μAFL: non-intrusive feedback-driven fuzzing for microcontroller firmware},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510208},
doi = {10.1145/3510003.3510208},
abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present μAFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, μAFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated μAFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {ETM, IoT, firmware security, fuzzing, microcontroller},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3466752.3480071,
author = {Truong, Minh S. Q. and Chen, Eric and Su, Deanyone and Shen, Liting and Glass, Alexander and Carley, L. Richard and Bain, James A. and Ghose, Saugata},
title = {RACER: Bit-Pipelined Processing Using Resistive Memory},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480071},
doi = {10.1145/3466752.3480071},
abstract = {To combat the high energy costs of moving data between main memory and the CPU, recent works have proposed to perform processing-using-memory (PUM), a type of processing-in-memory where operations are performed on data in situ (i.e., right at the memory cells holding the data). Several common and emerging memory technologies offer the ability to perform bitwise Boolean primitive functions by having interconnected cells interact with each other, eliminating the need to use discrete CMOS compute units for several common operations. Recent PUM architectures extend upon these Boolean primitives to perform bit-serial computation using memory. Unfortunately, several practical limitations of the underlying memory devices restrict how large emerging memory arrays can be, which hinders the ability of conventional bit-serial computation approaches to deliver high performance in addition to large energy savings. In this paper, we propose RACER, a cost-effective PUM architecture that delivers high performance and large energy savings using small arrays of resistive memories. RACER makes use of a bit-pipelining execution model, which can pipeline bit-serial w-bit computation across w small tiles. We fully design efficient control and peripheral circuitry, whose area can be amortized over small memory tiles without sacrificing memory density, and we propose an ISA abstraction for RACER to allow for easy program/compiler integration. We evaluate an implementation of RACER using NOR-capable ReRAM cells across a range of microbenchmarks extracted from data-intensive applications, and find that RACER provides 107 \texttimes{}, 12 \texttimes{}, and 7 \texttimes{} the performance of a 16-core CPU, a 2304-shader-core GPU, and a state-of-the-art in-SRAM compute substrate, respectively, with energy savings of 189 \texttimes{}, 17 \texttimes{}, and 1.3 \texttimes{}.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {100–116},
numpages = {17},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3686081,
title = {ICDSM '24: Proceedings of the International Conference on Decision Science &amp; Management},
year = {2024},
isbn = {9798400718151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3711129,
title = {EITCE '24: Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering},
year = {2024},
isbn = {9798400710094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3583740,
title = {SEC '23: Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing},
year = {2023},
isbn = {9798400701238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SEC is a premier forum for top researchers and practitioners to come together to discuss the opportunities and challenges of edge computing.},
location = {Wilmington, DE, USA}
}

@proceedings{10.1145/3670105,
title = {CNIOT '24: Proceedings of the 2024 5th International Conference on Computing, Networks and Internet of Things},
year = {2024},
isbn = {9798400716751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3524494,
title = {GAS '22: Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS explores how the growing adoption of gameful elements in various contexts can make the design and development of new technology increasingly complex, and provides a forum to explore these issues that crosscut the software engineering and games development communities. The goal of this one day workshop is to bring together interdisciplinary researchers and practitioners to discuss emerging and new research trends, challenges, costs, and benefits for entertainment games, serious games, and the gamification of traditional (non-game) applications and activities.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3623476.3623517,
author = {Jouneaux, Gwendal and Fr\"{o}lich, Damian and Barais, Olivier and Combemale, Benoit and Le Guernic, Gurvan and Mussbacher, Gunter and van Binsbergen, L. Thomas},
title = {Adaptive Structural Operational Semantics},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623517},
doi = {10.1145/3623476.3623517},
abstract = {Software systems evolve more and more in complex and changing environments, often requiring runtime adaptation  
to best deliver their services. When self-adaptation is the main concern of the system, a manual implementation of the underlying feedback loop and trade-off analysis may be desirable. However, the required expertise and substantial development effort make such implementations prohibitively difficult when it is only a secondary concern for the given domain. In this paper, we present ASOS, a metalanguage abstracting the runtime adaptation concern of a given domain in the behavioral semantics of a domain-specific language (DSL), freeing the language user from implementing it from scratch for each system in the domain. We demonstrate our approach on RobLANG, a procedural DSL for robotics, where we abstract a recurrent energy-saving behavior depending on the context. We provide formal semantics for ASOS and pave the way for checking properties such as determinism, completeness, and termination of the resulting self-adaptable language. We provide first results on the performance of our approach compared to a manual implementation of this self-adaptable behavior. We demonstrate, for RobLANG, that our approach provides suitable abstractions for specifying sound adaptive operational semantics while being more efficient.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {DSL, Operational Semantics, Self-Adaptation},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/3544548.3580652,
author = {Liao, Q. Vera and Subramonyam, Hariharan and Wang, Jennifer and Wortman Vaughan, Jennifer},
title = {Designerly Understanding: Information Needs for Model Transparency to Support Design Ideation for AI-Powered User Experience},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580652},
doi = {10.1145/3544548.3580652},
abstract = {Despite the widespread use of artificial intelligence (AI), designing user experiences (UX) for AI-powered systems remains challenging. UX designers face hurdles understanding AI technologies, such as pre-trained language models, as design materials. This limits their ability to ideate and make decisions about whether, where, and how to use AI. To address this problem, we bridge the literature on AI design and AI transparency to explore whether and how frameworks for transparent model reporting can support design ideation with pre-trained models. By interviewing 23 UX practitioners, we find that practitioners frequently work with pre-trained models, but lack support for UX-led ideation. Through a scenario-based design task, we identify common goals that designers seek model understanding for and pinpoint their model transparency information needs. Our study highlights the pivotal role that UX designers can play in Responsible AI and calls for supporting their understanding of AI limitations through model transparency and interrogation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {9},
numpages = {21},
keywords = {AI design, AI documentation, AI transparency, explainability, pre-trained models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3368089.3409727,
author = {Chen, Qingrong and Wang, Teng and Legunsen, Owolabi and Li, Shanshan and Xu, Tianyin},
title = {Understanding and discovering software configuration dependencies in cloud and datacenter systems},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409727},
doi = {10.1145/3368089.3409727},
abstract = {A large percentage of real-world software configuration issues, such as misconfigurations, involve multiple interdependent configuration parameters. However, existing techniques and tools either do not consider dependencies among configuration parameters— termed configuration dependencies—or rely on one or two dependency types and code patterns as input. Without rigorous understanding of configuration dependencies, it is hard to deal with many resulting configuration issues.  This paper presents our study of software configuration dependencies in 16 widely-used cloud and datacenter systems, including dependencies within and across software components. To understand types of configuration dependencies, we conduct an exhaustive search of descriptions in structured configuration metadata and unstructured user manuals. We find and manually analyze 521 configuration dependencies. We define five types of configuration dependencies and identify their common code patterns. We report on consequences of not satisfying these dependencies and current software engineering practices for handling the consequences.  We mechanize the knowledge gained from our study in a tool, cDep, which detects configuration dependencies. cDep automatically discovers five types of configuration dependencies from bytecode using static program analysis. We apply cDep to the eight Java and Scala software systems in our study. cDep finds 87.9% (275/313) of the related subset of dependencies from our study. cDep also finds 448 previously undocumented dependencies, with a 6.0% average false positive rate. Overall, our results show that configuration dependencies are more prevalent and diverse than previously reported and should henceforth be considered a first-class issue in software configuration engineering.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {362–374},
numpages = {13},
keywords = {Configuration, cloud systems, datacenter systems, dependency},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3658271,
title = {SBSI '24: Proceedings of the 20th Brazilian Symposium on Information Systems},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Juiz de Fora, Brazil}
}

@inproceedings{10.1145/3341105.3374036,
author = {de Castro-Cabrera, M. del Carmen and Garc\'{\i}a-Dominguez, Antonio and Medina-Bulo, Inmaculada},
title = {Trends in prioritization of test cases: 2017-2019},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374036},
doi = {10.1145/3341105.3374036},
abstract = {A core task in software testing is the design of test suites. Large test suites may take too long to run frequently, and test case prioritization (TCP) techniques have been proposed to speed up the detection of faults. These techniques have become increasingly popular and the number of publications has grown in recent years. Surveys have covered most of the techniques, but the latest included only publications until 2016: interest is growing, and new proposals have been developed in the last three years. This paper aims to complete that survey by providing the latest developments in TCP to respond to this growing interest. Specifically we use the taxonomy proposed by Khatibsyarbin et al. on the most important publications from 2017 to the present day (2019). All in all, we found 320 papers in this period about test case prioritization. The results show that the main techniques used are search-, coverage- and similarity-based.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2005–2011},
numpages = {7},
keywords = {TCP, regression testing, software testing, systematic literature review, test case prioritization},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3526071.3527518,
author = {Blender, Timo and Schlegel, Christian},
title = {Dynamic allocation of service robot resources to an order picking task considering functional and non-functional properties},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527518},
doi = {10.1145/3526071.3527518},
abstract = {Industry 4.0 processes have often varying requirements. A service robot and a team of service robots respectively represent a flexible resource. That means, it possesses variability that can possibly be configured in such a way that it is able to fulfill the requirements of industry 4.0 processes. Determining whether that is the case and how that has to happen is an important part of variability management. Based on a model-driven general method for variability management in a robotics software ecosystem, we present here a concrete use case (model) in which we allocate for an order picking task with specific time requirements either a single fitting service robot or a collaboration of two fitting service robots. Relevant properties of the service robots considered are both functional (are the capabilities to execute the tasks available?) as well as non-functional (the desired velocity parameterization while executing the individual navigation sub tasks limited by the respective maximum speed of a service robot).},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {25–32},
numpages = {8},
keywords = {model-driven software development, non-functional properties, robotics software ecosystem, service robot collaboration, variability management},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@proceedings{10.1109/3655039,
title = {ASPDAC '24: Proceedings of the 29th Asia and South Pacific Design Automation Conference},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
abstract = {ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) area like other sister conferences such as Design Automation Conference (DAC), Design, Automation &amp; Test in Europe (DATE), International Conference on Computer-Aided Design (ICCAD), and Embedded Systems Week (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunity to know the recent advanced technologies on LSI design and design automation areas, and to communicate each other for researchers and designers around Asia and South Pacific regions.},
location = {Incheon, Republic of Korea}
}

@proceedings{10.1145/3675417,
title = {DEAI '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
year = {2024},
isbn = {9798400717147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hongkong, China}
}

@inproceedings{10.1145/3236024.3264838,
author = {Brun, Yuriy and Meliou, Alexandra},
title = {Software fairness},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264838},
doi = {10.1145/3236024.3264838},
abstract = {A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {754–759},
numpages = {6},
keywords = {Software fairness, software bias, software process},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3313831.3376590,
author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
title = {Questioning the AI: Informing Design Practices for Explainable AI User Experiences},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376590},
doi = {10.1145/3313831.3376590},
abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {explainable AI, human-AI interaction, user experience},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3494519,
author = {Marijan, Dusica and Sen, Sagar},
title = {Industry–Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3494519},
doi = {10.1145/3494519},
abstract = {Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry–academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research–practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry–academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {45},
numpages = {52},
keywords = {Industry-academia collaboration, research collaboration, research co-creation, software engineering, technology transfer, knowledge transfer, collaboration gap, collaboration model, patterns, anti-patterns}
}

@article{10.1145/209910.209912,
author = {Gaines, Brian R. and Shaw, Mildred L. G.},
title = {Knowledge acquisition and representation techniques in scholarly communication},
year = {1995},
issue_date = {June 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0731-1001},
url = {https://doi.org/10.1145/209910.209912},
doi = {10.1145/209910.209912},
abstract = {Paper journals, conferences and workshops have been the major media for scholarly discourse for 300 years. However, in the 1990s access to low-cost personal computing and Internet communications is leading to radical changes in the operation of scholarly communities. Electronic publication and conferencing is becoming common in all disciplines using commonly available Internet facilities such as ftp archives, list servers, gopher and world-wide web. Some scholarly communities that had not previously achieved a critical mass have done so through the net, others have launched major collaborative projects managed through the net, and others are questioning the value of conventional conferences that are limited by being localized in space and time compared with the flexibility of continuous international electronic conferencing through the web. However, the majority of current electronic scholarly discourse emulates paper-based media in relying primarily on text and diagrams for knowledge communication. It is beginning to take advantage of some of the multimedia capabilities of electronic publishing for color diagrams, pictures, movies and sound. Hypertext and hypermedia capabilities are being used to develop webs of linked material. Concept maps and formal knowledge structures are being used to provide a framework for knowledge expression, interchange and collaborative development. This article focuses on the extension of current documentation technologies to provide knowledge-level support for scholarly communities.},
journal = {SIGDOC Asterisk J. Comput. Doc.},
month = jun,
pages = {23–36},
numpages = {14}
}

@proceedings{10.1145/3649411,
title = {GPGPU '24: Proceedings of the 16th Workshop on General Purpose Processing Using GPU},
year = {2024},
isbn = {9798400718175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@article{10.1145/3591286,
author = {Jin, Ende and Amin, Nada and Zhang, Yizhou},
title = {Extensible Metatheory Mechanization via Family Polymorphism},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591286},
doi = {10.1145/3591286},
abstract = {With the growing practice of mechanizing language metatheories, it has become ever more pressing that interactive theorem provers make it easy to write reusable, extensible code and proofs. This paper presents a novel language design geared towards extensible metatheory mechanization in a proof assistant. The new design achieves reuse and extensibility via a form of family polymorphism, an object-oriented idea, that allows code and proofs to be polymorphic to their enclosing families. Our development addresses technical challenges that arise from the underlying language of a proof assistant being simultaneously functional, dependently typed, a logic, and an interactive tool. Our results include (1) a prototypical implementation of the language design as a Coq plugin, (2) a dependent type theory capturing the essence of the language mechanism and its consistency and canonicity results, and (3) case studies showing how the new expressiveness naturally addresses real programming challenges in metatheory mechanization.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {172},
numpages = {25},
keywords = {Coq, Proof engineering, dependent type theory, expression problem, extensible frameworks, inductive types, interactive theorem proving, late binding, mixins, modules, reuse}
}

@article{10.1145/3569927,
author = {Broy, Manfred and Rumpe, Bernhard},
title = {Development Use Cases for Semantics-Driven Modeling Languages},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3569927},
doi = {10.1145/3569927},
abstract = {Choosing underlying semantic theories and definition techniques must closely follow intended use cases for the modeling language.},
journal = {Commun. ACM},
month = apr,
pages = {62–71},
numpages = {10}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@inproceedings{10.1145/3477244.3477985,
author = {van der Sanden, Bram and Li, Yonghui and van den Aker, Joris and Akesson, Benny and Bijlsma, Tjerk and Hendriks, Martijn and Triantafyllidis, Kostas and Verriet, Jacques and Voeten, Jeroen and Basten, Twan},
title = {Model-driven system-performance engineering for cyber-physical systems},
year = {2021},
isbn = {9781450387125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477244.3477985},
doi = {10.1145/3477244.3477985},
abstract = {System-Performance Engineering (SysPE) encompasses modeling formalisms, methods, techniques, and industrial practices to design systems for performance, where performance is taken integrally into account during the whole system life cycle. Industrial SysPE state of practice is generally model-based. Due to the rapidly increasing complexity of systems, there is a need to develop and establish model-driven methods and techniques. To structure the field of SysPE, we identify (1) industrial challenges motivating the importance of SysPE, (2) scientific challenges that need to be addressed to establish model-driven SysPE, (3) important focus areas for SysPE and (4) best practices. We conducted a survey to collect feedback on our views. The responses were used to update and validate the identified challenges, focus areas, and best practices. The final result is presented in this paper. Interesting observations are that industry sees a need for better design-space exploration support, more than for additional performance modeling and analysis techniques. Also tools and integral methods for SysPE need attention. From the identified focus areas, scheduling and supervisory control is seen as lacking established best practices.},
booktitle = {Proceedings of the 2021 International Conference on Embedded Software},
pages = {11–22},
numpages = {12},
keywords = {CPS, model-driven design, system-performance engineering},
location = {Virtual Event},
series = {EMSOFT '21}
}

@inproceedings{10.1145/1321631.1321676,
author = {Menzies, Tim and Elrawas, Oussama and Hihn, Jairus and Feather, Martin and Madachy, Ray and Boehm, Barry},
title = {The business case for automated software engineering},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321676},
doi = {10.1145/1321631.1321676},
abstract = {Adoption of advanced automated SE (ASE) tools would be favored if a business case could be made that these tools are more valuable than alternate methods. In theory, software prediction models can be used to make that case. In practice, this is complicated by the "local tuning" problem. Normally, predictors for software effort and defects and threat use local data to tune their predictions. Such local tuning data is often unavailable.This paper shows that assessing the relative merits of different SE methods need not require precise local tunings. STAR1 is a simulated annealer plus a Bayesian post-processor that explores the space of possible local tunings within software prediction models. STAR1 ranks project decisions by their effects on effort and defects and threats. In experiments with two NASA systems, STAR1 found that ASE tools were necessary to minimize effort/ defect/ threats.},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {303–312},
numpages = {10},
keywords = {COCOMO, COQUALMO, bayes, simulated annealing},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@article{10.1145/3699839.3699841,
author = {Awad, Hiba and Alidra, Abdelghani and Bruneliere, Hugo and Ledoux, Thomas and Rivalan, Jonathan},
title = {VeriFog: A Generic Model-based Approach for Verifying Fog Systems at Design Time and Generating Deployment Configurations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3699839.3699841},
doi = {10.1145/3699839.3699841},
abstract = {Fog Computing is a paradigm decentralizing the Cloud by geographically distributing computation, storage, network resources and related services. It provides benefits such as reducing the number of bottlenecks, limiting unwanted data movements, etc. However, managing the size, complexity and heterogeneity of the Fog systems to be engineered is challenging and can quickly become costly. According to best practices in software engineering, verification tasks could be performed on a system design prior to its implementation and deployment. We propose a generic model-based approach for verifying Fog systems at design time, also enabling the automatic generation of corresponding deployment configuration files. Named VeriFog, this approach is notably based on a customizable Fog Modeling Language (FML). We experimented in practice by modeling three use cases, from three different application domains, and by considering three main types of non-functional properties to be verified. From this modeling and verification effort, we show that we are able to automatically generate usable deployment configuration files for different deployment tools. In direct collaboration with our industrial partner Smile, the approach and underlying language presented in this paper are necessary steps towards a more global model-based support for the complete life cycle of Fog systems.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {18–36},
numpages = {19},
keywords = {deployment configuration, design time, fog computing, generation, model-based engineering, modeling language, non-functional properties, verification}
}

@proceedings{10.1145/3701625,
title = {SBQS '24: Proceedings of the XXIII Brazilian Symposium on Software Quality},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3651640,
title = {ESSE '23: Proceedings of the 4th European Symposium on Software Engineering},
year = {2023},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Napoli, Italy}
}

@proceedings{10.1145/3708282,
title = {AITC '24: Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
year = {2024},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.5555/3373669.3373682,
author = {Knote, Robin and S\"{o}llner, Matthias and Leimeister, Jan Marco},
title = {Towards a pattern language for smart personal assistants},
year = {2020},
publisher = {The Hillside Group},
address = {USA},
abstract = {Supporting users in their daily activities, thus, making their lives more comfortable, has long been a goal for consumer-oriented systems development. With the rise of smart personal assistants (SPAs), however, we have reached a new milestone along the path towards this goal. These systems assist their owners by providing personalized and context-dependent information and service. Today's implementations reach from conversational agents, such as Siri, Cortana or Google Assistant, over chatbots, which are primarily text-based, to cognitive assistants, which assist according to a user's current cognitive or emotional state. However, although both research and practice proceed with full pace, recurring design elements of SPAs have not yet been investigated. We hence propose a pattern language for smart personal assistants to guide further empirical and design efforts. Therefore, we review existing information systems, computer science and human-computer interaction literature to find recurring design characteristics among 115 different assistants. The resulting pattern language contains 22 patterns that specify the interaction behavior and the intelligence of smart personal assistants.},
booktitle = {Proceedings of the 25th Conference on Pattern Languages of Programs},
articleno = {14},
numpages = {16},
keywords = {pattern language, smart personal assistants},
location = {Portland, Oregon},
series = {PLoP '18}
}

@proceedings{10.1145/3570353,
title = {COP '22: Proceedings of the 14th ACM International Workshop on Context-Oriented Programming and Advanced Modularity},
year = {2022},
isbn = {9781450399869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Berlin, Germany}
}

@inproceedings{10.1145/3025453.3025626,
author = {Kery, Mary Beth and Horvath, Amber and Myers, Brad},
title = {Variolite: Supporting Exploratory Programming by Data Scientists},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025626},
doi = {10.1145/3025453.3025626},
abstract = {How do people ideate through code? Using semi-structured interviews and a survey, we studied data scientists who program, often with small scripts, to experiment with data. These studies show that data scientists frequently code new analysis ideas by building off of their code from a previous idea. They often rely on informal versioning interactions like copying code, keeping unused code, and commenting out code to repurpose older analysis code while attempting to keep those older analyses intact. Unlike conventional version control, these informal practices allow for fast versioning of any size code snippet, and quick comparisons by interchanging which versions are run. However, data scientists must maintain a strong mental map of their code in order to distinguish versions, leading to errors and confusion. We explore the needs for improving version control tools for exploratory tasks, and demonstrate a tool for lightweight local versioning, called Variolite, which programmers found usable and desirable in a preliminary usability study.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1265–1276},
numpages = {12},
keywords = {end-user programming, exploratory data analysis, variants, variations, version control systems (vcs)},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3593013.3594012,
author = {Widder, David Gray and Zhen, Derrick and Dabbish, Laura and Herbsleb, James},
title = {It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594012},
doi = {10.1145/3593013.3594012},
abstract = {How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {467–479},
numpages = {13},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3643690.3648242,
author = {Holmstr\"{o}m Olsson, Helena and Bosch, Jan},
title = {How To Get Good At Data: 5 Steps},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648242},
doi = {10.1145/3643690.3648242},
abstract = {Data allows companies to transition towards data-driven organizations and this is, in our experience, one of the highest-priority goals that many companies have. However, despite the prominence of data and the many opportunities associated with collection, analysis and use of data, the adoption of data-driven practices is slow. In our experience, companies fail to transition from their current state to a fully data-driven approach as the transformation is perceived as so large, complex and multi-dimensional that it becomes overwhelming and therefore, impossible to achieve in one step. We address this challenge by presenting a step-by-step process for how to transition towards fully data-driven practices. Consequently, the contribution of this paper is a model in which we outline five maturity steps for evolving towards fully data-driven practices.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {32–39},
numpages = {8},
keywords = {data-driven development, data practices, software instrumentation, value modeling, data maturity model},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@proceedings{10.1145/3569966,
title = {CSSE '22: Proceedings of the 5th International Conference on Computer Science and Software Engineering},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@inproceedings{10.1145/1409540.1409596,
author = {Berg, Kimmo and Ehtamo, Harri},
title = {Multidimensional screening: online computation and limited information},
year = {2008},
isbn = {9781605580753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409540.1409596},
doi = {10.1145/1409540.1409596},
abstract = {Optimal screening has been studied in economics, game theory, and recently computer science. We study the problem in a nonlinear pricing application, where a monopoly designs a price schedule from which the buyers self-select the quality they wish to consume. We formulate a multidimensional model with buyers' utility functions that need not satisfy the standard single-crossing assumption. We characterize the solution with the first-order optimality conditions and present a framework for analyzing the solution. With the framework, the structure of the solution is easily illustrated and the sensitivity analysis can be done. We give numerical examples that demonstrate the properties of the solution. With these observations, we discuss the complexity of the problem and solving the problem under limited information. We examine what information the monopoly needs when adjusting the price schedule to increase the profit. This paper applies, e.g., to pricing situations in electronic commerce where the seller may have limited information available, and the seller learns about the buyers' preferences online when doing the business.},
booktitle = {Proceedings of the 10th International Conference on Electronic Commerce},
articleno = {41},
numpages = {10},
keywords = {limited information, mechanism design, multidimensional screening, nonlinear pricing, online computation},
location = {Innsbruck, Austria},
series = {ICEC '08}
}

@proceedings{10.5555/3581644,
title = {CNSM '22: Proceedings of the 18th International Conference on Network and Service Management},
year = {2022},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {CNSM 2022 focuses on the theme "Intelligent Management of Disruptive Network Technologies and Services", that aims at capturing emerging approaches and intelligent solutions for dealing with disruptive network technologies, as well as associated services and applications.},
location = {Thessaloniki, Greece}
}

@article{10.1145/3487919,
author = {Pfannem\"{u}ller, Martin and Breitbach, Martin and Weckesser, Markus and Becker, Christian and Schmerl, Bradley and Sch\"{u}rr, Andy and Krupitzer, Christian},
title = {REACT-ION: A Model-based Runtime Environment for Situation-aware Adaptations},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487919},
doi = {10.1145/3487919},
abstract = {Trends such as the Internet of Things lead to a growing number of networked devices and to a variety of communication systems. Adding self-adaptive capabilities to these communication systems is one approach to reducing administrative effort and coping with changing execution contexts. Existing frameworks can help reducing development effort but are neither tailored toward the use in communication systems nor easily usable without knowledge in self-adaptive systems development. Accordingly, in previous work, we proposed REACT, a reusable, model-based runtime environment to complement communication systems with adaptive behavior. REACT addresses heterogeneity and distribution aspects of such systems and reduces development effort. In this article, we propose REACT-ION—an extension of REACT for situation awareness. REACT-ION offers a context management module that is able to acquire, store, disseminate, and reason on context data. The context management module is the basis for (i) proactive adaptation with REACT-ION and (ii) self-improvement of the underlying feedback loop. REACT-ION can be used to optimize adaptation decisions at runtime based on the current situation. Therefore, it can cope with uncertainty and situations that were not foreseeable at design time. We show and evaluate in two case studies how REACT-ION’s situation awareness enables proactive adaptation and self-improvement.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {12},
numpages = {29},
keywords = {Self-adaptive systems, model-based, runtime environment, framework, situation awareness}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@article{10.1145/3533818,
author = {Birchler, Christian and Khatiri, Sajad and Derakhshanfar, Pouria and Panichella, Sebastiano and Panichella, Annibale},
title = {Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533818},
doi = {10.1145/3533818},
abstract = {Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {28},
numpages = {30},
keywords = {Autonomous systems, software simulation, test case prioritization}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@article{10.1145/176979.176981,
author = {Chen, Peter M. and Lee, Edward K. and Gibson, Garth A. and Katz, Randy H. and Patterson, David A.},
title = {RAID: high-performance, reliable secondary storage},
year = {1994},
issue_date = {June 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/176979.176981},
doi = {10.1145/176979.176981},
abstract = {Disk arrays were proposed in the 1980s as a way to use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of most major computer manufacturers. This article gives a comprehensive overview of disk arrays and provides a framework in which to organize current and future work. First, the article introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the article describes seven disk array architectures, called RAID (Redundant Arrays of Inexpensive Disks) levels 0–6 and compares their  performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to improve performance and designing algorithms to maintain data consistency. Last, the article describes six disk array prototypes of products and discusses future opportunities for research, with an annotated bibliography disk array-related literature.},
journal = {ACM Comput. Surv.},
month = jun,
pages = {145–185},
numpages = {41},
keywords = {RAID, disk array, parallel I/O, redundancy, storage, striping}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@article{10.1007/s00165-021-00547-2,
author = {\v{C}e\v{s}ka, Milan and Hensel, Christian and Junges, Sebastian and Katoen, Joost-Pieter},
title = {Counterexample-guided inductive synthesis for probabilistic systems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4–5},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00547-2},
doi = {10.1007/s00165-021-00547-2},
abstract = {This paper presents counterexample-guided inductive synthesis (CEGIS) to automatically synthesise probabilistic models. The starting point is a family of finite-stateMarkov chains with related but distinct topologies. Such families can succinctly be described by a sketch of a probabilistic program. Program sketches are programs containing holes. Every hole has a finite repertoire of possible program snippets by which it can be filled.We study several synthesis problems—feasibility, optimal synthesis, and complete partitioning—for a given quantitative specification φ. Feasibility amounts to determine a family member satisfying φ, optimal synthesis amounts to find a family member that maximises the probability to satisfy φ, and complete partitioning splits the family in satisfying and refuting members. Each of these problems can be considered under the additional constraint of minimising the total cost of instantiations, e.g., what are all possible instantiations for φ that are within a certain budget? The synthesis problems are tackled using a CEGIS approach. The crux is to aggressively prune the search space by using counterexamples provided by a probabilistic model checker. Counterexamples can be viewed as sub-Markov chains that rule out all family members that share this sub-chain. Our CEGIS approach leverages efficient probabilisticmodel checking,modern SMT solving, and programsnippets as counterexamples. Experiments on case studies froma diverse nature—controller synthesis, program sketching, and security—show that synthesis among up to a million candidate designs can be done using a few thousand verification queries.},
journal = {Form. Asp. Comput.},
month = aug,
pages = {637–667},
numpages = {31},
keywords = {Program Synthesis, Markov Chains, Probabilistic Model Checking, Counterexamples, CEGIS}
}

@inproceedings{10.1109/ECASE.2019.00010,
author = {Keim, Jan and Schneider, Yves and Koziolek, Anne},
title = {Towards consistency analysis between formal and informal software architecture artefacts},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ECASE.2019.00010},
doi = {10.1109/ECASE.2019.00010},
abstract = {Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to address consistency analyses between models and textual natural language artefacts using natural language understanding and plan to include knowledge bases to improve these analyses. After extracting information out of the natural language documents, we plan to create traceability links and check whether statements within the textual documentation are consistent with the software architecture models. In this paper, we also outline our requirements for evaluating our approach with the help of a community-wide infrastructure and how our approach can be used to maintain community-wide case studies.},
booktitle = {Proceedings of the 2nd International Workshop on Establishing a Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {6–12},
numpages = {7},
keywords = {consistency, natural language processing, natural language understanding, software architecture, software architecture documentation, software engineering},
location = {Montreal, Quebec, Canada},
series = {ECASE '19}
}

@inproceedings{10.5555/3400397.3400402,
author = {Kulkarni, Vinay and Barat, Souvik and Clark, Tony},
title = {Towards adaptive enterprises using digital twins},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Modern enterprises are large complex systems operating in highly dynamic environments thus requiring quick response to a variety of change drivers. Moreover, they are systems of systems wherein understanding is available in localized contexts only and that too is typically partial and uncertain. With the overall system behaviour hard to know a-priori and conventional techniques for system-wide analysis either lacking in rigour or defeated by the scale of the problem, the current practice often exclusively relies on human expertise for monitoring and adaptation. We present an approach that combines ideas from modeling &amp; simulation, reinforcement learning and control theory to make enterprises adaptive. The approach hinges on the concept of Digital Twin - a set of relevant models that are amenable to analysis and simulation. The paper describes illustration of approach in two real world use cases.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {60–74},
numpages = {15},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3308560.3316605,
author = {Goswami, Anjan and Mohapatra, Prasant and Zhai, Chengxiang},
title = {Quantifying and Visualizing the Demand and Supply Gap from E-commerce Search Data using Topic Models},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316605},
doi = {10.1145/3308560.3316605},
abstract = {The demand generation and assortment planning are two critical components of running a retail business. Traditionally, retail companies use the historical sales data for modeling and optimization of assortment selection, and they use a marketing strategy for demand generation. However, today, most retail businesses have e-commerce sites with rapidly growing online sales. An e-commerce site typically has to maintain a large amount of digitized product data, and it also keeps a vast amount of historical customer interaction data that includes search, browse, click, purchase and many other different interactions. In this paper, we show how this digitized product data and the historical search logs can be used in understanding and quantifying the gap between the supply and demand side of a retail market. This gap helps in making an effective strategy for both demand generation and assortment selection. We construct topic models of the historical search queries and the digitized product data from the catalog. We use the former to model the customer demand and the later to model the supply side of the retail business. We then create a tool to visualize the topic models to understand the differences between the supply and demand side. We also quantify the supply and demand gap by defining a metric based on Kullback-Leibler (KL) divergence of topic distributions of queries and the products. The quantification helps us identifying the topics related to excess or less demand and thereby in designing effective strategies for demand generation and assortment selection. Application of this work by e-Commerce retailers can result in the development of product innovations that can be utilized to achieve economic equilibrium. We can identify the excess demand and can provide insight to the teams responsible for improving assortment and catalog quality. Similarly, we can also identify excess supply and can provide that intelligence to the teams responsible for demand generation. Tools of this nature can be developed to systematically drive efficiency in achieving better economic gains for the entire e-commerce engine. We conduct several experiments collecting data from Walmart.com to validate the effectiveness of our approach.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {348–353},
numpages = {6},
keywords = {Business Analytics, E-commerce search, Information retrieval, Marketplace economics, Topic Models},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/223984.224018,
author = {Gaines, Brian R. and Shaw, Mildred L. G.},
title = {Knowledge acquisition and representation techniques in scholarly communication},
year = {1996},
isbn = {0897917138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223984.224018},
doi = {10.1145/223984.224018},
booktitle = {Proceedings of the 13th Annual International Conference on Systems Documentation: Emerging from Chaos: Solutions for the Growing Complexity of Our Jobs},
pages = {197–206},
numpages = {10},
location = {Savannah, Georgia, USA},
series = {SIGDOC '95}
}

@inproceedings{10.5555/2820518.2820523,
author = {Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
title = {Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {13–23},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@proceedings{10.1145/3626772,
title = {SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 47th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024), taking place in Washington D.C., USA, from July 14 to 18, 2024.SIGIR serves as the foremost international forum for the presentation of groundbreaking research findings, the demonstration of innovative systems and techniques, and the exploration of forwardthinking research directions in the field of information retrieval.This year's SIGIR is an in-person conference. We believe that an in-person conference is beneficial for several reasons: it fosters direct engagement and networking opportunities, enhances the exchange of research ideas, contributes to a more dynamic and productive conference experience, and nurtures our research community by welcoming newcomers, providing them with the opportunity to become acquainted with SIGIR traditions. This decision has not been made lightly. We understand the challenges that can pose in the aftermath of a pandemic and amidst the uncertainties of the world around us. To accommodate those who cannot attend, we have implemented a series of measures such as proxy presenters, livestreaming, and recording sessions. These steps are taken to ensure that everyone has access to the valuable content that the conference offers.},
location = {Washington DC, USA}
}

@proceedings{10.1145/3660829,
title = {Programming '24: Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
year = {2024},
isbn = {9798400706349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lund, Sweden}
}

@proceedings{10.1145/3644033,
title = {FormaliSE '24: Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Historically, formal methods academic research and practical software development have had limited mutual interactions—except possibly in specialized domains such as safety-critical software. In recent times, the outlook has considerably improved: on the one hand, formal methods research has delivered more flexible techniques and tools that can support various aspects of the software development process—from user requirements elicitation, to design, implementation, verification and validation, as well as the creation of documentation. On the other hand, software engineering has developed a growing interest in rigorous techniques applied at scale.This evolution, and the desire to further improve it, motivated the creation of FormaliSE: a well-established annual conference whose main goal is to promote work at the intersection of the formal methods and software engineering communities, providing a venue to exchange ideas, experiences, techniques, and results. The collaboration between these two communities can be mutually beneficial by fostering the creation of formal methods that are practically useful and by helping develop higher-quality software.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3180155.3180244,
author = {Kwon, Jung-Hyun and Ko, In-Young and Rothermel, Gregg},
title = {Prioritizing browser environments for web application test execution},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180244},
doi = {10.1145/3180155.3180244},
abstract = {When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from −12.24% to 39.05% for no ordering, and from −0.04% to 45.85% for random ordering.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {468–479},
numpages = {12},
keywords = {browser environments, regression testing, web application testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3631882,
title = {MEMSYS '23: Proceedings of the International Symposium on Memory Systems},
year = {2023},
isbn = {9798400716447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Alexandria, VA, USA}
}

@inproceedings{10.1145/3011784.3011810,
author = {Javed, Muhammad Atif and Stevanetic, Srdjan and Zdun, Uwe},
title = {Towards a pattern language for construction and maintenance of software architecture traceability links},
year = {2016},
isbn = {9781450340748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011784.3011810},
doi = {10.1145/3011784.3011810},
abstract = {The documentation of software architecture traceability links is the foundation for many important architecture management activities, such as verification and validation, reuse evaluation and impact analysis. In practice, the construction and maintenance of traceability links is mostly manual, which is labor-intensive and error prone. Although the costs of manual traceability in terms of the time, effort and money required can be mitigated by automated construction, the completeness and correctness of traceability links tends to be negatively affected by automation in their creation and maintenance. This paper presents a pattern language for construction and maintenance of software architecture traceability links to requirements and source code. As a foundation for deriving the pattern language, we have performed systematic literature reviews, investigations of traceability links for multiple open-source software systems, and empirical studies. In particular, we studied the nature of the software architecture traceability phenomenon and its driving factors and impacts, as well as the methods that provide the means to control software architecture traceability. The derived pattern language provides support for addressing multiple decision categories for construction and maintenance of software architecture traceability links. To illustrate the patterns, their application is shown in the context of constructing and maintaining traceability links for an open source framework for mobile games.},
booktitle = {Proceedings of the 21st European Conference on Pattern Languages of Programs},
articleno = {24},
numpages = {20},
keywords = {4 (trusted) of traceability, grand challenge 2 (cost-effective), software architecture, traceability patterns},
location = {Kaufbeuren, Germany},
series = {EuroPlop '16}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/1030397.1030431,
author = {Simske, Steven J. and Baggs, Scott C.},
title = {Digital capture for automated scanner workflows},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030431},
doi = {10.1145/1030397.1030431},
abstract = {The use of scanners and other capture devices to incorporate film- and paper-based materials into digital workflows is an important part of "digital convergence", or the bringing of paper-based and electronic documents together into the same electronic workflows. The diversity of captured information-from text and mixed-type documents to photos, negatives, slides and transparencies-requires a combination of document analysis techniques to perform, automatically, the segmentation, classification and workflow assignment of the scanned images. We herein present technologies that provide fast (&lt; 1.0 sec) and reliable (&gt; 95% job accuracy) capture solutions for all of these input content types. These solutions offer near real-time capture that provides automated workflow capabilities to a repertoire of scanning hardware: scanners, all-in-one devices, copiers and multifunctional printers. The techniques used to categorize the documents, perform zoning analysis on the documents, and then perform closed loop quality assurance on the documents are presented.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {171–177},
numpages = {7},
keywords = {classification, negatives, photos, scanning, segmentation, slides, user interface, zoning},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@proceedings{10.1145/3643655,
title = {SESoS '24: Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2024 will provide a forum for researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for the development and evolution of complex software-intensive systems.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@inproceedings{10.1007/11424529_4,
author = {Fredriksson, Johan and Sandstr\"{o}m, Kristian and \r{A}kerholm, Mikael},
title = {Optimizing resource usage in component-based real-time systems},
year = {2005},
isbn = {3540258779},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424529_4},
doi = {10.1007/11424529_4},
abstract = {The embedded systems domain represents a class of systems that have high requirements on cost efficiency as well as run-time properties such as timeliness and dependability. The research on component-based systems has produced component technologies for guaranteeing real-time properties. However, the issue of saving resources by allocating several components to real-time tasks has gained little focus. Trade-offs when allocating components to tasks are, e.g., CPU-overhead, footprint and integrity. In this paper we present a general approach for allocating components to real-time tasks, while utilizing existing real-time analysis to ensure a feasible allocation. We demonstrate that CPU-overhead and memory consumption can be reduced by as much as 48% and 32% respectively for industrially representative systems.},
booktitle = {Proceedings of the 8th International Conference on Component-Based Software Engineering},
pages = {49–65},
numpages = {17},
location = {St. Louis, MO},
series = {CBSE'05}
}

@inproceedings{10.1145/3540250.3549144,
author = {Martin-Lopez, Alberto and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio},
title = {Online testing of RESTful APIs: promises and challenges},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549144},
doi = {10.1145/3540250.3549144},
abstract = {Online testing of web APIs—testing APIs in production—is gaining traction in industry. Platforms such as RapidAPI and Sauce Labs provide online testing and monitoring services of web APIs 24/7, typically by re-executing manually designed test cases on the target APIs on a regular basis. In parallel, research on the automated generation of test cases for RESTful APIs has seen significant advances in recent years. However, despite their promising results in the lab, it is unclear whether research tools would scale to industrial-size settings and, more importantly, how they would perform in an online testing setup, increasingly common in practice. In this paper, we report the results of an empirical study on the use of automated test case generation methods for online testing of RESTful APIs. Specifically, we used the RESTest framework to automatically generate and execute test cases in 13 industrial APIs for 15 days non-stop, resulting in over one million test cases. To scale at this level, we had to transition from a monolithic tool approach to a multi-bot architecture with over 200 bots working cooperatively in tasks like test generation and reporting. As a result, we uncovered about 390K failures, which we conservatively triaged into 254 bugs, 65 of which have been acknowledged or fixed by developers to date. Among others, we identified confirmed faults in the APIs of Amadeus, Foursquare, Yelp, and YouTube, accessed by millions of applications worldwide. More importantly, our reports have guided developers on improving their APIs, including bug fixes and documentation updates in the APIs of Amadeus and YouTube. Our results show the potential of online testing of RESTful APIs as the next must-have feature in industry, but also some of the key challenges to overcome for its full adoption in practice.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {408–420},
numpages = {13},
keywords = {REST, black-box testing, bot, online testing, web API},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3467896,
author = {Vogel-Heuser, Birgit and Neumann, Eva-Maria and Fischer, Juliane},
title = {MICOSE4aPS: Industrially Applicable Maturity Metric to Improve Systematic Reuse of Control Software},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467896},
doi = {10.1145/3467896},
abstract = {automated Production Systems (aPS) are highly complex, mechatronic systems that usually have to operate reliably for many decades. Standardization and reuse of control software modules is a core prerequisite to achieve the required system quality in increasingly shorter development cycles. However, industrial case studies in aPS show that many aPS companies still struggle with strategically reusing software. This paper proposes a metric-based approach to objectively measure the maturity of industrial IEC 61131-based control software in aPS (MICOSE4aPS) to identify potential weaknesses and quality issues hampering systematic reuse. Module developers in the machine and plant manufacturing industry can directly benefit as the metric calculation is integrated into the software engineering workflow. An in-depth industrial evaluation in a top-ranked machine manufacturing company in food packaging and an expert evaluation with different companies confirmed the benefit of efficiently managing the quality of control software.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {5},
numpages = {24},
keywords = {Automated production systems, programmable logic controllers, IEC 61131-3, software quality}
}

@inproceedings{10.1145/3236024.3236069,
author = {Amar, Hen and Bao, Lingfeng and Busany, Nimrod and Lo, David and Maoz, Shahar},
title = {Using finite-state models for log differencing},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236069},
doi = {10.1145/3236024.3236069},
abstract = {Much work has been published on extracting various kinds of models from logs that document the execution of running systems. In many cases, however, for example in the context of evolution, testing, or malware analysis, engineers are interested not only in a single log but in a set of several logs, each of which originated from a different set of runs of the system at hand. Then, the difference between the logs is the main target of interest.  In this work we investigate the use of finite-state models for log differencing. Rather than comparing the logs directly, we generate concise models to describe and highlight their differences. Specifically, we present two algorithms based on the classic k-Tails algorithm: 2KDiff, which computes and highlights simple traces containing sequences of k events that belong to one log but not the other, and nKDiff, which extends k-Tails from one to many logs, and distinguishes the sequences of length k that are common to all logs from the ones found in only some of them, all on top of a single, rich model. Both algorithms are sound and complete modulo the abstraction defined by the use of k-Tails.  We implemented both algorithms and evaluated their performance on mutated logs that we generated based on models from the literature. We conducted a user study including 60 participants demonstrating the effectiveness of the approach in log differencing tasks. We have further performed a case study to examine the use of our approach in malware analysis. Finally, we have made our work available in a prototype web-application, for experiments.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {49–59},
numpages = {11},
keywords = {log analysis, model inference},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@proceedings{10.1145/3579371,
title = {ISCA '23: Proceedings of the 50th Annual International Symposium on Computer Architecture},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3106237.3106277,
author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
title = {Fairness testing: testing software for discrimination},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106277},
doi = {10.1145/3106237.3106277},
abstract = {This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {498–510},
numpages = {13},
keywords = {Discrimination testing, fairness testing, software bias, testing},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3422392.3422439,
author = {Soares, Vin\'{\i}cius and Oliveira, Anderson and Pereira, Juliana Alves and Bibano, Ana Carla and Garcia, Alessandro and Farah, Paulo Roberto and Vergilio, Silvia Regina and Schots, Marcelo and Silva, Caio and Coutinho, Daniel and Oliveira, Daniel and Uch\^{o}a, Anderson},
title = {On the Relation between Complexity, Explicitness, Effectiveness of Refactorings and Non-Functional Concerns},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422439},
doi = {10.1145/3422392.3422439},
abstract = {Developers need to consistently improve the internal structural quality of a program to address its maintainability and possibly other non-functional concerns. Refactoring is the main practice to improve code quality. Typical refactoring factors, such as their complexity and explicitness (i.e., their self-affirmation), may influence its effectiveness in improving key internal code attributes, such as enhancing cohesion or reducing its coupling, complexity and size. However, we still lack an understanding of whether such concerns and factors play a role on improving the code structural quality. Thus, this paper investigates the relationship between complexity, explicitness and effectiveness of refactorings and non-functional concerns in four projects. We study four non-functional concerns, namely maintainability, security, performance, and robustness. Our findings reveal that complex refactorings indeed have an impactful effect on the code structure, either improving or reducing the code structural quality. We also found that both self-affirmed refactorings and non-functional concerns are usually accompanied by complex refactorings, but tend to have a negative effect on code structural quality. Our findings can: (i) help researchers to improve the design of empirical studies and refactoring-related tools, and (ii) warn practitioners on which circumstances their refactorings may cause a negative impact on code quality.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {788–797},
numpages = {10},
keywords = {internal quality attributes, non-functional concerns, refactoring, self-affirmed refactorings},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3427228.3427248,
author = {Pennekamp, Jan and Buchholz, Erik and Lockner, Yannik and Dahlmanns, Markus and Xi, Tiandong and Fey, Marcel and Brecher, Christian and Hopmann, Christian and Wehrle, Klaus},
title = {Privacy-Preserving Production Process Parameter Exchange},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427248},
doi = {10.1145/3427228.3427248},
abstract = {Nowadays, collaborations between industrial companies always go hand in hand with trust issues, i.e., exchanging valuable production data entails the risk of improper use of potentially sensitive information. Therefore, companies hesitate to offer their production data, e.g., process parameters that would allow other companies to establish new production lines faster, against a quid pro quo. Nevertheless, the expected benefits of industrial collaboration, data exchanges, and the utilization of external knowledge are significant. In this paper, we introduce our Bloom filter-based Parameter Exchange (BPE), which enables companies to exchange process parameters privacy-preservingly. We demonstrate the applicability of our platform based on two distinct real-world use cases: injection molding and machine tools. We show that BPE is both scalable and deployable for different needs to foster industrial collaborations. Thereby, we reward data-providing companies with payments while preserving their valuable data and reducing the risks of data leakage.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {510–525},
numpages = {16},
keywords = {Bloom filter, Internet of Production, oblivious transfer, secure industrial collaboration},
location = {Austin, USA},
series = {ACSAC '20}
}

@proceedings{10.1145/3633500,
title = {NSPW '23: Proceedings of the 2023 New Security Paradigms Workshop},
year = {2023},
isbn = {9798400716201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Segovia, Spain}
}

@proceedings{10.1145/3613904,
title = {CHI '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1109/ICSE43902.2021.00076,
author = {Hata, Hideaki and Kula, Raula Gaikovina and Ishio, Takashi and Treude, Christoph},
title = {Same File, Different Changes: The Potential of Meta-Maintenance on GitHub},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00076},
doi = {10.1109/ICSE43902.2021.00076},
abstract = {Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance---i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {773–784},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/51909.51910,
author = {Chu, Wei-Han},
title = {Generic expert system shell for diagnostic reasoning},
year = {1988},
isbn = {0897912713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/51909.51910},
doi = {10.1145/51909.51910},
abstract = {Rule based expert systems provide a modular and uniform approach to representing knowledge, however it has been recognized that rule-based systems become increasingly difficult to understand and maintain as the number of rules grow. Expert systems today are developed on general purpose inference shells that offer general purpose paradigms which do not take into considerations the type of problems being solved. It is up to the users to create the meta level control to prevent rule interference, and for the rules to function properly. This task tends to become increasingly difficult in direct proportion to the size of the accumulated knowledge.The solution is in a new generation of Application Specific Expert System Tools that are designed with specific paradigms and knowledge representation methodology that meet the requirements of a specific domain. This concept is examplified in the work presented here that introduces a generic expert systems shell for diagnostic reasoning. Domain knowledge is represented as five different classes of objects. A paradigm for diagnostic reasoning is built into the inference algorithm to become part of the inference shell, replacing the usual general purpose forward or backward chaining algorithm.},
booktitle = {Proceedings of the 1st International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 1},
pages = {7–12},
numpages = {6},
location = {Tullahoma, Tennessee, USA},
series = {IEA/AIE '88}
}

@proceedings{10.1145/3640794,
title = {CUI '24: Proceedings of the 6th ACM Conference on Conversational User Interfaces},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Luxembourg, Luxembourg}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.5555/3026877.3026904,
author = {Porter, Barry and Grieves, Matthew and Filho, Roberto Rodrigues and Leslie, David},
title = {REX: a development platform and online learning approach for runtime emergent software systems},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {Conventional approaches to self-adaptive software architectures require human experts to specify models, policies and processes by which software can adapt to its environment. We present REX, a complete platform and online learning approach for runtime emergent software systems, in which all decisions about the assembly and adaptation of software are machine-derived. REX is built with three major, integrated layers: (i) a novel component-based programming language called Dana, enabling discovered assembly of systems and very low cost adaptation of those systems for dynamic re-assembly; (ii) a perception, assembly and learning framework (PAL) built on Dana, which abstracts emergent software into configurations and perception streams; and (iii) an online learning implementation based on a linear bandit model, which helps solve the search space explosion problem inherent in runtime emergent software. Using an emergent web server as a case study, we show how software can be autonomously self-assembled from discovered parts, and continually optimized over time (by using alternative parts) as it is subjected to different deployment conditions. Our system begins with no knowledge that it is specifically assembling a web server, nor with knowledge of the deployment conditions that may occur at runtime.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {333–348},
numpages = {16},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@inproceedings{10.1109/MODELS-C.2019.00018,
author = {Rodr\'{\i}guez, Alejandro and Rutle, Adrian and Kristensen, Lars Michael and Dur\'{a}n, Francisco},
title = {A foundation for the composition of multilevel domain-specific languages},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00018},
doi = {10.1109/MODELS-C.2019.00018},
abstract = {In this paper, we provide a foundation for the definition and composition of multilevel domain-specific modelling languages. We will introduce modularization techniques such as composition, aggregation and referencing to enhance flexibility and reusability of these languages. To explain this foundation, we use Coloured Petri Nets (CPN) as a paradigmatic case study and define two CPN variants motivated by industrial collaboration projects: one used for the definition of protocols and the other one for robot controllers.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {88–97},
numpages = {10},
keywords = {coloured petri nets, model transformations, model-driven software engineering, multilevel modelling},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@proceedings{10.1145/3688867,
title = {McGE '24: Proceedings of the 2nd International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice},
year = {2024},
isbn = {9798400711947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice- McGE 2024We believe that this workshop will provide a valuable platform for researchers and practitioners to discuss and exchange ideas on the latest advancements, challenges, and opportunities in the rapidly evolving field of multimedia content generation.},
location = {Melbourne VIC, Australia}
}

@inproceedings{10.1145/3194133.3194147,
author = {Rodrigues, Arthur and Caldas, Ricardo Diniz and Rodrigues, Gena\'{\i}na Nunes and Vogel, Thomas and Pelliccione, Patrizio},
title = {A learning approach to enhance assurances for real-time self-adaptive systems},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194147},
doi = {10.1145/3194133.3194147},
abstract = {The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design- and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the system's goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {206–216},
numpages = {11},
keywords = {assurance evidence, data mining, goal-oriented, learning approach, real-time systems, self-adaptive systems},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1145/3603109,
author = {Zhang, Jingxuan and Luo, Junpeng and Liang, Jiahui and Gong, Lina and Huang, Zhiqiu},
title = {An Accurate Identifier Renaming Prediction and Suggestion Approach},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3603109},
doi = {10.1145/3603109},
abstract = {Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {148},
numpages = {51},
keywords = {Identifier renaming, source code analysis, code refactoring, mining code repository}
}

@inproceedings{10.1145/2745802.2745815,
author = {Zhou, You and Zhang, He and Huang, Xin and Yang, Song and Babar, Muhammad Ali and Tang, Hao},
title = {Quality assessment of systematic reviews in software engineering: a tertiary study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745815},
doi = {10.1145/2745802.2745815},
abstract = {Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {14},
numpages = {14},
keywords = {quality assessment, software engineering, systematic (literature) review},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/3377811.3380365,
author = {Stevens, Clay and Bagheri, Hamid},
title = {Reducing run-time adaptation space via analysis of possible utility bounds},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380365},
doi = {10.1145/3377811.3380365},
abstract = {Self-adaptive systems often employ dynamic programming or similar techniques to select optimal adaptations at run-time. These techniques suffer from the "curse of dimensionality", increasing the cost of run-time adaptation decisions. We propose a novel approach that improves upon the state-of-the-art proactive self-adaptation techniques to reduce the number of possible adaptations that need be considered for each run-time adaptation decision. The approach, realized in a tool called Thallium, employs a combination of automated formal modeling techniques to (i) analyze a structural model of the system showing which configurations are reachable from other configurations and (ii) compute the utility that can be generated by the optimal adaptation over a bounded horizon in both the best- and worst-case scenarios. It then constructs triangular possibility values using those optimized bounds to automatically compare adjacent adaptations for each configuration, keeping only the alternatives with the best range of potential results. The experimental results corroborate Thallium's ability to significantly reduce the number of states that need to be considered with each adaptation decision, freeing up vital resources at run-time.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1522–1534},
numpages = {13},
keywords = {formal methods, multi-objective optimization, run-time adaptation, self-adaptive systems},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/3320516.3320573,
author = {Galpin, Vashti and Georgoulas, Anastasis and Loreti, Michele and Vandin, Andrea},
title = {Statistical analysis of CARMA models: an advanced tutorial},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {CARMA (Collective Adaptive Resource-sharing Markovian Agents) is a process-algebra-based quantitative language developed for the modeling of collective adaptive systems. A CARMA model consists of an environment in which a collective of components with attribute stores interact via unicast and broadcast communication, providing a rich modeling formalism. The semantics of a CARMA model are given by a continuous-time Markov chain which can be simulated using the CARMA Eclipse Plug-in. Furthermore, statistical model checking can be applied to the trajectories generated through simulation using the MultiVeStA tool. This advanced tutorial will introduce some of the theory behind CARMA and MultiVeStA as well as demonstrate its application to collective adaptive system modeling.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {395–409},
numpages = {15},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@inproceedings{10.1145/3238147.3238192,
author = {Abdessalem, Raja Ben and Panichella, Annibale and Nejati, Shiva and Briand, Lionel C. and Stifter, Thomas},
title = {Testing autonomous cars for feature interaction failures using many-objective search},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238192},
doi = {10.1145/3238147.3238192},
abstract = {Complex systems such as autonomous cars are typically built as a composition of features that are independent units of functionality. Features tend to interact and impact one another's behavior in unknown ways. A challenge is to detect and manage feature interactions, in particular, those that violate system requirements, hence leading to failures. In this paper, we propose a technique to detect feature interaction failures by casting this problem into a search-based test generation problem. We define a set of hybrid test objectives (distance functions) that combine traditional coverage-based heuristics with new heuristics specifically aimed at revealing feature interaction failures. We develop a new search-based test generation algorithm, called FITEST, that is guided by our hybrid test objectives. FITEST extends recently proposed many-objective evolutionary algorithms to reduce the time required to compute fitness values. We evaluate our approach using two versions of an industrial self-driving system. Our results show that our hybrid test objectives are able to identify more than twice as many feature interaction failures as two baseline test objectives used in the software testing literature (i.e., coverage-based and failure-based test objectives). Further, the feedback from domain experts indicates that the detected feature interaction failures represent real faults in their systems that were not previously identified based on analysis of the system features and their requirements.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {143–154},
numpages = {12},
keywords = {Automotive Systems, Feature Interaction Problem, Many-Objective Optimization, Search-based Software Testing},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3385032.3385039,
author = {Bhatia, Kushagra and Mishra, Siba and Sharma, Arpit},
title = {Clustering Glossary Terms Extracted from Large-Sized Software Requirements using FastText},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385039},
doi = {10.1145/3385032.3385039},
abstract = {Specialized terms used in the requirements document should be defined in a glossary. We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents. We use text chunking combined with WordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster. We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {5},
numpages = {11},
keywords = {Clustering, FastText, Glossary, Natural Language Processing, Requirements Engineering, Word Embeddings},
location = {Jabalpur, India},
series = {ISEC '20}
}

@inproceedings{10.1145/2597008.2597156,
author = {Zapalowski, Vanius and Nunes, Ingrid and Nunes, Daltro Jos\'{e}},
title = {Revealing the relationship between architectural elements and source code characteristics},
year = {2014},
isbn = {9781450328791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597008.2597156},
doi = {10.1145/2597008.2597156},
abstract = {Understanding how a software system is structured, i.e. its architecture, is crucial for software comprehension. It allows developers to understand an implemented system and reason about how non-functional requirements are addressed. Yet, many systems lack any architectural documentation, or it is often outdated due to software evolution. In current practice, the process of recovering a system's architecture relies primarily on developer knowledge. Although existing architecture recovery approaches can help to identify architectural elements, these approaches require improvement to identify architectural concepts of a system automatically. Towards this goal, we analyze the usefulness of adopting different code-level characteristics to group elements into architectural modules. Our main contributions are an evaluation of the relationships between different sets of characteristics and their corresponding accuracies, and the evaluation results, which help us to understand which characteristics reveal information about the source code structure. Our experiment shows that an identified set of characteristics achieves an average accuracy of 80%, which indicates the usefulness of the considered characteristics for architecture recovery and thus to improving software comprehension.},
booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
pages = {14–25},
numpages = {12},
keywords = {Software architecture, architecture reconstruction, architecture recovery, source code characteristics},
location = {Hyderabad, India},
series = {ICPC 2014}
}

@inproceedings{10.1145/2020408.2020447,
author = {Mukerjee, Kunal and Porter, Todd and Gherman, Sorin},
title = {Linear scale semantic mining algorithms in microsoft SQL server's semantic platform},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020447},
doi = {10.1145/2020408.2020447},
abstract = {This paper describes three linear scale, incremental, and fully automatic semantic mining algorithms that are at the foundation of the new Semantic Platform being released in the next version of SQL Server. The target workload is large (10 -- 100 million) Enterprise document corpuses. At these scales, anything short of linear scale and incremental is costly to deploy. These three algorithms give rise to three weighted physical indexes: Tag Index (top keywords in each document); Document Similarity Index (top closely related documents given any document); and Semantic Phrase Similarity Index (top semantically related phrases, given any phrase), which are then query-able through the SQL interface. The need for specifically creating these three indexes was motivated by observing typical stages of document research, and gap analysis, given current tools and technology at the Enterprise. We describe the mining algorithms and architecture, and also outline some compelling user experiences that are enabled by the indexes.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {213–221},
numpages = {9},
keywords = {document similarity, incremental, keyword extraction, linear scale, semantic mining, semantic platform},
location = {San Diego, California, USA},
series = {KDD '11}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3357766.3359533,
author = {Izquierdo, Javier Luis C\'{a}novas and Cabot, Jordi},
title = {Analysis and modeling of the governance in general programming languages},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359533},
doi = {10.1145/3357766.3359533},
abstract = {General Programming Languages (GPLs) continuously evolve to adapt to the ever changing technology landscape. The evolution is rooted on technical aspects but it is ultimately decided by the group of people governing the language and working together to solve, vote and approve the new language extensions and modifications. As in any community, governance rules are used to manage the community, help to prioritize their tasks and come to a decision. Typically, these rules specify the decision-making mechanism used in the project, thus contributing to its long-term sustainability by clarifying how core language developers (external contributors and even end-users of the language) can work together. Despite their importance, this core topic has been largely ignored in the study of GPLs. In this paper we study eight well-known GPLs and analyze how they govern their evolution. We believe this study helps to clarify the different approaches GPLs use in this regard. These governance models, depicted as a feature model, can then be reused and mixed by developers of new languages to define their own governance.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {179–183},
numpages = {5},
keywords = {General Programming Language, Governance},
location = {Athens, Greece},
series = {SLE 2019}
}

@inproceedings{10.1109/SEAMS.2019.00023,
author = {Khakpour, Narges and Skandylas, Charilaos and Nariman, Goran Saman and Weyns, Danny},
title = {Towards secure architecture-based adaptations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00023},
doi = {10.1109/SEAMS.2019.00023},
abstract = {As any software system, a self-adaptive system is subject to security threats. However, applying self-adaptation may introduce additional threats. So far, little research has been devoted to this important problem. In this paper, we propose an approach for vulnerability analysis of architecture-based adaptations in self-adaptive systems using threat modeling and analysis techniques. To this end, we specify components' vulnerabilities and the system architecture formally and generate an attack model that describes the attacker's strategies to attack the system by exploiting different types of vulnerabilities. We use a set of security metrics to quantitatively assess the security risks of adaptations based on the produced attack model which enables the system to consider security aspects while choosing an adaptation to apply to the system. We automate and incorporate our approach into the Rainbow framework, allowing secure architectural adaptations at runtime. To evaluate the effectiveness of our approach, we apply it on a simple document storage system and on the ZNN system.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {114–125},
numpages = {12},
keywords = {graph-based threat models, security metrics, self-adaptation, vulnerability analysis},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@inbook{10.1145/3640508.3640525,
title = {The Deans},
year = {2024},
isbn = {9798400717741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3640508.3640525},
booktitle = {Rendering History: The Women of ACM-W},
pages = {241–282},
numpages = {42}
}

@proceedings{10.1145/3628516,
title = {IDC '24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@inproceedings{10.1145/2847263.2847269,
author = {Weisz, Gabriel and Melber, Joseph and Wang, Yu and Fleming, Kermin and Nurvitadhi, Eriko and Hoe, James C.},
title = {A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847269},
doi = {10.1145/2847263.2847269},
abstract = {The advent of FPGA acceleration platforms with direct coherent access to processor memory creates an opportunity for accelerating applications with irregular parallelism governed by large in-memory pointer-based data structures. This paper uses the simple reference behavior of a linked-list traversal as a proxy to study the performance potentials of accelerating these applications on shared-memory processor-FPGA systems. The linked-list traversal is parameterized by node layout in memory, per-node data payload size, payload dependence, and traversal concurrency to capture the main performance effects of different pointer-based data structures and algorithms. The paper explores the trade-offs over a wide range of implementation options available on shared-memory processor-FPGA architectures, including using tightly-coupled processor assistance. We make observations of the key effects on currently available systems including the Xilinx Zynq, the Intel QuickAssist QPI FPGA Platform, and the Convey HC-2. The key results show: (1) the FPGA fabric is least efficient when traversing a single list with non-sequential node layout and a small payload size; (2) processor assistance can help alleviate this shortcoming; and (3) when appropriate, a fabric only approach that interleaves multiple linked list traversals is an effective way to maximize traversal performance.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {264–273},
numpages = {10},
keywords = {cache coherence, fpga, heterogeneous systems, pointer chasing, shared memory},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/3053600.3053636,
author = {Ferme, Vincenzo and Pautasso, Cesare},
title = {Towards Holistic Continuous Software Performance Assessment},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053636},
doi = {10.1145/3053600.3053636},
abstract = {In agile, fast and continuous development lifecycles, software performance analysis is fundamental to confidently release continuously improved software versions. Researchers and industry practitioners have identified the importance of integrating performance testing in agile development processes in a timely and efficient way. However, existing techniques are fragmented and not integrated taking into account the heterogeneous skills of the users developing polyglot distributed software, and their need to automate performance practices as they are integrated in the whole lifecycle without breaking its intrinsic velocity. In this paper we present our vision for holistic continuous software performance assessment, which is being implemented in the BenchFlow tool. BenchFlow enables performance testing and analysis practices to be pervasively integrated in continuous development lifecycle activities. Users can specify performance activities (e.g., standard performance tests) by relying on an expressive Domain Specific Language for objective-driven performance analysis. Collected performance knowledge can be thus reused to speed up performance activities throughout the entire process.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {159–164},
numpages = {6},
keywords = {continuous integration, continuous software performance assessment, performance analysis, performance test},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {Software product line, actor, cross-domain reference architecture, multi product line, reference architecture},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@article{10.1145/3394602,
author = {Wang, Junjie and Yang, Ye and Menzies, Tim and Wang, Qing},
title = {iSENSE2.0: Improving Completion-aware Crowdtesting Management with Duplicate Tagger and Sanity Checker},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3394602},
doi = {10.1145/3394602},
abstract = {Software engineers get questions of “how much testing is enough” on a regular basis. Existing approaches in software testing management employ experience-, risk-, or value-based analysis to prioritize and manage testing processes. However, very few is applicable to the emerging crowdtesting paradigm to cope with extremely limited information and control over unknown, online crowdworkers. In practice, deciding when to close a crowdtesting task is largely done by experience-based guesswork and frequently results in ineffective crowdtesting. More specifically, it is found that an average of 32% testing cost was wasteful spending in current crowdtesting practice. This article intends to address this challenge by introducing automated decision support for monitoring and determining appropriate time to close crowdtesting tasks.To that end, it first investigates the necessity and feasibility of close prediction of crowdtesting tasks based on an industrial dataset. Next, it proposes a close prediction approach named iSENSE2.0, which applies incremental sampling technique to process crowdtesting reports arriving in chronological order and organizes them into fixed-sized groups as dynamic inputs. Then, a duplicate tagger analyzes the duplicate status of received crowd reports, and a CRC-based (Capture-ReCapture) close estimator generates the close decision based on the dynamic bug arrival status. In addition, a coverage-based sanity checker is designed to reinforce the stability and performance of close prediction. Finally, the evaluation of iSENSE2.0 is conducted on 56,920 reports of 306 crowdtesting tasks from one of the largest crowdtesting platforms. The results show that a median of 100% bugs can be detected with 30% saved cost. The performance of iSENSE2.0 does not demonstrate significant difference with the state-of-the-art approach iSENSE, while the later one relies on the duplicate tag, which is generally considered as time-consuming and tedious to obtain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {24},
numpages = {27},
keywords = {Crowdsourced testing, capture-recapture, close prediction, term coverage, test management}
}

@article{10.1145/3241383,
author = {Avrahami, Daniel and Patel, Mitesh and Yamaura, Yusuke and Kratz, Sven and Cooper, Matthew},
title = {Unobtrusive Activity Recognition and Position Estimation for Work Surfaces Using RF-Radar Sensing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3241383},
doi = {10.1145/3241383},
abstract = {Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users’ privacy concerns associated with cameras and is useful for a wide range of intelligent systems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {11},
numpages = {28},
keywords = {Activity recognition, IMU, deskwork, radio frequency radar sensor, retail, sensing}
}

@inproceedings{10.1145/3377811.3380927,
author = {Alrajeh, Dalal and Cailliau, Antoine and van Lamsweerde, Axel},
title = {Adapting requirements models to varying environments},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380927},
doi = {10.1145/3377811.3380927},
abstract = {The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set.The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {50–61},
numpages = {12},
keywords = {context-dependent requirements, formal verification, logic-based learning, requirements adaptation, requirements evolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/62972.63013,
author = {Worlton, J.},
title = {Some patterns of technological change in high-performance computers},
year = {1988},
isbn = {081860882X},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {High-performance computer technology is undergoing a period of unusually rapid change, and this paper attempts to describe the patterns of these changes in a systematic way. Pattern recognition is the basis of technology forecasting, and it is through technology forecasting that we obtain the anticipatory information that allows us to avoid problems and create opportunities. We will first identify the stages in which technological changes occur, and then define “change” as the first derivative of an information function that describes the state of a technology. We then explore the driving forces that cause three generic patterns of technological change: incremental, exponential, and logistic. Some areas in high-performance computer technology that are following these patterns are then identified, and a forecast is developed. Finally, some limits of high-performance computing are discussed.},
booktitle = {Proceedings of the 1988 ACM/IEEE Conference on Supercomputing},
pages = {312–320},
numpages = {9},
location = {Orlando, Florida, USA},
series = {Supercomputing '88}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@inproceedings{10.1145/3276604.3276983,
author = {Cimini, Matteo},
title = {Languages as first-class citizens (vision paper)},
year = {2018},
isbn = {9781450360296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276604.3276983},
doi = {10.1145/3276604.3276983},
abstract = {In this paper, we introduce languages as first-class citizens as a sub-paradigm of language-oriented programming. In this approach, language definitions are in the context of a general purpose programming language with the same status as any other expression. In particular, language definitions are elevated to be run-time values, that can be assigned to variables, passed to functions, returned by functions, and inserted into lists, to name a few possibilities. This approach offers flexible features in the run-time creation and modification of languages, and may promote new idioms in language-oriented programming. As a proof of concept, we have designed and implemented lang-n-play, a functional language with languages as first-class citizens. We present the features of lang-n-play with an example, and show that they naturally enable dynamic programming scenarios.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {65–69},
numpages = {5},
keywords = {functional languages, language design, language-oriented programming},
location = {Boston, MA, USA},
series = {SLE 2018}
}

@article{10.1145/3143561,
author = {Chen, Tsong Yueh and Kuo, Fei-Ching and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Tse, T. H. and Zhou, Zhi Quan},
title = {Metamorphic Testing: A Review of Challenges and Opportunities},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3143561},
doi = {10.1145/3143561},
abstract = {Metamorphic testing is an approach to both test case generation and test result verification. A central element is a set of metamorphic relations, which are necessary properties of the target function or algorithm in relation to multiple inputs and their expected outputs. Since its first publication, we have witnessed a rapidly increasing body of work examining metamorphic testing from various perspectives, including metamorphic relation identification, test case generation, integration with other software engineering techniques, and the validation and evaluation of software systems. In this article, we review the current research of metamorphic testing and discuss the challenges yet to be addressed. We also present visions for further improvement of metamorphic testing and highlight opportunities for new research.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {4},
numpages = {27},
keywords = {Metamorphic testing, metamorphic relation, oracle problem, test case generation}
}

@article{10.1145/3287070,
author = {Volanschi, Nic and Serpette, Bernard and Carteron, Adrien and Consel, Charles},
title = {A Language for Online State Processing of Binary Sensors, Applied to Ambient Assisted Living},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287070},
doi = {10.1145/3287070},
abstract = {There is a large variety of binary sensors in use today, and useful context-aware services can be defined using such binary sensors. However, the currently available approaches for programming context-aware services do not conveniently support binary sensors. Indeed, no existing approach simultaneously supports a notion of state, central to binary sensors, offers a complete set of operators to compose states, allows to define reusable abstractions by means of such compositions, and implements efficient online processing of these operators.This paper proposes a new language for event processing specifically targeted to binary sensors. The central contributions of this language are a native notion of state and semi-causal operators for temporal state composition including: Allen's interval relations generalized for handling multiple intervals, and temporal filters for handling delays. Compared to other approaches such as CEP (complex event processing), our language provides less discontinued information, allows less restricted compositions, and supports reusable abstractions. We implemented an interpreter for our language and applied it to successfully rewrite a full set of real Ambient Assisted Living services. The performance of our prototype interpreter is shown to compete well with a commercial CEP engine when expressing the same services.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {192},
numpages = {26},
keywords = {Allen interval algebra, Ambient assisted living, Binary sensors, Smart homes}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@inproceedings{10.1145/2593882.2593885,
author = {Orso, Alessandro and Rothermel, Gregg},
title = {Software testing: a research travelogue (2000–2014)},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593885},
doi = {10.1145/2593882.2593885},
abstract = {Despite decades of work by researchers and practitioners on numerous software quality assurance techniques, testing remains one of the most widely practiced and studied approaches for assessing and improving software quality. Our goal, in this paper, is to provide an accounting of some of the most successful research performed in software testing since the year 2000, and to present what appear to be some of the most significant challenges and opportunities in this area. To be more inclusive in this effort, and to go beyond our own personal opinions and biases, we began by contacting over 50 of our colleagues who are active in the testing research area, and asked them what they believed were (1) the most significant contributions to software testing since 2000 and (2) the greatest open challenges and opportunities for future research in this area. While our colleagues’ input (consisting of about 30 responses) helped guide our choice of topics to cover and ultimately the writing of this paper, we by no means claim that our paper represents all the relevant and noteworthy research performed in the area of software testing in the time period considered—a task that would require far more space and time than we have available. Nevertheless, we hope that the approach we followed helps this paper better reflect not only our views, but also those of the software testing community in general.},
booktitle = {Future of Software Engineering Proceedings},
pages = {117–132},
numpages = {16},
keywords = {Software testing},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up architectural SW health builds in a new product line generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {architectural checks, architectural technical debt, embedded software, product line development, software architecture, software erosion, technical debt},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1145/2642803.2642830,
author = {Petrov, Plamen and Nord, Robert L. and Buy, Ugo},
title = {Probabilistic Macro-Architectural Decision Framework},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642830},
doi = {10.1145/2642803.2642830},
abstract = {Experience with system-level concerns demonstrates that fitness for context is a consideration that is equally significant in making architectural decisions as is fitness for purpose. This requires architects to consider contextual factors in making decisions. These decisions are probabilistic in nature and they represent the subjective belief of the architect or the prior probability which is likely to change as new evidence becomes available during the course of the system design. They serve as recommendations and directional inputs to other decisions in the design process. In this paper, we introduce a macro-architectural decision framework we developed to enable the architect for a software-reliant system to model and reason about contextual factors. At the core of our framework is an adaptation of a Bayesian belief network that is augmented with decision and utility nodes. The framework captures contextual factors and their influence on decisions and utilities. We applied our approach in the study of a software system implementation at a healthcare company. The results show promise that such decision support tools help explore the space of factors involved in decision making and provide sensible suggestions for making architectural decisions.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {27},
numpages = {8},
keywords = {Bayesian belief network, Bayesian net, architecture decision network, contextual factors, decision network, enterprise, influence diagram, macro-architecture, probabilistic graphical model, software architecture, systems},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2593801.2593802,
author = {Cleland-Huang, Jane and Guo, Jin},
title = {Towards more intelligent trace retrieval algorithms},
year = {2014},
isbn = {9781450328463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593801.2593802},
doi = {10.1145/2593801.2593802},
abstract = {Automated trace creation techniques are based on a variety of algorithms ranging from basic term matching approaches to more sophisticated expert systems. In this position paper we propose a classification scheme for categorizing the intelligence level of automated traceability techniques. We show that the vast majority of relevant work in the past decade has been focused at the lowest level of the Traceability Intelligence Quotient (tIQ) and posit that achieving high quality automated traceability will require re-focusing research efforts on the development of more intelligent algorithms capable of reasoning about concepts, their relationships and constraints, and the contexts in which they occur.},
booktitle = {Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {Domain Ontology, Expert System, Traceability},
location = {Hyderabad, India},
series = {RAISE 2014}
}

@inproceedings{10.1145/98894.98916,
author = {Miyabe, Yutaka and Dairiki, Osamu and Minami, Etsuro},
title = {ESTO: a practical environment for industrial diagnostics systems},
year = {1990},
isbn = {0897913728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98894.98916},
doi = {10.1145/98894.98916},
abstract = {This paper describes an integrated environment for industrial diagnostic expert system development.},
booktitle = {Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2},
pages = {684–691},
numpages = {8},
location = {Charleston, South Carolina, USA},
series = {IEA/AIE '90}
}

@inproceedings{10.1145/2797433.2797494,
author = {Herold, Sebastian and Buckley, Jim},
title = {Feature-Oriented Reflexion Modelling},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797494},
doi = {10.1145/2797433.2797494},
abstract = {Reflexion Modelling is an industrially proven and well-established technique for the reconstruction of software architectures. Studies have shown, users of this technique not only envisage it as a means of recovering the logical module structure of the system but also envisage it as a means of modelling a high-level functional view on the system, in which architectural entities represent user-observable functionality. In this case, the challenging task of manual mapping source code to modules in the reflexion model becomes the task of feature location.In this article, we describe the idea of Feature-Oriented Reflexion Modelling in which just-in-time Reflexion Modelling is combined with automatic Feature Location techniques, helping the system expert define that mapping efficiently and thus subsequently analyse, explore, and model the inter-feature dependencies in systems. Feature Location techniques are applied to suggest mapping extensions to the reflexion model to the user, leading to an incremental and interactive mapping process. Furthermore, we outline a research agenda towards an industrially applicable realisation.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {59},
numpages = {7},
keywords = {feature location, reflexion modelling, software architecture reconstruction},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/2897053.2897060,
author = {Incerto, Emilio and Tribastone, Mirco and Trubiani, Catia},
title = {Symbolic performance adaptation},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897060},
doi = {10.1145/2897053.2897060},
abstract = {Quality-of-Service attributes such as performance and reliability heavily depend on the run-time conditions under which software is executed (e.g., workload fluctuation and resources availability). Therefore, it is important to design systems able to adapt their setting and behavior due to these run-time variabilities. In this paper we propose a novel approach based on queuing networks as the quantitative model to represent system configurations. To find a model that fits with continuous changes in run-time conditions we rely on an innovative combination of symbolic analysis and satisfiability modulo theory (SMT). Through symbolic analysis we represent all possible system configurations as a set of nonlinear real constraints. By formulating an SMT problem we are able to devise feasible system configurations at a small computational cost. We study the effectiveness and scalability of our approach on a three-tier web system featuring different levels of redundancy.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {140–150},
numpages = {11},
keywords = {performance-based adaptation, queueing networks, satisfiability modulo theories, symbolic analysis},
location = {Austin, Texas},
series = {SEAMS '16}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/2184751.2184832,
author = {Yoon, Jae Yoel and Kim, Iee Joon and Lim, Ji Yeon and Kim, Seung Kwan and Kim, Ung Mo},
title = {Research to recommend influencially product group about interest through the TKMA (Transformed K-Means Algorithm)},
year = {2012},
isbn = {9781450311724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2184751.2184832},
doi = {10.1145/2184751.2184832},
abstract = {The form of the knowledge information service has changed. The social network service, that is the new form giving and can take the information based on the user's relation, appeared. Recently, social-commerce emerges based on the social network. Recently, the pavement of the Facebook, Social shopping likes the Groupon, that is the partial form and etc., shows up as one example of the Social Commerce. In the social network, this paper utilizes the clustering analysis among data mining technology and tries to provide information for recommending the goods group which is effective and where there is the influence to the social users in the weak tie.},
booktitle = {Proceedings of the 6th International Conference on Ubiquitous Information Management and Communication},
articleno = {66},
numpages = {4},
keywords = {clustering, recommendation, social networks},
location = {Kuala Lumpur, Malaysia},
series = {ICUIMC '12}
}

@inproceedings{10.1145/3180155.3180210,
author = {Miranda, Breno and Cruciani, Emilio and Verdecchia, Roberto and Bertolino, Antonia},
title = {FAST approaches to scalable similarity-based test case prioritization},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180210},
doi = {10.1145/3180155.3180210},
abstract = {Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {222–232},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/3398665,
author = {Cerrolaza, Jon Perez and Obermaisser, Roman and Abella, Jaume and Cazorla, Francisco J. and Gr\"{u}ttner, Kim and Agirre, Irune and Ahmadian, Hamidreza and Allende, Imanol},
title = {Multi-core Devices for Safety-critical Systems: A Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398665},
doi = {10.1145/3398665},
abstract = {Multi-core devices are envisioned to support the development of next-generation safety-critical systems, enabling the on-chip integration of functions of different criticality. This integration provides multiple system-level potential benefits such as cost, size, power, and weight reduction. However, safety certification becomes a challenge and several fundamental safety technical requirements must be addressed, such as temporal and spatial independence, reliability, and diagnostic coverage. This survey provides a categorization and overview at different device abstraction levels (nanoscale, component, and device) of selected key research contributions that support the compliance with these fundamental safety requirements.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {38},
keywords = {Fault tolerance, diagnostic coverage, spatial independence, time independence}
}

@inproceedings{10.1145/2884781.2884853,
author = {Mirzaei, Nariman and Garcia, Joshua and Bagheri, Hamid and Sadeghi, Alireza and Malek, Sam},
title = {Reducing combinatorics in GUI testing of android applications},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884853},
doi = {10.1145/2884781.2884853},
abstract = {The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible combinations is the ideal upper bound in combinatorial testing, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These specifications express the app's behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid's ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {559–570},
numpages = {12},
keywords = {android, input generation, software testing},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/2659118.2659136,
author = {Zhou, Jingang and Yin, Kun},
title = {Automated web testing based on textual-visual UI patterns: the UTF approach},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659136},
doi = {10.1145/2659118.2659136},
abstract = {Automated software testing is the only resort for delivering quality software, since there are usually large test suites to be executed, especially for regression testing. Though many automated testing tools and techniques have been developed, they still do not solve all problems like cost and maintenance, and they can even be brittle in some situations, thus confining their adoption. To address these issues, we develop a pattern-based automated testing framework, called UTF (User-oriented Testing Framework), for Web applications. UTF encodes textual-visual information about and relationships between widgets into a domain specific language for test scripts based on the underlying invariant structural patterns in the DOM, which allows test scripts to be easily created and maintained. In addition, UTF provides flexible extension and customization capabilities to make it adaptable for various Web-application scenarios. Our experiences show UTF can greatly reduce the cost of adopting automated testing and facilitate its institutionalization.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–6},
numpages = {6},
keywords = {automated testing, domain-specific language, user-interface pattern, web application}
}

@inproceedings{10.1145/2024445.2024450,
author = {Ernst, Neil A. and Borgida, Alexander and Mylopoulos, John},
title = {Requirements evolution drives software evolution},
year = {2011},
isbn = {9781450308489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024445.2024450},
doi = {10.1145/2024445.2024450},
abstract = {Changes to software should be made with reference to the requirements of that software, as these requirements provide the reasons for a change. Requirements serve to tie the implementation world of the developers to the problem world of the stakeholders. Most empirical studies of requirements have shown that misunderstood and changing requirements cause the majority of failures and costs in software. However, research in software evolution has typically focused on how to evolve software and not why. In our view, evolving software is about solving requirements problems, that is, finding new implementations which will satisfy the requirements while respecting domain assumptions. We argue that by describing this relationship, an implementation choice that best meets stakeholder needs can be made. We describe a tool that models requirements problems. This tool can find incremental solutions to evolving requirements problems quickly.},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution},
pages = {16–20},
numpages = {5},
keywords = {goal-oriented modeling, requirements evolution, unanticipated change},
location = {Szeged, Hungary},
series = {IWPSE-EVOL '11}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@proceedings{10.1145/3593663,
title = {ECSEE '23: Proceedings of the 5th European Conference on Software Engineering Education},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seeon/Bavaria, Germany}
}

@proceedings{10.1145/3563835,
title = {Onward! 2022: Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2022},
isbn = {9781450399098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to Onward! 2022. Onward! is a premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities, and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research. Onward! 2022 is part of SPLASH 2022, taking place from Monday 5th to Saturday 10th December 2022 in Auckland, New Zealand.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3641237,
title = {SIGDOC '24: Proceedings of the 42nd ACM International Conference on Design of Communication},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fairfax, VA, USA}
}

@inproceedings{10.1145/2254129.2254183,
author = {Duan, Huiying and Liu, Feifei},
title = {Building and managing reputation in the environment of Chinese e-commerce: a case study on Taobao},
year = {2012},
isbn = {9781450309158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254129.2254183},
doi = {10.1145/2254129.2254183},
abstract = {We propose a lightweight reputation model R-Rep, for resisting manipulative behavior in Reputation Systems. We present a manipulative behavior detection system CSI to detect the customers who provide manipulative ratings, and the vendors who intend to increase their reputation value in a strategic manner. Via analysing motivation of manipulative behavior, we specify features for identifying suspicious customers using clustering algorithm. Utilizing the inherent relationship between suspicious customers and suspicious vendors, the first set of suspicious vendors is identified by CSI. Meanwhile, using different pieces of information, which refer to non-anonymous ratings and anonymous ratings, the second and the third sets of suspicious vendors are detected by CSI. We designed two universal approaches RVA and BVA to compare different reputation models with regard to resisting manipulative behavior. The comparison approaches are applied to a set of suspicious vendors identified by CSI. Results show that, R-Rep outperforms two existing models, the reputation model employed by Taobao (the largest e-commerce site in China) and a Bayesian System. The two comparing approaches RVA and BVA have inherent consistency.},
booktitle = {Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics},
articleno = {43},
numpages = {10},
keywords = {Taobao, manipulative behavior detection, reputation model, reputation system, trust management system},
location = {Craiova, Romania},
series = {WIMS '12}
}

@proceedings{10.1145/3628034,
title = {EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs},
year = {2023},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@inproceedings{10.1145/1858996.1859096,
author = {Becker, Michael and Gruhn, Volker},
title = {Automated model grouping},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859096},
doi = {10.1145/1858996.1859096},
abstract = {A tremendous amount of software models has been created so far. This growing number of models adds to the fact that it gets more and more difficult to organise, structure, and reuse them. Thereby new software development projects cannot profit from existing knowledge. In our research we will study existing natural language processing techniques for their adaptability in the reuse of software models. We will research methods to group existing models according to their functionality.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {493–498},
numpages = {6},
keywords = {grouping, model reusing, structuring},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@proceedings{10.1145/3526071,
title = {RoSE '22: Proceedings of the 4th International Workshop on Robotics Software Engineering},
year = {2022},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. The research communities advancing software engineering in robotics, however, are spread over various spe-cialized conferences, such as ICRA, IROS, SIMPAR - each attended mostly by robotics researchers and practitioners - or ICSE andMODELS - mostly attended by software engineering researchers and practitioners. At robotics conferences, software engineering lacks visibility and vice versa.The objective of RoSE is bringing together researchers and practitioners from both domains at a prominent conference to foster cross-fertilization between the two domains. Being the most prominent conference in software engineering, ICSE is the best venue to attract experts from both domains. Hosting this workshop at ICSE enables software engineering researchers to learn more about the challenges of robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions are unnoticed by roboticists, yet.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.5555/2337223.2337301,
author = {Balasubramaniam, Dharini and Jefferson, Christopher and Kotthoff, Lars and Miguel, Ian and Nightingale, Peter},
title = {An automated approach to generating efficient constraint solvers},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Combinatorial problems appear in numerous settings, from timetabling to industrial design. Constraint solving aims to find solutions to such problems efficiently and automatically. Current constraint solvers are monolithic in design, accepting a broad range of problems. The cost of this convenience is a complex architecture, inhibiting efficiency, extensibility and scalability. Solver components are also tightly coupled with complex restrictions on their configuration, making automated generation of solvers difficult.  We describe a novel, automated, model-driven approach to generating efficient solvers tailored to individual problems and present some results from applying the approach. The main contribution of this work is a solver generation framework called Dominion, which analyses a problem and, based on its characteristics, generates a solver using components chosen from a library. The key benefit of this approach is the ability to solve larger and more difficult problems as a result of applying finer-grained optimisations and using specialised techniques as required.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {661–671},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2611040.2611091,
author = {Cohen, Stephen and Money, William and Quick, Michele},
title = {Improving Integration and Insight in Smart Cities with Policy and Trust},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611091},
doi = {10.1145/2611040.2611091},
abstract = {This paper examines the issues of policy and trust in the context of IT infrastructures for Smart Cities. This paper proposes that trusted Smart city policies can lead to the development of trusted foundational service underlying all smart city solutions. Such a set services are critical for architectural choices of data integration and use within smart city domains, and will lead to the development of a marketplace where service providers and consumers engage in a free and fully informed exchange to choose worthy and reliable experiences to address everything from reporting street light outage to identifying economic advantages during city planning. It argues that two usually mutually exclusive architectural meta-models; Centralization and Federation, are required to achieve a set of trusted foundational services. It reviews the large array of options for implementing the marketplace component of the foundational services to support scenarios varying from fully isolated well known analytics to the anonymous access that allows potential users to browse for services without any controls. It concludes that Trusted Policies are highly important as successful ingredients in the development of foundational services and the following developmental stage, and in the operations and maintenance stages for integrated Smart city systems. It is critical that Smart cities systems implement city-wide policies that improve and sustain trust that in turn help Smart cities manage across the multitude of systems that are in both developmental and operational stages simultaneously, and will be so for many decades to come.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {57},
numpages = {9},
keywords = {IT infrastructures, Policy, foundational services, integrated solutions, trust},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@proceedings{10.1145/3568813,
title = {ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3524614,
title = {IWSiB '22: Proceedings of the 5th International Workshop on Software-intensive Business: Towards Sustainable Software Business},
year = {2022},
isbn = {9781450393027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {There are many researchers and practitioners whose work is related to the field of software-intensive business. However, they are often not fully aware of each other's work as the research is scattered. For example, individual research contributions have emerged related to, for example, software engineering economics, digital ecosystems and software startups. The goal of the workshop on Software-intensive Business is to bring these different sub-fields together and strengthen their ties.},
location = {Pittsburgh, Pennsylvania}
}

@article{10.1145/3563294,
author = {Sun, Yaozhu and Dhandhania, Utkarsh and Oliveira, Bruno C. d. S.},
title = {Compositional embeddings of domain-specific languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563294},
doi = {10.1145/3563294},
abstract = {A common approach to defining domain-specific languages (DSLs) is via a direct embedding into a host language. There are several well-known techniques to do such embeddings, including shallow and deep embeddings. However, such embeddings come with various trade-offs in existing programming languages. Owing to such trade-offs, many embedded DSLs end up using a mix of approaches in practice, requiring a substantial amount of code, as well as some advanced coding techniques.  

In this paper, we show that the recently proposed Compositional Programming paradigm and the CP language provide improved support for embedded DSLs. In CP we obtain a new form of embedding, which we call a compositional embedding, that has most of the advantages of both shallow and deep embeddings. On the one hand, compositional embeddings enable various forms of linguistic reuse that are characteristic of shallow embeddings, including the ability to reuse host-language optimizations in the DSL and add new DSL constructs easily. On the other hand, similarly to deep embeddings, compositional embeddings support definitions by pattern matching or dynamic dispatching (including dependent interpretations, transformations, and optimizations) over the abstract syntax of the DSL and have the ability to add new interpretations. We illustrate an instance of compositional embeddings with a DSL for document authoring called ExT. The DSL is highly flexible and extensible, allowing users to create various non-trivial extensions easily. For instance, ExT supports various extensions that enable the production of wiki-like documents, LaTeX documents, vector graphics or charts. The viability of compositional embeddings for ExT is evaluated with three applications.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {131},
numpages = {29},
keywords = {Compositional Programming, Extensible Typesetting}
}

@inproceedings{10.1145/2568225.2568251,
author = {Zhang, Sai and Ernst, Michael D.},
title = {Which configuration option should I change?},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568251},
doi = {10.1145/2568225.2568251},
abstract = {Modern software often exposes configuration options that enable users to customize its behavior. During software evolution, developers may change how the configuration options behave. When upgrading to a new software version, users may need to re-configure the software by changing the values of certain configuration options.  This paper addresses the following question during the evolution of a configurable software system: which configuration options should a user change to maintain the software's desired behavior? This paper presents a technique (and its tool implementation, called ConfSuggester) to troubleshoot configuration errors caused by software evolution. ConfSuggester uses dynamic profiling, execution trace comparison, and static analysis to link the undesired behavior to its root cause - a configuration option whose value can be changed to produce desired behavior from the new software version.  We evaluated ConfSuggester on 8 configuration errors from 6 configurable software systems written in Java. For 6 errors, the rootcause configuration option was ConfSuggester's first suggestion. For 1 error, the root cause was ConfSuggester's third suggestion. The root cause of the remaining error was ConfSuggester's sixth suggestion. Overall, ConfSuggester produced significantly better results than two existing techniques. ConfSuggester runs in just a few minutes, making it an attractive alternative to manual debugging.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {152–163},
numpages = {12},
keywords = {Configuration error diagnosis, Software evolution},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1145/3386331,
author = {Moler, Cleve and Little, Jack},
title = {A history of MATLAB},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386331},
doi = {10.1145/3386331},
abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {81},
numpages = {67},
keywords = {MATLAB, linear algebra, matrix computation}
}

@inproceedings{10.1145/2896941.2896945,
author = {Smiley, Karen and Harding, Jeff and Patel, Pankesh},
title = {From ideas to implementations: closing the gaps between technical experts and software solutions},
year = {2016},
isbn = {9781450341578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896941.2896945},
doi = {10.1145/2896941.2896945},
abstract = {Rapid delivery strategies strive to balance critical performance qualities vs. reducing the time between an idea and deployment of a software implementation of that idea. For industrial software solutions that encapsulate expertise in deliverable components, technical SMEs (Subject Matter Experts) with ideas and knowledge have traditionally partnered as requirements providers with software development teams. These human processes are not optimally fast, are vulnerable to errors in translating or interpreting requirements, and do not scale when software teams need to integrate the knowledge of many SMEs into multiple software solutions and deployments. To address these limitations, ABB has pursued an industrial research initiative for innovative SME toolsets with focus on two goals: to accelerate the creation, evolution, reuse, and delivery of expert algorithms, and to streamline the deployment of these algorithms into releases and fielded solutions. The vision underpinning the initiative is to empower technical SMEs as "end-user developers" to convert their knowledge into reusable software solution components without having to learn, perform, or partner on traditional software development, integration, or deployment. In this paper, we summarize our experiences and lessons learned to date from this initiative, key continuing challenges, and some positional thoughts on how end-user development by technical SMEs aligns with emerging approaches for rapid delivery and evolution.},
booktitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery},
pages = {1–4},
numpages = {4},
keywords = {continuous deployment, end-user programming, end-user software engineering, industrial analytics, subject matter expert, visual programming environments},
location = {Austin, Texas},
series = {CSED '16}
}

@article{10.1145/882240.882242,
author = {staff, ACM SIGSOFT Software Engineering Notes},
title = {Back matter},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/882240.882242},
doi = {10.1145/882240.882242},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {38–48},
numpages = {11}
}

@inproceedings{10.1145/2933242.2935866,
author = {Cauchi, Abigail and Pace, Gordon},
title = {Using testing techniques to classify user interface designs},
year = {2016},
isbn = {9781450343220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933242.2935866},
doi = {10.1145/2933242.2935866},
abstract = {Number entry systems in medical devices such as infusion pumps are used to input drug doses that will be administered to patients. They are safety critical since if the drug dose is too high or too low, this may cause harm to patients.Previous work shows that number entry systems with the same hardware layout can have software that is implemented in different ways. This means that devices with the same hardware layout may lead to different results after the same keystrokes are pressed. Previous work also shows that choosing the best software implementation over the worst can reduce the likelihood of human error eight-fold in directional number entry systems.Determining whether a software implementation abides by the requirements is a time consuming task for regulatory bodies and hospital procurement departments. In this paper we show how software testing techniques can be used to classify various software implementations in order to determine whether the given number entry system satisfies specifications.},
booktitle = {Proceedings of the 8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {159–164},
numpages = {6},
keywords = {medical devices, program classification, testing, user interfaces},
location = {Brussels, Belgium},
series = {EICS '16}
}

@inproceedings{10.1145/1858996.1859010,
author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {Variability modeling in the real: a perspective from the operating systems domain},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859010},
doi = {10.1145/1858996.1859010},
abstract = {Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {configuration, empirical software engineering, feature models, product line architectures, variability modeling},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1007/978-3-642-12261-3_9,
author = {Becker, Basil and Giese, Holger and Neumann, Stefan and Schenck, Martin and Treffer, Arian},
title = {Model-Based extension of AUTOSAR for architectural online reconfiguration},
year = {2009},
isbn = {3642122604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12261-3_9},
doi = {10.1007/978-3-642-12261-3_9},
abstract = {In the last few years innovations in the automotive domain have been realized by software, leading to a dramatically increased complexity of such systems. Additionally, automotive systems have to be flexible and robust, e.g., to be able to deal with failures of sensors, actuators or other constituents of an automotive system. One possibility to achieve robustness and flexibility in automotive systems is the usage of reconfiguration capabilities. However, adding such capabilities introduces an even higher degree of complexity. To avoid this drawback we propose to integrate reconfiguration capabilities into AUTOSAR, an existing framework supporting the management of such a complex system at the architectural level. Elaborated and expensive tools and toolchains assist during the development of automotive systems. Hence, we present how our reconfiguration solution has been seamlessly integrated into such a toolchain.},
booktitle = {Proceedings of the 2009 International Conference on Models in Software Engineering},
pages = {83–97},
numpages = {15},
location = {Denver, CO},
series = {MODELS'09}
}

@proceedings{10.1145/3580507,
title = {EC '23: Proceedings of the 24th ACM Conference on Economics and Computation},
year = {2023},
isbn = {9798400701047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the course of two decades, EC has established itself as one of the few truly successful interdisciplinary conferences, attracting papers and participants with a broad range of interests in economics and computer science, and fostering work in the intersection.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/2866614.2866615,
author = {Melo, Jean and Flesborg, Elvis and Brabrand, Claus and W\k{a}sowski, Andrzej},
title = {A Quantitative Analysis of Variability Warnings in Linux},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866615},
doi = {10.1145/2866614.2866615},
abstract = {In order to get insight into challenges with quality in highly-configurable software, we analyze one of the largest open source projects, the Linux kernel, and quantify basic properties of configuration-related warnings. We automatically analyze more than 20 thousand valid and distinct random configurations, in a computation that lasted more than a month. We count and classify a total of 400,000 warnings to get an insight in the distribution of warning types, and the location of the warnings. We run both on a stable and unstable version of the Linux kernel. The results show that Linux contains a significant amount of configuration-dependent warnings, including many that appear harmful. In fact, it appears that there are no configuration-independent warnings in the kernel at all, adding to our knowledge about relevance of family-based analyses.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–8},
numpages = {6},
keywords = {Highly-Configurable Systems, Linux, Preprocessors, Quantitative Analysis, Variability Warnings},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/333175.333882,
author = {Kumar, Amruth},
title = {Announcements},
year = {2000},
issue_date = {Spring 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1523-8822},
url = {https://doi.org/10.1145/333175.333882},
doi = {10.1145/333175.333882},
journal = {Intelligence},
month = apr,
pages = {41–48},
numpages = {8}
}

@inproceedings{10.5555/2814058.3252438,
author = {Cappelli, Claudia and Ferreira, Arnaldo Alves},
title = {Session details: Special Track - Experience Reports in Industry and Case Studies},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@article{10.1145/1148094.1148102,
author = {Ebling, Maria R.},
title = {HotMobile 2006: workshop on mobile computing systems and applications overview of workshop},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1559-1662},
url = {https://doi.org/10.1145/1148094.1148102},
doi = {10.1145/1148094.1148102},
abstract = {Like the first WMCSA, the goal of this workshop was to foster interaction between practitioners of mobile computing. In keeping with this goal, we decided to return a small, informal workshop with few papers, but significant discussions. We accepted just nine papers, but we had two significant group discussions, two exciting panels, and an insightful keynote address. Approximately 40 people attended the two-day event.},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = jul,
pages = {70–78},
numpages = {9}
}

@inproceedings{10.1145/1282280.1282355,
author = {Schietse, Jan and Eakins, John P. and Veltkamp, Remco C.},
title = {Practice and challenges in trademark image retrieval},
year = {2007},
isbn = {9781595937339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1282280.1282355},
doi = {10.1145/1282280.1282355},
abstract = {In this paper, we outline some of the main challenges facing trademark searchers today, and discuss the extent to which current automated systems are meeting those challenges.},
booktitle = {Proceedings of the 6th ACM International Conference on Image and Video Retrieval},
pages = {518–524},
numpages = {7},
keywords = {content-based image retrieval, pattern matching, trademark similarity},
location = {Amsterdam, The Netherlands},
series = {CIVR '07}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2487575.2488190,
author = {Aggour, Kareem S. and Hoogs, Bethany},
title = {Financing lead triggers: empowering sales reps through knowledge discovery and fusion},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488190},
doi = {10.1145/2487575.2488190},
abstract = {Sales representatives must have access to meaningful and actionable intelligence about potential customers to be effective in their roles. Historically, GE Capital Americas sales reps identified leads by manually searching through news reports and financial statements either in print or online. Here we describe a system built to automate the collection and aggregation of information on companies, which is then mined to identify actionable sales leads. The Financing Lead Triggers system is comprised of three core components that perform information fusion, knowledge discovery and information visualization. Together these components extract raw data from disparate sources, fuse that data into information, and then automatically mine that information for actionable sales leads driven by a combination of expert-defined and statistically derived triggers. A web-based interface provides sales reps access to the company information and sales leads in a single location. The use of the Lead Triggers system has significantly improved the performance of the sales reps, providing them with actionable intelligence that has improved their productivity by 30-50%. In 2010, Lead Triggers provided leads on opportunities that represented over $44B in new deal commitments for GE Capital.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1141–1149},
numpages = {9},
keywords = {commercial finance, information fusion, knowledge discovery, lead generation, sales leads},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A retrospective analysis of SAC requirements: engineering track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {26–41},
numpages = {16},
keywords = {SAC, relevance, requirements engineering, retrospective, scoping study, symposium on applied computing, systematic mapping study, trends}
}

@inproceedings{10.1145/2642937.2643002,
author = {Campos, Jos\'{e} and Arcuri, Andrea and Fraser, Gordon and Abreu, Rui},
title = {Continuous test generation: enhancing continuous integration with automated test generation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643002},
doi = {10.1145/2642937.2643002},
abstract = {In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {55–66},
numpages = {12},
keywords = {automated test generation, continuous integration, continuous testing, unit testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/3276492,
author = {Koppel, James and Premtoon, Varot and Solar-Lezama, Armando},
title = {One tool, many languages: language-parametric transformation with incremental parametric syntax},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276492},
doi = {10.1145/3276492},
abstract = {We present a new approach for building source-to-source transformations that can run on multiple programming languages, based on a new way of representing programs called incremental parametric syntax. We implement this approach in Haskell in our Cubix system, and construct incremental parametric syntaxes for C, Java, JavaScript, Lua, and Python. We demonstrate a whole-program refactoring tool that runs on all of them, along with three smaller transformations that each run on several. Our evaluation shows that (1) once a transformation is written, little work is required to configure it for a new language (2) transformations built this way output readable code which preserve the structure of the original, according to participants in our human study, and (3) our transformations can still handle language corner-cases, as validated on compiler test suites.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {122},
numpages = {28},
keywords = {expression problem, program transformation, refactoring}
}

@book{10.1145/3382097,
author = {Allemang, Dean and Hendler, Jim and Gandon, Fabien},
title = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {3},
volume = {33},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.}
}

@inproceedings{10.1145/2976767.2976775,
author = {Daniel, Gwendal and Suny\'{e}, Gerson and Cabot, Jordi},
title = {PrefetchML: a framework for prefetching and caching models},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2976775},
doi = {10.1145/2976767.2976775},
abstract = {Prefetching and caching are well-known techniques integrated in database engines and file systems in order to speed-up data access. They have been studied for decades and have proven their efficiency to improve the performance of I/O intensive applications. Existing solutions do not fit well with scalable model persistence frameworks because the prefetcher operates at the data level, ignoring potential optimizations based on the information available at the metamodel level. Furthermore, prefetching components are common in relational databases but typically missing (or rather limited) in NoSQL databases, a common option for model storage nowadays. To overcome this situation we propose PrefetchML, a framework that executes prefetching and caching strategies over models. Our solution embeds a DSL to precisely configure the prefetching rules to follow. Our experiments show that PrefetchML provides a significant execution time speedup. Tool support is fully available online.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {318–328},
numpages = {11},
keywords = {DSL, MDE, NoSQL, persistence framework, prefetching, scalability},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.5555/1274236.1274238,
author = {Garlan, David and Schmerl, Bradley},
title = {Architecture-driven modelling and analysis},
year = {2007},
isbn = {1920682503},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Over the past 15 years there has been increasing recognition that careful attention to the design of a system's software architecture is critical to satisfying its requirements for quality attributes such as performance, security, and dependability. As a consequence, during this period the field of software architecture has matured significantly. However, current practices of software architecture rely on relatively informal methods, limiting the potential for fully exploiting architectural designs to gain insight and improve the quality of the resulting system. In this paper we draw from a variety of research results to illustrate how formal approaches to software architecture can lead to enhancements in software quality, including improved clarity of design, support for analysis, and assurance that implementations conform to their intended architecture.},
booktitle = {Proceedings of the Eleventh Australian Workshop on Safety Critical Systems and Software - Volume 69},
pages = {3–17},
numpages = {15},
keywords = {architecture analysis, software architecture},
location = {Melbourne, Australia},
series = {SCS '06}
}

@proceedings{10.1145/2986012,
title = {Onward! 2016: Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2819009.2819029,
author = {Kl\"{a}s, Michael and Bauer, Thomas and Dereani, Andreas and S\"{o}derqvist, Thomas and Helle, Philipp},
title = {A large-scale technology evaluation study: effects of model-based analysis and testing},
year = {2015},
publisher = {IEEE Press},
abstract = {Besides model-based development, model-based quality assurance and the tighter integration of static and dynamic quality assurance activities are becoming increasingly relevant in the development of software-intensive systems. Thus, this paper reports on an empirical study aimed at investigating the promises regarding quality improvements and cost savings. The evaluation comprises data from 13 industry case studies conducted during a three-year large-scale research project in the transportation domain (automotive, avionics, rail system). During the evaluation, we identified major goals and strategies associated with (integrated) model-based analysis and testing and evaluated the improvements achieved. The aggregated results indicate an average cost reduction of between 29% and 34% for verification and validation and of between 22% and 32% for defect removal. Compared with these cost savings, improvements regarding test coverage (~8%), number of remaining defects (~13%), and time to market (~8%) appear less noticeable.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {119–128},
numpages = {10},
keywords = {GQM+Strategies, embedded software quality assurance, empirical study, internal baselines, model-based testing, multiple case study, quantitative technology evaluation},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {annotations, correctness by construction, program generation, programming languages, specification languages, tools, verification, verified software grand challenge},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.1145/2038642.2038667,
author = {Kim, BaekGyu and Ayoub, Anaheed and Sokolsky, Oleg and Lee, Insup and Jones, Paul and Zhang, Yi and Jetley, Raoul},
title = {Safety-assured development of the GPCA infusion pump software},
year = {2011},
isbn = {9781450307147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038642.2038667},
doi = {10.1145/2038642.2038667},
abstract = {This paper presents our effort of using model-driven engineering to establish a safety-assured implementation of Patient-Controlled Analgesic (PCA) infusion pump software based on the generic PCA reference model provided by the U.S. Food and Drug Administration (FDA). The reference model was first translated into a network of timed automata using the UPPAAL tool. Its safety properties were then assured according to the set of generic safety requirements also provided by the FDA. Once the safety of the reference model was established, we applied the TIMES tool to automatically generate platform-independent code as its preliminary implementation. The code was then equipped with auxiliary facilities to interface with pump hardware and deployed onto a real PCA pump. Experiments show that the code worked correctly and effectively with the real pump. To assure that the code does not introduce any violation of the safety requirements, we also developed a testbed to check the consistency between the reference model and the code through conformance testing. Challenges encountered and lessons learned during our work are also discussed in this paper.},
booktitle = {Proceedings of the Ninth ACM International Conference on Embedded Software},
pages = {155–164},
numpages = {10},
keywords = {code synthesis, formalization, model-based engineering, pca infusion pump, timed automata, verification},
location = {Taipei, Taiwan},
series = {EMSOFT '11}
}

@article{10.1145/3340108,
author = {Lara, Juan De and Guerra, Esther and Ruscio, Davide Di and Rocco, Juri Di and Cuadrado, Jes\'{u}s S\'{a}nchez and Iovino, Ludovico and Pierantonio, Alfonso},
title = {Automated Reuse of Model Transformations through Typing Requirements Models},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3340108},
doi = {10.1145/3340108},
abstract = {Model transformations are key elements of model-driven engineering, where they are used to automate the manipulation of models. However, they are typed with respect to concrete source and target meta-models, making their reuse for other (even similar) meta-models challenging.To improve this situation, we propose capturing the typing requirements for reusing a transformation with other meta-models by the notion of a typing requirements model (TRM). A TRM describes the prerequisites that a model transformation imposes on the source and target meta-models to obtain a correct typing. The key observation is that any meta-model pair that satisfies the TRM is a valid reuse context for the transformation at hand.A TRM is made of two domain requirement models (DRMs) describing the requirements for the source and target meta-models, and a compatibility model expressing dependencies between them. We define a notion of refinement between DRMs and see meta-models as a special case of DRM. We provide a catalogue of valid refinements and describe how to automatically extract a TRM from an ATL transformation. The approach is supported by our tool TOTEM. We report on two experiments—based on transformations developed by third parties and meta-model mutation techniques—validating the correctness and completeness of our TRM extraction procedure and confirming the power of TRMs to encode variability and support flexible reuse.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {21},
numpages = {62},
keywords = {ATL, Model transformation, meta-modelling, model transformation reuse, refinement}
}

@article{10.1145/270849.270855,
author = {Waugh, Doug},
title = {Description of EDCS technology clusters},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270855},
doi = {10.1145/270849.270855},
abstract = {Evolutionary Systems are those that are capable of accomodating change over an extended system lifetime with reduced risk and cost/schedule impact. Most of our complex defense systems depend on software for their successful operation and, as a result, the software in those systems is the prmary vehicle for adapting to change. The EDCS (Evolutionary Design of Complex Software) Program is providing for the development and experimental application of new software technologies which can enable significant improvements in miltary mission effectiveness and information superiority. The goal is the capability to produce software intensive military systmes that are highly flexible and adaptable to meet changing requirements --- evolutionary systems.In addition to DARPA, the EDCS Program is co-sponsored by USAF Rome Laboratory, USAF Wright Laboratory, US Army Missile Command, and the National Science Foundation (NSF). Technical management is provided jointly by the DoD Software Engineering Institute (SEI) and Rome Laboratory. Rome Laboratory is DARPA's primary contracting agent.EDCS is organized into 5 Technology Clusters:1. Rationale Capture and Software Understanding2. Architecture and Generation3. High Assurance / Real-Time4. Information Management5. Dynamic LanguagesThis paper contains a short description of each cluster.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {33–42},
numpages = {10}
}

@inproceedings{10.1145/2347583.2347592,
author = {Weyns, Danny and Iftikhar, M. Usman and de la Iglesia, Didac Gil and Ahmad, Tanvir},
title = {A survey of formal methods in self-adaptive systems},
year = {2012},
isbn = {9781450310840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347583.2347592},
doi = {10.1145/2347583.2347592},
abstract = {One major challenge in self-adaptive systems is to assure the required quality properties. Formal methods provide the means to rigorously specify and reason about the behaviors of self-adaptive systems, both at design time and runtime. To the best of our knowledge, no systematic study has been performed on the use of formal methods in self-adaptive systems. As a result, there is no clear view on what methods have been used to verify self-adaptive systems, and what support these methods offer to software developers. As such insight is important for researchers and engineers, we performed a systematic literature review covering 12 main software engineering venues and 4 journals, resulting in 75 papers used for data collection. The study shows that the attention for self-adaptive software systems is gradually increasing, but the number of studies that employ formal methods remains low. The main focus of formalization is on modeling and reasoning. Model checking and theorem proving have gained limited attention. The main concerns of interest in formalization of self-adaptation are efficiency/performance and reliability. Important adaptation concerns, such as security and scalability, are hardly considered. To verify the concerns of interest, a set of new properties are defined, such as interference freedom, responsiveness, mismatch, and loss-tolerance. A relevant part of the studies use formal methods at runtime, but the use is limited to modeling and analysis. Formal methods can be applied to other runtime activities of self-adaptation, and there is a need for light-weight tools to support runtime verification.},
booktitle = {Proceedings of the Fifth International C* Conference on Computer Science and Software Engineering},
pages = {67–79},
numpages = {13},
keywords = {self-adaptive systems, systematic literature review},
location = {Montreal, Quebec, Canada},
series = {C3S2E '12}
}

@article{10.1145/299157.299171,
author = {Sipper, Moshe and Mange, Daniel and Sanchez, Eduardo},
title = {Quo Vadis evolvable hardware?},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/299157.299171},
doi = {10.1145/299157.299171},
journal = {Commun. ACM},
month = apr,
pages = {50–56},
numpages = {7}
}

@inproceedings{10.5555/2050655.2050659,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {Towards quality driven exploration of model transformation spaces},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Verifying that a software system has certain nonfunctional properties is a primary concern in many engineering fields. Although several model-driven approaches exist to predict quality attributes from system models, they still lack the proper level of automation envisioned by Model Driven Software Development. When a potential issue concerning non-functional properties is discovered, the identification of a solution is still entirely up to the engineer and to his/her experience. This paper presents QVT-Rational, our multi-modeling solution to automate the detection-solution loop. We leverage and extend existing model transformation techniques with constructs to elicit the space of the alternative solutions and to bind quality properties to them. Our framework is highly customizable, it supports the definition of nonfunctional requirements and provides an engine to automatically explore the solution space. We evaluate our approach by applying it to two well-known software engineering problems -- Object-Relational Mapping and components allocation -- and by showing how several solutions that satisfy given performance requirements can be automatically identified.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {2–16},
numpages = {15},
keywords = {feedback provisioning, model transformations},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/352029.352035,
author = {van Deursen, Arie and Klint, Paul and Visser, Joost},
title = {Domain-specific languages: an annotated bibliography},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/352029.352035},
doi = {10.1145/352029.352035},
abstract = {We survey the literature available on the topic of domain-specific languages as used for the construction and maintenance of software systems. We list a selection of 75 key publications in the area, and provide a summary for each of the papers. Moreover, we discuss terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.},
journal = {SIGPLAN Not.},
month = jun,
pages = {26–36},
numpages = {11}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@book{10.1145/3502372,
author = {Pelkey, James L. and Russell, Andrew L. and Robbins, Loring G.},
title = {Circuits, Packets, and Protocols: Entrepreneurs and Computer Communications, 1968–1988},
year = {2022},
isbn = {9781450397261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {40},
abstract = {As recently as 1968, computer scientists were uncertain how best to interconnect even two computers. The notion that within a few decades the challenge would be how to interconnect millions of computers around the globe was too far-fetched to contemplate. Yet, by 1988, that is precisely what was happening. The products and devices developed in the intervening years—such as modems, multiplexers, local area networks, and routers—became the linchpins of the global digital society. How did such revolutionary innovation occur? This book tells the story of the entrepreneurs who were able to harness and join two factors: the energy of computer science researchers supported by governments and universities, and the tremendous commercial demand for Internetworking computers. The centerpiece of this history comes from unpublished interviews from the late 1980s with over 80 computing industry pioneers, including Paul Baran, J.C.R. Licklider, Vint Cerf, Robert Kahn, Larry Roberts, and Robert Metcalfe. These individuals give us unique insights into the creation of multi-billion dollar markets for computer-communications equipment, and they reveal how entrepreneurs struggled with failure, uncertainty, and the limits of knowledge.“The key technologies that brought us our modern networked society—routers, packet switching, multiplexers, Internet protocols—were all invented by people in the short period between 1968 and 1988. James Pelkey interviewed these people at that time and recorded their stories. This book is the result: a detailed and up-close personal history of a world being born. Fascinating.” - W. Brian Arthur, Author of The Nature of Technology: What It Is and How It Evolves“Circuits, Packets, and Protocols is full of revelations for me even though I was there. Never had it explained so clearly how my distributed computing strategy was the wrong one for 3Com in the 1980s.” - Bob Metcalfe, Internet Pioneer, Ethernet inventor, 3Com founder; University of Texas at Austin Professor of Innovation}
}

@inproceedings{10.5555/1162708.1163053,
author = {Luce, Karl and Trepanier, Lucie and Ciochetto, Fred and Goldman, Lawrence},
title = {Simulation and optimization as effective DFSS tools},
year = {2005},
isbn = {0780395190},
publisher = {Winter Simulation Conference},
abstract = {Simulation and optimization techniques can provide Design for Six Sigma (DFSS) practitioners with reduced reliance on physical prototypes, rapid time-to-market, minimal defects and post-design rework. These advantages lead to quantifiable benefits within the product development life-cycle, in terms of time and cost. Through one case study, this paper will provide Six Sigma, Process Excellence and Lean practitioners with the rationale for spreadsheet simulation and optimization in DFSS initiatives. Discussion topics include the role of simulation and optimization in the DMADV methodology, disadvantages of not quantifying uncertainty in DFSS projects, differences between deterministic and stochastic optimization, and tradeoff considerations when running optimizations. Practical techniques for efficiently identifying robust, high quality solutions are demonstrated through the use of Monte Carlo simulation and optimization.},
booktitle = {Proceedings of the 37th Conference on Winter Simulation},
pages = {1993–1999},
numpages = {7},
location = {Orlando, Florida},
series = {WSC '05}
}

@article{10.1145/1050849.1057988,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Letters, Open Source Software (OSS) Patent Search Engine, Calendar of Events, Workshop and Conference Information)},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1057988},
doi = {10.1145/1050849.1057988},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {0},
numpages = {19}
}

@inproceedings{10.1145/643603.643620,
author = {Tucker, David B. and Krishnamurthi, Shriram},
title = {Pointcuts and advice in higher-order languages},
year = {2003},
isbn = {1581136609},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/643603.643620},
doi = {10.1145/643603.643620},
abstract = {Aspect-oriented software design will need to support languages with first-class and higher-order procedures, such as Python, Perl, ML and Scheme. These language features present both challenges and benefits for aspects. On the one hand, they force the designer to carefully address issues of scope that do not arise in first-order languages. On the other hand, these distinctions of scope make it possible to define a much richer variety of policies than first-order aspect languages permit.In this paper, we describe the subtleties of pointcuts and advice for higher-order languages, particularly Scheme. We then resolve these subtleties by alluding to traditional notions of scope. In particular, programmers can now define both dynamic aspects traditional to AOP and static aspects that can capture common security-control paradigms. We also describe the implementation of this language as an extension to Scheme. By exploiting two novel features of our Scheme system---continuation marks and language-defining macros---the implementation is lightweight and integrates well into the programmer's toolkit.},
booktitle = {Proceedings of the 2nd International Conference on Aspect-Oriented Software Development},
pages = {158–167},
numpages = {10},
location = {Boston, Massachusetts},
series = {AOSD '03}
}

@inproceedings{10.1145/568760.568881,
author = {Fresa, A. and Nucera, G. and Peciola, E. and Santucci, G.},
title = {Assessment of software architectures: a case study},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568881},
doi = {10.1145/568760.568881},
abstract = {Producing high quality software is a very hard task. In the last years a big effort has been spent in devising techniques for estimating and/or measuring software properties. This ranges from forecasting, in a very early stage, the cost of software production to measuring several subcharacterics in order to assess the internal and external software quality. The role of predicting vs. measuring is gaining an increas ing relevance. As an example, the recently revised ISO 9126 standard [1] introduces the concept of Estimated (Predicted) Product Quality. It is clear that the sooner estimated figures are available, the better is possible to modify some design choices. Among all the aspects involved in software developing, a central role is played by the chosen software architecture. Estimating the quality characteristics of such architecture is a strategic activity that can drive several following design decision. In this paper we report the experience of an architectural assessment performed in Ericsson Lab Italy. The assessment was performed according to the framework presented by Jan Bosch in [2].},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {699–706},
numpages = {8},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1145/197978.197981,
author = {Davis, M. J. and Hawley, H. G.},
title = {Dialogue-specified reuse of domain engineering work products},
year = {1994},
isbn = {0897916840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/197978.197981},
doi = {10.1145/197978.197981},
abstract = {This paper reports on experience gained in implementing a search and retrieval strategy for reusable software work products stored in a domain-specific reuse library. The strategy utilizes a knowledge base associated with a domain architecture, together capturing domain knowledge. Both the architecture and the knowledge base are maintained in the library. The work products (reusable parts, knowledge base, domain architecture, etc.) are created and managed by domain engineers (domain experts). The same items are utilized by application engineers to instantiate a specific application/system within the domain. Reuse of the domain engineering work products can be automated to support specification of what to retrieve and adapt through a dialogue between the library and the application engineer.},
booktitle = {Proceedings of the Eleventh Annual Washington Ada Symposium &amp; Summer ACM SIGAda Meeting on Ada},
pages = {28–36},
numpages = {9},
location = {McLean, Virginia, USA},
series = {WADAS '94}
}

@book{10.1145/3544564,
author = {Ullmer, Brygg and Shaer, Orit and Mazalek, Ali and Hummels, Caroline},
title = {Weaving Fire into Form: Aspirations for Tangible and Embodied Interaction},
year = {2022},
isbn = {9781450397698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {44},
abstract = {This book investigates multiple facets of the emerging discipline of Tangible, Embodied, and Embedded Interaction (TEI). This is a story of atoms and bits. We explore the interweaving of the physical and digital, toward understanding some of their wildly varying hybrid forms and behaviors. Spanning conceptual, philosophical, cognitive, design, and technical aspects of interaction, this book charts both history and aspirations for the future of TEI. We examine and celebrate diverse trailblazing works, and provide wide-ranging conceptual and pragmatic tools toward weaving the animating fires of computation and technology into evocative tangible forms. We also chart a path forward for TEI engagement with broader societal and sustainability challenges that will profoundly (re)shape our children’s and grandchildren’s futures. We invite you all to join this quest.}
}

@inproceedings{10.1145/1869459.1869534,
author = {Quillien, Jenny and West, Dave},
title = {Rubber ducks, nightmares, and unsaturated predicates: proto-scientific schemata are good for agile},
year = {2010},
isbn = {9781450302036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869459.1869534},
doi = {10.1145/1869459.1869534},
abstract = {Fine-grain case studies of scientific inquiry, lessons from linguistics on metaphoric thinking, the epistemology of Charles Sanders Peirce, recent work on architectural image-schemata, along with the computer world's own theorist, Peter Naur, all suggest that software developers (frequently dulled and desiccated from overdosing on 'Cartesian' methodologies) could benefit from imbibing a little 'mysticism' not the wave-your-hands woo-woo kind but the more ineffable hunch and gut side of human cognition. Scholarly publications in their final polished forms rarely admit that stories, jokes, eroticism, and dreams were the fertile seeds that germinated into 'serious' results. This essay looks to these 'closet' sources, non-reductionist, non-self conscious, metaphorical, aformal modes of thought as the salvation of a profession gone awry. It is notably proto-scientific image-schemata that retain our attention as a pragmatic tool for improving the fecundity of Agile methodology, at its roots, so to speak. The necessary context is provided by Peter Naur's fundamental insights about software development as 'theory building' coupled with an elaboration of the Agile concept of storytelling.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {901–917},
numpages = {17},
keywords = {agile, alexander, stories, theory-building},
location = {Reno/Tahoe, Nevada, USA},
series = {OOPSLA '10}
}

@inproceedings{10.1145/2000064.2000115,
author = {Udipi, Aniruddha N. and Muralimanohar, Naveen and Balasubramonian, Rajeev and Davis, Al and Jouppi, Norman P.},
title = {Combining memory and a controller with photonics through 3D-stacking to enable scalable and energy-efficient systems},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000115},
doi = {10.1145/2000064.2000115},
abstract = {It is well-known that memory latency, energy, capacity, bandwidth, and scalability will be critical bottlenecks in future large-scale systems. This paper addresses these problems, focusing on the interface between the compute cores and memory, comprising the physical interconnect and the memory access protocol. For the physical interconnect, we study the prudent use of emerging silicon-photonic technology to reduce energy consumption and improve capacity scaling. We conclude that photonics are effective primarily to improve socket-edge bandwidth by breaking the pin barrier, and for use on heavily utilized links. For the access protocol, we propose a novel packet based interface that relinquishes most of the tight control that the memory controller holds in current systems and allows the memory modules to be more autonomous, improving flexibility and interoperability. The key enabler here is the introduction of a 3D-stacked interface die that allows both these optimizations without modifying commodity memory dies. The interface die handles all conversion between optics and electronics, as well as all low-level memory device control functionality. Communication beyond the interface die is fully electrical, with TSVs between dies and low-swing wires on-die. We show that such an approach results in substantially lowered energy consumption, reduced latency, better scalability to large capacities, and better support for heterogeneity and interoperability.},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {425–436},
numpages = {12},
keywords = {3d stacking, communication protocols, dram, photonics},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@inproceedings{10.5555/317825.317973,
author = {van Laarhoven, Peter J. M. and Aarts, Emile H. L. and Davio, Marc},
title = {PHIPLA—a new algorithm for logic minimization},
year = {1985},
isbn = {0818606355},
publisher = {IEEE Press},
abstract = {PHIPLA, a new algorithm for logic minimization, is presented. The algorithm sets out to find optimal sum-of-products representations for a set of Boolean functions, thus contributing to area minimization of the Programmable Logic Array corresponding to the set of functions.The results of a comparative study of PHIPLA and two other algorithms, SPAM and PRESTOL-II, are presented. From these results it is concluded that PHIPLA generates representations which are competitive with those generated by SPAM and PRESTOL-II, whilst the algorithm is extremely fast for small problems (up to 12 variables).},
booktitle = {Proceedings of the 22nd ACM/IEEE Design Automation Conference},
pages = {739–743},
numpages = {5},
location = {Las Vegas, Nevada, USA},
series = {DAC '85}
}

@inproceedings{10.1145/243327.243623,
author = {Scherlis, William L.},
title = {Small-scale structural reengineering of software},
year = {1996},
isbn = {0897918673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/243327.243623},
doi = {10.1145/243327.243623},
booktitle = {Joint Proceedings of the Second International Software Architecture Workshop (ISAW-2) and International Workshop on Multiple Perspectives in Software Development (Viewpoints '96) on SIGSOFT '96 Workshops},
pages = {116–120},
numpages = {5},
location = {San Francisco, California, USA},
series = {ISAW '96}
}

@inproceedings{10.1145/1134285.1134404,
author = {Bishop, Judith},
title = {Multi-platform user interface construction: a challenge for software engineering-in-the-small},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134404},
doi = {10.1145/1134285.1134404},
abstract = {The popular view of software engineering focuses on managing teams of people to produce large systems. This paper addresses a different angle of software engineering, that of development for re-use and portability. We consider how an essential part of most software products - the user interface - can be successfully engineered so that it can be portable across multiple platforms and on multiple devices. Our research has identified the structure of the problem domain, and we have filled in some of the answers. We investigate promising solutions from the model-driven frameworks of the 1990s, to modern XML-based specification notations (Views, XUL, XIML, XAML), multi-platform toolkits (Qt and Gtk), and our new work, Mirrors which pioneers reflective libraries. The methodology on which Views and Mirrors is based enables existing GUI libraries to be transported to new operating systems. The paper also identifies cross-cutting challenges related to education, standardization and the impact of mobile and tangible devices on the future design of UIs. This paper seeks to position user interface construction as an important challenge in software engineering, worthy of ongoing research.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {751–760},
numpages = {10},
keywords = {.NET, GUI library reuse, XAML, XUL, graphical user interfaces, mirrors, mobile devices, platform independence, portability, reflection, tangible user interfaces, views},
location = {Shanghai, China},
series = {ICSE '06}
}

@article{10.1145/174800.174803,
author = {Curtis, Bill and Hefley, Bill},
title = {A wimp no more: the maturing of user interface engineering},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1072-5520},
url = {https://doi.org/10.1145/174800.174803},
doi = {10.1145/174800.174803},
journal = {Interactions},
month = jan,
pages = {22–34},
numpages = {13}
}

@inproceedings{10.1145/168466.168503,
author = {Lelu, Alain and Francois, Claire},
title = {Hypertext paradigm in the field of information retrieval: a neural approach},
year = {1993},
isbn = {089791547X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168466.168503},
doi = {10.1145/168466.168503},
booktitle = {Proceedings of the ACM Conference on Hypertext},
pages = {112–121},
numpages = {10},
keywords = {cluster analysis, graphic user interface, hypertext, information retrieval, neural networks},
location = {Milan, Italy},
series = {ECHT '92}
}

@article{10.1145/50087.50089,
author = {Curtis, Bill and Krasner, Herb and Iscoe, Neil},
title = {A field study of the software design process for large systems},
year = {1988},
issue_date = {Nov. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/50087.50089},
doi = {10.1145/50087.50089},
abstract = {The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three of these problems—the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns—affected software productivity and quality through their impact on cognitive, social, and organizational processes.},
journal = {Commun. ACM},
month = nov,
pages = {1268–1287},
numpages = {20}
}

@inproceedings{10.1145/67449.67488,
author = {Poltrock, S. E.},
title = {Innovation in user interface development: obstacles and opportunities},
year = {1989},
isbn = {0897913019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67449.67488},
doi = {10.1145/67449.67488},
abstract = {Case studies of two software development organizations suggest that common practices of these organizations pose obstacles to innovation. Although software development organizations have good reasons to be conservative and resist innovation, they recognize the importance of innovations to the competitiveness of their products. But organizations experienced at development of regularly scheduled releases are not well suited to development of innovations. In this research investigators worked with the user interface teams in two organizations while interviewing people throughout the organizations. Both organizations developed prototypes, but only small design changes were prototyped and tested early in development. Innovative changes were evaluated late, when resistance to iteration was great. User interface designs and prototypes were often not shown to users. Mechanisms for coordinating development were another conservative influence. Both organizations successfully overcame these obstacles by departing from established practices.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {191–195},
numpages = {5},
series = {CHI '89}
}

@book{10.1145/3368274,
author = {Halvorson, Michael J.},
title = {Code Nation: Personal Computing and the Learn to Program Movement in America},
year = {2020},
isbn = {9781450377584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
abstract = {Code Nation explores the rise of software development as a social, cultural, and technical phenomenon in American history. The movement germinated in government and university labs during the 1950s, gained momentum through corporate and counterculture experiments in the 1960s and 1970s, and became a broad-based computer literacy movement in the 1980s. As personal computing came to the fore, learning to program was transformed by a groundswell of popular enthusiasm, exciting new platforms, and an array of commercial practices that have been further amplified by distributed computing and the Internet. The resulting society can be depicted as a “Code Nation”—a globally-connected world that is saturated with computer technology and enchanted by software and its creation.Code Nation is a new history of personal computing that emphasizes the technical and business challenges that software developers faced when building applications for CP/M, MS-DOS, UNIX, Microsoft Windows, the Apple Macintosh, and other emerging platforms. It is a popular history of computing that explores the experiences of novice computer users, tinkerers, hackers, and power users, as well as the ideals and aspirations of leading computer scientists, engineers, educators, and entrepreneurs. Computer book and magazine publishers also played important, if overlooked, roles in the diffusion of new technical skills, and this book highlights their creative work and influence.Code Nation offers a “behind-the-scenes” look at application and operating-system programming practices, the diversity of historic computer languages, the rise of user communities, early attempts to market PC software, and the origins of “enterprise” computing systems. Code samples and over 80 historic photographs support the text. The book concludes with an assessment of contemporary efforts to teach computational thinking to young people.}
}

@techreport{10.1145/2594168,
author = {The Joint Task Force on Computing Curricula},
title = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
year = {2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The primary purpose of this volume is to provide guidance to academic institutions and accreditation agencies about what should constitute an undergraduate software engineering education. These recommendations have been developed by a broad, internationally based group of volunteer participants. This group has taken into account much of the work that has been done in software engineering education over the last quarter of a century. Software engineering curriculum recommendations are of particular relevance, since there is currently a surge in the creation of software engineering degree programs and accreditation processes for such programs have been established in a number of countries.}
}

@article{10.1145/505145.505148,
author = {Smaragdakis, Yannis and Batory, Don},
title = {Mixin layers: an object-oriented implementation technique for refinements and collaboration-based designs},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/505145.505148},
doi = {10.1145/505145.505148},
abstract = {A "refinement" is a functionality addition to a software project that can affect multiple dispersed implementation entities (functions, classes, etc.). In this paper, we examine large-scale refinements in terms of a fundamental object-oriented technique called collaboration-based design. We explain how collaborations can be expressed in existing programming languages or can be supported with new language constructs (which we have implemented as extensions to the Java language). We present a specific expression of large-scale refinements called mixin layers, and demonstrate how it overcomes the scalability difficulties that plagued prior work. We also show how we used mixin layers as the primary implementation technique for building an extensible Java compiler, JTS.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
pages = {215–255},
numpages = {41},
keywords = {Collaboration-based design, component-based software, product-line architectures}
}

@inproceedings{10.1145/336512.336586,
author = {Perry, Dewayne E. and Porter, Adam A. and Votta, Lawrence G.},
title = {Empirical studies of software engineering: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336586},
doi = {10.1145/336512.336586},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {345–355},
numpages = {11},
keywords = {empirical studies, software engineering},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@article{10.1145/358141.358148,
author = {Markus, M. Lynne},
title = {Power, politics, and MIS implementation},
year = {1983},
issue_date = {June 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/358141.358148},
doi = {10.1145/358141.358148},
abstract = {Theories of resistance to management information systems (MIS) are important because they guide the implementation strategies and tactics chosen by implementors. Three basic theories of the causes of resistance underlie many prescriptions and rules for MIS implementation. Simply stated, people resist MIS because of their own internal factors, because of poor system design, and because of the interaction of specific system design features with aspects of the organizational context of system use. These theories differ in their basic assumptions about systems, organizations, and resistance; they also differ in predictions that can be derived from them and in their implications for the implementation process. These differences are described and the task of evaluating the theories on the bases of the differences is begun. Data from a case study are used to illustrate the theories and to demonstrate the superiority, for implementors, of the interaction theory.},
journal = {Commun. ACM},
month = jun,
pages = {430–444},
numpages = {15},
keywords = {implementation, intraorganizational power, politics, resistance}
}

@article{10.1145/641788.641789,
author = {Fuller, S. H. and Mathew, G. A.},
title = {Implementing microprogram storage with PLA's},
year = {1976},
issue_date = {June 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/641788.641789},
doi = {10.1145/641788.641789},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {6–11},
numpages = {6}
}

@inproceedings{10.1145/336512.336584,
author = {Boehm, Barry W. and Sullivan, Kevin J.},
title = {Software economics: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336584},
doi = {10.1145/336512.336584},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {319–343},
numpages = {25},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.5555/324493.324638,
author = {Lo, T. Leo.},
title = {The evolution of workload management in data processing industry: a survey},
year = {1986},
isbn = {0818647434},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of 1986 ACM Fall Joint Computer Conference},
pages = {768–777},
numpages = {10},
location = {Dallas, Texas, USA},
series = {ACM '86}
}

@inproceedings{10.5555/800134.804370,
author = {Perdue, Robert K.},
title = {Construction of econometric planning models for business units},
year = {1979},
publisher = {IEEE Press},
abstract = {This paper describes a method for constructing econometric planning models that can translate macroeconomic information into projected income statements for each of a corporation's various business units. Each business unit is modeled by equations expressing historical relationships between micro variables of interest (sales, various costs, net income) and their macro and/or internal determinants. The text provides guidelines for designing a model structure, specifying and estimating major equations, and using the model.},
booktitle = {Proceedings of the 11th Conference on Winter Simulation - Volume 2},
pages = {409–416},
numpages = {8},
location = {San Diego, CA, USA},
series = {WSC '79}
}

@inproceedings{10.1145/1499586.1499672,
author = {Brainin, Jack},
title = {Use of COMRADE in engineering design},
year = {1973},
isbn = {9781450379168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1499586.1499672},
doi = {10.1145/1499586.1499672},
abstract = {The Naval Ship Research and Development Center began formal work in computer aided design in 1965. The initial tasks undertaken were the development of individual batch application programs which were little more than the computerization of manual design methods. The programs developed included those shown in Figure 1.},
booktitle = {Proceedings of the June 4-8, 1973, National Computer Conference and Exposition},
pages = {325–329},
numpages = {5},
location = {New York, New York},
series = {AFIPS '73}
}

@inproceedings{10.1145/1028976.1028990,
author = {Dufour, Bruno and Goard, Christopher and Hendren, Laurie and de Moor, Oege and Sittampalam, Ganesh and Verbrugge, Clark},
title = {Measuring the dynamic behaviour of AspectJ programs},
year = {2004},
isbn = {1581138318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028976.1028990},
doi = {10.1145/1028976.1028990},
abstract = {This paper proposes and implements a rigorous method for studying the dynamic behaviour of AspectJ programs. As part of this methodology several new metrics specific to AspectJ programs are proposed and tools for collecting the relevant metrics are presented. The major tools consist of: (1) a modified version of the AspectJ compiler that tags bytecode instructions with an indication of the cause of their generation, such as a particular feature of AspectJ; and (2) a modified version of the *J dynamic metrics collection tool which is composed of a JVMPI-based trace generator and an analyzer which propagates tags and computes the proposed metrics. This dynamic propagation is essential, and thus this paper contributes not only new metrics, but also non-trivial ways of computing them.We furthermore present a set of benchmarks that exercise a wide range of AspectJ's features, and the metrics that we measured on these benchmarks. The results provide guidance to AspectJ users on how to avoid efficiency pitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on ways to understand the runtime behaviour of AspectJ.},
booktitle = {Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {150–169},
numpages = {20},
keywords = {AspectJ, aspect-oriented programming, dynamic metrics, java, optimization, performance, program analysis},
location = {Vancouver, BC, Canada},
series = {OOPSLA '04}
}

@inproceedings{10.1145/800283.811115,
author = {Bowen, John B.},
title = {Are current approaches sufficient for measuring software quality?},
year = {1978},
isbn = {9781450373944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800283.811115},
doi = {10.1145/800283.811115},
abstract = {Numerous software quality studies have been performed over the past three years-mostly sponsored by the Rome Air Development Center. It is proposed by the author that more emphasis should be placed on devising and validating quantitative metrics that are indicative of the quality of software when it is being designed and coded. Such measures could be applied effectively, as relative guidelines without formal validation. However for such measures to be predictive of the quality of the delivered software, they must be validated with actual operational error data or data gathered in a simulated operational environment. This paper includes a review of proposed metrics from the literature a report of a Hughes intramodule metric study, and recommendations for refining proposed software quality assurance criteria.},
booktitle = {Proceedings of the Software Quality Assurance Workshop on Functional and Performance Issues},
pages = {148–155},
numpages = {8}
}

@article{10.1145/32232.32233,
author = {Gifford, David and Spector, Alfred},
title = {Case study: IBM's system/360-370 architecture},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/32232.32233},
doi = {10.1145/32232.32233},
abstract = {The architecture of IBM's System/360-370 series of compatible processors is one of the most durable artifacts of the computer age. Through two major revisions of the product line and 23 years of technological change, it has remained a viable and versatile interface between machine and user.},
journal = {Commun. ACM},
month = apr,
pages = {291–307},
numpages = {17}
}

@inproceedings{10.1145/1478559.1478586,
author = {Pariser, J. J. and Maurer, H. E.},
title = {Implementation of the NASA modular computer with LSI functional characters},
year = {1969},
isbn = {9781450379069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1478559.1478586},
doi = {10.1145/1478559.1478586},
abstract = {The NASA Electronics Research Center (ERC) in Cambridge, Massachusetts, has undertaken a broad program to satisfy flight computer system requirements for future missions, including versatility and long term reliability. Specific attention to these requirements is necessary because flight qualified aerospace computers and even some still under development, have been designed for increased computational speed and arithmetic capability, but not for the long life reliability and application flexibility that will be required for future space missions. For example, the mean time between failure (MTBF) of available aerospace computers lies in the range of 2,000 to 5,000 hours, whereas long space missions will require an MTBF of 10 hours.},
booktitle = {Proceedings of the November 18-20, 1969, Fall Joint Computer Conference},
pages = {231–245},
numpages = {15},
location = {Las Vegas, Nevada},
series = {AFIPS '69 (Fall)}
}

@inproceedings{10.5555/782010.782022,
author = {Kunz, Thomas and Seuren, Michiel F. H.},
title = {Fast detection of communication patterns in distributed executions},
year = {1997},
publisher = {IBM Press},
abstract = {Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.},
booktitle = {Proceedings of the 1997 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {12},
location = {Toronto, Ontario, Canada},
series = {CASCON '97}
}

@article{10.1145/356850.356853,
author = {Harris Cheheyl, Maureen and Gasser, Morrie and Huff, George A. and Millen, Jonathan K.},
title = {Verifying Security},
year = {1981},
issue_date = {Sept. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/356850.356853},
doi = {10.1145/356850.356853},
journal = {ACM Comput. Surv.},
month = sep,
pages = {279–339},
numpages = {61}
}

