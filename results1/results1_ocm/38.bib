@inproceedings{10.1145/3646548.3676599,
author = {Gomez-Vazquez, Marcos and Cabot, Jordi},
title = {Exploring the Use of Software Product Lines for the Combination of Machine Learning Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676599},
doi = {10.1145/3646548.3676599},
abstract = {The size of Large Language Models (LLMs), and Machine Learning (ML) models in general, is a key factor of their capacity and quality of their responses. But it comes with a high cost, both during the training and the model execution phase. Recently, various model merging techniques and Mixture of Experts (MoE) architectures are gaining popularity as they enable the creation of large models by combining other existing ones (the "experts" in the MoE approach). Creating these combinations remains a deep technical task with many possible configurations to consider. In this sense, this paper aims to democratize the creation of combined ML models by presenting a product line approach to the specification and training of this type of ML architectures from an initial feature model that helps users define, among other aspects, the type of models they want to combine, the combination strategy and even, for the MoE approach, the tasks that should be associated to each expert.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {26–29},
numpages = {4},
keywords = {Feature Model, Large Language Model, Machine Learning, Mixture of Experts, Model Merging, Software Product Line},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3547014,
author = {Tavassoli, Shaghayegh and Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Khosravi, Ramtin},
title = {A benchmark for active learning of variability-intensive systems},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547014},
doi = {10.1145/3546932.3547014},
abstract = {Behavioral models are the key enablers for behavioral analysis of Software Product Lines (SPL), including testing and model checking. Active model learning comes to the rescue when family behavioral models are non-existent or outdated. A key challenge on active model learning is to detect commonalities and variability efficiently and combine them into concise family models. Benchmarks and their associated metrics will play a key role in shaping the research agenda in this promising field and provide an effective means for comparing and identifying relative strengths and weaknesses in the forthcoming techniques. In this challenge, we seek benchmarks to evaluate the efficiency (e.g., learning time and memory footprint) and effectiveness (e.g., conciseness and accuracy of family models) of active model learning methods in the software product line context. These benchmark sets must contain the structural and behavioral variability models of at least one SPL. Each SPL in a benchmark must contain products that requires more than one round of model learning with respect to the basic active learning L* algorithm. Alternatively, tools supporting the synthesis of artificial benchmark models are also welcome.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {245–249},
numpages = {5},
keywords = {model learning, featured finite state machines, benchmarking, behavioral variability},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3634713.3634720,
author = {Klikovits, Stefan and Gambi, Alessio and Dhungana, Deepak and Rabiser, Rick},
title = {Leveraging Software Product Lines for Testing Autonomous Vehicles},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634720},
doi = {10.1145/3634713.3634720},
abstract = {Extensive testing of Automated Driving Systems (ADS), such as Advanced Driver Assistance Systems and Autonomous Vehicles, is commonly conducted using simulators programmed to implement various driving scenarios, a technique known as scenario-based testing. ADS scenario-based testing using simulations is challenging because it requires identifying scenarios that can effectively test ADS functionalities while ensuring that driving simulators’ features match the driving scenarios’ requirements. This short paper discusses the main challenges of systematically conducting simulation-based testing and proposes leveraging Software Product Line techniques to address them. Specifically, we argue that variability models can be used to support testers in generating test scenarios by effectively capturing and relating the variability in driving simulators, testing scenarios, and ADS implementations. We conclude by outlining an agenda for future research in this important area.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {56–60},
numpages = {5},
keywords = {Autonomous Vehicles, Scenario- and Simulation-based Testing, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3546932.3546998,
author = {Trasobares, Jose Ignacio and Domingo, \'{A}frica and Arcega, Lorena and Cetina, Carlos},
title = {Evaluating the benefits of software product lines in game software engineering},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546998},
doi = {10.1145/3546932.3546998},
abstract = {Video game development is one of the fastest-growing industries in the world. The use of software product lines (SPLs) has proven to be effective in developing different types of software at a lower cost, in less time, and with higher quality. There are recent research efforts that propose to apply SPLs in the domain of video games. Video games present characteristics that differentiate their development from the development of classic software; for example, game developers perceive more difficulties than other non-game developers when reusing code. In this paper, we evaluate if the adoption of an SPL in game software engineering (GSE) can generate the same benefits as in classic software engineering (CSE) considering the case study of Kromaia. As in other disciplines dealing with human behaviour, empirical research allows for building a reliable knowledge base in software engineering. We present an experiment comparing two development approaches, Clone and Own (CaO) and an SPL in terms of correctness, efficiency, and satisfaction when subjects develop elements of a commercial video game. The results indicate that the elements developed using the SPL are more correct than those developed with CaO but do not indicate significant improvement in efficiency or satisfaction. Our findings suggest that SPLs in GSE may play a different role than the one they have played for decades in CSE. Specifically, SPLs can be relevant to generating new video game content or to balancing video game difficulty.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {120–130},
numpages = {11},
keywords = {software product line engineering, game software engineering, empirical comparison},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609016,
author = {Acher, Mathieu and Martinez, Jabier},
title = {Generative AI for Reengineering Variants into Software Product Lines: An Experience Report},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609016},
doi = {10.1145/3579028.3609016},
abstract = {The migration and reengineering of existing variants into a software product line (SPL) is an error-prone and time-consuming activity. Many extractive approaches have been proposed, spanning different activities from feature identification and naming to the synthesis of reusable artefacts. In this paper, we explore how large language model (LLM)-based assistants can support domain analysts and developers. We revisit four illustrative cases of the literature where the challenge is to migrate variants written in different formalism (UML class diagrams, Java, GraphML, statecharts). We systematically report on our experience with ChatGPT-4, describing our strategy to prompt LLMs and documenting positive aspects but also failures. We compare the use of LLMs with state-of-the-art approach, BUT4Reuse. While LLMs offer potential in assisting domain analysts and developers in transitioning software variants into SPLs, their intrinsic stochastic nature and restricted ability to manage large variants or complex structures necessitate a semiautomatic approach, complete with careful review, to counteract inaccuracies.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {57–66},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {software product lines, machine learning, configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {performance-influence models, machine learning, dynamic software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {systematic literature review, software engineering, machine learning, Tertiary study}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1109/ASE56229.2023.00026,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Bridging the Gap between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00026},
doi = {10.1109/ASE56229.2023.00026},
abstract = {This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world --- Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1098–1110},
numpages = {13},
keywords = {machine learning, software defect prediction, nokia 5G, industry introduction, experience paper},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@article{10.1145/3467477,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3467477},
doi = {10.1145/3467477},
abstract = {Evolutionary Computation (EC) approaches are inspired by nature and solve optimization problems in a stochastic manner. They can offer a reliable and effective approach to address complex problems in real-world applications. EC algorithms have recently been used to improve the performance of Machine Learning (ML) models and the quality of their results. Evolutionary approaches can be used in all three parts of ML: preprocessing (e.g., feature selection and resampling), learning (e.g., parameter setting, membership functions, and neural network topology), and postprocessing (e.g., rule optimization, decision tree/support vectors pruning, and ensemble learning). This article investigates the role of EC algorithms in solving different ML challenges. We do not provide a comprehensive review of evolutionary ML approaches here; instead, we discuss how EC algorithms can contribute to ML by addressing conventional challenges of the artificial intelligence and ML communities. We look at the contributions of EC to ML in nine sub-fields: feature selection, resampling, classifiers, neural networks, reinforcement learning, clustering, association rule mining, and ensemble methods. For each category, we discuss evolutionary machine learning in terms of three aspects: problem formulation, search mechanisms, and fitness value computation. We also consider open issues and challenges that should be addressed in future work.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {161},
numpages = {35},
keywords = {swarm intelligence, learning optimization, Evolutionary computation}
}

@inproceedings{10.1145/3302333.3302345,
author = {Ali, Shaukat and Arcaini, Paolo and Hasuo, Ichiro and Ishikawa, Fuyuki and Lee, Nian-Ze},
title = {Towards a Framework for the Analysis of Multi-Product Lines in the Automotive Domain},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302345},
doi = {10.1145/3302333.3302345},
abstract = {Safety analyses in the automotive domain (in particular automated driving) present unprecedented challenges due to its complexity and tight integration with the physical environment. Given the diversity in the types of cars, potentially unlimited number of possible environmental and driving conditions, it is crucial to devise a systematic way of managing variability in hazards, driving and environmental conditions in individual cars, families of cars, and families of families of cars to facilitate analyses efficiently. To this end, we present our ongoing work in a research project that focuses on devising a model-based reasoning framework for systematically managing hazards in the automotive domain and supporting safety analyses (e.g., falsification).},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {12},
numpages = {6},
keywords = {Simulink, Product Lines, Hazard analysis, Falsification, Automotive domain},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3511805,
author = {Ram\'{\i}rez, Aurora and Feldt, Robert and Romero, Jos\'{e} Ra\'{u}l},
title = {A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511805},
doi = {10.1145/3511805},
abstract = {Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {21},
numpages = {42},
keywords = {industry, test case prioritisation, machine learning, taxonomy, Regression testing}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {feedback loops, Self-adaptation, MAPE-K}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {variability models, software product lines, metrics, implementation, feature models, SPL},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {software product lines, model learning, family model, 150% model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3307650.3322267,
author = {Tarsa, Stephen J. and Chowdhury, Rangeen Basu Roy and Sebot, Julien and Chinya, Gautham and Gaur, Jayesh and Sankaranarayanan, Karthik and Lin, Chit-Kwan and Chappell, Robert and Singhal, Ronak and Wang, Hong},
title = {Post-silicon CPU adaptation made practical using machine learning},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322267},
doi = {10.1145/3307650.3322267},
abstract = {Processors that adapt architecture to workloads at runtime promise compelling performance per watt (PPW) gains, offering one way to mitigate diminishing returns from pipeline scaling. State-of-the-art adaptive CPUs deploy machine learning (ML) models on-chip to optimize hardware by recognizing workload patterns in event counter data. However, despite breakthrough PPW gains, such designs are not yet widely adopted due to the potential for systematic adaptation errors in the field.This paper presents an adaptive CPU based on Intel SkyLake that (1) closes the loop to deployment, and (2) provides a novel mechanism for post-silicon customization. Our CPU performs predictive cluster gating, dynamically setting the issue width of a clustered architecture while clock-gating unused resources. Gating decisions are driven by ML adaptation models that execute on an existing microcontroller, minimizing design complexity and allowing performance characteristics to be adjusted with the ease of a firmware update. Crucially, we show that although adaptation models can suffer from statistical blindspots that risk degrading performance on new workloads, these can be reduced to minimal impact with careful design and training.Our adaptive CPU improves PPW by 31.4% over a comparable non-adaptive CPU on SPEC2017, and exhibits two orders of magnitude fewer Service Level Agreement (SLA) violations than the state-of-the-art. We show how to optimize PPW using models trained to different SLAs or to specific applications, e.g. to improve datacenter hardware in situ. The resulting CPU meets real world deployment criteria for the first time and provides a new means to tailor hardware to individual customers, even as their needs change.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {adaptive hardware, clustered architectures, machine learning, runtime optimization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3646548.3676543,
author = {Stummer, Alexander and Hager, Anna-Lena and Rabiser, Rick},
title = {Towards a Flexible Approach for Variability Mining},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676543},
doi = {10.1145/3646548.3676543},
abstract = {Software Product Line (SPL) Engineering relies on explicitly documenting and managing variability information, typically in variability models such as feature models, and relating them with reusable artifacts, e.g., software components. Many companies still rely on clone-and-own reuse approaches and could benefit from adopting an SPL approach instead. However, extracting variability information from existing systems is often challenging due to their size and complexity. Also, manually creating and maintaining variability models and relating them with reusable artifacts is very expensive and requires expert knowledge. To address this problem, SPL reverse-engineering approaches try to automatically extract variability information from existing systems to populate variability models with it. Unfortunately, many existing approaches are limited to a single artifact type and only a few more widely applicable methods have been proposed. In this short paper, we present our vision of a flexible, extensible and artifact-independent approach, which provides a framework for users to mine the variability of their existing system variants and automatically reverse-engineer SPLs. We discuss challenges and give an overview of a possible architecture as well as the steps necessary to realize our approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {75–81},
numpages = {7},
keywords = {Reverse Engineering, Software Product Lines, Variability Mining},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {variability modeling, software product lines, design patterns},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3634713.3634734,
author = {Fortz, Sophie and Temple, Paul and Devroey, Xavier and Perrouin, Gilles},
title = {Towards Feature-based ML-enabled Behaviour Location},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634734},
doi = {10.1145/3634713.3634734},
abstract = {Mapping behaviours to the features they relate to is a prerequisite for variability-intensive systems (VIS) reverse engineering. Manually providing this whole mapping is labour-intensive. In black-box scenarios, only execution traces are available (e.g., process mining). In our previous work, we successfully experimented with variant-based mapping using supervised machine learning (ML) to identify the variants responsible of the production of a given execution trace, and demonstrated that recurrent neural networks (RNNs) work well (≥ 80% accuracy) when trained on datasets in which we label execution traces with variants. However, this mapping (i) may not scale to large VIS because of combinatorial explosion and (ii) makes the internal ML representation hard to understand. In this short paper, we discuss the design of a novel approach: feature-based mapping learning.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {152–154},
numpages = {3},
keywords = {Feature, Machine Learning, Software Variability, Variability Exploration},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3634713.3634732,
author = {Acher, Mathieu},
title = {A Demonstration of End-User Code Customization Using Generative AI},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634732},
doi = {10.1145/3634713.3634732},
abstract = {Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configurators. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {139–145},
numpages = {7},
keywords = {LLM, code synthesis, customization, end-user programming, generative AI, generator, software product lines, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3646548.3672597,
author = {Purandare, Salil and Cohen, Myra B.},
title = {Exploration of Failures in an sUAS Controller Software Product Line},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672597},
doi = {10.1145/3646548.3672597},
abstract = {Small uncrewed aerial systems (sUAS) are growing in their use for commercial, scientific, recreational, and emergency management purposes. A critical part of a successful flight is a correctly tuned controller which manages the physics of the vehicle. If improperly configured, it can lead to flight instability, deviation, or crashes. These types of misconfigurations are often within the valid ranges specified in the documentation; hence, they are hard to identify. Recent research has used fuzzing or explored only a small part of the parameter space, providing little understanding of the configuration landscape itself. In this work we leverage software product line engineering to model a subset of the parameter space of a widely used flight control software, using it to guide a systematic exploration of the controller space. Via simulation, we test over 20,000 configurations from a feature model with 50 features and 8.88 \texttimes{} 1034 products, covering all single parameter value changes and all pairs of changes from their default values. Our results show that only a small number of single configuration changes fail (15%), however almost 40% fail when we evaluate changes to two-parameters at a time. We explore the interactions between parameters in more detail, finding what appear to be many dependencies and interactions between parameters which are not well documented. We then explore a smaller, exhaustive product line model, with eight of the most important features (and 6,561 configurations) and uncover a complex set of interactions; over 48% of all configurations fail.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {125–135},
numpages = {11},
keywords = {Configurability, Software Product Lines, sUAS},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3546932.3547008,
author = {Amraoui, Yassine El and Blay-Fornarino, Mireille and Collet, Philippe and Precioso, Fr\'{e}d\'{e}ric and Muller, Julien},
title = {Evolvable SPL management with partial knowledge: an application to anomaly detection in time series},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547008},
doi = {10.1145/3546932.3547008},
abstract = {In Machine Learning (ML), the resolution of anomaly detection problems in time series presents a great diversity of practices as it can correspond to many different contexts. These practices cover both grasping the business problem and designing the solution itself. By practice, we designate explicit and implicit steps toward resolving a problem, while a solution corresponds to a combination of algorithms selected for their performance on a given problem. Two related issues arise. The first one is that the practices are individual and not explicitly mutualized. The second one is that choosing one solution over another is all the more difficult to justify because the space of solutions and the evaluation criteria are vast and evolve rapidly with the advances in ML. To solve these issues and tame the evolving diversity in ML, a Software Product Line (SPL) approach can be envisaged to represent the variable set of solutions. However, this requires characterizing an ML business problem through an explicit set of criteria and justifying one ML solution over all others. The resolution of anomaly detection problems is thus different from finding the best configuration workflow from past configurations but lies more in guiding the configuration towards a solution that may never have been studied before. This paper proposes an SPL approach that capitalizes on past practices by exploiting a variability-aware representation to detect new criteria and constraints when practices adopt different solutions to seemingly similar problems. We report on the evaluation of our approach using a set of applications from the literature and an ML software company. We show how the analysis of practices makes it possible to consolidate the knowledge contained in the SPL.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {222–233},
numpages = {12},
keywords = {software product line, metrics, machine learning, evolution},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3672590,
author = {Sundermann, Chico and Brancaccio, Vincenzo Francesco and Kuiter, Elias and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas},
title = {Collecting Feature Models from the Literature: A Comprehensive Dataset for Benchmarking},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672590},
doi = {10.1145/3646548.3672590},
abstract = {Feature models are widely used for specifying the valid configurations of product lines. Many automated analyses on feature models have been considered, but they often depend on computationally complex algorithms (e.g., solving satisfiability problems). To identify and develop efficient reasoning engines, it is necessary to compare their performance on practically relevant feature models. However, empirical evaluations on feature-model analysis often suffer from the limitations of available feature-model datasets in terms of transferability. A major problem is the accessibility of relevant feature models as they are scattered over numerous publications. In this work, we perform a literature survey on empirical evaluations that target the performance of feature-model analyses to examine common evaluation practices and collect feature models for future evaluations. Furthermore, we examine the suitability of the derived collection for benchmarking performance. To improve accessibility, we provide a repository including all 2,518 identified feature models from 13 application domains, such as system software.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {54–65},
numpages = {12},
keywords = {benchmark, evaluation, feature model, product line, survey},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Software Product Lines, Requirement Documents, Feature Terms Identification, Feature Extraction},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {software product lines, performance prediction, machine learning, configurable systems},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3691620.3695594,
author = {Sundermann, Chico and Loth, Jacob and Th\"{u}m, Thomas},
title = {Efficient Slicing of Feature Models via Projected d-DNNF Compilation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695594},
doi = {10.1145/3691620.3695594},
abstract = {Configurable systems often contain components from different fields or disciplines that are relevant for distinct stakeholders. For instance, tests or analyses targeting interactions of the software of a cyber-physical system may be only applicable for software components. However, managing such components in isolation is not trivial due, for instance, interdependencies between features. Feature models are a common formalism to specify such dependencies. Feature-model slicing corresponds to creating a subset of the feature model (e.g., with only components relevant to a particular stakeholder) that still preserves transitive dependencies from discarded features. However, slicing is computationally expensive and subsequent analyses often depend on complex computations, such as SAT or #SAT. With knowledge compilation, the original feature model can be translated to a beneficial format (e.g., d-DNNF or BDD) with an initial effort that accelerates subsequent analyses. Consequentially, acquiring a sliced target format depends on two expensive subsequent algorithms. In this work, we merge both steps by proposing projected d-DNNF compilation; a novel way to slice feature models that coincidently performs knowledge compilation to d-DNNF. Our empirical evaluation on real-world feature models shows that our tool pd4 often reduces runtimes substantially compared to existing techniques and scales to more input instances.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1332–1344},
numpages = {13},
keywords = {feature models, configurable systems, product lines, d-DNNF, knowledge compilation, slicing, projection},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3238147.3238201,
author = {Mukelabai, Mukelabai and Ne\v{s}i\'{c}, Damir and Maro, Salome and Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp},
title = {Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238201},
doi = {10.1145/3238147.3238201},
abstract = {Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {155–166},
numpages = {12},
keywords = {Product Lines, Highly Configurable Systems, Analysis},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3579027.3608989,
author = {K\"{o}nig, Christoph and Rosiak, Kamil and Cleophas, Loek and Schaefer, Ina},
title = {True Variability Shining Through Taxonomy Mining},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608989},
doi = {10.1145/3579027.3608989},
abstract = {Software variants of a Software Product Line (SPL) consist of a set of artifacts specified by features. Variability models document the valid relationships between features and their mapping to artifacts. However, research has shown inconsistencies between the variability of variants in features and artifacts, with negative effects on system safety and development effort. To analyze this mismatch in variability, the causal relationships between features, artifacts, and variants must be uncovered, which has only been addressed to a limited extent. In this paper, we propose taxonomy graphs as novel variability models that reflect the composition of variants from artifacts and features, making mismatches in variability explicit. Our evaluation with two SPL case studies demonstrates the usefulness of our variability model and shows that mismatches in variability can vary significantly in detail and severity.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {182–193},
numpages = {12},
keywords = {Variability Modeling, Taxonomy, Software Product Lines},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1145/3611663,
author = {Oh, Jeho and Batory, Don and Heradio, Rub\'{e}n},
title = {Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611663},
doi = {10.1145/3611663},
abstract = {A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {7},
numpages = {36},
keywords = {Software product lines, configuration optimization, product spaces, machine learning, uniform random sampling, random search, order statistics}
}

@inproceedings{10.1145/3579027.3608974,
author = {Brault, Yann and El Amraoui, Yassine and Blay-Fornarino, Mireille and Collet, Philippe and Jaillet, Florent and Precioso, Fr\'{e}d\'{e}ric},
title = {Taming the Diversity of Computational Notebooks},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608974},
doi = {10.1145/3579027.3608974},
abstract = {In many applications of Computational Science and especially Data Science, notebooks are the cornerstone of knowledge and experiment sharing. Their diversity is multiple (problem addressed, input data, algorithm used, overall quality) and is not made explicit at all. As they are heavily reused through a clone-and-own approach, the tailoring process from an existing notebook to a specific problem is cumbersome, error-prone, and particularly uncertain. In this paper, we propose a tooled approach that captures the different dimensions of variability in computational notebooks. It allows one to seek an existing notebook that suits her requirements, or to generate most parts of a new one.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {27–33},
numpages = {7},
keywords = {software variability, computational science, clone-and-own},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3546996,
author = {Uta, Mathias and Felfernig, Alexander and Helic, Denis and Le, Viet-Man},
title = {Accuracy- and consistency-aware recommendation of configurations},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546996},
doi = {10.1145/3546932.3546996},
abstract = {Constraint-based configurators support users in deciding which components and features should be included in a configuration. Due to the increasing size and complexity of configurable products and services, recommender systems are used to personalize the interaction with configurators. Since basic recommendation approaches such as collaborative filtering do not take into account constraints between variable values, recommendations can induce inconsistencies between user requirements and the underlying configuration knowledge base. In this paper, we introduce a constraint-based configuration approach that integrates the results of model-based collaborative filtering (e.g., implemented as feed forward neural network) into constraint solving in such a way that the solver (configurator) is able to determine consistency-preserving and user-relevant configurations.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {79–84},
numpages = {6},
keywords = {neural networks, feature models, constraint solving, configuration, collaborative filtering},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3652620.3687798,
author = {Sousa, Tiago and Ries, Beno\^{\i}t and Guelfi, Nicolas},
title = {Model-Driven Software Product Line Engineering of AI-Based Applications for Achieving Sustainable Development Goals: Vision Paper},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687798},
doi = {10.1145/3652620.3687798},
abstract = {Achieving the Sustainable Development Goals (SDGs) set by the United Nations requires innovative solutions to address the related complex and interconnected challenges. The application of AI has demonstrated the potential to significantly contribute to these efforts by providing advanced analytics and decision-making capabilities. However, integrating AI into sustainability initiatives faces several challenges, including the need for flexible and reusable solutions that can be adapted to diverse and evolving SDG contexts, as well as the challenge of making these technologies accessible to nonexpert stakeholders. This paper proposes an integrated approach that combines Model-Driven Engineering (MDE) with Software Product Line Engineering (SPLE) to address these challenges. The proposed process includes key activities such as domain analysis, metamodel-driven requirements specification, product derivation, and AI model training. This approach aims to automate the derivation of flexible and reusable AI architectures tailored to specific SDG contexts, thus reducing the development time of AI-based software solutions for sustainability efforts.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {523–527},
numpages = {5},
keywords = {model-driven engineering, software product line, artificial intelligence, sustainable development goals, vision paper},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3634713.3634733,
author = {Kuiter, Elias and He\ss{}, Tobias and Sundermann, Chico and Krieter, Sebastian and Th\"{u}m, Thomas and Saake, Gunter},
title = {How Easy is SAT-Based Analysis of a Feature Model?},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634733},
doi = {10.1145/3634713.3634733},
abstract = {With feature-model analyses, stakeholders can improve their understanding of complex configuration spaces. Computationally, these analyses are typically reduced to solving satisfiability problems. While this has been found to perform reasonably well on many models, estimating the efficiency of a given analysis on a given model is still difficult. We argue that such estimates are necessary due to the heterogeneity of feature models. We discuss inherently influential factors and suggest potential algorithmic solutions.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {149–151},
numpages = {3},
keywords = {SAT solving, algorithm selection, feature modeling},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3583740.3626628,
author = {Gropengie\ss{}er, Uwe and Liphardt, Julian and Matthe, Michael and M\"{u}hlh\"{a}user, Max},
title = {Poster: (Re)-Configuration Framework for Mission-Critical Applications in Edge Environments},
year = {2024},
isbn = {9798400701238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583740.3626628},
doi = {10.1145/3583740.3626628},
abstract = {Mission-critical applications, which must adhere to processing deadlines, can benefit from low latencies offered by the edge. Adapting the Quality of Result allows for targeted processing times by selecting various approximations. Feature models can be employed to manage the resulting multitude of possible configurations. However, the deployment and (re)-configuration process is very time-consuming, making it impractical for mission-critical applications. In this work, we introduce a processing pipeline with components that significantly accelerate online (re)-configuration based on changing latencies compared to the state-of-the-art. Additionally, we address the edge-specific discovery of potential microservice chains capable of executing the application.},
booktitle = {Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing},
pages = {264–266},
numpages = {3},
keywords = {feature model, edge computing, approximate computing, quality of result},
location = {Wilmington, DE, USA},
series = {SEC '23}
}

@inproceedings{10.1145/3579027.3608971,
author = {Eichhorn, Domenik and Pett, Tobias and Osborne, Tobias and Schaefer, Ina},
title = {Quantum Computing for Feature Model Analysis: Potentials and Challenges},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608971},
doi = {10.1145/3579027.3608971},
abstract = {Feature modeling is a technique to model the variability of configurable systems. When working with feature models, it is possible to analyze them, for instance, by counting the number of valid configurations, searching feature model anomalies, or creating samples of configurations for testing. Classical feature model analysis techniques are based on solving algorithmic problems such as boolean satisfiability, satisfiability modulo theories, or integer linear programming. Existing analysis approaches provide satisfactory solutions for small and medium-sized problem instances, but scaling issues are observed for large-sized feature models. Quantum computers provide up to superpolynomial speedups for specific algorithmic problems and have the potential to solve those scaling issues. This paper analyzes the algorithmic techniques used in classical product line analysis and identifies potentials and challenges for quantum speedups. Our findings show that quantum algorithms like QAOA and Grover have the potential to speed up SAT and ILP-based feature model analysis techniques, but only after additional improvements in quantum hardware have been made.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {1–7},
numpages = {7},
keywords = {quantum computing, quantum algorithms, feature model analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608987,
author = {Jacobs, Jef and Nicolay, Jens and De Meuter, Wolfgang},
title = {VariMod: A Structured Approach to Variability in 3D Modelling},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608987},
doi = {10.1145/3579027.3608987},
abstract = {Today's manufacturing industry is confronted with an increasing demand for product variability that stems from product customisation needs and the engineering process. Different customer demands and the mass-customisation of physical products require designing multiple variants of products, and additional requirements may be introduced when the product reaches subsequent stages (simulation, manufacturing, assembly...) in its engineering process.The state-of-the-art 3D modelling software deals with variability in a mostly ad-hoc fashion. Designing products typically involves creating digital 3D models using Computer-Aided Design (CAD) software, and implementing variability requires duplication of entire models or parts thereof that then require changes without any identification of or distinction between the different requirements that caused them. Parametric CAD approaches do enable designing 3D models that contain modifiable parameters, but designers must still ensure that the 3D model with updated parameter values satisfies all requirements. It is therefore difficult or impossible with current approaches and tools to design variants of products in a structured and efficient manner.In this work, we present VariMod, a 3D modelling approach that distinguishes between invariant requirements that each variant of a 3D model must satisfy, and variant-specific requirements that individual variants must satisfy. Hereby, VariMod enables the specification of 'generic' 3D models that satisfy invariant requirements, of which the parameter values can be optimised so that they also satisfy variant-specific requirements. To this end, VariMod represents both types of requirements as bidirectional constraints that are solved to find optimal parameter values that satisfy all constraints. VariMod features a constraint-solving process that aims to minimise the modifications made to parameter values when optimising a 3D model, thereby preventing unexpected modifications to the 3D model. We use PrintTalk, a programmatic CAD language for parametric 3D modelling, as a vehicle for implementing and validating VariMod by demonstrating how it can be used for designing variants of 3D models in a structured and efficient manner.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {160–169},
numpages = {10},
keywords = {Variational Design, PrintTalk, Parametric CAD, Non-Functional Requirements, DFX, Constraints, 3D Modelling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {variability extraction, reverse engineering, relational concept analysis, complex software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547050,
author = {Le, Viet-Man and Tran, Thi Ngoc Trang and Felfernig, Alexander},
title = {Consistency-based integration of multi-stakeholder recommender systems with feature model configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547050},
doi = {10.1145/3503229.3547050},
abstract = {Feature models are used to represent variability properties of complex items. In most of the cases, the assumption in feature model configuration is that single users/stakeholders are interacting with the underlying configurator. However, there are many scenarios where multiple stakeholders need to jointly complete a configuration task, for example, when selecting the features to be included in a company-wide software service or when deciding about the software features to be included in upcoming releases. In such cases, decisions have to be taken jointly where the constraints and preferences of individual stakeholders have to be taken into account. In this paper, we show how multi-stakeholder recommender systems can be integrated in feature model configuration scenarios.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {178–182},
numpages = {5},
keywords = {feature model configuration, feature models, group decision making, multi-stakeholder recommendation},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {variability model, system evolution, fault, automatic repair},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3472674.3473980,
author = {Fortz, Sophie and Temple, Paul and Devroey, Xavier and Heymans, Patrick and Perrouin, Gilles},
title = {VaryMinions: leveraging RNNs to identify variants in event logs},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473980},
doi = {10.1145/3472674.3473980},
abstract = {Business processes have to manage variability in their execution, e.g., to deliver the correct building permit in different municipalities. This variability is visible in event logs, where sequences of events are shared by the core process (building permit authorisation) but may also be specific to each municipality. To rationalise resources (e.g., derive a configurable business process capturing all municipalities’ permit variants) or to debug anomalous behaviour, it is mandatory to identify to which variant a given trace belongs. This paper supports this task by training Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) algorithms on two datasets: a configurable municipality and a travel expenses workflow. We demonstrate that variability can be identified accurately (&gt;87%) and discuss the challenges of learning highly entangled variants.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {13–18},
numpages = {6},
keywords = {Variability Mining, Recurrent Neural Networks, Configurable processes},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/3579027.3608985,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Greiner, Sandra and Moosherr, Benjamin and Krieter, Sebastian and Tinnes, Christof and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Views on Edits to Variational Software},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608985},
doi = {10.1145/3579027.3608985},
abstract = {Software systems are subject to frequent changes, for example to fix bugs or meet new customer requirements. In variational software systems, developers are confronted with the complexity of evolution and configurability on a daily basis; essentially handling changes to many distinct software variants simultaneously. To reduce the complexity of configurability for developers, filtered or projectional editing was introduced: By providing a partial or complete configuration, developers can interact with a simpler view of the variational system that shows only artifacts belonging to that configuration. Yet, such views are available for individual revisions only but not for edits performed across revisions. To reduce the complexity of evolution in variational software for developers, we extend the concept of views to edits. We formulate a correctness criterion for views on edits and introduce two correct operators for view generation, one operator suitable for formal reasoning, and a runtime optimized operator. In an empirical study, we demonstrate the feasibility of our operators by applying them to the change histories of 44 open-source software systems.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {141–152},
numpages = {12},
keywords = {variation control, software variability, software product lines, software evolution, projectional editing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3550356.3561575,
author = {Shi, Yechuan and Kienzle, J\"{o}rg and Guo, Jin L. C.},
title = {Feature-oriented modularization of deep learning APIs},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561575},
doi = {10.1145/3550356.3561575},
abstract = {Deep learning libraries provide vast APIs because of the multitude of supported input data types, pre-processing operations, and neural network types and configuration options. However, developers working on one concrete application typically use only a small subset of the API at any one given time. Newcomers hence have to read through tutorials and API documentation, gathering scattered information, trying to find the API that fits their needs. This is time consuming and error prone. To remedy this, we show how we modularized the API of a popular Java DL framework Deeplearning4j (DL4J) according to features. Beginner developers can interactively select desired high level features, and our tool generates the subset of the DL library API that corresponds to the selected features. We evaluate our modularization on DL4J code samples, demonstrating an average recall of 98.9% for API classes and 98.0% for API methods. The respective precision is 19.3% and 13.8%, which represents an improvement of two orders of magnitude compared to the complete DL4J API.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {367–374},
numpages = {8},
keywords = {feature interaction, concern-oriented reuse, DL4J, API generation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Software Product Line, Requirements Engineering, Natural Language Processing, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3637528.3671604,
author = {Mukerji, Abhimanyu and More, Sushant and Kannan, Ashwin Viswanathan and Ravi, Lakshmi and Chen, Hua and Kohli, Naman and Khawand, Chris and Mandalapu, Dinesh},
title = {Valuing an Engagement Surface using a Large Scale Dynamic Causal Model},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671604},
doi = {10.1145/3637528.3671604},
abstract = {With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5556–5565},
numpages = {10},
keywords = {causal inference, causal modeling, dynamic causal model, engagement surface, investment decisions, large-scale modeling, observational causal model, program valuation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@inproceedings{10.1145/3652620.3687816,
author = {Chiang, Thomas and Paige, Richard and Wassyng, Alan and Mosser, Sebastien},
title = {A Tool For Feature-Requirement Traceability Using Requirement Canvas and Encapsulation},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687816},
doi = {10.1145/3652620.3687816},
abstract = {System traceability from requirements to implementation is desirable, often mandatory, for complex and safety-critical system development. However, maintaining traceability documentation can be extremely taxing and tedious through iterative and incremental development life cycles. The problem is further exacerbated when product variability is introduced in the system development. Feature modelling and product line engineering are becoming increasingly common in safety-critical domains such as the automotive domain when manufacturers must consider maintaining documentation for multiple variations of a single vehicle. This problem is made even more complex by the yearly iterations they have on their vehicles. Thus, we introduce CyclicL, a tool for developing, managing, and modelling feature-requirement traceability through iterative and incremental development strategies. CyclicL is designed to facilitate traceability between features and requirements through various product iterations, and multiple incremental feature and requirement changes.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {725–734},
numpages = {10},
keywords = {model-driven engineering, requirement engineering, product line engineering, product families, domain-specific languages, traceability generation, traceability management, feature modelling, requirement diagrams, encapsulation},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/2695664.2695907,
author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han\^{e}ne},
title = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695907},
doi = {10.1145/2695664.2695907},
abstract = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1602–1609},
numpages = {8},
keywords = {software product lines, measurement, feature model, evaluation},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3646548.3672596,
author = {Chueca, Jorge and Blasco, Daniel and Cetina, Carlos and Font, Jaime},
title = {Leveraging Phylogenetics in Software Product Families: The Case of Latent Content Generation in Video Games},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672596},
doi = {10.1145/3646548.3672596},
abstract = {A family of software products comprises similar products within a defined scope that share common characteristics, often due to reuse techniques applied during development. This paper introduces an approach that applies biological insights to map the landscape of a software product family, identifying potential gaps within its scope. Phylogenetics studies the gene similarity among groups of organisms to understand ancestry among species. Leveraging Phylogenetics in software, our approach offers a structured view of a product family, aiding in the discovery of unexplored areas fitting the scope of the family. Our approach creates a phylogenetic tree that enables to easily identify latent products (ancestors) that did not exist in the original family. Those ancestors can then be reconstructed from existing products (descendants). The product family evaluated is a set of industry-scale video game non-playable characters. We assess this approach through video game simulations and scope metrics to determine how closely the reconstructed products align with the family’s scope. The results confirm that the content generated with phylogenetics aligns better with the family scope than the state-of-the-art procedural content generation techniques using evolutionary algorithms. Phylogenetics enhances content generation by providing a framework to understand and expand the product family with new content.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {113–124},
numpages = {12},
keywords = {Game Software Engineering, Phylogenetics, Procedural Content Generation, Software Product Families},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {recommender systems, feature models, evaluation, configuration},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3550355.3552397,
author = {G\"{o}ttmann, Hendrik and Caesar, Birte and Beers, Lasse and Lochau, Malte and Sch\"{u}rr, Andy and Fay, Alexander},
title = {Precomputing reconfiguration strategies based on stochastic timed game automata},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552397},
doi = {10.1145/3550355.3552397},
abstract = {Many modern software systems continuously reconfigure themselves to (self-)adapt to ever-changing environmental contexts. Selecting presumably best-fitting next configurations is, however, very challenging, depending on functional and non-functional criteria like real-time constraints as well as inherently uncertain future contexts which makes greedy one-step decision heuristics ineffective. In addition, the computational overhead caused by reconfiguration planning at run-time should not outweigh its benefits. On the other hand, completely pre-planning reconfiguration decisions at design time is also infeasible due to the lack of knowledge about the context behavior. In this paper, we propose a game-theoretic setting for precomputing reconfiguration decisions under partially uncertain real-time behavior. We employ stochastic timed game automata as reconfiguration model to derive winning strategies which enable the first player (the system) to make fast look-ups for presumably best-fitting reconfiguration decisions satisfying the second player (the context). To cope with the high computational complexity of finding winning strategies, our tool implementation1 utilizes the statistical model-checker Uppaal Stratego to approximate near-optimal solutions. In our evaluation, we investigate efficiency/effectiveness trade-offs by considering a real-world example consisting of a reconfigurable robot support system for the construction of aircraft fuselages.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {31–42},
numpages = {12},
keywords = {strategy synthesis, stochastic timed game automata, statistical model-checking, proactive self-adaptation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00037,
author = {Yan, Ming and Chen, Junjie and Mao, Hangyu and Jiang, Jiajun and Hao, Jianye and Li, Xingjian and Tian, Zhao and Chen, Zhichao and Li, Dong and Xian, Zhangkong and Guo, Yanwei and Liu, Wulong and Wang, Bin and Sun, Yuefeng and Cui, Yongshun},
title = {Achieving Last-Mile Functional Coverage in Testing Chip Design Software Implementations},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00037},
doi = {10.1109/ICSE-SEIP58684.2023.00037},
abstract = {Defective chips may cause huge losses (even disasters), and thus ensuring the reliability of chips is fundamentally important. To ensure the functional correctness of chips, adequate testing is essential on the chip design implementation (CDI), which is the software implementation of the chip under design in hardware description languages, before putting on fabrication. Over the years, while some techniques targeting CDI functional testing have been proposed, there are still a number of hard-to-cover functionality points due to huge input space and complex constraints among variables in a test input. We call the coverage of these points last-mile functional coverage.Here, we propose the first technique targeting the significant challenge of improving last-mile functional coverage in CDI functional testing, called LMT, which does not rely on domain knowledge and CDI internal information. LMT first identifies the relevant variables in test inputs to the coverage of last-mile functionality points inspired by the idea of feature selection in machine learning, so as to largely reduce the search space. It then incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs (that satisfy complex constraints among variables) with a larger possibility. We conducted a practical study on two industrial CDIs in Huawei to evaluate LMT. The results show that LMT achieves at least 49.27% and 75.09% higher last-mile functional coverage than the state-of-the-art CDI test input generation techniques under the same number of test inputs, and saves at least 94.24% and 84.45% testing time to achieve the same functional coverage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {343–354},
numpages = {12},
keywords = {machine learning, functional coverage, test generation, chip design testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Product Line Architecture, Multi-Objective Optimization, Human-computer interaction},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {symmetries, product line, knowledge compilation, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {software product line, requirements engineering, natural language processing, feature model extraction, NLTK},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3644033.3644371,
author = {Zeyen, Olivier and Cordy, Maxime and Perrouin, Gilles and Acher, Mathieu},
title = {Preprocessing is What You Need: Understanding and Predicting the Complexity of SAT-based Uniform Random Sampling},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644371},
doi = {10.1145/3644033.3644371},
abstract = {Despite its NP-completeness, the Boolean satisfiability problem gave birth to highly efficient tools that are able to find solutions to a Boolean formula and compute their number. Boolean formulae compactly encode huge, constrained search spaces for variability-intensive systems, e.g., the possible configurations of the Linux kernel. These search spaces are generally too big to explore exhaustively, leading most testing approaches to sample a few solutions before analysing them. A desirable property of such samples is uniformity: each solution should get the same selection probability. This property motivated the design of uniform random samplers, relying on SAT solvers and counters and achieving different tradeoffs between uniformity and scalability. Though we can observe their performance in practice, understanding the complexity these tools face and accurately predicting it is an under-explored problem. Indeed, structural metrics such as the number of variables and clauses involved in a formula poorly predict the sampling complexity. More elaborated ones, such as minimal independent support (MIS), are intractable to compute on large formulae. We provide an efficient parallel algorithm to compute a related metric, the number of equivalence classes, and demonstrate that this metric is highly correlated to time and memory usage of uniform random sampling and model counting tools. We explore the role of formula preprocessing on various metrics and show its positive influence on correlations. Relying on these correlations, we train an efficient classifier (F1-score 0.97) to predict whether uniformly sampling a given formula will exceed a specified budget. Our results allow us to characterise the similarities and differences between (uniform) sampling, solving and counting.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {23–32},
numpages = {10},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3385412.3386016,
author = {He, Jingxuan and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
title = {Learning fast and precise numerical analysis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386016},
doi = {10.1145/3385412.3386016},
abstract = {Numerical abstract domains are a key component of modern static analyzers. Despite recent advances, precise analysis with highly expressive domains remains too costly for many real-world programs. To address this challenge, we introduce a new data-driven method, called LAIT, that produces a faster and more scalable numerical analysis without significant loss of precision. Our approach is based on the key insight that sequences of abstract elements produced by the analyzer contain redundancy which can be exploited to increase performance without compromising precision significantly. Concretely, we present an iterative learning algorithm that learns a neural policy that identifies and removes redundant constraints at various points in the sequence. We believe that our method is generic and can be applied to various numerical domains.  We instantiate LAIT for the widely used Polyhedra and Octagon domains. Our evaluation of LAIT on a range of real-world applications with both domains shows that while the approach is designed to be generic, it is orders of magnitude faster on the most costly benchmarks than a state-of-the-art numerical library while maintaining close-to-original analysis precision. Further, LAIT outperforms hand-crafted heuristics and a domain-specific learning approach in terms of both precision and speed.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1112–1127},
numpages = {16},
keywords = {Abstract interpretation, Machine learning, Numerical domains, Performance optimization},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {vulnerable configuration, vulnerabilities, testing, reasoning, pentesting, feature model, cybersecurity},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3546932.3547007,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Quality-aware analysis and optimisation of virtual network functions},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547007},
doi = {10.1145/3546932.3547007},
abstract = {The softwarisation and virtualisation of network functionality is the last milestone in the networking industry. Software-Defined Networks (SDN) and Network Function Virtualization (NFV) offer the possibility of using software to manage computer and mobile networks and build novel Virtual Network Functions (VNFs) deployed in heterogeneous devices. To reason about the variability of network functions and especially about the quality of a software product defined as a set of VNFs instantiated as part of a service (i.e., Service Function Chaining), a variability model along with a quality model is required.However, this domain imposes certain challenges to quality-aware reasoning of service function chains, such as numerical features or configuration-level Quality Attributes (QAs) (e.g., energy consumption). Incorporating numerical reasoning with quality data into SPL analyses is challenging and tool support is rare. In this work, we present 3 groups of operations: model report, aggregate functions to dynamically convert QAs at the feature-level into the configuration-level, and quality-aware optimisation. Our objective is to test the most complete reasoning tools to exploit the extended variability with quality attributes needed for VNFs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210–221},
numpages = {12},
keywords = {virtual network function, variability, reasoning, quality attribute, optimization, numerical feature},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3643690.3648595,
author = {Kedziora, Damian and Siemon, Dominik and Elshan, Edona and So\'{n}ta, Monika},
title = {Towards stability, predictability, and quality of intelligent automation services: ECIT product journey from on-premise to as-a-service},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648595},
doi = {10.1145/3643690.3648595},
abstract = {The intensive transformations of software products have been widely discussed in academia and business from various perspectives, yet with limited reference to low-code technologies, such as robotic process automation (RPA). The intensive growth in the size and importance of RPA and low-code industry puts their offering transitions from traditional on-premise to modern software-as-a-service models at the core of its paradigm evolution. Our single case study presents the transformation from 'On-Prem' to 'RaaS-P product for intelligent automation (IA) services, conducted by the leading Nordic consultancy ECIT Group, elaborating on its triggers, journey, stakeholders, as well as implications on cost, quality, and customer satisfaction. Triggered by internal experiences, cost, and market pressures, the case allowed us to discover that while RPA technology has rapidly grown to become a commodity at a wide amount of business organizations, the RPA service providers need to aim at resolving the issues and addressing challenges of their customers, not technology itself. It brings forward the 'Robot as a Service - Process Automation' product, with a novel approach to SLA, focused on availability, job stability, recovery, and transaction quality.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {15–23},
numpages = {9},
keywords = {robotic process automation, low code, business models},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {similarity metrics, product lines, feature-based representation, commonality and variability, case-based reasoning},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {IoT, time series forecasting, software architecture evaluation, Continuous evaluation}
}

@inproceedings{10.1145/2384616.2384673,
author = {K\"{a}stner, Christian and Ostermann, Klaus and Erdweg, Sebastian},
title = {A variability-aware module system},
year = {2012},
isbn = {9781450315616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384616.2384673},
doi = {10.1145/2384616.2384673},
abstract = {Module systems enable a divide and conquer strategy to software development. To implement compile-time variability in software product lines, modules can be composed in different combinations. However, this way, variability dictates a dominant decomposition. As an alternative, we introduce a variability-aware module system that supports compile-time variability inside a module and its interface. So, each module can be considered a product line that can be type checked in isolation. Variability can crosscut multiple modules. The module system breaks with the antimodular tradition of a global variability model in product-line development and provides a path toward software ecosystems and product lines of product lines developed in an open fashion. We discuss the design and implementation of such a module system on a core calculus and provide an implementation for C as part of the TypeChef project. Our implementation supports variability inside modules from #ifdef preprocessor directives and variable linking at the composition level. With our implementation, we type check all configurations of all modules of the open source product line Busybox with 811~compile-time options, perform linker check of all configurations, and report found type and linker errors -- without resorting to a brute-force strategy.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {773–792},
numpages = {20},
keywords = {variability, software product lines, preprocessor, module system, linker, conditional compilation, composition, C, #ifdef},
location = {Tucson, Arizona, USA},
series = {OOPSLA '12}
}

@inproceedings{10.1145/3579028.3609017,
author = {Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {On the Reuse of Existing Configurations for Testing Evolving Feature Models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609017},
doi = {10.1145/3579028.3609017},
abstract = {Software Product Lines (SPLs) are used for representing a variety of highly configurable systems or families of systems. They are commonly represented by feature models (FMs). Starting from FMs, configurations, used as test cases, can be generated to identify the products of interest for further activities. As the other types of software, SPLs and their FMs may evolve due to changing requirements or bug-fixing. However, no guidance is usually given on what to do with derived configurations when an FM evolves. The common approach is based on generating all configurations from scratch, which is not optimal since a greater effort is required for concretizing the new tests, and some of the old ones may be still applicable.In this paper, we present the use of a technique for generating combinatorial tests for evolving feature models: this technique incrementally builds the new combinatorial configuration set starting from the one generated from the previous model. Furthermore, we present a novel definition of dissimilarity among configuration sets that can be used to evaluate how much an evolved test suite differs from the previous one and thus allows evaluating the effort required for adapting old test cases to the new ones.Our experiments confirm that using the proposed technique, in general, leads to lower dissimilarity and test suite size w.r.t. the generation of tests from scratch.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {67–76},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579028.3609008,
author = {Galindo, Jos\'{e} A. and Horcas, Jose-Miguel and Felferning, Alexander and Fernandez-Amoros, David and Benavides, David},
title = {FLAMA: A collaborative effort to build a new framework for the automated analysis of feature models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609008},
doi = {10.1145/3579028.3609008},
abstract = {Nowadays, feature models are the de facto standard when representing commonalities and variability, with modern examples spanning up to 7000 features. Manual analysis of such models is challenging and error-prone due to sheer size. To help in this task, automated analysis of feature models (AAFM) has emerged over the past three decades. However, the diversity of these tools and their supported languages presents a significant challenge that motivated the MOD-EVAR community to initiate a project for a new tool that supports the UVL language. Despite the rise of machine learning and data science, along with robust Python-based libraries, most AAFM tools have been implemented in Java, creating a collaboration gap. This paper introduces Flama, an innovative framework that automates the analysis of variability models. It focuses on UVL model analysis and aims for easy integration and extensibility to bridge this gap and foster better community and cross-community collaboration.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {16–19},
numpages = {4},
keywords = {visualization design process, variability, software product line, graphs and tables, effective communication, data visualization},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3555776.3578611,
author = {Limaylla-Lunarejo, Maria-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
title = {Towards a FAIR Dataset for non-functional requirements},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578611},
doi = {10.1145/3555776.3578611},
abstract = {In the last years, the application of supervised Machine Learning (ML) algorithms in Requirements Engineering (RE) has allowed increasing the performance (e.g. accuracy, precision) and scalability of automatic requirements classification. However, the lack of publicly labeled datasets is still one concern when conducting ML experiments. Few publicly labeled datasets for non-functional requirements classification are available, and even less in the Spanish language. Moreover, most of the available datasets present some limitations, such as imbalanced classes (e.g. PROMISE NFR). This study aims to generate a FAIR dataset of non-functional requirements in the Spanish language for facilitating reuse in ML classification experiments. 109 non-functional requirements were collected from final degree projects from the University of A Coru\~{n}a. We conducted a pilot quasi-experiment for non-functional requirements labeling in the categories and subcategories of the ISO/IEC 25010 quality model. The labeling process was accomplished by 7 annotators. The inter-annotator agreement using a Fleiss' Kappa test obtained a substantial agreement in the category level (0.78) and a moderate agreement (0.48) when the classification is per subcategory.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1414–1421},
numpages = {8},
keywords = {FAIR principles, spanish dataset, non-functional requirements, data labeling},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3522664.3528603,
author = {Husom, Erik Johannes and Tverdal, Simeon and Goknil, Arda and Sen, Sagar},
title = {UDAVA: an unsupervised learning pipeline for sensor data validation in manufacturing},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528603},
doi = {10.1145/3522664.3528603},
abstract = {Manufacturing has enabled the mechanized mass production of the same (or similar) products by replacing craftsmen with assembly lines of machines. The quality of each product in an assembly line greatly hinges on continual observation and error compensation during machining using sensors that measure quantities such as position and torque of a cutting tool and vibrations due to possible imperfections in the cutting tool and raw material. Patterns observed in sensor data from a (near-)optimal production cycle should ideally recur in subsequent production cycles with minimal deviation. Manually labeling and comparing such patterns is an insurmountable task due to the massive amount of streaming data that can be generated from a production process. We present UDAVA, an unsupervised machine learning pipeline that automatically discovers process behavior patterns in sensor data for a reference production cycle. UDAVA performs clustering of reduced dimensionality summary statistics of raw sensor data to enable high-speed clustering of dense time-series data. It deploys the model as a service to verify batch data from subsequent production cycles to detect recurring behavior patterns and quantify deviation from the reference behavior. We have evaluated UDAVA from an AI Engineering perspective using two industrial case studies.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {159–169},
numpages = {11},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3338906.3342508,
author = {Radavelli, Marco},
title = {Using software testing to repair models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342508},
doi = {10.1145/3338906.3342508},
abstract = {Software testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1253–1255},
numpages = {3},
keywords = {timed automata, software testing, software product lines, search-based software engineering, mutation, model repair, CIT},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@article{10.1145/3704905,
author = {Cai, Yufan and Hou, Zhe and Sanan, David and Luan, Xiaokun and Lin, Yun and Sun, Jun and Dong, Jin Song},
title = {Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704905},
doi = {10.1145/3704905},
abstract = {Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To initiate this vision, we propose Refine4LLM, an approach that aims to:
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Formally refine the specifications,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(2) Automatically prompt and guide the LLM using refinement calculus,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(3) Interact with the LLM to generate the code,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(5) Learn and build more advanced refinement laws to extend the refinement calculus.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {69},
numpages = {33},
keywords = {Large Language Model, Program Refinement, Program Synthesis}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.5555/1953048.2021053,
author = {Bubeck, S\'{e}bastien and Munos, R\'{e}mi and Stoltz, Gilles and Szepesv\'{a}ri, Csaba},
title = {X-Armed Bandits},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by √n, that is, the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {1655–1695},
numpages = {41}
}

@article{10.1145/3630252,
author = {Lustosa, Andre and Menzies, Tim},
title = {Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630252},
doi = {10.1145/3630252},
abstract = {When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK’s 12-month prediction errors are {I=0%, R=33%&nbsp;C=47%}, whereas other methods have far larger errors of {I=61%,R=119%&nbsp;C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {58},
numpages = {22},
keywords = {Hyperparameter tuning, software health, indepedent variable clustering}
}

@article{10.1145/3609333,
author = {Li, Zhong and Zhu, Yuxuan and Van Leeuwen, Matthijs},
title = {A Survey on Explainable Anomaly Detection},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3609333},
doi = {10.1145/3609333},
abstract = {In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {23},
numpages = {54},
keywords = {explainable artificial intelligence, explainable machine learning, outlier detection, anomaly detection, anomaly explanation, interpretable anomaly detection, Explainable anomaly detection}
}

@inproceedings{10.1145/3038912.3052704,
author = {Wu, Chao-Yuan and Ahmed, Amr and Kumar, Gowtham Ramani and Datta, Ritendra},
title = {Predicting Latent Structured Intents from Shopping Queries},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052704},
doi = {10.1145/3038912.3052704},
abstract = {In online shopping, users usually express their intent through search queries. However, these queries are often ambiguous. For example, it is more likely (and easier) for users to write a query like "high-end bike" than "21 speed carbon frames jamis or giant road bike". It is challenging to interpret these ambiguous queries and thus search result accuracy suffers. A user oftentimes needs to go through the frustrating process of refining search queries or self-teaching from possibly unstructured information. However, shopping is indeed a structured domain, that is composed of category hierarchy, brands, product lines, features, etc. It would be much better if a shopping site could understand users' intent through this structure, present organized information, and then find the items with the right categories, brands or features.In this paper we study the problem of inferring the latent intent from unstructured queries and mapping them to structured attributes. We present a novel framework that jointly learns this knowledge from user consumption behaviors and product metadata. We present a hybrid Long Short-term Memory (LSTM) joint model that is accurate and robust, even though user queries are noisy and product catalog is rapidly growing. Our study is conducted on a large-scale dataset from Google Shopping, that is composed of millions of items and user queries along with their click responses. Extensive qualitative and quantitative evaluation shows that the proposed model is more accurate, concise, and robust than multiple possible alternatives. In terms of information retrieval (IR) performance, our model is able to improve the quality of current Google Shopping production system, which is a very strong baseline.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1133–1141},
numpages = {9},
keywords = {shopping, recurrent neural networks, query understanding, entity relationship modeling, autoencoder},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/3715111,
author = {Abrah\~{a}o, Silvia and Grundy, John and Pezz\`{e}, Mauro and Storey, Margaret-Anne and Andrew Tamburri, Damian},
title = {Software Engineering by and for Humans in an AI Era},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715111},
doi = {10.1145/3715111},
abstract = {The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb
}

@inproceedings{10.1145/3358960.3379134,
author = {Garbi, Giulio and Incerto, Emilio and Tribastone, Mirco},
title = {Learning Queuing Networks by Recurrent Neural Networks},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379134},
doi = {10.1145/3358960.3379134},
abstract = {It is well known that building analytical performance models in practice is difficult because it requires a considerable degree of proficiency in the underlying mathematics. In this paper, we pro- pose a machine-learning approach to derive performance models from data. We focus on queuing networks, and crucially exploit a deterministic approximation of their average dynamics in terms of a compact system of ordinary differential equations. We encode these equations into a recurrent neural network whose weights can be directly related to model parameters. This allows for an inter- pretable structure of the neural network, which can be trained from system measurements to yield a white-box parameterized model that can be used for prediction purposes such as what-if analyses and capacity planning. Using synthetic models as well as a real case study of a load-balancing system, we show the effectiveness of our technique in yielding models with high predictive power.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {56–66},
numpages = {11},
keywords = {software performance, recurrent neural networks, queuing networks},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1145/3464305,
author = {Sobhy, Dalia and Bahsoon, Rami and Minku, Leandro and Kazman, Rick},
title = {Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464305},
doi = {10.1145/3464305},
abstract = {Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {51},
numpages = {50},
keywords = {uncertainty, run-time software architecture evaluation, design-time software architecture evaluation, Continuous software architecture evaluation}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.5555/3466184.3466402,
author = {Ratusny, Marco and Ay, Alican and Ponsignon, Thomas},
title = {Characterizing customer ordering behaviors in semiconductor supply chains with convolutional neural networks},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Advancements in the semiconductor industry have resulted in the need for extracting vital information from vast amounts of data. In the operational processes of demand planning and order management, it is important to understand customer demand data due to its potential to provide insights for managing supply chains. For this purpose, customer ordering behaviors are visualized in the form of two-dimensional heat maps. The goal is to classify the customers into predefined ordering patterns on the example of a semiconductor manufacturing, namely Infineon Technologies. Therefore, a convolutional neural network is used. By classifying the customers into preselected ordering patterns, a better understanding on how the customer demand develops over time is achieved. The results show that customers have a certain ordering pattern, but their behavior can be meaningfully classified only to a certain extend due to unidentified behaviors in the data. Further research could identify additional ordering patterns.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1931–1942},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1145/3464939,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat},
title = {Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464939},
doi = {10.1145/3464939},
abstract = {Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {53},
numpages = {36},
keywords = {testing, multi-objective search, mined rules, interacting products, configuration recommendation, Product line}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@inproceedings{10.1145/3316782.3321549,
author = {Gollasch, David and Engel, Christin and Branig, Meinhardt and Weber, Gerhard},
title = {Applying software variability methods to design adaptive assistance robots},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3321549},
doi = {10.1145/3316782.3321549},
abstract = {We believe that assistance and service robots are a major technological trend that could lead to a spread similar to smartphones today. Thus, it could be normal to have such a robot at home to support people during their daily life. The main challenge with today's developments in this segment is a lack of features and a lack of adaptivity. Most service robots are specialised systems (vacuum cleaners) or do not consider the user's needs and preferences when fulfilling their tasks. Our goal is to cope with these challenges and propose a solution by applying principles of software variability engineering to create an adaptive and accessible robot that can be extended and re-configured to fit the user's needs better. Therefore, we modified feature and variability modelling known from product lines and software ecosystems and developed an approach to configure the robot based on user requirements (needs and preferences).},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {313–314},
numpages = {2},
keywords = {software variability, software ecosystems, personal robot, needs and preferences, human-robot interaction, human-computer interaction, assistive technology, adaptive technology, accessibility},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/1993478.1993495,
author = {Singer, Jeremy and Kovoor, George and Brown, Gavin and Luj\'{a}n, Mikel},
title = {Garbage collection auto-tuning for Java mapreduce on multi-cores},
year = {2011},
isbn = {9781450302630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993478.1993495},
doi = {10.1145/1993478.1993495},
abstract = {MapReduce has been widely accepted as a simple programming pattern that can form the basis for efficient, large-scale, distributed data processing. The success of the MapReduce pattern has led to a variety of implementations for different computational scenarios. In this paper we present MRJ, a MapReduce Java framework for multi-core architectures. We evaluate its scalability on a four-core, hyperthreaded Intel Core i7 processor, using a set of standard MapReduce benchmarks. We investigate the significant impact that Java runtime garbage collection has on the performance and scalability of MRJ. We propose the use of memory management auto-tuning techniques based on machine learning. With our auto-tuning approach, we are able to achieve MRJ performance within 10% of optimal on 75% of our benchmark tests.},
booktitle = {Proceedings of the International Symposium on Memory Management},
pages = {109–118},
numpages = {10},
keywords = {mapreduce, machine learning, java, garbage collection},
location = {San Jose, California, USA},
series = {ISMM '11}
}

@inproceedings{10.1145/2372233.2372237,
author = {Mendes, Emilia and Azhar, Damir},
title = {The role of systematic reviews in identifying the state of the art in web resource estimation},
year = {2012},
isbn = {9781450315098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372233.2372237},
doi = {10.1145/2372233.2372237},
abstract = {The goal of this position paper is to motivate the importance of SRs, and to present a SR of Web resource estimation. The SR results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique.},
booktitle = {Proceedings of the 2nd International Workshop on Evidential Assessment of Software Technologies},
pages = {3–8},
numpages = {6},
keywords = {web resource estimation, systematic review},
location = {Lund, Sweden},
series = {EAST '12}
}

@inproceedings{10.1145/3397271.3401041,
author = {Chen, Fanglin and Liu, Xiao and Proserpio, Davide and Troncoso, Isamar and Xiong, Feiyu},
title = {Studying Product Competition Using Representation Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401041},
doi = {10.1145/3397271.3401041},
abstract = {Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1261–1268},
numpages = {8},
keywords = {representation learning, product2vec, product competition},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@article{10.1145/3649319,
author = {Quattrocchi, Giovanni and Heuvel, Willem-Jan van den and Tamburri, Damian Andrew},
title = {The Data Product-service Composition Frontier: A Hybrid Learning Approach},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3649319},
doi = {10.1145/3649319},
abstract = {The service dominant logic is a base concept behind modern economies and software products, with service composition being a well-known practice for companies to gain a competitive edge over others by joining differentiated services together, typically assembled according to a number of features. At the other end of the spectrum, product compositions are a marketing device to sell products together in bundles that often augment the value for the customer, e.g., with suggested product interactions, sharing, and so on. Unfortunately, currently each of these two streams—namely, product and service composition—are carried out and delivered individually in splendid isolation: anything is being offered as a product and as a service, disjointly. We argue that the next wave of services computing features more and more service fusion with physical counterparts as well as data around them. Therefore a need emerges to investigate the interactive engagement of both (data) products and services. This manuscript offers a real-life implementation in support of this argument, using (1) genetic algorithms (GA) to shape product-service clusters, (2) end-user feedback to make the GAs interactive with a data-driven fashion, and (3) a hybridized approach which factors into our solution an ensemble machine-learning method considering additional features. All this research was conducted in an industrial environment. With such a cross-fertilized, data-driven, and multi-disciplinary approach, practitioners from both fields may benefit from their mutual state of the art as well as learn new strategies for product, service, and data product-service placement for increased value to the customer as well as the service provider. Results show promise but also highlight plenty of avenues for further research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {22},
keywords = {Anomaly detection, time series, unsupervised, literature review}
}

@article{10.1145/3612918,
author = {Sun, Danfeng and Hu, Junjie and Wu, Huifeng and Wu, Jia and Yang, Jian and Sheng, Quan Z. and Dustdar, Schahram},
title = {A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3612918},
doi = {10.1145/3612918},
abstract = {The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {50},
numpages = {37}
}

@inproceedings{10.1145/2365324.2365332,
author = {Azhar, Damir and Mendes, Emilia and Riddle, Patricia},
title = {A systematic review of web resource estimation},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365332},
doi = {10.1145/2365324.2365332},
abstract = {Background: Web development plays an important role in today's industry, so an in depth view into Web resource estimation would be valuable. However a systematic review (SR) on Web resource estimation in its entirety has not been done.Aim: The aim of this paper is to present a SR of Web resource estimation in order to define the current state of the art, and to identify any research gaps that may be present.Method: Research questions that would address the current state of the art in Web resource estimation were first identified. A comprehensive literature search was then executed resulting in the retrieval of 84 empirical studies that investigated any aspect of Web resource estimation. Data extraction and synthesis was performed on these studies with these research questions in mind.Results: We have found that there are no guidelines with regards to what resource estimation technique should be used in a particular estimation scenario, how it should be implemented, and how its effectiveness should be evaluated. Accuracy results vary widely and are dependent on numerous factors. Research has focused on development effort/cost estimation, neglecting other facets of resource estimation like quality and maintenance. Size measures have been used in all but one study as a resource predictor.Conclusions: Our results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {49–58},
numpages = {10},
keywords = {systematic review, web resource estimation},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1145/3450445,
author = {Roy, Soumyadeep and Sural, Shamik and Chhaya, Niyati and Natarajan, Anandhavelu and Ganguly, Niloy},
title = {An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis, and Recommendation},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3450445},
doi = {10.1145/3450445},
abstract = {A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.},
journal = {ACM Trans. Web},
month = may,
articleno = {9},
numpages = {25},
keywords = {text classification, sentence ranking, online reputation management, Brand personality}
}

@inproceedings{10.1145/1595696.1595716,
author = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
title = {Fair and balanced? bias in bug-fix datasets},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595716},
doi = {10.1145/1595696.1595716},
abstract = {Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical data from several software projects, and find strong evidence of systematic bias. We then investigate the potential effects of "unfair, imbalanced" datasets on the performance of prediction techniques. We draw the lesson that bias is a critical problem that threatens both the effectiveness of processes that rely on biased datasets to build prediction models and the generalizability of hypotheses tested on biased data.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {121–130},
numpages = {10},
keywords = {bias},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/1868328.1868335,
author = {Corazza, A. and Di Martino, S. and Ferrucci, F. and Gravino, C. and Sarro, F. and Mendes, E.},
title = {How effective is Tabu search to configure support vector regression for effort estimation?},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868335},
doi = {10.1145/1868328.1868335},
abstract = {Background. Recent studies have shown that Support Vector Regression (SVR) has an interesting potential in the field of effort estimation. However applying SVR requires to carefully set some parameters that heavily affect the prediction accuracy. No general guidelines are available to select these parameters, whose choice also depends on the characteristics of the data set used. This motivates the work described in this paper. Aims. We have investigated the use of an optimization technique in combination with SVR to select a suitable subset of parameters to be used for effort estimation. This technique is named Tabu Search (TS), which is a meta-heuristic approach used to address several optimization problems. Method. We employed SVR with linear and RBF kernels, and used variables' preprocessing strategies (i.e., logarithmic). As for the data set, we employed the Tukutuku cross-company database, which is widely adopted in Web effort estimation studies, and performed a hold-out validation using two different splits of the data set. As benchmark, results are compared to those obtained with Manual StepWise Regression, Case-Based Reasoning, and Bayesian Networks. Results. Our results show that TS provides a good choice of parameters, so that the combination of TS and SVR outperforms any other technique applied on this data set. Conclusions. The use of the meta-heuristic Tabu Search allowed us to obtain (I) an automatic choice of the parameters required to run SVR, and (II) a significant improvement on prediction accuracy for SVR. While we are not guaranteed that this is the global optimum, the results we are presenting are the best performance ever obtained on the problem at the hand, up to now. Of course, the experimental results here presented should be assessed on further data. However, they are surely interesting enough to suggest the use of SVR among the techniques that are suitable for effort estimation, especially when using a cross-company database.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {4},
numpages = {10},
keywords = {Tabu search, development effort estimation, empirical studies, support vector machines, support vector regression},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3192366.3192416,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {A data-driven CHC solver},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192416},
doi = {10.1145/3192366.3192416},
abstract = {We present a data-driven technique to solve Constrained Horn Clauses (CHCs) that encode verification conditions of programs containing unconstrained loops and recursions. Our CHC solver neither constrains the search space from which a predicate's components are inferred (e.g., by constraining the number of variables or the values of coefficients used to specify an invariant), nor fixes the shape of the predicate itself (e.g., by bounding the number and kind of logical connectives). Instead, our approach is based on a novel machine learning-inspired tool chain that synthesizes CHC solutions in terms of arbitrary Boolean combinations of unrestricted atomic predicates. A CEGAR-based verification loop inside the solver progressively samples representative positive and negative data from recursive CHCs, which is fed to the machine learning tool chain. Our solver is implemented as an LLVM pass in the SeaHorn verification framework and has been used to successfully verify a large number of nontrivial and challenging C programs from the literature and well-known benchmark suites (e.g., SV-COMP).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {707–721},
numpages = {15},
keywords = {Program Verification, Invariant Inference, Data-Driven Analysis, Constrained Horn Clauses (CHCs)},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {software product line testing, search-based software engineering, preference-based multi-objective algorithms},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/1101908.1101941,
author = {Langelier, Guillaume and Sahraoui, Houari and Poulin, Pierre},
title = {Visualization-based analysis of quality for large-scale software systems},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101941},
doi = {10.1145/1101908.1101941},
abstract = {We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {214–223},
numpages = {10},
keywords = {software visualization, quality assessment, metrics},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@proceedings{10.1145/3568834,
title = {ICIBE '22: Proceedings of the 8th International Conference on Industrial and Business Engineering},
year = {2022},
isbn = {9781450397582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@inproceedings{10.1145/2460999.2461002,
author = {Matos, Olavo and Fortaleza, Luiz and Conte, Tayana and Mendes, Emilia},
title = {Realising web effort estimation: a qualitative investigation},
year = {2013},
isbn = {9781450318488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460999.2461002},
doi = {10.1145/2460999.2461002},
abstract = {Context: Reliable effort estimation is essential for better management of Web projects, hence the need to identify what are the key factors that affect effort estimates for new Web projects and how they are inter-related. Objective: This paper improves our understanding of Web effort estimation using as basis the knowledge of Web effort estimation experts. Method: We employed qualitative research with the participation of four different Web development companies in Manaus (Brazil) using semi-structured interviews for data collection; our data analysis was carried out using Grounded Theory-based procedures to identify and combine factors affecting the estimation effort of Web projects. Results: We identified four main groupings (categories) of factors - Web project, Web development complexity, Web development team, and Clients. Each of these groupings contains a set of factors that impact upon Web effort estimation. Conclusions: This is the first time that qualitative research is employed in the field of Web effort estimation to further understand and help improve this process. In addition, some of the factors found had never been identified in any of the previous studies in this field, thus suggesting that the use of Grounded Theory-based procedures may provide a way to enrich our understanding of the phenomenon under investigation via the identification of factors that overlap and also complement those from previous studies.},
booktitle = {Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {web project management, web effort predictors, web effort estimation, qualitative studies, grounded-theory},
location = {Porto de Galinhas, Brazil},
series = {EASE '13}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1109/WI-IAT.2009.118,
author = {Ziegler, Cai-Nicolas and Viermetz, Maximilian},
title = {Discovery of Technology Synergies through Collective Wisdom},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.118},
doi = {10.1109/WI-IAT.2009.118},
abstract = {Spotting and quantifying technological synergies across organizational levels is of utter importance for corporate strategy departments. These efforts aim at saving resources by consolidating scattered expertise and by reusing technologies across multiple product lines. In the past, this task has been done in a manual process by domain experts. While feasible, the major drawback lies in the enormous cost of time: For a structured and complete analysis every combination of any two technologies has to be assessed. We present an approach that discovers those synergies in an automated fashion, using collective wisdom from the Web. Our method has been deployed for the synergy evaluation process within Siemens. We have also conducted evaluations based on randomly selected technology pairs so as to benchmark the accuracy of our approach, as compared to a group of general computer science technologists as well as a control group of domain experts.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {701–706},
numpages = {6},
keywords = {text mining, synergy, similarity, semantics},
series = {WI-IAT '09}
}

@proceedings{10.1145/3639856,
title = {AIMLSystems '23: Proceedings of the Third International Conference on AI-ML Systems},
year = {2023},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/2872518.2891070,
author = {Lakshminarayan, Choudur and Kosuru, Ram and Hsu, Meichun},
title = {Modeling Complex Clickstream Data by Stochastic Models: Theory and Methods},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891070},
doi = {10.1145/2872518.2891070},
abstract = {As the website is a primary customer touch-point, millions are spent to gather web data about customer visits. Sadly, the trove of data and corresponding analytics have not lived up to the promise. Current marketing practice relies on ambiguous summary statistics or small-sample usability studies. Idiosyncratic browsing and low conversion (browser-to-buyer) make modeling hard. In this paper, we model browsing patterns (sequence of clicks) via Markov chain theory to predict users' propensity to buy within a session. We focus on model complexity, imputing missing values, data augmentation, and other attendant issues that impact performance. The paper addresses the following aspects; (1) Determine appropriate order of the Markov chain (assess the influence of prior history in prediction), (2) Impute missing transitions by exploiting the inherent link structure in the page sequences, (3) predict the likelihood of a purchase based on variable-length page sequences, and (4) Augment the training set of buyers (which is typically very small: 2% by viewing the page transitions as a graph and exploiting its link structure to improve performance. The cocktail of solutions address important issues in practical digital marketing. Extensive analysis of data applied to a large commercial web-site shows that Markov chain based classifiers are useful predictors of user intent.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {879–884},
numpages = {6},
keywords = {prediction, markov chains, link analysis, imputation, click streams},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {software metrics, defect prediction, cost-sensitive classification},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/1571941.1572035,
author = {Ziegler, Cai-Nicolas and Jung, Stefan},
title = {Leveraging sources of collective wisdom on the web for discovering technology synergies},
year = {2009},
isbn = {9781605584836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1571941.1572035},
doi = {10.1145/1571941.1572035},
abstract = {One of the central tasks of R&amp;D strategy and portfolio management at large technology companies and research institutions refers to the identification of technological synergies throughout the organization. These efforts are geared towards saving resources by consolidating scattered expertise, sharing best practices, and reusing available technologies across multiple product lines. In the past, this task has been done in a manual evaluation process by technical domain experts. While feasible, the major drawback of this approach is the enormous effort in terms of availability and time: For a structured and complete analysis every combination of any two technologies has to be rated explicitly. We present a novel approach that recommends technological synergies in an automated fashion, making use of abundant collective wisdom from the Web, both in pure textual form as well as classification ontologies. Our method has been deployed for practical support of the synergy evaluation process within our company. We have also conducted empirical evaluations based on randomly selected technology pairs so as to benchmark the accuracy of our approach, as compared to a group of general computer science technologists as well as a control group of domain experts.},
booktitle = {Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {548–555},
numpages = {8},
keywords = {web 2.0, text mining, semantic similarity, collective wisdom},
location = {Boston, MA, USA},
series = {SIGIR '09}
}

@proceedings{10.1145/3643690,
title = {IWSiB '24: Proceedings of the 7th ACM/IEEE International Workshop on Software-intensive Business},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The workshop brings together research communities working on softwareintensive business and software engineering. It aims to bridge the gap between the research in these areas. This year's theme, "Software Business in the Era of Generative Artificial Intelligence," reflects our focus on exploring how generative artificial intelligence (GenAI) and the related large language models (LLMs) impact the established practices of software engineering and software business.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@article{10.1007/s00165-019-00479-y,
author = {Dimovski, Aleksandar S. and Brabrand, Claus and W\k{a}sowski, Andrzej},
title = {Finding suitable variability abstractions for lifted analysis},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-019-00479-y},
doi = {10.1007/s00165-019-00479-y},
abstract = {Many software systems are today variational: they are built as program families or Software Product Lines. They can produce a potentially huge number of related programs, known as products or variants, by selecting suitable configuration options (features) at compile time. Many such program families are safety critical, yet the appropriate tools only rarely are able to analyze them effeciently. Researchers have addressed this problem by designing specialized variability-aware static (dataflow) analyses, which allow analyzing all variants of the family, simultaneously, in a single run without generating any of the variants explicitly. They are also known as lifted or family-based analyses. They take as input the common code base, which encodes all variants of a program family, and produce precise analysis results corresponding to all variants. These analyses scale much better than “brute force” approach, where all individual variants are analyzed in isolation, one-by-one, using off-the-shelf single-program analyzers. Nevertheless, the computational cost of lifted analyses still greatly depends on the number of features and variants (which is often huge). For families with a large number of features and variants, the lifted analyses may be too costly or even infeasible. In order to speed up lifted analyses and make them computationally cheaper, variability abstractions which simplify variability away from program families and lifted analyses have been introduced. However, the space of possible variability abstractions is still intractably large to search naively, with most abstractions being either too imprecise or too costly. We introduce here a method to efficiently find suitable variability abstractions from a large space of possible abstractions for a lifted static analysis. The main idea is to use a pre-analysis to estimate the impact of variability-specific parts of the program family on the analysis’s precision. The pre-analysis is fully variability-aware while it aggressively abstracts the other semantics aspects. Then we use the pre-analysis results to find out when and where the subsequent abstract lifted analysis should turn off or on its variability-awareness. The abstraction constructed in this way is effective in discarding variability-specific program details that are irrelevant for showing the analysis’s ultimate goal. We formalize this approach and we illustrate its effectiveness on several Java case studies. The evaluation shows that our approach which consists of running a pre-analysis followed by a subsequent abstract lifted analysis achieves competitive the precision-speed tradeoff compared to the standard lifted analysis.},
journal = {Form. Asp. Comput.},
month = apr,
pages = {231–259},
numpages = {29},
keywords = {Abstract interpretation, Variability abstractions, Lifted static analysis, Program families}
}

@inproceedings{10.1109/MODELS-C.2019.00046,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {Inferring metamodel relaxations based on structural patterns to support model families},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00046},
doi = {10.1109/MODELS-C.2019.00046},
abstract = {A model family is a set of related models in a given language that results from the evolution of models over time and/or variations over the space (product) dimension. To enable a more efficient analysis of family members, all at once, we have already proposed union models to capture the union of all elements in all family members, in a compact and exact manner. However, despite having each model in a model family conforming to the same metamodel, there is still no guarantee that their union model will conform to the original metamodel of the family members. This paper aims to support the representation of union models (as valid instances of a metamodel) by inferring, from the structure of the original metamodel, a relaxed metamodel to which a union model conforms. In particular, instead of relaxing all metamodel constraints, the paper contributes a heuristic method that relaxes particular constraints (related only to multiplicities of attributes and association ends) by inferring where such relaxations are needed in the metamodel. To infer relaxation points, structural patterns are first identified in metamodels, then an evidence-based or an anticipation-based approach is applied to get the actual inference. The purpose behind inferring particular metamodel relaxation points is to be able to adapt the existing tools and analysis techniques once and minimally for all potential model families of a given modeling language.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {294–303},
numpages = {10},
keywords = {metamodel, metamodel relaxation, model, model family, relaxation point, structural pattern, union model},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/1370750.1370762,
author = {Layman, Lucas and Nagappan, Nachiappan and Guckenheimer, Sam and Beehler, Jeff and Begel, Andrew},
title = {Mining software effort data: preliminary analysis of visual studio team system data},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370762},
doi = {10.1145/1370750.1370762},
abstract = {In the software development process, scheduling and predictability are important components to delivering a product on time and within budget. Effort estimation artifacts offer a rich data set for improving scheduling accuracy and for understanding the development process. Effort estimation data for 55 features in the latest release of Visual Studio Team System (VSTS) were collected and analyzed for trends, patterns, and differences. Statistical analysis shows that actual estimation error was positively correlated with feature size, and that in-process metrics of estimation error were also correlated with the final estimation error. These findings suggest that smaller features can be estimated more accurately, and that in-process estimation error metrics can be provide a quantitative supplement to developer intuition regarding high-risk features during the development process.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {43–46},
numpages = {4},
keywords = {prediction, effort estimation},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/1150402.1150423,
author = {Forman, George},
title = {Quantifying trends accurately despite classifier error and class imbalance},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150423},
doi = {10.1145/1150402.1150423},
abstract = {This paper promotes a new task for supervised machine learning research: quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set, with no concern for predictions on individual cases. A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. These tasks cover a large and important family of applications that measure trends over time.The paper establishes a research methodology, and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications. In empirical tests, Median Sweep methods show outstanding ability to estimate the class distribution, despite wide disparity in testing and training conditions. The paper addresses shifting class priors and costs, but not concept drift in general.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {157–166},
numpages = {10},
keywords = {text mining, quantification, cost quantification, classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@techreport{10.1145/3185595,
author = {Larus, James and Hankin, Chris and Carson, Siri Granum and Christen, Markus and Crafa, Silvia and Grau, Oliver and Kirchner, Claude and Knowles, Bran and McGettrick, Andrew and Tamburri, Damian Andrew and Werthner, Hannes},
title = {When Computers Decide: European Recommendations on Machine-Learned Automated Decision Making},
year = {2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past two decades, the ability of machines to challenge and beat humans at complex games has made “quantum” leaps, rhetorically if not in technical computing terms.In 1997, IBM's Deep Blue supercomputer used “brute force” computing power to out-calculate Grand Master Garry Kasparov at chess. In 2011, the company's Watson employed “machine learning” (ML) techniques to beat several former Jeopardy champions at their own game. In early 2016, Google's DeepMind AlphaGo program-trained by a massive game history-repeatedly defeated the reigning European champion at Go: a game that has more possible board configurations than there are atoms in the universe [1]. It reached this milestone by employing two neural networks powered by sophisticated “automated decision making” (ADM) algorithms. And, in 2017, AlphaGo Zero became the strongest Go player on the planet-human or machine-after just a few months of game-play training alone. Incredibly, it was programmed initially only with the rules of the game [2].Automated decision making concerns decision making by purely technological means without human involvement. Article 22(1) of the European General Data Protection Regulation (GDPR) enshrines the right of data subjects not to be subject to decisions, which have legal or other significant effects, being based solely on automatic individual decision making. As a consequence, in this paper we consider applications of ADM to applications other than those based on personal information, for example the game-playing discussed above. We discuss other aspects of GDPR later in the paper. Whilst the game-playing results are impressive, the consequences of machine learning and automated decision making are themselves, however, no game. As of this writing, they have progressed to enable computers to rival humans' ability at even more challenging, ambiguous, and highly skilled tasks with profound “real world” applications, such as: recognizing images, understanding speech, and analysing X-rays among many others. As these techniques continue to improve rapidly, many new and established companies are utilizing them to build applications that reliably perform activities that previously were done (and doable) only by people. Today, such systems can both augment human decision making and, in some cases, replace it with a fully autonomous system.In this report, we review the principal implications of the coming widespread adoption of MLdriven automated decision making with a particular emphasis on its technical, ethical, legal, economic, societal and educational ramifications. We also make a number of recommendations that policy makers might wish to consider.}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3600006.3613175,
author = {Jayaram Subramanya, Suhas and Arfeen, Daiyaan and Lin, Shouxu and Qiao, Aurick and Jia, Zhihao and Ganger, Gregory R.},
title = {Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613175},
doi = {10.1145/3600006.3613175},
abstract = {The Sia scheduler efficiently assigns heterogeneous deep learning (DL) cluster resources to elastic resource-adaptive jobs. Although some recent schedulers address one aspect or another (e.g., heterogeneity or resource-adaptivity), none addresses all and most scale poorly to large clusters and/or heavy workloads even without the full complexity of the combined scheduling problem. Sia introduces a new scheduling formulation that can scale to the search-space sizes and intentionally match jobs and their configurations to GPU types and counts, while adapting to changes in cluster load and job mix over time. Sia also introduces a low-profiling-overhead approach to bootstrapping (for each new job) throughput models used to evaluate possible resource assignments, and it is the first cluster scheduler to support elastic scaling of hybrid parallel jobs.Extensive evaluations show that Sia outperforms state-of-the-art schedulers. For example, even on relatively small 44- to 64-GPU clusters with a mix of three GPU types, Sia reduces average job completion time (JCT) by 30--93%, 99th percentile JCT and makespan by 28--95%, and GPU hours used by 12--55% for workloads derived from 3 real-world environments. Additional experiments demonstrate that Sia scales to at least 2000-GPU clusters, provides improved fairness, and is not over-sensitive to scheduler parameter settings.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {642–657},
numpages = {16},
keywords = {deep learning training, resource allocation, cluster scheduling},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@article{10.1145/3708527,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Marchezan, Luciano and Arkoh, Lawrence and Egyed, Alexander and Ramler, Rudolf},
title = {Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708527},
doi = {10.1145/3708527},
abstract = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Evolution, Software Migration, Re-designing, Re-engineering}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {software product line engineering, industry case study, feature modeling, embedded systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3652620.3688199,
author = {F\"{o}ldi\'{a}k, M\'{a}t\'{e}},
title = {Probabilistic Graph Queries for Design Space Exploration Under Uncertainty},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688199},
doi = {10.1145/3652620.3688199},
abstract = {Critical cyber-physical systems have an increasingly significant role in the world, and ensuring their safety is a high priority objective. State of the art approaches and engineering tools can support the development process from very early stages, with high-level system modeling, analysis capabilities, and exploration of alternatives. However, these approaches are limited when it comes evaluation of complex extra-functional characteristics over designs with uncertainties, typical to early system designs. In my thesis project, I intend to introduce probabilistic graph queries for high level, scalable probabilistic analysis, for analysing system models with design uncertainty and applicable in design space exploration. The approach will be evaluated on external case studies, focusing on key performance metrics related to applicability in the target context, such as runtime, precision and formal guarantees.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {142–148},
numpages = {7},
keywords = {cyber-physical systems, probabilistic analysis, graph queries, design space exploration, design uncertainty, lifting, safety},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@book{10.1145/3640508,
editor = {Townsend, Gloria Childress},
title = {Rendering History: The Women of ACM-W},
year = {2024},
isbn = {9798400717741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {58},
abstract = {The Association for Computing Machinery (ACM) has more than 100,000 members circling the globe, including trailblazing women who created ACM-W (ACM’s Committee on Women in Computing) in 1993. This book, published in celebration of ACM-W’s 30th birthday, divides the history of ACM-W into three parts.The first section provides a traditional history that details the evolution of ACM-W’s projects. In the next section, Rendering History allows the women of ACM-W to tell their own stories. What motivated them to trade personal time and energy for work that would change the face of computing for women and young girls? Among many others, Sue Black relates a story that spans her escape from two abusive homes to recognition for her computing accomplishments by both the late Queen of England and the current King. Kathy Kleiman describes her contributions to the field, including helping to rescue the wireless spectrum (now used by WiFi) from the (US) Federal Communications Commission’s plan to sell it. Bhavani Thuraisingham writes about her birth in Sri Lanka, an arranged marriage to a man eight years her senior, and cutting-edge research in the integration of cyber security and machine learning. The final section of the book provides an annotated bibliography of the research that launched ACM-W and continued to inform its projects over the next 30 years.ACM-W advocates internationally for the full engagement of women in all aspects of the computing field, providing a wide range of programs and services to ACM members and working in the larger community to advance the contributions of technical women. The main theme of ACM-W’s 30-year history as detailed in this book is the organization’s maturation from a US-centric organization to a global leader in supporting the advancement of women in computer science.“This is, quite simply, a book you won’t want to put down! What a wonderful collection of stories – blending personal and professional reflections – of many of the women who have been pioneers in computing and its roles in society, and in ACM-W. Gloria Childress Townsend is one of those pioneers and we owe her great thanks for putting together this captivating collection of stories, and for telling the very impressive and influential history of ACM-W.” - Bobby Schnabel, University of Colorado Boulder – co-founder of the National Center for Women &amp; Information Technology}
}

@inproceedings{10.1145/3643655.3643876,
author = {Rossi, Maria Teresa and Tundo, Alessandro and Mariani, Leonardo},
title = {Towards Model-Driven Dashboard Generation for Systems-of-Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643876},
doi = {10.1145/3643655.3643876},
abstract = {Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {9–12},
numpages = {4},
keywords = {automatic dashboard generation, model-driven engineering, model-based dashboard, systems of systems, monitoring dashboard},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1109/ASE.2015.58,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Configuration-aware change impact analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.58},
doi = {10.1109/ASE.2015.58},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {program analysis, maintenance, configuration, change impact analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3640310,
title = {MODELS '24: Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Linz, Austria}
}

@inproceedings{10.1145/3579028.3609019,
author = {Bazin, Alexandre and Huchard, Marianne and Martin, Pierre},
title = {Towards Analyzing Variability in Space and Time of Products from a Product Line using Triadic Concept Analysis},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609019},
doi = {10.1145/3579028.3609019},
abstract = {In this paper, we report an ongoing work on exploring the ability of Triadic Concept Analysis to provide a framework for analyzing products evolution in time and space, and highlight possible usages in the lifecycle of a product line.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {85–89},
numpages = {5},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.5555/3712729.3712879,
author = {Surman, Richard and Nehl, Matt and Evanson, Cole and Low, Soo Leen and Chan, Kern Chern and Liu, Hui Sian and Gan, Boon Ping},
title = {Capacity Planning Accuracy and the Effect of Dyanmic Dedication Changes for a Single Wafer Lot Semiconductor Factory},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In the context of a single wafer lot semiconductor factory characterized by high levels of Research and Development (RD) work-in-progress (WIP), low levels of product-based lots and lengthy cycle times, this paper investigates the accuracy of capacity planning given the impact of dynamic changes in dedication. We delve into several critical aspects related to dedication planning, drawing insights from historical data and dispatch logic used. The experimental results show the improvement in model accuracy with the incorporation of dedication changes as distribution functions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1809–1817},
numpages = {9},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1145/3712004,
author = {Casadei, Roberto and Aguzzi, Gianluca and Audrito, Giorgio and Damiani, Ferruccio and Pianini, Danilo and Scarso, Giordano and Torta, Gianluca and Viroli, Mirko},
title = {Software Engineering for Collective Cyber-Physical Ecosystems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712004},
doi = {10.1145/3712004},
abstract = {Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {cyber-physical ecosystems, collective adaptive systems, swarm intelligence, macro-programming, edge-cloud continuum, multi-agent systems, distributed artificial intelligence}
}

@inproceedings{10.1145/3643660.3643949,
author = {Garlan, David and Schmerl, Bradley and Wohlrab, Rebekka and C\'{a}mara, Javier},
title = {Challenges in Creating Effective Automated Design Environments: An experience report from the domain of generative manufacturing},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643949},
doi = {10.1145/3643660.3643949},
abstract = {The emergence of powerful automated design tools in many domains is changing the nature of design, as human-intensive activities can be increasingly off-loaded to those tools. Rather than having a human consider only handful of options, as has been done historically, such tools now enable the generation of a large space of potential designs, exhibiting different tradeoffs among competing qualities of merit, and supporting systematic exploration of the design space. At the same time, this paradigm raises new challenges centered on enabling humans to effectively navigate that generated space in order to select a design that best meets their requirements. In this paper we describe our experience in the domain of generative manufacturing, in which we developed a novel design environment for airplane parts manufacturing that incorporates a number of sophisticated design tools and attempts to tackle the emergent problems of design space exploration that are faced by designers of those parts. We use this experience to highlight the challenges that we faced and reflect on their applicability more generally to tool-assisted software design environments.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {15–20},
numpages = {6},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3540250.3549151,
author = {Dong, Liming and Zhang, He and Liu, Wei and Weng, Zhiluo and Kuang, Hongyu},
title = {Semi-supervised pre-processing for learning-based traceability framework on real-world software projects},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549151},
doi = {10.1145/3540250.3549151},
abstract = {The traceability of software artifacts has been recognized as an important factor to support various activities in software development processes. However, traceability can be difficult and time-consuming to create and maintain manually, thereby automated approaches have gained much attention. Unfortunately, existing automated approaches for traceability suffer from practical issues. This paper aims to gain an understanding of the potential challenges for the underperforming of the state-of-the-art, ML-based trace link classifiers applied in real-world projects. By investigating different industrial datasets, we found that two critical (and classic) challenges, i.e. data imbalance and sparse problems, lie in real-world projects’ traceability automation. To overcome these challenges, we developed a framework called SPLINT to incorporate hybrid textual similarity measures and semi-supervised learning strategies as enhancements to the learning-based traceability approaches. We carried out experiments with six open-source platforms and ten industry datasets. The results confirm that SPLINT is able to operate at higher performance on two communities’ datasets. Specifically, the industrial datasets, which significantly suffer from data imbalance and sparsity problems, show an increase in F2-score over 14% and AUC over 8% on average. The adjusted class-balancing and self-training policies used in SPLINT (CBST-Adjust) also work effectively for the selection of pseudo-labels on minor classes from unlabeled trace sets, demonstrating SPLINT’s practicability.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {570–582},
numpages = {13},
keywords = {Software Traceability, Semi-supervised Learning, Learning-based Model, Industry Practice, Data Sparsity, Data Imbalance},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.5555/2818754.2818821,
author = {Jia, Yue and Cohen, Myra B. and Harman, Mark and Petke, Justyna},
title = {Learning combinatorial interaction test generation strategies using hyperheuristic search},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our algorithm's strong generic performance results from its unsupervised learning. Hyperheuristic search is thus a promising way to relocate CIT design intelligence from human to machine.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {540–550},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/3573074.3573096,
author = {Bucchiarone, Antonio and Cooper, Kendra M. L. and Lin, Dayi and Melcer, Edward F. and Sung, Kelvin},
title = {Games and Software Engineering: Engineering fun, inspiration, and motivation},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3573074.3573096},
doi = {10.1145/3573074.3573096},
abstract = {Games are a popular form of entertainment and, due to their nature (i.e., interactive, immersive, etc.), strongly lend themselves for use beyond this original intent. Serious games, or games with a purpose, have been introduced to integrate the entertainment value games with domain specific objectives on important topics within education, health, and the environment to mention a few. In addition, gamification has been used to enhance nonentertainment applications with game elements; it aspires to foster behavioral changes, engagement, motivation, and participation in activities. In this context, the actions performed have meaning/value in the game experience in order to improve workplace performance or learn something in real life.The growing adoption of gameful experiences in all of the previous contexts make their design and development increasingly complex due to, for example, the number and variety of users, and their potential mission criticality. This complexity is nurtured, among the other factors, by a lack of theoretical grounding and adequate frameworks to engineer the intended solutions. In this paper, we report the outcomes of the 6th International Workshop on Games and Software Engineering: Engineering fun, inspiration, and motivation (GAS 2023 ) 1, which was held as part of the 44th International Conference on Software Engineering (ICSE 2022) in Pittsburgh, PA, USA on May 20, 2022.The workshop program includes two exciting keynotes discussing topics related to training and learning, and fulfilling the promise and potential of gamification. The two paper sessions examined gamification from the perspectives of software project, testing, and, design. The conclusion of the workshop is anchored by a panel of four highly qualified researchers and practitioners discussing lessons learned and the future of gamification.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {85–89},
numpages = {5}
}

@article{10.1145/3670795,
author = {Broy, Manfred and Brucker, Achim D. and Fantechi, Alessandro and Gleirscher, Mario and Havelund, Klaus and Kuppe, Markus Alexander and Mendes, Alexandra and Platzer, Andr\'{e} and Ringert, Jan Oliver and Sullivan, Allison},
title = {Does Every Computer Scientist Need to Know Formal Methods?},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1145/3670795},
doi = {10.1145/3670795},
abstract = {We focus on the integration of Formal Methods as mandatory theme in any Computer Science University curriculum. In particular, when considering the ACM Curriculum for Computer Science, the inclusion of Formal Methods as a mandatory Knowledge Area needs arguing for why and how does every computer science graduate benefit from such knowledge. We do not agree with the sentence “While there is a belief that formal methods are important and they are growing in importance, we cannot state that every computer science graduate will need to use formal methods in their career.” We argue that formal methods are and have to be an integral part of every computer science curriculum. Just as not all graduates will need to know how to work with databases either, it is still important for students to have a basic understanding of how data is stored and managed efficiently. The same way, students have to understand why and how formal methods work, what their formal background is, and how they are justified. No engineer should be ignorant of the foundations of their subject and the formal methods based on these.In this article, we aim at highlighting why every computer scientist needs to be familiar with formal methods. We argue that education in formal methods plays a key role by shaping students' programming mindset, fostering an appreciation for underlying principles, and encouraging the practice of thoughtful program design and justification, rather than simply writing programs without reflection and deeper understanding. Since integrating formal methods into the computer science curriculum is not a straightforward process, we explore the additional question: what are the tradeoffs between one dedicated knowledge area of formal methods in a computer science curriculum versus having formal methods scattered across all knowledge areas? Solving problems while designing software and software-intensive systems demands an understanding of what is required, followed by a specification and formalizing a solution in a programming language. How to do this systematically and correctly on solid grounds is exactly supported by formal methods.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {6},
numpages = {17},
keywords = {Formal methods, software and systems engineering, computer science university curriculum}
}

@article{10.1145/3603502,
author = {Monjur, Mohammad and Calzadillas, Joshua and Yu, Qiaoyan},
title = {Hardware Security Risks and Threat Analyses in Advanced Manufacturing Industry},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3603502},
doi = {10.1145/3603502},
abstract = {The advanced manufacturing industry (AMI) faces many unique challenges from the cyber-physical domain. Security threats are originated from two integral parts: software and hardware. Over the past decade, software security has been addressed extensively, but hardware security has not received enough attention. This work analyzes the security vulnerabilities of typical electronic devices deployed to AMI and proposes three attack models for sensing nodes, local storage and processing edge devices, and wired/wireless communication interfaces, respectively. Practical security attacks on hardware are demonstrated in this work to inspire the development of feasible countermeasures against hardware Trojans, fault injection attacks, and external signal interference. Moreover, this work highlights the unique security challenges posed by advanced manufacturing applications. To mitigate those security attacks in AMI, this work suggests guidelines for the defense method design that can effectively protect the hardware in AMI.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {83},
numpages = {22},
keywords = {cybersecurity, communication, side-channel analysis, hardware Trojan, sensor, Security threats}
}

@inproceedings{10.1145/3603166.3632143,
author = {St\"{o}tzner, Miles and Klinaku, Floriment and Pesl, Robin Dominic and Becker, Steffen},
title = {Enhancing Deployment Variability Management by Pruning Elements in Deployment Models},
year = {2024},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603166.3632143},
doi = {10.1145/3603166.3632143},
abstract = {Since applications often need to be deployed in different variants, deployment technologies, such as Ansible and Terraform, support modeling variability. Unfortunately, applications typically need the combination of multiple deployment technologies, which have proprietary and non-interoperable variability concepts. Therefore, variable deployment models have been introduced to model deployment variability across different technologies by assigning variability conditions to elements to specify their presence. However, the manual modeling of these conditions is repetitive, error-prone, and time-consuming. In this paper, we propose to reduce the modeling effort by a pruning concept, i.e., the automated removal of elements due to consistency issues and semantic aspects. To validate the practical feasibility, we implemented a prototype based on Open-TOSCA Vintner. Moreover, we present a case study that shows that the number of conditions to be modeled is significantly decreased.},
booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
articleno = {18},
numpages = {11},
keywords = {pruning, deployment models, variability management, TOSCA},
location = {Taormina (Messina), Italy},
series = {UCC '23}
}

@article{10.1145/3617946.3617955,
author = {Bucchiarone, Antonio and Cooper, Kendra M. L. and Lin, Dayi and Smith, Adam and Wanick, Vanissa},
title = {Fostering Collaboration and Advancing Research in Software Engineering and Game Development for Serious Contexts},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617955},
doi = {10.1145/3617946.3617955},
abstract = {The potential benefits of using the engaging and interactive nature of games to achieve specific objectives have been recognized by researchers and professionals from numerous domains. Serious games have been developed to impart knowledge, skills, and awareness in areas such as education, healthcare and the environment, while gamification has been applied to enhance the engagement, motivation, and participation of users in non-game activities such as sustainability and learning. As a result, the fields of game engineering, software engineering, and user experience are increasingly converging to create innovative solutions that blend the strengths of games with real-world applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {46–50},
numpages = {5}
}

@article{10.1145/3673226,
author = {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle, Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
title = {Context, Composition, Automation, and Communication: The C2AC Roadmap for Modeling and Simulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3673226},
doi = {10.1145/3673226},
abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {23},
numpages = {51},
keywords = {Modeling, simulation, state of the art, open challenges, reuse, composition, communication, reproducibility, automation, intelligent modeling and simulation lifecycle}
}

@article{10.1145/3686903,
author = {Valentine, Melissa A. and Pratt, Amanda L. and Hinds, Rebecca and Bernstein, Michael S.},
title = {The Algorithm and the Org Chart: How Algorithms Can Conflict with Organizational Structures},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686903},
doi = {10.1145/3686903},
abstract = {Algorithms are introducing changes to individuals? jobs, but do algorithms also lead to changes in the structures of organizations themselves? Organizational structures, as often formalized into organization (org) charts, are meant to facilitate coordinated decision-making. Yet our 10-month ethnographic study of a large online retail company reveals why the organizational structures that facilitate effective decision-making by humans may be in tension with the organizational structures that facilitate effective decision-making using algorithms. Our findings show that the human decision-makers needed small, divided-up sets of decisions, and they had previously accomplished this through how they structured individuals' roles and teams in the org chart. In contrast, when data scientists developed a new algorithm and first deployed it within organizational structures meant to support human decision-making, they realized that these small divided-up decision spaces were arbitrarily constraining the algorithm's search space. When not constrained in this manner, the algorithm could identify and recommend better solutions, but those optimal solutions did not always align with the structure of roles and teams in the org chart. This study suggests that as algorithms are integrated into the workplace, organization designs may begin to more explicitly reflect the contours of those algorithms' behaviors.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {364},
numpages = {31},
keywords = {algorithms, automation, ethnography, hierarchy, organizational structure, planning}
}

@proceedings{10.1145/3603287,
title = {ACMSE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@inproceedings{10.1145/3597926.3605232,
author = {Mordahl, Austin},
title = {Automatic Testing and Benchmarking for Configurable Static Analysis Tools},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605232},
doi = {10.1145/3597926.3605232},
abstract = {Static analysis is an important tool for detecting bugs in real-world software. The advent of numerous analysis algorithms with their own tradeoffs has led to the proliferation of configurable static analysis tools, but their complex, undertested configuration spaces are obstacles to their widespread adoption. To improve the reliability of these tools, my research focuses on developing new approaches to automatically test and debug them. First, I describe an empirical study that helps to understand the performance and behavior of configurable taint analysis tools for Android. The findings of this study motivate the development of ECSTATIC, a framework for testing and debugging that goes beyond taint analysis to test any configurable static analysis tool. The next steps for this research involve the automatic creation of real-world benchmarks for static analysis with associated ground truths and analysis features.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1532–1536},
numpages = {5},
keywords = {testing, debugging, configurable static analysis, benchmarking},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3702231,
author = {ter Beek, Maurice and Broy, Manfred and Dongol, Brijesh},
title = {The Role of Formal Methods in Computer Science Education},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3702231},
doi = {10.1145/3702231},
journal = {ACM Inroads},
month = nov,
pages = {58–66},
numpages = {9}
}

@inproceedings{10.1145/3563835.3568737,
author = {Gazzillo, Paul and Cohen, Myra B.},
title = {Bringing Together Configuration Research: Towards a Common Ground},
year = {2022},
isbn = {9781450399098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563835.3568737},
doi = {10.1145/3563835.3568737},
abstract = {Configurable software makes up most of the software in use today. Configurability, i.e., the ability of software to be customized without additional programming, is pervasive, and due to the criticality of problems caused by misconfiguration, it has been an active topic researched by investigators in multiple, diverse areas. This broad reach of configurability means that much of the literature and latest results are dispersed, and researchers may not be collaborating or be aware of similar problems and solutions in other domains. We argue that this lack of a common ground leads to a missed opportunity for synergy between research domains and the synthesis of efforts to tackle configurability problems. In short, configurability cuts across software as a whole and needs to be treated as a first class programming element. To provide a foundation for addressing these concerns we make suggestions on how to bring the communities together and propose a common model of configurability and a platform, ACCORD, to facilitate collaboration among researchers and practitioners.},
booktitle = {Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {259–269},
numpages = {11},
keywords = {configurability, community building},
location = {Auckland, New Zealand},
series = {Onward! 2022}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3477314.3507004,
author = {Ribeiro, Quelita A. D. S. and Ribeiro, Moniky and Castro, Jaelson},
title = {Requirements engineering for autonomous vehicles: a systematic literature review},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507004},
doi = {10.1145/3477314.3507004},
abstract = {Context: Autonomous Vehicles (AVs) will transform the way we live and work. Several benefits are envisaged, including: reduction in traffic deaths, drop in harmful emissions, improvement in fuel economy, reduction in travel time, and consumer savings. Indeed, the trend in the automobile industry is the development of AVs in preparation for the introduction and mass implementation of driverless vehicles. However, given the complexity and increasing connectivity of the AV, the challenges for effective and efficient development are immense. Many problems are related to misconceptions in the requirements engineering phase for AVs. Hence, a Requirements Engineering (RE) process is crucial in the development of AVs. Objective: The purpose of this work is to identify and analyse the current RE approaches used for AVs development. The analysis is based on answers related to the type of RE problems addressed by the study, the RE phases covered by the approach, requirements modelling styles used, the type of requirements described in the study, the specific AVs considered in the study, and the open problems reported. Method: We conducted a Systematic Literature Review (SLR) as the basis for our work. Results: Our SLR draws on 31 studies in which we identified the RE problems addressed by the studies and the phases of the RE processes considered. We also uncovered the languages and description styles used to describe the requirementes of the AVs. Special attention was paid to the non-functional requirements of interest. Different types of AVs were identified. Last but not least, several challenges were revealed. Conclusions: This paper reports the current state of the art of RE for AVs and identifies some open issues that deserve further investigation.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1299–1308},
numpages = {10},
keywords = {autonomous vehicles, requirements engineering, systematic literature review},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3579371.3589349,
author = {Loh, Gabriel H. and Schulte, Michael J. and Ignatowski, Mike and Adhinarayanan, Vignesh and Aga, Shaizeen and Aguren, Derrick and Agrawal, Varun and Aji, Ashwin M. and Alsop, Johnathan and Bauman, Paul and Beckmann, Bradford M. and Beigi, Majed Valad and Blagodurov, Sergey and Boraten, Travis and Boyer, Michael and Brantley, William C. and Chalmers, Noel and Chen, Shaoming and Cheng, Kevin and Chu, Michael L. and Cownie, David and Curtis, Nicholas and Del Pino, Joris and Duong, Nam and Duundefinedu, Alexandru and Eckert, Yasuko and Erb, Christopher and Freitag, Chip and Greathouse, Joseph L. and Gurumurthi, Sudhanva and Gutierrez, Anthony and Hamidouche, Khaled and Hossamani, Sachin and Huang, Wei and Islam, Mahzabeen and Jayasena, Nuwan and Kalamatianos, John and Kayiran, Onur and Kotra, Jagadish and Lee, Alan and Lowell, Daniel and Madan, Niti and Majumdar, Abhinandan and Malaya, Nicholas and Manne, Srilatha and Mashimo, Susumu and McDougall, Damon and Mednick, Elliot and Mishkin, Michael and Nutter, Mark and Paul, Indrani and Poremba, Matthew and Potter, Brandon and Punniyamurthy, Kishore and Puthoor, Sooraj and Raasch, Steven E. and Rao, Karthik and Rodgers, Gregory and Scrbak, Marko and Seyedzadeh, Mohammad and Slice, John and Sridharan, Vilas and van Oostrum, Ren\'{e} and van Tassell, Eric and Vishnu, Abhinav and Wasmundt, Samuel and Wilkening, Mark and Wolfe, Noah and Wyse, Mark and Yalavarti, Adithya and Yudanov, Dmitri},
title = {A Research Retrospective on AMD's Exascale Computing Journey},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589349},
doi = {10.1145/3579371.3589349},
abstract = {The pace of advancement of the top-end supercomputers historically followed an exponential curve similar to (and driven in part by) Moore's Law. Shortly after hitting the petaflop mark, the community started looking ahead to the next milestone: Exascale. However, many obstacles were already looming on the horizon, such as the slowing of Moore's Law, and others like the end of Dennard Scaling had already arrived. Anticipating significant challenges for the overall high-performance computing (HPC) community to achieve the next 1000x improvement, the U.S. Department of Energy (DOE) launched the Exascale Computing Program to enable and accelerate fundamental research across the many technologies needed to achieve exascale computing.AMD had the opportunity to contribute to the so-called "*Forward" programs from the DOE, which were a series of public-private partnerships focused on research and co-design activities covering compute architectures, interconnects, memory systems, chiplets and packaging, software stacks, applications, and more. Some of the research from these programs can now be found in the world's first exascale supercomputer, some were a little ahead of their time and may have an impact in the coming years, and others simply did not pan out. In this paper, we provide a retrospective of AMD's nearly decade-long research journey covering how we tried to predict the architecture of a supercomputer a decade into the future, what we got right, what we got wrong, and some of the insights and learnings that we discovered along the way.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {81},
numpages = {14},
keywords = {exascale, HPC, high-performance computing, supercomputing, frontier, memory, chiplets, heterogeneous compute, accelerated processing unit, research},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{10.1145/3630614.3630621,
author = {Josephson, Colleen and Peill-Moelter, Nicola and Pan, Zhelong and Pfaff, Ben and Firoiu, Victor},
title = {The Sky is Not the Limit: Untapped Opportunities for Green Computing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3630614.3630621},
doi = {10.1145/3630614.3630621},
abstract = {The Information and Communications Technology (ICT) industry emits as much carbon as the aviation industry [11], and if we continue business as usual our share of emissions will grow manyfold in the coming decade [30]. At the same time, more and more businesses are making commitments to be zero carbon or carbon neutral by 2050 or sooner. The path to zero carbon ICT has four key pillars: prioritizing renewable energy, using resources like power and water more efficiently, addressing embodied carbon, and removing institutional barriers. We discuss from a broad industry perspective the challenges and opportunities within each pillar, as well as the role ICT can play in helping other industries achieve zero carbon goals.},
journal = {SIGENERGY Energy Inform. Rev.},
month = oct,
pages = {33–39},
numpages = {7},
keywords = {carbon, energy, green software, sustainability}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@proceedings{10.1145/3643665,
title = {FinanSE '24: Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software development has an integral role in every financial organisation; indeed, almost every service provided by a bank utilizes some form of software solution. While SE research has led to solutions and innovations for many popular SE problems, there remain unresolved challenges, particularly, those challenges faced in software development in financial firms. An example of such a challenge is defect prediction, where defects are not equal as some may lead to larger reputational and financial damage than others. Consequently, testing and verification is burdened with a further set of restraints for finance-based SE teams. Financial firms began automating processes as early as the 1960s, and as such, must maintain large legacy systems which may host critical operations. This problem is further exacerbated by the numerous mergers and acquisitions common in the financial sector, which leaves firms with a set of heterogeneous legacy systems that need to communicate with one another effectively and efficiently. Therefore, maintaining these systems while modernizing them leads to intriguing challenges, spanning from model extraction and process optimisation to code translation. Moreover, highly regulated institutions like financial firms require a high degree of transparency and accountability. This requirement facilitates the need for model fairness and explainability for any SE solution, in particular those that rely on AI.The 1st International Workshop on Software Engineering Challenges in Financial Firms (FinanSE 2024) is a forum to bring together academia and industry to share new ideas and results in tackling these challenges.},
location = {Lisbon, Portugal}
}

@article{10.1145/3589227,
author = {Weyns, Danny and Gerostathopoulos, Ilias and Abbas, Nadeem and Andersson, Jesper and Biffl, Stefan and Brada, Premek and Bures, Tomas and Di Salle, Amleto and Galster, Matthias and Lago, Patricia and Lewis, Grace and Litoiu, Marin and Musil, Angelika and Musil, Juergen and Patros, Panos and Pelliccione, Patrizio},
title = {Self-Adaptation in Industry: A Survey},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3589227},
doi = {10.1145/3589227},
abstract = {Computing systems form the backbone of many areas in our society, from manufacturing to traffic control, healthcare, and financial systems. When software plays a vital role in the design, construction, and operation, these systems are referred to as software-intensive systems. Self-adaptation equips a software-intensive system with a feedback loop that either automates tasks that otherwise need to be performed by human operators or deals with uncertain conditions. Such feedback loops have found their way to a variety of practical applications; typical examples are an elastic cloud to adapt computing resources and automated server management to respond quickly to business needs. To gain insight into the motivations for applying self-adaptation in practice, the problems solved using self-adaptation and how these problems are solved, and the difficulties and risks that industry faces in adopting self-adaptation, we performed a large-scale survey. We received 184 valid responses from practitioners spread over 21 countries. Based on the analysis of the survey data, we provide an empirically grounded overview the of state of the practice in the application of self-adaptation. From that, we derive insights for researchers to check their current research with industrial needs, and for practitioners to compare their current practice in applying self-adaptation. These insights also provide opportunities for applying self-adaptation in practice and pave the way for future industry-research collaborations.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {5},
numpages = {44},
keywords = {Self-adaptation, industry, survey}
}

@article{10.1145/3579851,
author = {Greca, Renan and Miranda, Breno and Bertolino, Antonia},
title = {State of Practical Applicability of Regression Testing Research: A Live Systematic Literature Review},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3579851},
doi = {10.1145/3579851},
abstract = {Context: Software regression testing refers to rerunning test cases after the system under test is modified, ascertaining that the changes have not (re-)introduced failures. Not all researchers’ approaches consider applicability and scalability concerns, and not many have produced an impact in practice. Objective: One goal is to investigate industrial relevance and applicability of proposed approaches. Another is providing a live review, open to continuous updates by the community. Method: A systematic review of regression testing studies that are clearly motivated by or validated against industrial relevance and applicability is conducted. It is complemented by follow-up surveys with authors of the selected papers and 23 practitioners. Results: A set of 79 primary studies published between 2016–2022 is collected and classified according to approaches and metrics. Aspects relative to their relevance and impact are discussed, also based on their authors’ feedback. All the data are made available from the live repository that accompanies the study. Conclusions: While widely motivated by industrial relevance and applicability, not many approaches are evaluated in industrial or large-scale open-source systems, and even fewer approaches have been adopted in practice. Some challenges hindering the implementation of relevant approaches are synthesized, also based on the practitioners’ feedback.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {274},
numpages = {36},
keywords = {Regression Testing, test case selection, test case prioritization, test suite reduction, test suite amplification, systematic literature review}
}

@proceedings{10.1145/3689930,
title = {RICSS '24: Proceedings of the 2024 Workshop on Re-design Industrial Control Systems with Security},
year = {2024},
isbn = {9798400712265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Re-design Industrial Control Systems with Security (RICSS).As ICS software and systems are foundational to the critical infrastructure relied upon by industry, academia, and government, addressing their security challenges is more crucial than ever. Historically, these systems were not designed with security in mind, and the rapid expansion of interconnectivity has exposed significant vulnerabilities, leaving many practitioners reliant on a patchwork of security measures. While some proprietary ICS software providers have begun incorporating security features, the security of free and open-source ICS software remains underdeveloped and underappreciated. This workshop aims to change that.},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3350768.3352455,
author = {de Sousa, Amanda Oliveira and Bezerra, Carla I. M. and Andrade, Rossana M. C. and Filho, Jos\'{e} M. S. M.},
title = {Quality Evaluation of Self-Adaptive Systems: Challenges and Opportunities},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3352455},
doi = {10.1145/3350768.3352455},
abstract = {Self-adaptive systems (SAS) can adapt their behavior to suit user preferences or contexts, as well as monitor their performance and adjust it if necessary. In addition to adaptation operations, self-adaptive systems communicate with sensors, actuators, and other devices. Due to the complexity and dynamism of SAS, many situations can compromise the functioning of the system, such as faults in adaptations, low performance to execute tasks, and context inconsistencies. To prevent the system of these problems, it is essential to ensure high levels of quality. However, due to the peculiarities of these systems, there are still challenges to perform quality evaluations in these systems. In this sense, this paper proposes a discussion about the quality evaluation of self-adaptive systems in the last years. As a result, we identify challenges, limitations and research opportunities related to SAS quality evaluation.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {213–218},
numpages = {6},
keywords = {measurement, quality evaluation, self-adaptive systems},
location = {Salvador, Brazil},
series = {SBES '19}
}

@proceedings{10.1145/3705677,
title = {CITCE '24: Proceedings of the 4th International Conference on Computer, Internet of Things and Control Engineering},
year = {2024},
isbn = {9798400711848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3526071.3527518,
author = {Blender, Timo and Schlegel, Christian},
title = {Dynamic allocation of service robot resources to an order picking task considering functional and non-functional properties},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527518},
doi = {10.1145/3526071.3527518},
abstract = {Industry 4.0 processes have often varying requirements. A service robot and a team of service robots respectively represent a flexible resource. That means, it possesses variability that can possibly be configured in such a way that it is able to fulfill the requirements of industry 4.0 processes. Determining whether that is the case and how that has to happen is an important part of variability management. Based on a model-driven general method for variability management in a robotics software ecosystem, we present here a concrete use case (model) in which we allocate for an order picking task with specific time requirements either a single fitting service robot or a collaboration of two fitting service robots. Relevant properties of the service robots considered are both functional (are the capabilities to execute the tasks available?) as well as non-functional (the desired velocity parameterization while executing the individual navigation sub tasks limited by the respective maximum speed of a service robot).},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {25–32},
numpages = {8},
keywords = {model-driven software development, non-functional properties, robotics software ecosystem, service robot collaboration, variability management},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3718491,
title = {AIBDF '24: Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
year = {2024},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3417990.3419626,
author = {Barriga, Angela and Di Ruscio, Davide and Iovino, Ludovico and Nguyen, Phuong T. and Pierantonio, Alfonso},
title = {An extensible tool-chain for analyzing datasets of metamodels},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3419626},
doi = {10.1145/3417990.3419626},
abstract = {Metamodels play a crucial role in any modeling environment as they formalize the modeling constructs underpinning the definition of conforming artifacts, including models, model transformations, code generators, and editors. Understanding the structural characteristics and the quality of the metamodels that are available in public repositories before their reuse is a critical task that demands the adoption of different tools, which might not be easy to adopt. Even the selection of metamodels to be used for experimenting with new tools is not straightforward as it involves exploring various sources of information and dig in each metamodel to check its appropriateness for the evaluation of the tool under development. In this paper, we present a dataset of metamodels, which has been collected for experimenting with different approaches conceived by the authors. The dataset has been automatically curated using a toolchain, which has been re-designed post-ante the definition of the proposed approaches to foster its future reuse.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {50},
numpages = {8},
keywords = {analysis, dataset, metamodels, repositories},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3686081,
title = {ICDSM '24: Proceedings of the International Conference on Decision Science &amp; Management},
year = {2024},
isbn = {9798400718151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3466752.3480071,
author = {Truong, Minh S. Q. and Chen, Eric and Su, Deanyone and Shen, Liting and Glass, Alexander and Carley, L. Richard and Bain, James A. and Ghose, Saugata},
title = {RACER: Bit-Pipelined Processing Using Resistive Memory},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480071},
doi = {10.1145/3466752.3480071},
abstract = {To combat the high energy costs of moving data between main memory and the CPU, recent works have proposed to perform processing-using-memory (PUM), a type of processing-in-memory where operations are performed on data in situ (i.e., right at the memory cells holding the data). Several common and emerging memory technologies offer the ability to perform bitwise Boolean primitive functions by having interconnected cells interact with each other, eliminating the need to use discrete CMOS compute units for several common operations. Recent PUM architectures extend upon these Boolean primitives to perform bit-serial computation using memory. Unfortunately, several practical limitations of the underlying memory devices restrict how large emerging memory arrays can be, which hinders the ability of conventional bit-serial computation approaches to deliver high performance in addition to large energy savings. In this paper, we propose RACER, a cost-effective PUM architecture that delivers high performance and large energy savings using small arrays of resistive memories. RACER makes use of a bit-pipelining execution model, which can pipeline bit-serial w-bit computation across w small tiles. We fully design efficient control and peripheral circuitry, whose area can be amortized over small memory tiles without sacrificing memory density, and we propose an ISA abstraction for RACER to allow for easy program/compiler integration. We evaluate an implementation of RACER using NOR-capable ReRAM cells across a range of microbenchmarks extracted from data-intensive applications, and find that RACER provides 107 \texttimes{}, 12 \texttimes{}, and 7 \texttimes{} the performance of a 16-core CPU, a 2304-shader-core GPU, and a state-of-the-art in-SRAM compute substrate, respectively, with energy savings of 189 \texttimes{}, 17 \texttimes{}, and 1.3 \texttimes{}.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {100–116},
numpages = {17},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@article{10.5555/948835.948836,
author = {Liu, Hong and Gluch, David P.},
title = {Conceptual modeling with the object-process methodology in software architecture},
year = {2004},
issue_date = {January 2004},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {19},
number = {3},
issn = {1937-4771},
abstract = {The ultimate goal of software design is to transform real world problems into software solutions. Architectural design is the earliest phase of this process. It is a phase in which conceptual modeling plays an important role. In conceptual modeling, designers pay more attention to accurately describing real world problems than making detailed design decisions. Object-Oriented Analysis and Design is the prevailing software development methodology. Using object-orientation for conceptual modeling is often difficult, especially because of the encapsulation of processes within objects. An alternative approach is the Object-Process Methodology (OPM). OPM is a systems engineering approach that, while recognizing the duality of objects and processes, establishes a peer relationship among them. This peer relationship enables conceptual modelers to describe real world problems more naturally. Using OPM and its support tool, designers can develop integrated conceptual models that faithfully capture the characteristics and interactions of real-world entities. This paper briefly introduces OPM and its support tools and uses case studies to show the advantages of OPM for conceptual modeling in software architecture design.},
journal = {J. Comput. Sci. Coll.},
month = jan,
pages = {10–21},
numpages = {12},
keywords = {software architecture, object-process methodology, conceptual modeling}
}

@article{10.1145/3533818,
author = {Birchler, Christian and Khatiri, Sajad and Derakhshanfar, Pouria and Panichella, Sebastiano and Panichella, Annibale},
title = {Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533818},
doi = {10.1145/3533818},
abstract = {Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {28},
numpages = {30},
keywords = {Autonomous systems, software simulation, test case prioritization}
}

@inproceedings{10.1145/3470482.3479628,
author = {Silvestre, Vit\'{o}ria R. N. and Gomes, Francisco A. A. and C\^{a}ndido, Adriano L. and Fernandes, Filipe and Rocha, Lincoln S. and Trinta, Fernando A. M.},
title = {DOP-MS: A Microservice-based Data Offloading Service with Support for Data Anonymisation},
year = {2021},
isbn = {9781450386098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470482.3479628},
doi = {10.1145/3470482.3479628},
abstract = {Due to mobile devices' growing presence in our daily routine, mobile applications are becoming increasingly complex. They require more powerful processing capability and more extensive data storage, which characterizes a challenge when computational constraints of these devices are taken into account. Several software infrastructures proposed to help the development of mobile applications with data offloading features. However, they lack essential features for data offloading, such as configurable data synchronization policy models, privacy mechanisms for the offloaded data, and scalability and performance analyses. This work presents DOP, a solution to assist the development of mobile applications that use data migration, including contextual data, from mobile devices to a remote environment, based on a microservice architecture. The data offloading technique enables data migration into a remote environment, allowing (i) storage savings on the mobile device and (ii) sharing data among users. The experiments on DOP showed benefits in storage savings on mobile devices and new possibilities for inferring situations based on shared data from multiple users.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {145–152},
numpages = {8},
keywords = {Data Offloading, Microservices, Mobile Cloud Computing},
location = {Belo Horizonte, Minas Gerais, Brazil},
series = {WebMedia '21}
}

@article{10.1145/3699839.3699841,
author = {Awad, Hiba and Alidra, Abdelghani and Bruneliere, Hugo and Ledoux, Thomas and Rivalan, Jonathan},
title = {VeriFog: A Generic Model-based Approach for Verifying Fog Systems at Design Time and Generating Deployment Configurations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3699839.3699841},
doi = {10.1145/3699839.3699841},
abstract = {Fog Computing is a paradigm decentralizing the Cloud by geographically distributing computation, storage, network resources and related services. It provides benefits such as reducing the number of bottlenecks, limiting unwanted data movements, etc. However, managing the size, complexity and heterogeneity of the Fog systems to be engineered is challenging and can quickly become costly. According to best practices in software engineering, verification tasks could be performed on a system design prior to its implementation and deployment. We propose a generic model-based approach for verifying Fog systems at design time, also enabling the automatic generation of corresponding deployment configuration files. Named VeriFog, this approach is notably based on a customizable Fog Modeling Language (FML). We experimented in practice by modeling three use cases, from three different application domains, and by considering three main types of non-functional properties to be verified. From this modeling and verification effort, we show that we are able to automatically generate usable deployment configuration files for different deployment tools. In direct collaboration with our industrial partner Smile, the approach and underlying language presented in this paper are necessary steps towards a more global model-based support for the complete life cycle of Fog systems.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {18–36},
numpages = {19},
keywords = {deployment configuration, design time, fog computing, generation, model-based engineering, modeling language, non-functional properties, verification}
}

@inproceedings{10.1145/3236024.3264838,
author = {Brun, Yuriy and Meliou, Alexandra},
title = {Software fairness},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264838},
doi = {10.1145/3236024.3264838},
abstract = {A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {754–759},
numpages = {6},
keywords = {Software fairness, software bias, software process},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3564121.3564133,
author = {Dutta, Jeet and Dey, Swarnava and Mukherjee, Arijit and Pal, Arpan},
title = {Acceleration-aware, Retraining-free Evolutionary Pruning for Automated Fitment of Deep Learning Models on Edge Devices},
year = {2023},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564121.3564133},
doi = {10.1145/3564121.3564133},
abstract = {Deep Learning architectures used in computer vision, natural language and speech processing, unsupervised clustering, etc. have become highly complex and application-specific in recent times. Despite existing automated feature engineering techniques, building such complex models still requires extensive domain knowledge or a huge infrastructure for employing techniques such as Neural Architecture Search (NAS). Further, many industrial applications need in-premises decision-making close to sensors, thus making deployment of deep learning models on edge devices a desirable and often necessary option. Instead of freshly designing application-specific Deep Learning models, the transformation of already built models can achieve faster time to market and cost reduction. In this work, we present an efficient re-training-free model compression method that searches for the best hyper-parameters to reduce the model size and latency without losing any accuracy. Moreover, our proposed method takes into account any drop in accuracy due to hardware acceleration, when a Deep Neural Network is executed on accelerator hardware.},
booktitle = {Proceedings of the Second International Conference on AI-ML Systems},
articleno = {10},
numpages = {10},
keywords = {deep learning, edge, nas, neural networks, pruning},
location = {Bangalore, India},
series = {AIMLSystems '22}
}

@article{10.1145/3542945,
author = {Araujo, Hugo and Mousavi, Mohammad Reza and Varshosaz, Mahsa},
title = {Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542945},
doi = {10.1145/3542945},
abstract = {We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {51},
numpages = {61},
keywords = {Verification and validation, robotics, autonomous systems, testing, literature survey}
}

@proceedings{10.1145/3716895,
title = {ICAICE '24: Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering},
year = {2024},
isbn = {9798400718007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@book{10.1145/3664191,
author = {Kumar, Amruth N. and Raj, Rajendra K. and Aly, Sherif G. and Anderson, Monica D. and Becker, Brett A. and Blumenthal, Richard L. and Eaton, Eric and Epstein, Susan L. and Goldweber, Michael and Jalote, Pankaj and Lea, Douglas and Oudshoorn, Michael and Pias, Marcelo and Reiser, Susan and Servin, Christian and Simha, Rahul and Winters, Titus and Xiang, Qiao},
title = {Computer Science Curricula 2023},
year = {2024},
isbn = {9798400710339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@proceedings{10.1145/3708036,
title = {ICCSMT '24: Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
year = {2024},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3623476.3623517,
author = {Jouneaux, Gwendal and Fr\"{o}lich, Damian and Barais, Olivier and Combemale, Benoit and Le Guernic, Gurvan and Mussbacher, Gunter and van Binsbergen, L. Thomas},
title = {Adaptive Structural Operational Semantics},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623517},
doi = {10.1145/3623476.3623517},
abstract = {Software systems evolve more and more in complex and changing environments, often requiring runtime adaptation  
to best deliver their services. When self-adaptation is the main concern of the system, a manual implementation of the underlying feedback loop and trade-off analysis may be desirable. However, the required expertise and substantial development effort make such implementations prohibitively difficult when it is only a secondary concern for the given domain. In this paper, we present ASOS, a metalanguage abstracting the runtime adaptation concern of a given domain in the behavioral semantics of a domain-specific language (DSL), freeing the language user from implementing it from scratch for each system in the domain. We demonstrate our approach on RobLANG, a procedural DSL for robotics, where we abstract a recurrent energy-saving behavior depending on the context. We provide formal semantics for ASOS and pave the way for checking properties such as determinism, completeness, and termination of the resulting self-adaptable language. We provide first results on the performance of our approach compared to a manual implementation of this self-adaptable behavior. We demonstrate, for RobLANG, that our approach provides suitable abstractions for specifying sound adaptive operational semantics while being more efficient.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {DSL, Operational Semantics, Self-Adaptation},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/1321631.1321676,
author = {Menzies, Tim and Elrawas, Oussama and Hihn, Jairus and Feather, Martin and Madachy, Ray and Boehm, Barry},
title = {The business case for automated software engineering},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321676},
doi = {10.1145/1321631.1321676},
abstract = {Adoption of advanced automated SE (ASE) tools would be favored if a business case could be made that these tools are more valuable than alternate methods. In theory, software prediction models can be used to make that case. In practice, this is complicated by the "local tuning" problem. Normally, predictors for software effort and defects and threat use local data to tune their predictions. Such local tuning data is often unavailable.This paper shows that assessing the relative merits of different SE methods need not require precise local tunings. STAR1 is a simulated annealer plus a Bayesian post-processor that explores the space of possible local tunings within software prediction models. STAR1 ranks project decisions by their effects on effort and defects and threats. In experiments with two NASA systems, STAR1 found that ASE tools were necessary to minimize effort/ defect/ threats.},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {303–312},
numpages = {10},
keywords = {COCOMO, COQUALMO, bayes, simulated annealing},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@proceedings{10.1145/3661904,
title = {ICETT '24: Proceedings of the 2024 10th International Conference on Education and Training Technologies},
year = {2024},
isbn = {9798400717895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {Taormina (Messina), Italy}
}

@proceedings{10.1145/3607947,
title = {IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Noida, India}
}

@inproceedings{10.1145/1409540.1409596,
author = {Berg, Kimmo and Ehtamo, Harri},
title = {Multidimensional screening: online computation and limited information},
year = {2008},
isbn = {9781605580753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409540.1409596},
doi = {10.1145/1409540.1409596},
abstract = {Optimal screening has been studied in economics, game theory, and recently computer science. We study the problem in a nonlinear pricing application, where a monopoly designs a price schedule from which the buyers self-select the quality they wish to consume. We formulate a multidimensional model with buyers' utility functions that need not satisfy the standard single-crossing assumption. We characterize the solution with the first-order optimality conditions and present a framework for analyzing the solution. With the framework, the structure of the solution is easily illustrated and the sensitivity analysis can be done. We give numerical examples that demonstrate the properties of the solution. With these observations, we discuss the complexity of the problem and solving the problem under limited information. We examine what information the monopoly needs when adjusting the price schedule to increase the profit. This paper applies, e.g., to pricing situations in electronic commerce where the seller may have limited information available, and the seller learns about the buyers' preferences online when doing the business.},
booktitle = {Proceedings of the 10th International Conference on Electronic Commerce},
articleno = {41},
numpages = {10},
keywords = {limited information, mechanism design, multidimensional screening, nonlinear pricing, online computation},
location = {Innsbruck, Austria},
series = {ICEC '08}
}

@proceedings{10.1145/3675417,
title = {DEAI '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
year = {2024},
isbn = {9798400717147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hongkong, China}
}

@proceedings{10.1145/3624007,
title = {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’23). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming languages research communities.},
location = {Cascais, Portugal}
}

@article{10.1145/3494519,
author = {Marijan, Dusica and Sen, Sagar},
title = {Industry–Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3494519},
doi = {10.1145/3494519},
abstract = {Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry–academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research–practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry–academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {45},
numpages = {52},
keywords = {Industry-academia collaboration, research collaboration, research co-creation, software engineering, technology transfer, knowledge transfer, collaboration gap, collaboration model, patterns, anti-patterns}
}

@proceedings{10.1145/3644033,
title = {FormaliSE '24: Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Historically, formal methods academic research and practical software development have had limited mutual interactions—except possibly in specialized domains such as safety-critical software. In recent times, the outlook has considerably improved: on the one hand, formal methods research has delivered more flexible techniques and tools that can support various aspects of the software development process—from user requirements elicitation, to design, implementation, verification and validation, as well as the creation of documentation. On the other hand, software engineering has developed a growing interest in rigorous techniques applied at scale.This evolution, and the desire to further improve it, motivated the creation of FormaliSE: a well-established annual conference whose main goal is to promote work at the intersection of the formal methods and software engineering communities, providing a venue to exchange ideas, experiences, techniques, and results. The collaboration between these two communities can be mutually beneficial by fostering the creation of formal methods that are practically useful and by helping develop higher-quality software.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3615834,
title = {iWOAR '23: Proceedings of the 8th international Workshop on Sensor-Based Activity Recognition and Artificial Intelligence},
year = {2023},
isbn = {9798400708169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {L\"{u}beck, Germany}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/3313831.3376590,
author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
title = {Questioning the AI: Informing Design Practices for Explainable AI User Experiences},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376590},
doi = {10.1145/3313831.3376590},
abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {explainable AI, human-AI interaction, user experience},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3487919,
author = {Pfannem\"{u}ller, Martin and Breitbach, Martin and Weckesser, Markus and Becker, Christian and Schmerl, Bradley and Sch\"{u}rr, Andy and Krupitzer, Christian},
title = {REACT-ION: A Model-based Runtime Environment for Situation-aware Adaptations},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487919},
doi = {10.1145/3487919},
abstract = {Trends such as the Internet of Things lead to a growing number of networked devices and to a variety of communication systems. Adding self-adaptive capabilities to these communication systems is one approach to reducing administrative effort and coping with changing execution contexts. Existing frameworks can help reducing development effort but are neither tailored toward the use in communication systems nor easily usable without knowledge in self-adaptive systems development. Accordingly, in previous work, we proposed REACT, a reusable, model-based runtime environment to complement communication systems with adaptive behavior. REACT addresses heterogeneity and distribution aspects of such systems and reduces development effort. In this article, we propose REACT-ION—an extension of REACT for situation awareness. REACT-ION offers a context management module that is able to acquire, store, disseminate, and reason on context data. The context management module is the basis for (i) proactive adaptation with REACT-ION and (ii) self-improvement of the underlying feedback loop. REACT-ION can be used to optimize adaptation decisions at runtime based on the current situation. Therefore, it can cope with uncertainty and situations that were not foreseeable at design time. We show and evaluate in two case studies how REACT-ION’s situation awareness enables proactive adaptation and self-improvement.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {12},
numpages = {29},
keywords = {Self-adaptive systems, model-based, runtime environment, framework, situation awareness}
}

@inproceedings{10.1145/3477244.3477985,
author = {van der Sanden, Bram and Li, Yonghui and van den Aker, Joris and Akesson, Benny and Bijlsma, Tjerk and Hendriks, Martijn and Triantafyllidis, Kostas and Verriet, Jacques and Voeten, Jeroen and Basten, Twan},
title = {Model-driven system-performance engineering for cyber-physical systems},
year = {2021},
isbn = {9781450387125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477244.3477985},
doi = {10.1145/3477244.3477985},
abstract = {System-Performance Engineering (SysPE) encompasses modeling formalisms, methods, techniques, and industrial practices to design systems for performance, where performance is taken integrally into account during the whole system life cycle. Industrial SysPE state of practice is generally model-based. Due to the rapidly increasing complexity of systems, there is a need to develop and establish model-driven methods and techniques. To structure the field of SysPE, we identify (1) industrial challenges motivating the importance of SysPE, (2) scientific challenges that need to be addressed to establish model-driven SysPE, (3) important focus areas for SysPE and (4) best practices. We conducted a survey to collect feedback on our views. The responses were used to update and validate the identified challenges, focus areas, and best practices. The final result is presented in this paper. Interesting observations are that industry sees a need for better design-space exploration support, more than for additional performance modeling and analysis techniques. Also tools and integral methods for SysPE need attention. From the identified focus areas, scheduling and supervisory control is seen as lacking established best practices.},
booktitle = {Proceedings of the 2021 International Conference on Embedded Software},
pages = {11–22},
numpages = {12},
keywords = {CPS, model-driven design, system-performance engineering},
location = {Virtual Event},
series = {EMSOFT '21}
}

@proceedings{10.1145/3652037,
title = {PETRA '24: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Crete, Greece}
}

@proceedings{10.1145/3711129,
title = {EITCE '24: Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering},
year = {2024},
isbn = {9798400710094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@inproceedings{10.1145/3490099.3511119,
author = {Sun, Jiao and Liao, Q. Vera and Muller, Michael and Agarwal, Mayank and Houde, Stephanie and Talamadupula, Kartik and Weisz, Justin D.},
title = {Investigating Explainability of Generative AI for Code through Scenario-based Design},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511119},
doi = {10.1145/3490099.3511119},
abstract = {What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {212–228},
numpages = {17},
keywords = {explainable AI, generative AI, human-centered AI, scenario based design, software engineering tooling},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3368089.3409727,
author = {Chen, Qingrong and Wang, Teng and Legunsen, Owolabi and Li, Shanshan and Xu, Tianyin},
title = {Understanding and discovering software configuration dependencies in cloud and datacenter systems},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409727},
doi = {10.1145/3368089.3409727},
abstract = {A large percentage of real-world software configuration issues, such as misconfigurations, involve multiple interdependent configuration parameters. However, existing techniques and tools either do not consider dependencies among configuration parameters— termed configuration dependencies—or rely on one or two dependency types and code patterns as input. Without rigorous understanding of configuration dependencies, it is hard to deal with many resulting configuration issues.  This paper presents our study of software configuration dependencies in 16 widely-used cloud and datacenter systems, including dependencies within and across software components. To understand types of configuration dependencies, we conduct an exhaustive search of descriptions in structured configuration metadata and unstructured user manuals. We find and manually analyze 521 configuration dependencies. We define five types of configuration dependencies and identify their common code patterns. We report on consequences of not satisfying these dependencies and current software engineering practices for handling the consequences.  We mechanize the knowledge gained from our study in a tool, cDep, which detects configuration dependencies. cDep automatically discovers five types of configuration dependencies from bytecode using static program analysis. We apply cDep to the eight Java and Scala software systems in our study. cDep finds 87.9% (275/313) of the related subset of dependencies from our study. cDep also finds 448 previously undocumented dependencies, with a 6.0% average false positive rate. Overall, our results show that configuration dependencies are more prevalent and diverse than previously reported and should henceforth be considered a first-class issue in software configuration engineering.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {362–374},
numpages = {13},
keywords = {Configuration, cloud systems, datacenter systems, dependency},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3593013.3594012,
author = {Widder, David Gray and Zhen, Derrick and Dabbish, Laura and Herbsleb, James},
title = {It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594012},
doi = {10.1145/3593013.3594012},
abstract = {How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {467–479},
numpages = {13},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@proceedings{10.1145/3570353,
title = {COP '22: Proceedings of the 14th ACM International Workshop on Context-Oriented Programming and Advanced Modularity},
year = {2022},
isbn = {9781450399869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Berlin, Germany}
}

@proceedings{10.1145/3660515,
title = {EICS '24 Companion: Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@proceedings{10.1145/3670105,
title = {CNIOT '24: Proceedings of the 2024 5th International Conference on Computing, Networks and Internet of Things},
year = {2024},
isbn = {9798400716751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3649411,
title = {GPGPU '24: Proceedings of the 16th Workshop on General Purpose Processing Using GPU},
year = {2024},
isbn = {9798400718175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@proceedings{10.1145/3708359,
title = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3524494,
title = {GAS '22: Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS explores how the growing adoption of gameful elements in various contexts can make the design and development of new technology increasingly complex, and provides a forum to explore these issues that crosscut the software engineering and games development communities. The goal of this one day workshop is to bring together interdisciplinary researchers and practitioners to discuss emerging and new research trends, challenges, costs, and benefits for entertainment games, serious games, and the gamification of traditional (non-game) applications and activities.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3564121,
title = {AIMLSystems '22: Proceedings of the Second International Conference on AI-ML Systems},
year = {2022},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1007/11424529_4,
author = {Fredriksson, Johan and Sandstr\"{o}m, Kristian and \r{A}kerholm, Mikael},
title = {Optimizing resource usage in component-based real-time systems},
year = {2005},
isbn = {3540258779},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424529_4},
doi = {10.1007/11424529_4},
abstract = {The embedded systems domain represents a class of systems that have high requirements on cost efficiency as well as run-time properties such as timeliness and dependability. The research on component-based systems has produced component technologies for guaranteeing real-time properties. However, the issue of saving resources by allocating several components to real-time tasks has gained little focus. Trade-offs when allocating components to tasks are, e.g., CPU-overhead, footprint and integrity. In this paper we present a general approach for allocating components to real-time tasks, while utilizing existing real-time analysis to ensure a feasible allocation. We demonstrate that CPU-overhead and memory consumption can be reduced by as much as 48% and 32% respectively for industrially representative systems.},
booktitle = {Proceedings of the 8th International Conference on Component-Based Software Engineering},
pages = {49–65},
numpages = {17},
location = {St. Louis, MO},
series = {CBSE'05}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@inproceedings{10.1145/3194133.3194147,
author = {Rodrigues, Arthur and Caldas, Ricardo Diniz and Rodrigues, Gena\'{\i}na Nunes and Vogel, Thomas and Pelliccione, Patrizio},
title = {A learning approach to enhance assurances for real-time self-adaptive systems},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194147},
doi = {10.1145/3194133.3194147},
abstract = {The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design- and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the system's goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {206–216},
numpages = {11},
keywords = {assurance evidence, data mining, goal-oriented, learning approach, real-time systems, self-adaptive systems},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3487552.3487813,
author = {Ma, Zane and Austgen, James and Mason, Joshua and Durumeric, Zakir and Bailey, Michael},
title = {Tracing your roots: exploring the TLS trust anchor ecosystem},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487813},
doi = {10.1145/3487552.3487813},
abstract = {Secure TLS server authentication depends on reliable trust anchors. The fault intolerant design of today's system---where a single compromised trust anchor can impersonate nearly all web entities---necessitates the careful assessment of each trust anchor found in a root store. In this work, we present a first look at the root store ecosystem that underlies the accelerating deployment of TLS. Our broad collection of TLS user agents, libraries, and operating systems reveals a surprisingly condensed root store ecosystem, with nearly all user agents ultimately deriving their roots from one of three root programs: Apple, Microsoft, and NSS. This inverted pyramid structure further magnifies the importance of judicious root store management by these foundational root programs.Our analysis of root store management presents evidence of NSS's relative operational agility, transparency, and rigorous inclusion policies. Unsurprisingly, all derivative root stores in our dataset (e.g., Linuxes, Android, NodeJS) draw their roots from NSS. Despite this solid footing, derivative root stores display lax update routines and often customize their root stores in questionable ways. By scrutinizing these practices, we highlight two fundamental obstacles to existing NSS-derived root stores: rigid on-or-off trust and multi-purpose root stores. Taken together, our study highlights the concentration of root store trust in TLS server authentication, exposes questionable root management practices, and proposes improvements for future TLS root stores.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {179–194},
numpages = {16},
location = {Virtual Event},
series = {IMC '21}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3651640,
title = {ESSE '23: Proceedings of the 4th European Symposium on Software Engineering},
year = {2023},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Napoli, Italy}
}

@proceedings{10.1145/3583740,
title = {SEC '23: Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing},
year = {2023},
isbn = {9798400701238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SEC is a premier forum for top researchers and practitioners to come together to discuss the opportunities and challenges of edge computing.},
location = {Wilmington, DE, USA}
}

@article{10.1007/s00165-021-00547-2,
author = {\v{C}e\v{s}ka, Milan and Hensel, Christian and Junges, Sebastian and Katoen, Joost-Pieter},
title = {Counterexample-guided inductive synthesis for probabilistic systems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4–5},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00547-2},
doi = {10.1007/s00165-021-00547-2},
abstract = {This paper presents counterexample-guided inductive synthesis (CEGIS) to automatically synthesise probabilistic models. The starting point is a family of finite-stateMarkov chains with related but distinct topologies. Such families can succinctly be described by a sketch of a probabilistic program. Program sketches are programs containing holes. Every hole has a finite repertoire of possible program snippets by which it can be filled.We study several synthesis problems—feasibility, optimal synthesis, and complete partitioning—for a given quantitative specification φ. Feasibility amounts to determine a family member satisfying φ, optimal synthesis amounts to find a family member that maximises the probability to satisfy φ, and complete partitioning splits the family in satisfying and refuting members. Each of these problems can be considered under the additional constraint of minimising the total cost of instantiations, e.g., what are all possible instantiations for φ that are within a certain budget? The synthesis problems are tackled using a CEGIS approach. The crux is to aggressively prune the search space by using counterexamples provided by a probabilistic model checker. Counterexamples can be viewed as sub-Markov chains that rule out all family members that share this sub-chain. Our CEGIS approach leverages efficient probabilisticmodel checking,modern SMT solving, and programsnippets as counterexamples. Experiments on case studies froma diverse nature—controller synthesis, program sketching, and security—show that synthesis among up to a million candidate designs can be done using a few thousand verification queries.},
journal = {Form. Asp. Comput.},
month = aug,
pages = {637–667},
numpages = {31},
keywords = {Program Synthesis, Markov Chains, Probabilistic Model Checking, Counterexamples, CEGIS}
}

@proceedings{10.1145/3689484,
title = {GPCE '24: Proceedings of the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2024},
isbn = {9798400712111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’24). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming language and software engineering research communities.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1109/ECASE.2019.00010,
author = {Keim, Jan and Schneider, Yves and Koziolek, Anne},
title = {Towards consistency analysis between formal and informal software architecture artefacts},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ECASE.2019.00010},
doi = {10.1109/ECASE.2019.00010},
abstract = {Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to address consistency analyses between models and textual natural language artefacts using natural language understanding and plan to include knowledge bases to improve these analyses. After extracting information out of the natural language documents, we plan to create traceability links and check whether statements within the textual documentation are consistent with the software architecture models. In this paper, we also outline our requirements for evaluating our approach with the help of a community-wide infrastructure and how our approach can be used to maintain community-wide case studies.},
booktitle = {Proceedings of the 2nd International Workshop on Establishing a Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {6–12},
numpages = {7},
keywords = {consistency, natural language processing, natural language understanding, software architecture, software architecture documentation, software engineering},
location = {Montreal, Quebec, Canada},
series = {ECASE '19}
}

@proceedings{10.1145/3708282,
title = {AITC '24: Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
year = {2024},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3658271,
title = {SBSI '24: Proceedings of the 20th Brazilian Symposium on Information Systems},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Juiz de Fora, Brazil}
}

@proceedings{10.1145/3569966,
title = {CSSE '22: Proceedings of the 5th International Conference on Computer Science and Software Engineering},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@article{10.1145/3467896,
author = {Vogel-Heuser, Birgit and Neumann, Eva-Maria and Fischer, Juliane},
title = {MICOSE4aPS: Industrially Applicable Maturity Metric to Improve Systematic Reuse of Control Software},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467896},
doi = {10.1145/3467896},
abstract = {automated Production Systems (aPS) are highly complex, mechatronic systems that usually have to operate reliably for many decades. Standardization and reuse of control software modules is a core prerequisite to achieve the required system quality in increasingly shorter development cycles. However, industrial case studies in aPS show that many aPS companies still struggle with strategically reusing software. This paper proposes a metric-based approach to objectively measure the maturity of industrial IEC 61131-based control software in aPS (MICOSE4aPS) to identify potential weaknesses and quality issues hampering systematic reuse. Module developers in the machine and plant manufacturing industry can directly benefit as the metric calculation is integrated into the software engineering workflow. An in-depth industrial evaluation in a top-ranked machine manufacturing company in food packaging and an expert evaluation with different companies confirmed the benefit of efficiently managing the quality of control software.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {5},
numpages = {24},
keywords = {Automated production systems, programmable logic controllers, IEC 61131-3, software quality}
}

@inproceedings{10.1145/3540250.3549144,
author = {Martin-Lopez, Alberto and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio},
title = {Online testing of RESTful APIs: promises and challenges},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549144},
doi = {10.1145/3540250.3549144},
abstract = {Online testing of web APIs—testing APIs in production—is gaining traction in industry. Platforms such as RapidAPI and Sauce Labs provide online testing and monitoring services of web APIs 24/7, typically by re-executing manually designed test cases on the target APIs on a regular basis. In parallel, research on the automated generation of test cases for RESTful APIs has seen significant advances in recent years. However, despite their promising results in the lab, it is unclear whether research tools would scale to industrial-size settings and, more importantly, how they would perform in an online testing setup, increasingly common in practice. In this paper, we report the results of an empirical study on the use of automated test case generation methods for online testing of RESTful APIs. Specifically, we used the RESTest framework to automatically generate and execute test cases in 13 industrial APIs for 15 days non-stop, resulting in over one million test cases. To scale at this level, we had to transition from a monolithic tool approach to a multi-bot architecture with over 200 bots working cooperatively in tasks like test generation and reporting. As a result, we uncovered about 390K failures, which we conservatively triaged into 254 bugs, 65 of which have been acknowledged or fixed by developers to date. Among others, we identified confirmed faults in the APIs of Amadeus, Foursquare, Yelp, and YouTube, accessed by millions of applications worldwide. More importantly, our reports have guided developers on improving their APIs, including bug fixes and documentation updates in the APIs of Amadeus and YouTube. Our results show the potential of online testing of RESTful APIs as the next must-have feature in industry, but also some of the key challenges to overcome for its full adoption in practice.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {408–420},
numpages = {13},
keywords = {REST, black-box testing, bot, online testing, web API},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3640457,
title = {RecSys '24: Proceedings of the 18th ACM Conference on Recommender Systems},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bari, Italy}
}

@inproceedings{10.1145/3544548.3580652,
author = {Liao, Q. Vera and Subramonyam, Hariharan and Wang, Jennifer and Wortman Vaughan, Jennifer},
title = {Designerly Understanding: Information Needs for Model Transparency to Support Design Ideation for AI-Powered User Experience},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580652},
doi = {10.1145/3544548.3580652},
abstract = {Despite the widespread use of artificial intelligence (AI), designing user experiences (UX) for AI-powered systems remains challenging. UX designers face hurdles understanding AI technologies, such as pre-trained language models, as design materials. This limits their ability to ideate and make decisions about whether, where, and how to use AI. To address this problem, we bridge the literature on AI design and AI transparency to explore whether and how frameworks for transparent model reporting can support design ideation with pre-trained models. By interviewing 23 UX practitioners, we find that practitioners frequently work with pre-trained models, but lack support for UX-led ideation. Through a scenario-based design task, we identify common goals that designers seek model understanding for and pinpoint their model transparency information needs. Our study highlights the pivotal role that UX designers can play in Responsible AI and calls for supporting their understanding of AI limitations through model transparency and interrogation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {9},
numpages = {21},
keywords = {AI design, AI documentation, AI transparency, explainability, pre-trained models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/256562.256940,
author = {Novels, Michael D. and Jonik, Stan},
title = {Scheduling with simulation in the food &amp; drinks industry},
year = {1996},
isbn = {0780333837},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1145/256562.256940},
doi = {10.1145/256562.256940},
abstract = {The ever increasing pressures from the market place has caused companies to re-assess how the production planning function is carried out and to look for tools that "I support the decision making process and generate achievable schedules in a timely manner. This paper describes the use of capacity planning and scheduling decision support tools that combine a simulation model generated with ARENA, with PREACTOR, a PC based finite scheduler. The use of this combination is illustrated by their use in food processing plant in the UK and then discusses how these may influence the way that variable costs are measured.},
booktitle = {Proceedings of the 28th Conference on Winter Simulation},
pages = {1252–1256},
numpages = {5},
location = {Coronado, California, USA},
series = {WSC '96}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@proceedings{10.1109/3655039,
title = {ASPDAC '24: Proceedings of the 29th Asia and South Pacific Design Automation Conference},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
abstract = {ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) area like other sister conferences such as Design Automation Conference (DAC), Design, Automation &amp; Test in Europe (DATE), International Conference on Computer-Aided Design (ICCAD), and Embedded Systems Week (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunity to know the recent advanced technologies on LSI design and design automation areas, and to communicate each other for researchers and designers around Asia and South Pacific regions.},
location = {Incheon, Republic of Korea}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.5555/3320516.3320573,
author = {Galpin, Vashti and Georgoulas, Anastasis and Loreti, Michele and Vandin, Andrea},
title = {Statistical analysis of CARMA models: an advanced tutorial},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {CARMA (Collective Adaptive Resource-sharing Markovian Agents) is a process-algebra-based quantitative language developed for the modeling of collective adaptive systems. A CARMA model consists of an environment in which a collective of components with attribute stores interact via unicast and broadcast communication, providing a rich modeling formalism. The semantics of a CARMA model are given by a continuous-time Markov chain which can be simulated using the CARMA Eclipse Plug-in. Furthermore, statistical model checking can be applied to the trajectories generated through simulation using the MultiVeStA tool. This advanced tutorial will introduce some of the theory behind CARMA and MultiVeStA as well as demonstrate its application to collective adaptive system modeling.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {395–409},
numpages = {15},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3238147.3238192,
author = {Abdessalem, Raja Ben and Panichella, Annibale and Nejati, Shiva and Briand, Lionel C. and Stifter, Thomas},
title = {Testing autonomous cars for feature interaction failures using many-objective search},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238192},
doi = {10.1145/3238147.3238192},
abstract = {Complex systems such as autonomous cars are typically built as a composition of features that are independent units of functionality. Features tend to interact and impact one another's behavior in unknown ways. A challenge is to detect and manage feature interactions, in particular, those that violate system requirements, hence leading to failures. In this paper, we propose a technique to detect feature interaction failures by casting this problem into a search-based test generation problem. We define a set of hybrid test objectives (distance functions) that combine traditional coverage-based heuristics with new heuristics specifically aimed at revealing feature interaction failures. We develop a new search-based test generation algorithm, called FITEST, that is guided by our hybrid test objectives. FITEST extends recently proposed many-objective evolutionary algorithms to reduce the time required to compute fitness values. We evaluate our approach using two versions of an industrial self-driving system. Our results show that our hybrid test objectives are able to identify more than twice as many feature interaction failures as two baseline test objectives used in the software testing literature (i.e., coverage-based and failure-based test objectives). Further, the feedback from domain experts indicates that the detected feature interaction failures represent real faults in their systems that were not previously identified based on analysis of the system features and their requirements.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {143–154},
numpages = {12},
keywords = {Automotive Systems, Feature Interaction Problem, Many-Objective Optimization, Search-based Software Testing},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3053600.3053636,
author = {Ferme, Vincenzo and Pautasso, Cesare},
title = {Towards Holistic Continuous Software Performance Assessment},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053636},
doi = {10.1145/3053600.3053636},
abstract = {In agile, fast and continuous development lifecycles, software performance analysis is fundamental to confidently release continuously improved software versions. Researchers and industry practitioners have identified the importance of integrating performance testing in agile development processes in a timely and efficient way. However, existing techniques are fragmented and not integrated taking into account the heterogeneous skills of the users developing polyglot distributed software, and their need to automate performance practices as they are integrated in the whole lifecycle without breaking its intrinsic velocity. In this paper we present our vision for holistic continuous software performance assessment, which is being implemented in the BenchFlow tool. BenchFlow enables performance testing and analysis practices to be pervasively integrated in continuous development lifecycle activities. Users can specify performance activities (e.g., standard performance tests) by relying on an expressive Domain Specific Language for objective-driven performance analysis. Collected performance knowledge can be thus reused to speed up performance activities throughout the entire process.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {159–164},
numpages = {6},
keywords = {continuous integration, continuous software performance assessment, performance analysis, performance test},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inbook{10.1145/3640508.3640525,
title = {The Deans},
year = {2024},
isbn = {9798400717741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3640508.3640525},
booktitle = {Rendering History: The Women of ACM-W},
pages = {241–282},
numpages = {42}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@inproceedings{10.1145/3377811.3380365,
author = {Stevens, Clay and Bagheri, Hamid},
title = {Reducing run-time adaptation space via analysis of possible utility bounds},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380365},
doi = {10.1145/3377811.3380365},
abstract = {Self-adaptive systems often employ dynamic programming or similar techniques to select optimal adaptations at run-time. These techniques suffer from the "curse of dimensionality", increasing the cost of run-time adaptation decisions. We propose a novel approach that improves upon the state-of-the-art proactive self-adaptation techniques to reduce the number of possible adaptations that need be considered for each run-time adaptation decision. The approach, realized in a tool called Thallium, employs a combination of automated formal modeling techniques to (i) analyze a structural model of the system showing which configurations are reachable from other configurations and (ii) compute the utility that can be generated by the optimal adaptation over a bounded horizon in both the best- and worst-case scenarios. It then constructs triangular possibility values using those optimized bounds to automatically compare adjacent adaptations for each configuration, keeping only the alternatives with the best range of potential results. The experimental results corroborate Thallium's ability to significantly reduce the number of states that need to be considered with each adaptation decision, freeing up vital resources at run-time.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1522–1534},
numpages = {13},
keywords = {formal methods, multi-objective optimization, run-time adaptation, self-adaptive systems},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2593801.2593802,
author = {Cleland-Huang, Jane and Guo, Jin},
title = {Towards more intelligent trace retrieval algorithms},
year = {2014},
isbn = {9781450328463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593801.2593802},
doi = {10.1145/2593801.2593802},
abstract = {Automated trace creation techniques are based on a variety of algorithms ranging from basic term matching approaches to more sophisticated expert systems. In this position paper we propose a classification scheme for categorizing the intelligence level of automated traceability techniques. We show that the vast majority of relevant work in the past decade has been focused at the lowest level of the Traceability Intelligence Quotient (tIQ) and posit that achieving high quality automated traceability will require re-focusing research efforts on the development of more intelligent algorithms capable of reasoning about concepts, their relationships and constraints, and the contexts in which they occur.},
booktitle = {Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {Domain Ontology, Expert System, Traceability},
location = {Hyderabad, India},
series = {RAISE 2014}
}

@inproceedings{10.1145/3427228.3427248,
author = {Pennekamp, Jan and Buchholz, Erik and Lockner, Yannik and Dahlmanns, Markus and Xi, Tiandong and Fey, Marcel and Brecher, Christian and Hopmann, Christian and Wehrle, Klaus},
title = {Privacy-Preserving Production Process Parameter Exchange},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427248},
doi = {10.1145/3427228.3427248},
abstract = {Nowadays, collaborations between industrial companies always go hand in hand with trust issues, i.e., exchanging valuable production data entails the risk of improper use of potentially sensitive information. Therefore, companies hesitate to offer their production data, e.g., process parameters that would allow other companies to establish new production lines faster, against a quid pro quo. Nevertheless, the expected benefits of industrial collaboration, data exchanges, and the utilization of external knowledge are significant. In this paper, we introduce our Bloom filter-based Parameter Exchange (BPE), which enables companies to exchange process parameters privacy-preservingly. We demonstrate the applicability of our platform based on two distinct real-world use cases: injection molding and machine tools. We show that BPE is both scalable and deployable for different needs to foster industrial collaborations. Thereby, we reward data-providing companies with payments while preserving their valuable data and reducing the risks of data leakage.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {510–525},
numpages = {16},
keywords = {Bloom filter, Internet of Production, oblivious transfer, secure industrial collaboration},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3643655,
title = {SESoS '24: Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2024 will provide a forum for researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for the development and evolution of complex software-intensive systems.},
location = {Lisbon, Portugal}
}

@proceedings{10.5555/3581644,
title = {CNSM '22: Proceedings of the 18th International Conference on Network and Service Management},
year = {2022},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {CNSM 2022 focuses on the theme "Intelligent Management of Disruptive Network Technologies and Services", that aims at capturing emerging approaches and intelligent solutions for dealing with disruptive network technologies, as well as associated services and applications.},
location = {Thessaloniki, Greece}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1145/2897053.2897060,
author = {Incerto, Emilio and Tribastone, Mirco and Trubiani, Catia},
title = {Symbolic performance adaptation},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897060},
doi = {10.1145/2897053.2897060},
abstract = {Quality-of-Service attributes such as performance and reliability heavily depend on the run-time conditions under which software is executed (e.g., workload fluctuation and resources availability). Therefore, it is important to design systems able to adapt their setting and behavior due to these run-time variabilities. In this paper we propose a novel approach based on queuing networks as the quantitative model to represent system configurations. To find a model that fits with continuous changes in run-time conditions we rely on an innovative combination of symbolic analysis and satisfiability modulo theory (SMT). Through symbolic analysis we represent all possible system configurations as a set of nonlinear real constraints. By formulating an SMT problem we are able to devise feasible system configurations at a small computational cost. We study the effectiveness and scalability of our approach on a three-tier web system featuring different levels of redundancy.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {140–150},
numpages = {11},
keywords = {performance-based adaptation, queueing networks, satisfiability modulo theories, symbolic analysis},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/2896825.2896834,
author = {Klein, John and Buglak, Ross and Blockow, David and Wuttke, Troy and Cooper, Brenton},
title = {A reference architecture for big data systems in the national security domain},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896834},
doi = {10.1145/2896825.2896834},
abstract = {Acquirers, system builders, and other stakeholders of big data systems need to define requirements, develop and evaluate solutions, and integrate systems together. A reference architecture enables these software engineering activities by standardizing nomenclature, defining key solution elements and their relationships, collecting relevant solution patterns, and classifying existing technologies. Within the national security domain, existing reference architectures for big data systems have not been useful because they are too general or are not vendor-neutral. We present a reference architecture for big data systems that is focused on addressing typical national defence requirements and that is vendor-neutral, and we demonstrate how to use this reference architecture to define solutions in one mission area.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {51–57},
numpages = {7},
keywords = {big data, reference architecture},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@proceedings{10.1145/3660829,
title = {Programming '24: Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
year = {2024},
isbn = {9798400706349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lund, Sweden}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.1145/3701625,
title = {SBQS '24: Proceedings of the XXIII Brazilian Symposium on Software Quality},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@article{10.1145/2674632.2674634,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2674632.2674634},
doi = {10.1145/2674632.2674634},
journal = {SIGSOFT Softw. Eng. Notes},
month = dec,
pages = {8–16},
numpages = {9}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@inproceedings{10.1145/3377811.3380927,
author = {Alrajeh, Dalal and Cailliau, Antoine and van Lamsweerde, Axel},
title = {Adapting requirements models to varying environments},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380927},
doi = {10.1145/3377811.3380927},
abstract = {The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set.The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {50–61},
numpages = {12},
keywords = {context-dependent requirements, formal verification, logic-based learning, requirements adaptation, requirements evolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@proceedings{10.1145/3631882,
title = {MEMSYS '23: Proceedings of the International Symposium on Memory Systems},
year = {2023},
isbn = {9798400716447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Alexandria, VA, USA}
}

@inproceedings{10.1145/2593882.2593885,
author = {Orso, Alessandro and Rothermel, Gregg},
title = {Software testing: a research travelogue (2000–2014)},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593885},
doi = {10.1145/2593882.2593885},
abstract = {Despite decades of work by researchers and practitioners on numerous software quality assurance techniques, testing remains one of the most widely practiced and studied approaches for assessing and improving software quality. Our goal, in this paper, is to provide an accounting of some of the most successful research performed in software testing since the year 2000, and to present what appear to be some of the most significant challenges and opportunities in this area. To be more inclusive in this effort, and to go beyond our own personal opinions and biases, we began by contacting over 50 of our colleagues who are active in the testing research area, and asked them what they believed were (1) the most significant contributions to software testing since 2000 and (2) the greatest open challenges and opportunities for future research in this area. While our colleagues’ input (consisting of about 30 responses) helped guide our choice of topics to cover and ultimately the writing of this paper, we by no means claim that our paper represents all the relevant and noteworthy research performed in the area of software testing in the time period considered—a task that would require far more space and time than we have available. Nevertheless, we hope that the approach we followed helps this paper better reflect not only our views, but also those of the software testing community in general.},
booktitle = {Future of Software Engineering Proceedings},
pages = {117–132},
numpages = {16},
keywords = {Software testing},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3175731.3175733,
author = {Pelliccione, Patrizio and Kobetski, Avenir and Larsson, Tony and Aramrattana, Maytheewat and Aderum, Tobias and \r{A}gren, S. Magnus and Jonsson, G\"{o}ran and Heldal, Rogardt and Bergenhem, Carl and Thors\'{e}n, Anders},
title = {Architecting cars as constituents of a system of systems},
year = {2016},
isbn = {9781450363990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175731.3175733},
doi = {10.1145/3175731.3175733},
abstract = {Future transportation systems will be a heterogeneous mix of items with varying connectivity and interoperability. A mix of new technologies and legacy systems will co-exist to realize a variety of scenarios involving not only connected cars but also road infrastructures, pedestrians, cyclists, etc. Future transportation systems can be seen as a System of Systems (SoS), where each constituent system - one of the units that compose an SoS - can act as a standalone system, but the cooperation among the constituent systems enables new emerging and promising scenarios. In this paper we investigate how to architect cars so that they can be constituents of future transportation systems. This work is realized in the context of two Swedish projects coordinated by Volvo Cars and involving some universities and research centers in Sweden and many suppliers of the OEM, including Autoliv, Arccore, Combitech, Cybercom, Knowit, Prevas, \r{A}F-Technology, Semcom, and Qamcom.},
booktitle = {Proceedings of the International Colloquium on Software-Intensive Systems-of-Systems at 10th European Conference on Software Architecture},
articleno = {5},
numpages = {7},
keywords = {automotive, software architecture, systems of systems},
location = {Copenhagen, Denmark},
series = {SiSoS@ECSA '16}
}

@inproceedings{10.1145/2020408.2020447,
author = {Mukerjee, Kunal and Porter, Todd and Gherman, Sorin},
title = {Linear scale semantic mining algorithms in microsoft SQL server's semantic platform},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020447},
doi = {10.1145/2020408.2020447},
abstract = {This paper describes three linear scale, incremental, and fully automatic semantic mining algorithms that are at the foundation of the new Semantic Platform being released in the next version of SQL Server. The target workload is large (10 -- 100 million) Enterprise document corpuses. At these scales, anything short of linear scale and incremental is costly to deploy. These three algorithms give rise to three weighted physical indexes: Tag Index (top keywords in each document); Document Similarity Index (top closely related documents given any document); and Semantic Phrase Similarity Index (top semantically related phrases, given any phrase), which are then query-able through the SQL interface. The need for specifically creating these three indexes was motivated by observing typical stages of document research, and gap analysis, given current tools and technology at the Enterprise. We describe the mining algorithms and architecture, and also outline some compelling user experiences that are enabled by the indexes.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {213–221},
numpages = {9},
keywords = {document similarity, incremental, keyword extraction, linear scale, semantic mining, semantic platform},
location = {San Diego, California, USA},
series = {KDD '11}
}

@article{10.1145/3143561,
author = {Chen, Tsong Yueh and Kuo, Fei-Ching and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Tse, T. H. and Zhou, Zhi Quan},
title = {Metamorphic Testing: A Review of Challenges and Opportunities},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3143561},
doi = {10.1145/3143561},
abstract = {Metamorphic testing is an approach to both test case generation and test result verification. A central element is a set of metamorphic relations, which are necessary properties of the target function or algorithm in relation to multiple inputs and their expected outputs. Since its first publication, we have witnessed a rapidly increasing body of work examining metamorphic testing from various perspectives, including metamorphic relation identification, test case generation, integration with other software engineering techniques, and the validation and evaluation of software systems. In this article, we review the current research of metamorphic testing and discuss the challenges yet to be addressed. We also present visions for further improvement of metamorphic testing and highlight opportunities for new research.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {4},
numpages = {27},
keywords = {Metamorphic testing, metamorphic relation, oracle problem, test case generation}
}

@article{10.1145/3287070,
author = {Volanschi, Nic and Serpette, Bernard and Carteron, Adrien and Consel, Charles},
title = {A Language for Online State Processing of Binary Sensors, Applied to Ambient Assisted Living},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287070},
doi = {10.1145/3287070},
abstract = {There is a large variety of binary sensors in use today, and useful context-aware services can be defined using such binary sensors. However, the currently available approaches for programming context-aware services do not conveniently support binary sensors. Indeed, no existing approach simultaneously supports a notion of state, central to binary sensors, offers a complete set of operators to compose states, allows to define reusable abstractions by means of such compositions, and implements efficient online processing of these operators.This paper proposes a new language for event processing specifically targeted to binary sensors. The central contributions of this language are a native notion of state and semi-causal operators for temporal state composition including: Allen's interval relations generalized for handling multiple intervals, and temporal filters for handling delays. Compared to other approaches such as CEP (complex event processing), our language provides less discontinued information, allows less restricted compositions, and supports reusable abstractions. We implemented an interpreter for our language and applied it to successfully rewrite a full set of real Ambient Assisted Living services. The performance of our prototype interpreter is shown to compete well with a commercial CEP engine when expressing the same services.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {192},
numpages = {26},
keywords = {Allen interval algebra, Ambient assisted living, Binary sensors, Smart homes}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@inproceedings{10.1145/2642803.2642830,
author = {Petrov, Plamen and Nord, Robert L. and Buy, Ugo},
title = {Probabilistic Macro-Architectural Decision Framework},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642830},
doi = {10.1145/2642803.2642830},
abstract = {Experience with system-level concerns demonstrates that fitness for context is a consideration that is equally significant in making architectural decisions as is fitness for purpose. This requires architects to consider contextual factors in making decisions. These decisions are probabilistic in nature and they represent the subjective belief of the architect or the prior probability which is likely to change as new evidence becomes available during the course of the system design. They serve as recommendations and directional inputs to other decisions in the design process. In this paper, we introduce a macro-architectural decision framework we developed to enable the architect for a software-reliant system to model and reason about contextual factors. At the core of our framework is an adaptation of a Bayesian belief network that is augmented with decision and utility nodes. The framework captures contextual factors and their influence on decisions and utilities. We applied our approach in the study of a software system implementation at a healthcare company. The results show promise that such decision support tools help explore the space of factors involved in decision making and provide sensible suggestions for making architectural decisions.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {27},
numpages = {8},
keywords = {Bayesian belief network, Bayesian net, architecture decision network, contextual factors, decision network, enterprise, influence diagram, macro-architecture, probabilistic graphical model, software architecture, systems},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3180155.3180210,
author = {Miranda, Breno and Cruciani, Emilio and Verdecchia, Roberto and Bertolino, Antonia},
title = {FAST approaches to scalable similarity-based test case prioritization},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180210},
doi = {10.1145/3180155.3180210},
abstract = {Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {222–232},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3633500,
title = {NSPW '23: Proceedings of the 2023 New Security Paradigms Workshop},
year = {2023},
isbn = {9798400716201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Segovia, Spain}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/2024445.2024450,
author = {Ernst, Neil A. and Borgida, Alexander and Mylopoulos, John},
title = {Requirements evolution drives software evolution},
year = {2011},
isbn = {9781450308489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024445.2024450},
doi = {10.1145/2024445.2024450},
abstract = {Changes to software should be made with reference to the requirements of that software, as these requirements provide the reasons for a change. Requirements serve to tie the implementation world of the developers to the problem world of the stakeholders. Most empirical studies of requirements have shown that misunderstood and changing requirements cause the majority of failures and costs in software. However, research in software evolution has typically focused on how to evolve software and not why. In our view, evolving software is about solving requirements problems, that is, finding new implementations which will satisfy the requirements while respecting domain assumptions. We argue that by describing this relationship, an implementation choice that best meets stakeholder needs can be made. We describe a tool that models requirements problems. This tool can find incremental solutions to evolving requirements problems quickly.},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution},
pages = {16–20},
numpages = {5},
keywords = {goal-oriented modeling, requirements evolution, unanticipated change},
location = {Szeged, Hungary},
series = {IWPSE-EVOL '11}
}

@proceedings{10.1145/3626772,
title = {SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 47th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024), taking place in Washington D.C., USA, from July 14 to 18, 2024.SIGIR serves as the foremost international forum for the presentation of groundbreaking research findings, the demonstration of innovative systems and techniques, and the exploration of forwardthinking research directions in the field of information retrieval.This year's SIGIR is an in-person conference. We believe that an in-person conference is beneficial for several reasons: it fosters direct engagement and networking opportunities, enhances the exchange of research ideas, contributes to a more dynamic and productive conference experience, and nurtures our research community by welcoming newcomers, providing them with the opportunity to become acquainted with SIGIR traditions. This decision has not been made lightly. We understand the challenges that can pose in the aftermath of a pandemic and amidst the uncertainties of the world around us. To accommodate those who cannot attend, we have implemented a series of measures such as proxy presenters, livestreaming, and recording sessions. These steps are taken to ensure that everyone has access to the valuable content that the conference offers.},
location = {Washington DC, USA}
}

@proceedings{10.1145/3579371,
title = {ISCA '23: Proceedings of the 50th Annual International Symposium on Computer Architecture},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@proceedings{10.1145/3688867,
title = {McGE '24: Proceedings of the 2nd International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice},
year = {2024},
isbn = {9798400711947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice- McGE 2024We believe that this workshop will provide a valuable platform for researchers and practitioners to discuss and exchange ideas on the latest advancements, challenges, and opportunities in the rapidly evolving field of multimedia content generation.},
location = {Melbourne VIC, Australia}
}

@proceedings{10.1145/3640794,
title = {CUI '24: Proceedings of the 6th ACM Conference on Conversational User Interfaces},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Luxembourg, Luxembourg}
}

@inproceedings{10.5555/2337223.2337301,
author = {Balasubramaniam, Dharini and Jefferson, Christopher and Kotthoff, Lars and Miguel, Ian and Nightingale, Peter},
title = {An automated approach to generating efficient constraint solvers},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Combinatorial problems appear in numerous settings, from timetabling to industrial design. Constraint solving aims to find solutions to such problems efficiently and automatically. Current constraint solvers are monolithic in design, accepting a broad range of problems. The cost of this convenience is a complex architecture, inhibiting efficiency, extensibility and scalability. Solver components are also tightly coupled with complex restrictions on their configuration, making automated generation of solvers difficult.  We describe a novel, automated, model-driven approach to generating efficient solvers tailored to individual problems and present some results from applying the approach. The main contribution of this work is a solver generation framework called Dominion, which analyses a problem and, based on its characteristics, generates a solver using components chosen from a library. The key benefit of this approach is the ability to solve larger and more difficult problems as a result of applying finer-grained optimisations and using specialised techniques as required.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {661–671},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1145/3398665,
author = {Cerrolaza, Jon Perez and Obermaisser, Roman and Abella, Jaume and Cazorla, Francisco J. and Gr\"{u}ttner, Kim and Agirre, Irune and Ahmadian, Hamidreza and Allende, Imanol},
title = {Multi-core Devices for Safety-critical Systems: A Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398665},
doi = {10.1145/3398665},
abstract = {Multi-core devices are envisioned to support the development of next-generation safety-critical systems, enabling the on-chip integration of functions of different criticality. This integration provides multiple system-level potential benefits such as cost, size, power, and weight reduction. However, safety certification becomes a challenge and several fundamental safety technical requirements must be addressed, such as temporal and spatial independence, reliability, and diagnostic coverage. This survey provides a categorization and overview at different device abstraction levels (nanoscale, component, and device) of selected key research contributions that support the compliance with these fundamental safety requirements.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {38},
keywords = {Fault tolerance, diagnostic coverage, spatial independence, time independence}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@proceedings{10.1145/3613904,
title = {CHI '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/2884781.2884853,
author = {Mirzaei, Nariman and Garcia, Joshua and Bagheri, Hamid and Sadeghi, Alireza and Malek, Sam},
title = {Reducing combinatorics in GUI testing of android applications},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884853},
doi = {10.1145/2884781.2884853},
abstract = {The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible combinations is the ideal upper bound in combinatorial testing, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These specifications express the app's behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid's ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {559–570},
numpages = {12},
keywords = {android, input generation, software testing},
location = {Austin, Texas},
series = {ICSE '16}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@article{10.1145/3386331,
author = {Moler, Cleve and Little, Jack},
title = {A history of MATLAB},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386331},
doi = {10.1145/3386331},
abstract = {The first MATLAB (the name is short for “Matrix Laboratory”) was not a programming language. Written in Fortran in the late 1970s, it was a simple interactive matrix calculator built on top of about a dozen subroutines from the LINPACK and EISPACK matrix software libraries. There were only 71 reserved words and built-in functions. It could be extended only by modifying the Fortran source code and recompiling it. The programming language appeared in 1984 when MATLAB became a commercial product. The calculator was reimplemented in C and significantly enhanced with the addition of user functions, toolboxes, and graphics. It was available initially on the IBM PC and clones; versions for Unix workstations and the Apple Macintosh soon followed. In addition to the matrix functions from the calculator, the 1984 MATLAB included fast Fourier transforms (FFT). The Control System Toolbox appeared in 1985 and the Signal Processing Toolbox in 1987. Built-in support for the numerical solution of ordinary differential equations also appeared in 1987. The first significant new data structure, the sparse matrix, was introduced in 1992. The Image Processing Toolbox and the Symbolic Math Toolbox were both introduced in 1993. Several new data types and data structures, including single precision floating point, various integer and logical types, cell arrays, structures, and objects were introduced in the late 1990s. Enhancements to the MATLAB computing environment have dominated development in recent years. Included are extensions to the desktop, major enhancements to the object and graphics systems, support for parallel computing and GPUs, and the “Live Editor”, which combines programs, descriptive text, output and graphics into a single interactive, formatted document. Today there are over 60 Toolboxes, many programmed in the MATLAB language, providing extended capabilities in specialized technical fields.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {81},
numpages = {67},
keywords = {MATLAB, linear algebra, matrix computation}
}

@article{10.1145/882240.882242,
author = {staff, ACM SIGSOFT Software Engineering Notes},
title = {Back matter},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/882240.882242},
doi = {10.1145/882240.882242},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {38–48},
numpages = {11}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3628516,
title = {IDC '24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@article{10.1145/333175.333882,
author = {Kumar, Amruth},
title = {Announcements},
year = {2000},
issue_date = {Spring 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1523-8822},
url = {https://doi.org/10.1145/333175.333882},
doi = {10.1145/333175.333882},
journal = {Intelligence},
month = apr,
pages = {41–48},
numpages = {8}
}

@inproceedings{10.1145/1282280.1282355,
author = {Schietse, Jan and Eakins, John P. and Veltkamp, Remco C.},
title = {Practice and challenges in trademark image retrieval},
year = {2007},
isbn = {9781595937339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1282280.1282355},
doi = {10.1145/1282280.1282355},
abstract = {In this paper, we outline some of the main challenges facing trademark searchers today, and discuss the extent to which current automated systems are meeting those challenges.},
booktitle = {Proceedings of the 6th ACM International Conference on Image and Video Retrieval},
pages = {518–524},
numpages = {7},
keywords = {content-based image retrieval, pattern matching, trademark similarity},
location = {Amsterdam, The Netherlands},
series = {CIVR '07}
}

@inproceedings{10.1145/2642937.2643002,
author = {Campos, Jos\'{e} and Arcuri, Andrea and Fraser, Gordon and Abreu, Rui},
title = {Continuous test generation: enhancing continuous integration with automated test generation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643002},
doi = {10.1145/2642937.2643002},
abstract = {In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {55–66},
numpages = {12},
keywords = {automated test generation, continuous integration, continuous testing, unit testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@proceedings{10.1145/3628034,
title = {EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs},
year = {2023},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@article{10.1145/384076.384082,
author = {Dray, Susan M.},
title = {Business: going global with the product design process: does it make sense?},
year = {2001},
issue_date = {Nov./Dec. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {1072-5520},
url = {https://doi.org/10.1145/384076.384082},
doi = {10.1145/384076.384082},
journal = {Interactions},
month = oct,
pages = {21–26},
numpages = {6}
}

@proceedings{10.1145/3563835,
title = {Onward! 2022: Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2022},
isbn = {9781450399098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to Onward! 2022. Onward! is a premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities, and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research. Onward! 2022 is part of SPLASH 2022, taking place from Monday 5th to Saturday 10th December 2022 in Auckland, New Zealand.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3641237,
title = {SIGDOC '24: Proceedings of the 42nd ACM International Conference on Design of Communication},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fairfax, VA, USA}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@proceedings{10.1145/3526071,
title = {RoSE '22: Proceedings of the 4th International Workshop on Robotics Software Engineering},
year = {2022},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. The research communities advancing software engineering in robotics, however, are spread over various spe-cialized conferences, such as ICRA, IROS, SIMPAR - each attended mostly by robotics researchers and practitioners - or ICSE andMODELS - mostly attended by software engineering researchers and practitioners. At robotics conferences, software engineering lacks visibility and vice versa.The objective of RoSE is bringing together researchers and practitioners from both domains at a prominent conference to foster cross-fertilization between the two domains. Being the most prominent conference in software engineering, ICSE is the best venue to attract experts from both domains. Hosting this workshop at ICSE enables software engineering researchers to learn more about the challenges of robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions are unnoticed by roboticists, yet.},
location = {Pittsburgh, Pennsylvania}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.5555/1274236.1274238,
author = {Garlan, David and Schmerl, Bradley},
title = {Architecture-driven modelling and analysis},
year = {2007},
isbn = {1920682503},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Over the past 15 years there has been increasing recognition that careful attention to the design of a system's software architecture is critical to satisfying its requirements for quality attributes such as performance, security, and dependability. As a consequence, during this period the field of software architecture has matured significantly. However, current practices of software architecture rely on relatively informal methods, limiting the potential for fully exploiting architectural designs to gain insight and improve the quality of the resulting system. In this paper we draw from a variety of research results to illustrate how formal approaches to software architecture can lead to enhancements in software quality, including improved clarity of design, support for analysis, and assurance that implementations conform to their intended architecture.},
booktitle = {Proceedings of the Eleventh Australian Workshop on Safety Critical Systems and Software - Volume 69},
pages = {3–17},
numpages = {15},
keywords = {architecture analysis, software architecture},
location = {Melbourne, Australia},
series = {SCS '06}
}

@proceedings{10.1145/3593663,
title = {ECSEE '23: Proceedings of the 5th European Conference on Software Engineering Education},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seeon/Bavaria, Germany}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3524614,
title = {IWSiB '22: Proceedings of the 5th International Workshop on Software-intensive Business: Towards Sustainable Software Business},
year = {2022},
isbn = {9781450393027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {There are many researchers and practitioners whose work is related to the field of software-intensive business. However, they are often not fully aware of each other's work as the research is scattered. For example, individual research contributions have emerged related to, for example, software engineering economics, digital ecosystems and software startups. The goal of the workshop on Software-intensive Business is to bring these different sub-fields together and strengthen their ties.},
location = {Pittsburgh, Pennsylvania}
}

@article{10.1145/270849.270855,
author = {Waugh, Doug},
title = {Description of EDCS technology clusters},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270855},
doi = {10.1145/270849.270855},
abstract = {Evolutionary Systems are those that are capable of accomodating change over an extended system lifetime with reduced risk and cost/schedule impact. Most of our complex defense systems depend on software for their successful operation and, as a result, the software in those systems is the prmary vehicle for adapting to change. The EDCS (Evolutionary Design of Complex Software) Program is providing for the development and experimental application of new software technologies which can enable significant improvements in miltary mission effectiveness and information superiority. The goal is the capability to produce software intensive military systmes that are highly flexible and adaptable to meet changing requirements --- evolutionary systems.In addition to DARPA, the EDCS Program is co-sponsored by USAF Rome Laboratory, USAF Wright Laboratory, US Army Missile Command, and the National Science Foundation (NSF). Technical management is provided jointly by the DoD Software Engineering Institute (SEI) and Rome Laboratory. Rome Laboratory is DARPA's primary contracting agent.EDCS is organized into 5 Technology Clusters:1. Rationale Capture and Software Understanding2. Architecture and Generation3. High Assurance / Real-Time4. Information Management5. Dynamic LanguagesThis paper contains a short description of each cluster.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {33–42},
numpages = {10}
}

@proceedings{10.1145/3568813,
title = {ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Chicago, IL, USA}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@proceedings{10.1145/2986012,
title = {Onward! 2016: Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3580507,
title = {EC '23: Proceedings of the 24th ACM Conference on Economics and Computation},
year = {2023},
isbn = {9798400701047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the course of two decades, EC has established itself as one of the few truly successful interdisciplinary conferences, attracting papers and participants with a broad range of interests in economics and computer science, and fostering work in the intersection.},
location = {London, United Kingdom}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@book{10.1145/3502372,
author = {Pelkey, James L. and Russell, Andrew L. and Robbins, Loring G.},
title = {Circuits, Packets, and Protocols: Entrepreneurs and Computer Communications, 1968–1988},
year = {2022},
isbn = {9781450397261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {40},
abstract = {As recently as 1968, computer scientists were uncertain how best to interconnect even two computers. The notion that within a few decades the challenge would be how to interconnect millions of computers around the globe was too far-fetched to contemplate. Yet, by 1988, that is precisely what was happening. The products and devices developed in the intervening years—such as modems, multiplexers, local area networks, and routers—became the linchpins of the global digital society. How did such revolutionary innovation occur? This book tells the story of the entrepreneurs who were able to harness and join two factors: the energy of computer science researchers supported by governments and universities, and the tremendous commercial demand for Internetworking computers. The centerpiece of this history comes from unpublished interviews from the late 1980s with over 80 computing industry pioneers, including Paul Baran, J.C.R. Licklider, Vint Cerf, Robert Kahn, Larry Roberts, and Robert Metcalfe. These individuals give us unique insights into the creation of multi-billion dollar markets for computer-communications equipment, and they reveal how entrepreneurs struggled with failure, uncertainty, and the limits of knowledge.“The key technologies that brought us our modern networked society—routers, packet switching, multiplexers, Internet protocols—were all invented by people in the short period between 1968 and 1988. James Pelkey interviewed these people at that time and recorded their stories. This book is the result: a detailed and up-close personal history of a world being born. Fascinating.” - W. Brian Arthur, Author of The Nature of Technology: What It Is and How It Evolves“Circuits, Packets, and Protocols is full of revelations for me even though I was there. Never had it explained so clearly how my distributed computing strategy was the wrong one for 3Com in the 1980s.” - Bob Metcalfe, Internet Pioneer, Ethernet inventor, 3Com founder; University of Texas at Austin Professor of Innovation}
}

@book{10.1145/3382097,
author = {Allemang, Dean and Hendler, Jim and Gandon, Fabien},
title = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {3},
volume = {33},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@book{10.1145/3544564,
author = {Ullmer, Brygg and Shaer, Orit and Mazalek, Ali and Hummels, Caroline},
title = {Weaving Fire into Form: Aspirations for Tangible and Embodied Interaction},
year = {2022},
isbn = {9781450397698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {44},
abstract = {This book investigates multiple facets of the emerging discipline of Tangible, Embodied, and Embedded Interaction (TEI). This is a story of atoms and bits. We explore the interweaving of the physical and digital, toward understanding some of their wildly varying hybrid forms and behaviors. Spanning conceptual, philosophical, cognitive, design, and technical aspects of interaction, this book charts both history and aspirations for the future of TEI. We examine and celebrate diverse trailblazing works, and provide wide-ranging conceptual and pragmatic tools toward weaving the animating fires of computation and technology into evocative tangible forms. We also chart a path forward for TEI engagement with broader societal and sustainability challenges that will profoundly (re)shape our children’s and grandchildren’s futures. We invite you all to join this quest.}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@techreport{10.1145/2594168,
author = {The Joint Task Force on Computing Curricula},
title = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
year = {2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The primary purpose of this volume is to provide guidance to academic institutions and accreditation agencies about what should constitute an undergraduate software engineering education. These recommendations have been developed by a broad, internationally based group of volunteer participants. This group has taken into account much of the work that has been done in software engineering education over the last quarter of a century. Software engineering curriculum recommendations are of particular relevance, since there is currently a surge in the creation of software engineering degree programs and accreditation processes for such programs have been established in a number of countries.}
}

@book{10.1145/3368274,
author = {Halvorson, Michael J.},
title = {Code Nation: Personal Computing and the Learn to Program Movement in America},
year = {2020},
isbn = {9781450377584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
abstract = {Code Nation explores the rise of software development as a social, cultural, and technical phenomenon in American history. The movement germinated in government and university labs during the 1950s, gained momentum through corporate and counterculture experiments in the 1960s and 1970s, and became a broad-based computer literacy movement in the 1980s. As personal computing came to the fore, learning to program was transformed by a groundswell of popular enthusiasm, exciting new platforms, and an array of commercial practices that have been further amplified by distributed computing and the Internet. The resulting society can be depicted as a “Code Nation”—a globally-connected world that is saturated with computer technology and enchanted by software and its creation.Code Nation is a new history of personal computing that emphasizes the technical and business challenges that software developers faced when building applications for CP/M, MS-DOS, UNIX, Microsoft Windows, the Apple Macintosh, and other emerging platforms. It is a popular history of computing that explores the experiences of novice computer users, tinkerers, hackers, and power users, as well as the ideals and aspirations of leading computer scientists, engineers, educators, and entrepreneurs. Computer book and magazine publishers also played important, if overlooked, roles in the diffusion of new technical skills, and this book highlights their creative work and influence.Code Nation offers a “behind-the-scenes” look at application and operating-system programming practices, the diversity of historic computer languages, the rise of user communities, early attempts to market PC software, and the origins of “enterprise” computing systems. Code samples and over 80 historic photographs support the text. The book concludes with an assessment of contemporary efforts to teach computational thinking to young people.}
}

@inproceedings{10.5555/782010.782022,
author = {Kunz, Thomas and Seuren, Michiel F. H.},
title = {Fast detection of communication patterns in distributed executions},
year = {1997},
publisher = {IBM Press},
abstract = {Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.},
booktitle = {Proceedings of the 1997 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {12},
location = {Toronto, Ontario, Canada},
series = {CASCON '97}
}

