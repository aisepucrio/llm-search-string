@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Feature interaction, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1007/978-3-031-17587-9_1,
author = {Shahbaz, Ajmal and Khan, Salman and Hossain, Mohammad Asiful and Lomonaco, Vincenzo and Cannons, Kevin and Xu, Zhan and Cuzzolin, Fabio},
title = {International Workshop on&nbsp;Continual Semi-Supervised Learning: Introduction, Benchmarks and&nbsp;Baselines},
year = {2021},
isbn = {978-3-031-17586-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17587-9_1},
doi = {10.1007/978-3-031-17587-9_1},
abstract = {The aim of this paper is to formalise a new continual semi-supervised learning (CSSL) paradigm, proposed to the attention of the machine learning community via the IJCAI 2021 International Workshop on Continual Semi-Supervised Learning (CSSL@IJCAI) (), with the aim of raising the field’s awareness about this problem and mobilising its effort in this direction. After a formal definition of continual semi-supervised learning and the appropriate training and testing protocols, the paper introduces two new benchmarks specifically designed to assess CSSL on two important computer vision tasks: activity recognition and crowd counting. We describe the Continual Activity Recognition (CAR) and Continual Crowd Counting (CCC) challenges built upon those benchmarks, the baseline models proposed for the challenges, and describe a simple CSSL baseline which consists in applying batch self-training in temporal sessions, for a limited number of rounds. The results show that learning from unlabelled data streams is extremely challenging, and stimulate the search for methods that can encode the dynamics of the data stream.},
booktitle = {Continual Semi-Supervised Learning: First International Workshop, CSSL 2021, Virtual Event, August 19–20, 2021, Revised Selected Papers},
pages = {1–14},
numpages = {14},
keywords = {Continual learning, Semi-supervised learning, Artificial intelligence}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-87626-5_5,
author = {Davidson, Padraig and Buckermann, Florian and Steininger, Michael and Krause, Anna and Hotho, Andreas},
title = {Semi-unsupervised Learning: An In-depth Parameter Analysis},
year = {2021},
isbn = {978-3-030-87625-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87626-5_5},
doi = {10.1007/978-3-030-87626-5_5},
abstract = {Creating datasets for supervised learning is a very challenging and expensive task, in which each input example has to be annotated with its expected output (e.g. object class). By combining unsupervised and semi-supervised learning, semi-unsupervised learning proposes a new paradigm for partially labeled datasets with additional unknown classes. In this paper we focus on a better understanding of this new learning paradigm and analyze the impact of the amount of labeled data, the number of augmented classes and the selection of hidden classes on the quality of prediction. Especially the number of augmented classes highly influences classification accuracy, which needs tuning for each dataset, since too few and too many augmented classes are detrimental to classifier performance. We also show that we can improve results on a large variety of datasets when using convolutional networks as feature extractors while applying output driven entropy regularization instead of a simple weight based L2 norm.},
booktitle = {KI 2021: Advances in Artificial Intelligence: 44th German Conference on AI, Virtual Event, September 27 – October 1, 2021, Proceedings},
pages = {51–66},
numpages = {16},
keywords = {Semi-unsupervised learning, Deep generative models, Classification}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {cyber physical system (CPS), domain analysis, model based system engineering, product line engineering (PLE), requirements engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s00521-020-05529-8,
author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco},
title = {Machine learning for landslides prevention: a survey},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05529-8},
doi = {10.1007/s00521-020-05529-8},
abstract = {Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {10881–10907},
numpages = {27},
keywords = {Natural disasters, Landslides prevention, Machine learning, Supervised learning, Unsupervised learning, Deep learning}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.5555/3540261.3541394,
author = {Balcan, Maria-Florina and Sharma, Dravyansh},
title = {Data driven semi-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a novel data driven approach for designing semi-supervised learning algorithms that can effectively learn with only a small number of labeled examples. We focus on graph-based techniques, where the unlabeled examples are connected in a graph under the implicit assumption that similar nodes likely have similar labels. Over the past two decades, several elegant graph-based semi-supervised learning algorithms for inferring the labels of the unlabeled examples given the graph and a few labeled examples have been proposed. However, the problem of how to create the graph (which impacts the practical usefulness of these methods significantly) has been relegated to heuristics and domain-specific art, and no general principles have been proposed. In this work we present a novel data driven approach for learning the graph and provide strong formal guarantees in both the distributional and online learning formalizations. We show how to leverage problem instances coming from an underlying problem domain to learn the graph hyperparameters for commonly used parametric families of graphs that provably perform well on new instances from the same domain. We obtain low regret and efficient algorithms in the online setting, and generalization guarantees in the distributional setting. We also show how to combine several very different similarity metrics and learn multiple hyperparameters, our results hold for large classes of problems. We expect some of the tools and techniques we develop along the way to be of independent interest, for data driven algorithms more generally.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1133},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.engappai.2021.104504,
author = {Vaish, Rachna and Dwivedi, U.D. and Tewari, Saurabh and Tripathi, S.M.},
title = {Machine learning applications in power system fault diagnosis: Research advancements and perspectives},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104504},
doi = {10.1016/j.engappai.2021.104504},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {33},
keywords = {Machine learning (ML), Reinforcement learning, Supervised learning, Transfer learning, Unsupervised learning}
}

@inproceedings{10.5555/3540261.3541402,
author = {Yang, Longqi and Zhang, Liangliang and Yang, Wenjing},
title = {Graph adversarial self-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies a long-standing problem of learning the representations of a whole graph without human supervision. The recent self-supervised learning methods train models to be invariant to the transformations (views) of the inputs. However, designing these views requires the experience of human experts. Inspired by adversarial training, we propose an adversarial self-supervised learning (GASSL) framework for learning unsupervised representations of graph data without any handcrafted views. GASSL automatically generates challenging views by adding perturbations to the input and are adversarially trained with respect to the encoder. Our method optimizes the min-max problem and utilizes a gradient accumulation strategy to accelerate the training process. Experimental on ten graph classification datasets show that the proposed approach is superior to state-of-the-art self-supervised learning baselines, which are competitive with supervised models.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1141},
numpages = {13},
series = {NIPS '21}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.5555/3524938.3525621,
author = {Nock, Richard and Menon, Aditya Krishna},
title = {Supervised learning: no loss no cry},
year = {2020},
publisher = {JMLR.org},
abstract = {Supervised learning requires the specification of a loss function to minimise. While the theory of admissible losses from both a computational and statistical perspective is well-developed, these offer a panoply of different choices. In practice, this choice is typically made in an ad hoc manner. In hopes of making this procedure more principled, the problem of learning the loss function for a downstream task (e.g., classification) has garnered recent interest. However, works in this area have been generally empirical in nature.In this paper, we revisit the SLISOTRON algorithm of Kakade et al. (2011) through a novel lens, derive a generalisation based on Bregman divergences, and show how it provides a principled procedure for learning the loss. In detail, we cast SLISOTRON as learning a loss from a family of composite square losses. By interpreting this through the lens of proper losses, we derive a generalisation of SLISOTRON based on Bregman divergences. The resulting BREGMANTRON algorithm jointly learns the loss along with the classifier. It comes equipped with a simple guarantee of convergence for the loss it learns, and its set of possible outputs comes with a guarantee of agnostic approximability of Bayes rule. Experiments indicate that the BREGMANTRON outperforms the SLISOTRON, and that the loss it learns can be minimized by other algorithms for different tasks, thereby opening the interesting problem of loss transfer between domains.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {683},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.neunet.2021.08.003,
author = {Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
title = {Hebbian semi-supervised learning in a sample efficiency setting},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {143},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.08.003},
doi = {10.1016/j.neunet.2021.08.003},
journal = {Neural Netw.},
month = nov,
pages = {719–731},
numpages = {13},
keywords = {Convolutional Neural Networks, Computer vision, Semi-supervised learning, Hebbian learning, Sample efficiency}
}

@inproceedings{10.5555/3327345.3327407,
author = {Garg, Vikas K. and Kalai, Adam},
title = {Supervising unsupervised learning},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm, remove the outliers, and provably circumvent Kleinberg's impossibility result. Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4996–5006},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1613/jair.1.12839,
author = {Jurewicz, Mateusz and Derczynski, Leon},
title = {Set-to-Sequence Methods in Machine Learning: A Review},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12839},
doi = {10.1613/jair.1.12839},
abstract = {Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modelling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {885–924},
numpages = {40},
keywords = {neural networks, machine learning}
}

@inproceedings{10.5555/3540261.3542199,
author = {Araslanov, Nikita and Schaub-Meyer, Simone and Roth, Stefan},
title = {Dense unsupervised learning for video segmentation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1938},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/3540261.3542376,
author = {Wang, Yu and Lin, Jingyang and Zou, Jingjing and Pan, Yingwei and Yao, Ting and Mei, Tao},
title = {Improving self-supervised learning with automated unsupervised outlier arbitration},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Our work reveals a structured shortcoming of the existing mainstream selfsupervised learning methods. Whereas self-supervised learning frameworks usually take the prevailing perfect instance level invariance hypothesis for granted, we carefully investigate the pitfalls behind. Particularly, we argue that the existing augmentation pipeline for generating multiple positive views naturally introduces out-of-distribution (OOD) samples that undermine the learning of the downstream tasks. Generating diverse positive augmentations on the input does not always pay off in benefiting downstream tasks. To overcome this inherent deficiency, we introduce a lightweight latent variable model UOTA, targeting the view sampling issue for self-supervised learning. UOTA adaptively searches for the most important sampling region to produce views, and provides viable choice for outlier-robust self-supervised learning approaches. Our method directly generalizes to many mainstream self-supervised learning approaches, regardless of the loss's nature contrastive or not. We empirically show UOTA's advantage over the state-of-the-art self-supervised paradigms with evident margin, which well justifies the existence of the OOD sample issue embedded in the existing approaches. Especially, we theoretically prove that the merits of the proposal boil down to guaranteed estimator variance and bias reduction.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2115},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.5555/3540261.3542417,
author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
title = {Fair sequential selection using supervised learning models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classifica-tion problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfes the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions. We introduce a new fairness notion, "Equal Selection (ES)," suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the perfect ES fairness can still be attained under certain conditions.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2156},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-36718-3_53,
author = {Rastin, Parisa and Cabanes, Gu\'{e}na\"{e}l and Verde, Rosanna and Bennani, Youn\`{e}s and Couronne, Thierry},
title = {Generative Histogram-Based Model Using Unsupervised Learning},
year = {2019},
isbn = {978-3-030-36717-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-36718-3_53},
doi = {10.1007/978-3-030-36718-3_53},
abstract = {This paper presents a new generative unsupervised learning algorithm based on a representation of the clusters distribution by histograms. The main idea is to reduce the model complexity through cluster-defined projections of the data on independent axes. The results show that the proposed approach performs efficiently compared with other algorithms. In addition, it is more efficient to generate new instances with the same distribution than the training data.},
booktitle = {Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III},
pages = {634–646},
numpages = {13},
keywords = {Unsupervised learning, Clustering, Generative model, Histogram distribution},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1007/978-3-030-58799-4_57,
author = {Nakao, Eduardo K. and Levada, Alexandre L. M.},
title = {Unsupervised Learning and Feature Extraction in Hyperspectral Imagery},
year = {2020},
isbn = {978-3-030-58798-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58799-4_57},
doi = {10.1007/978-3-030-58799-4_57},
abstract = {Remotely sensed hyperspectral scenes are typically defined by large area coverage and hundreds of spectral bands. Those characteristics imply smooth transitions in the spectral-spatio domains. As consequence, subtle differences in the scene are evidenced, benefiting precision applications, but values in neighboring locations and wavelengths are highly correlated. Nondiagonal covariance matrices and wide autocorrelation functions can be observed this way, implying increased intraclass and decreased interclass variation, in both spectral and spatial domains. This leads to lower interpretation accuracies and makes it reasonable to investigate if hyperspectral imagery suffer from Curse of Dimensionality. Moreover, as this Curse can compromise linear method’s Euclidean behavior assumption, it is relevant to compare linear and nonlinear dimensionality reduction performance. So, in this work we verify these two aspects empirically using multiple nonparametric statistical comparisons of Gaussian Mixture Model clustering performances in the cases of: absence, linear and nonlinear unsupervised feature extraction. Experimental results indicate Curse of Dimensionality presence and nonlinear adequacy.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part I},
pages = {792–806},
numpages = {15},
keywords = {Hyperspectral imagery, Curse of dimensionality, Unsupervised feature extraction, Nonlinear dimensionality reduction, Unsupervised learning},
location = {Cagliari, Italy}
}

@article{10.1007/s00521-020-05343-2,
author = {Erkan, U\u{g}ur},
title = {A precise and stable machine learning algorithm: eigenvalue classification (EigenClass)},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05343-2},
doi = {10.1007/s00521-020-05343-2},
abstract = {In this study, a precise and efficient eigenvalue-based machine learning algorithm, particularly denoted as Eigenvalue Classification (EigenClass) algorithm, has been presented to deal with classification problems. The EigenClass algorithm is constructed by exploiting an eigenvalue-based proximity evaluation. To appreciate the classification performance of EigenClass, it is compared with the well-known algorithms, such as k-nearest neighbours, fuzzy k-nearest neighbours, random forest (RF) and multi-support vector machines. Number of 20 different datasets with various attributes and classes are used for the comparison. Every algorithm is trained and tested for 30 runs through 5-fold cross-validation. The results are then compared among each other in terms of the most used measures, such as accuracy, precision, recall, micro-F-measure, and macro-F-measure. It is demonstrated that EigenClass exhibits the best classification performance for 15 datasets in terms of every metric and, in a pairwise comparison, outperforms the other algorithms for at least 16 datasets in consideration of each metric. Moreover, the algorithms are also compared through statistical analysis and computational complexity. Therefore, the achieved results show that EigenClass is a precise and stable algorithm as well as the most successful algorithm considering the overall classification performances.},
journal = {Neural Comput. Appl.},
month = may,
pages = {5381–5392},
numpages = {12},
keywords = {Data classification, Eigenvalues, Learning algorithm, Machine learning, Supervised learning}
}

@article{10.1007/s00500-018-3655-2,
author = {Aldana-Bobadila, Edwin and Kuri-Morales, Angel and Lopez-Arevalo, Ivan and Rios-Alvarado, Ana B.},
title = {An unsupervised learning approach for multilayer perceptron networks},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3655-2},
doi = {10.1007/s00500-018-3655-2},
abstract = {Multilayer perceptron networks have been designed to solve supervised learning problems in which there is a set of known labeled training feature vectors. The resulting model allows us to infer adequate labels for unknown input vectors. Traditionally, the optimal model is the one that minimizes the error between the known labels and those inferred labels via such a model. The training process results in those weights that achieve the most adequate labels. Training implies a search process which is usually determined by the descent gradient of the error. In this work, we propose to replace the known labels by a set of such labels induced by a validity index. The validity index represents a measure of the adequateness of the model relative only to intrinsic structures and relationships of the set of feature vectors and not to previously known labels. Since, in general, there is no guarantee of the differentiability of such an index, we resort to heuristic optimization techniques. Our proposal results in an unsupervised learning approach for multilayer perceptron networks that allows us to infer the best model relative to labels derived from such a validity index which uncovers the hidden relationships of an unlabeled dataset.},
journal = {Soft Comput.},
month = nov,
pages = {11001–11013},
numpages = {13},
keywords = {Neural networks, Clustering, Unsupervised learning}
}

@inproceedings{10.5555/3540261.3542103,
author = {Kaku, Aakash and Upadhya, Sahana and Razavian, Narges},
title = {Intermediate layers matter in momentum contrastive self supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that bringing intermediate layers' representations of two augmented versions of an image closer together in self supervised learning helps to improve the momentum contrastive (MoCo) method. To this end, in addition to the con-trastive loss, we minimize the mean squared error between the intermediate layer representations or make their cross-correlation matrix closer to an identity matrix. Both loss objectives either outperform standard MoCo, or achieve similar performances on three diverse medical imaging datasets: NIH-Chest Xrays, Breast Cancer Histopathology, and Diabetic Retinopathy. The gains of the improved MoCo are especially large in a low-labeled data regime (e.g. 1% labeled data) with an average gain of 5% across three datasets. We analyze the models trained using our novel approach via feature similarity analysis and layer-wise probing. Our analysis reveals that models trained via our approach have higher feature reuse compared to a standard MoCo and learn informative features earlier in the network. Finally, by comparing the output probability distribution of models fine tuned on small versus large labeled data, we conclude that our proposed method of pre-training leads to lower Kolmogorov–Smirnov distance, as compared to a standard MoCo. This provides additional evidence that our proposed method learns more informative features in the pre-training phase which could be leveraged in a low-labeled data regime.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1842},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-031-17587-9_3,
author = {Monorchio, Luca and Capotondi, Marco and Corsanici, Mario and Villa, Wilson and De Luca, Alessandro and Puja, Francesco},
title = {Transfer and&nbsp;Continual Supervised Learning for&nbsp;Robotic Grasping Through Grasping Features},
year = {2021},
isbn = {978-3-031-17586-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17587-9_3},
doi = {10.1007/978-3-031-17587-9_3},
abstract = {We present a Transfer and Continual Learning method for robotic grasping tasks, based on small vision-depth (RGBD) datasets and realized through the use of Grasping Features. Given a network architecture composed by a CNN (Convolutional Neural Network) followed by a FCC (Fully Connected Cascade Neural Network), we exploit high-level features specific of the grasping tasks, as extracted by the convolutional network from RGBD images. These features are more descriptive of a grasping task than just visual ones, and thus more efficient for transfer learning purposes. Being datasets for visual grasping less common than those for image recognition, we also propose an efficient way to generate these data using only simple geometric structures. This reduces the computational burden of the FCC and allows to obtain a better performance with the same amount of data. Simulation results using the collaborative UR-10 robot and a jaw gripper are reported to show the quality of the proposed method.},
booktitle = {Continual Semi-Supervised Learning: First International Workshop, CSSL 2021, Virtual Event, August 19–20, 2021, Revised Selected Papers},
pages = {33–47},
numpages = {15},
keywords = {Transfer Learning, Continual Learning, Robotic grasping}
}

@article{10.1007/s00521-021-05968-x,
author = {Esqu\'{\i}vel, Manuel L. and Krasii, Nadezhda P.},
title = {A wavelet-based neural network scheme for supervised and unsupervised learning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05968-x},
doi = {10.1007/s00521-021-05968-x},
abstract = {We introduce a scheme for supervised and unsupervised learning based on successive decomposition of random inputs by means of wavelet basis. To each successive layer corresponds a different wavelet basis with more null moments than its predecessor. We consider two types of operations in the scheme: firstly, a input signal treatment phase—the awake phase—and, secondly, a reorganizing phase of the random wavelet coefficients obtained in the previous awake phase—the asleep phase. The next awake phase input treatment will include a feedback derived from the previous asleep phase. The set of random wavelet coefficients of the deepest layer—at each stage of the learning process—is supposed to be Gaussian distributed, and the corresponding sequence of Gaussian distributions constitutes the inner representation of the world in the scheme. We show that in the case of a constant average value of the inputs in each successive awake phases, the mean value of the feedback converges to a multiple of the constant expected value of the inputs. We show a general result on the stabilization of the Gaussian distribution corresponding to the deepest layer, and we show that when the estimated means and covariances converge, then the sequence of Gaussian distributions of the inner representation of the world in the scheme also converges. We present an example of a neural network scheme for supervised learning corresponding to extracting a signal from data corrupted with Gaussian noise.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {13433–13448},
numpages = {16},
keywords = {Neural networks, Supervised and unsupervised learning, Wavelet theory}
}

@inproceedings{10.1007/978-3-030-59861-7_38,
author = {Zhao, Fenqiang and Wu, Zhengwang and Wang, Li and Lin, Weili and Xia, Shunren and Shen, Dinggang and Li, Gang},
title = {Unsupervised Learning for Spherical Surface Registration},
year = {2020},
isbn = {978-3-030-59860-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59861-7_38},
doi = {10.1007/978-3-030-59861-7_38},
abstract = {Current spherical surface registration methods achieve good performance on alignment and spatial normalization of cortical surfaces across individuals in neuroimaging analysis. However, they are computationally intensive, since they have to optimize an objective function independently for each pair of surfaces. In this paper, we present a fast learning-based algorithm that makes use of the recent development in spherical Convolutional Neural Networks (CNNs) for spherical cortical surface registration. Given a set of surface pairs without supervised information such as ground truth deformation fields or anatomical landmarks, we formulate the registration as a parametric function and learn its parameters by enforcing the feature similarity between one surface and the other one warped by the estimated deformation field using the function. Then, given a new pair of surfaces, we can quickly infer the spherical deformation field registering one surface to the other one. We model this parametric function using three orthogonal Spherical U-Nets and use spherical transform layers to warp the spherical surfaces, while imposing smoothness constraints on the deformation field. All the layers in the network are well-defined and differentiable, thus the parameters can be effectively learned. We show that our method achieves accurate cortical alignment results on 102 subjects, comparable to two state-of-the-art methods: Spherical Demons and MSM, while runs much faster.},
booktitle = {Machine Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {373–383},
numpages = {11},
keywords = {Spherical U-Net, Cortical surface registration},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-61616-8_67,
author = {Axenie, Cristian and Kurz, Daria},
title = {Tumor Characterization Using Unsupervised Learning of Mathematical Relations Within Breast Cancer Data},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_67},
doi = {10.1007/978-3-030-61616-8_67},
abstract = {Despite the variety of imaging, genetic and histopathological data used to assess tumors, there is still an unmet need for patient-specific tumor growth profile extraction and tumor volume prediction, for use in surgery planning. Models of tumor growth predict tumor size and require tumor biology-dependent parametrization, which hardly generalizes to cope with tumor variability among patients. In addition, the datasets are limited in size, owing to the restricted or single-time measurements. In this work, we address the shortcomings that incomplete biological specifications, the inter-patient variability of tumors, and the limited size of the data bring to mechanistic tumor growth models. We introduce a machine learning model that alleviates these shortcomings and is capable of characterizing a tumor’s growth pattern, phenotypical transitions, and volume. The model learns without supervision, from different types of breast cancer data the underlying mathematical relations describing tumor growth curves more accurate than three state-of-the-art models. Experiments performed on publicly available clinical breast cancer datasets, demonstrate the versatility of the approach among breast cancer types. Moreover, the model can also, without modification, learn the mathematical relations among, for instance, histopathological and morphological parameters of the tumor and, combined with the growth curve, capture the (phenotypical) growth transitions of the tumor from a small amount of data. Finally, given the tumor growth curve and its transitions, our model can learn the relation among tumor proliferation-to-apoptosis ratio, tumor radius, and tumor nutrient diffusion length, used to estimate tumor volume. Such a quantity can be readily incorporated within current clinical practice, for surgery planning.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {838–849},
numpages = {12},
keywords = {Artificial neural networks, Breast cancer, Unsupervised learning, Prediction algorithms},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.eswa.2021.115662,
author = {Wiwatcharakoses, Chayut and Berrar, Daniel},
title = {A self-organizing incremental neural network for continual supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115662},
doi = {10.1016/j.eswa.2021.115662},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {9},
keywords = {Catastrophic forgetting, Concept drift, Continual learning, Incremental learning, Supervised learning}
}

@inproceedings{10.5555/3327345.3327355,
author = {Sun, Haitian and Bing, Lidong and Cohen, William W.},
title = {Semi-supervised learning with declaratively specified entropy constraints},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4430–4440},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1007/978-3-030-93944-1_8,
author = {Goldsteen, Abigail and Ezov, Gilad and Shmelkin, Ron and Moffie, Micha and Farkash, Ariel},
title = {Anonymizing Machine Learning Models},
year = {2021},
isbn = {978-3-030-93943-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93944-1_8},
doi = {10.1007/978-3-030-93944-1_8},
abstract = {There is a known tension between the need to analyze personal data to drive business and the need to preserve the privacy of data subjects. Many data protection regulations, including the EU General Data Protection Regulation (GDPR) and the California Consumer Protection Act (CCPA), set out strict restrictions and obligations on the collection and processing of personal data. Moreover, machine learning models themselves can be used to derive personal information, as demonstrated by recent membership and attribute inference attacks. Anonymized data, however, is exempt from the obligations set out in these regulations. It is therefore desirable to be able to create models that are anonymized, thus also exempting them from those obligations, in addition to providing better protection against attacks.Learning on anonymized data typically results in significant degradation in accuracy. In this work, we propose a method that is able to achieve better model accuracy by using the knowledge encoded within the trained model, and guiding our anonymization process to minimize the impact on the model’s accuracy, a process we call accuracy-guided anonymization. We demonstrate that by focusing on the model’s accuracy rather than generic information loss measures, our method outperforms state of the art k-anonymity methods in terms of the achieved utility, in particular with high values of k and large numbers of quasi-identifiers.We also demonstrate that our approach has a similar, and sometimes even better ability to prevent membership inference attacks as approaches based on differential privacy, while averting some of their drawbacks such as complexity, performance overhead and model-specific implementations. In addition, since our approach does not rely on making modifications to the training algorithm, it can even work with “black-box” models where the data owner does not have full control over the training process, or within complex machine learning pipelines where it may be difficult to replace existing learning algorithms with new ones. This makes model-guided anonymization a legitimate substitute for such methods and a practical approach to creating privacy-preserving models.},
booktitle = {Data Privacy Management, Cryptocurrencies and Blockchain Technology: ESORICS 2021 International Workshops, DPM 2021 and CBT 2021, Darmstadt, Germany, October 8, 2021, Revised Selected Papers},
pages = {121–136},
numpages = {16},
keywords = {GDPR, Anonymization, k-anonymity, Compliance, Privacy, Machine learning},
location = {Darmstadt, Germany}
}

@inproceedings{10.1007/978-3-031-08147-7_13,
author = {Vitorino, Jo\~{a}o and Andrade, Rui and Pra\c{c}a, Isabel and Sousa, Orlando and Maia, Eva},
title = {A Comparative Analysis of Machine Learning Techniques for IoT Intrusion Detection},
year = {2021},
isbn = {978-3-031-08146-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08147-7_13},
doi = {10.1007/978-3-031-08147-7_13},
abstract = {The digital transformation faces tremendous security challenges. In particular, the growing number of cyber-attacks targeting Internet of Things (IoT) systems restates the need for a reliable detection of malicious network activity. This paper presents a comparative analysis of supervised, unsupervised and reinforcement learning techniques on nine malware captures of the IoT-23 dataset, considering both binary and multi-class classification scenarios. The developed models consisted of Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), Isolation Forest (iForest), Local Outlier Factor (LOF) and a Deep Reinforcement Learning (DRL) model based on a Double Deep Q-Network (DDQIN), adapted to the intrusion detection context. The most reliable performance was achieved by LightGBM. Nonetheless, iForest displayed good anomaly detection results and the DRL model demonstrated the possible benefits of employing this methodology to continuously improve the detection. Overall, the obtained results indicate that the analyzed techniques are well suited for IoT intrusion detection.},
booktitle = {Foundations and Practice of Security: 14th International Symposium, FPS 2021, Paris, France, December 7–10, 2021, Revised Selected Papers},
pages = {191–207},
numpages = {17},
keywords = {Internet of Things, Intrusion detection, Supervised learning, Unsupervised learning, Reinforcement learning},
location = {Paris, France}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/2362536.2362580,
author = {Hamza, Haitham S. and Martinez, Jabier and Thurimella, Anil Kumar and Deogun, Jitender S.},
title = {Third International Workshop on Knowledge-Oriented Product Line Engineering (KOPLE 2012)},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362580},
doi = {10.1145/2362536.2362580},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing Product Lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, Knowledge plays a paramount role in the success of the various activities of PLE. The objective of the KOPLE workshop series is to bring together SPL researchers and practitioners from academia and industry to investigate the role of Knowledge in PLE. Knowledge is usually encapsulated in PL architectures in a tacit or implicit way, and this may appear to be sufficient for industry to implement successful product lines. Nevertheless, KOPLE also aims to become a discussion forum about techniques and methods to convert from tacit to explicit Knowledge in PLE and to process and use this Knowledge for optimizing and innovating PLE processes.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {292–293},
numpages = {2},
keywords = {conceptual graphs, knowledge engineering, ontology, product lines, software reuse, tacit knowledge},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1561/2200000081,
author = {Holden, Sean B.},
title = {Machine Learning for Automated Theorem Proving: Learning to Solve SAT and QSAT},
year = {2021},
issue_date = {Nov 2021},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {14},
number = {6},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000081},
doi = {10.1561/2200000081},
abstract = {The decision problem for Boolean satisfiability, generally
    referred to as SAT, is the archetypal NP-complete problem,
    and encodings of many problems of practical interest exist
    allowing them to be treated as SAT problems. Its generalization
    to quantified SAT (QSAT) is PSPACE-complete, and
    is useful for the same reason. Despite the computational
    complexity of SAT and QSAT, methods have been developed
    allowing large instances to be solved within reasonable
    resource constraints. These techniques have largely exploited
    algorithmic developments; however machine learning also
    exerts a significant influence in the development of state-ofthe-
    art solvers. Here, the application of machine learning
    is delicate, as in many cases, even if a relevant learning
    problem can be solved, it may be that incorporating the
    result into a SAT or QSAT solver is counterproductive, because
    the run-time of such solvers can be sensitive to small
    implementation changes. The application of better machine
    learning methods in this area is thus an ongoing challenge,
    with characteristics unique to the field. This work provides
    a comprehensive review of the research to date on incorporating
    machine learning into SAT and QSAT solvers, as a
    resource for those interested in further advancing the field.},
journal = {Found. Trends Mach. Learn.},
month = nov,
pages = {807–989},
numpages = {187}
}

@article{10.1145/3360601,
author = {Cambronero, Jos\'{e} P. and Rinard, Martin C.},
title = {AL: autogenerating supervised learning programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360601},
doi = {10.1145/3360601},
abstract = {We present AL, a novel automated machine learning system that learns to generate new supervised learning pipelines from an existing corpus of supervised learning programs. In contrast to existing automated machine learning tools, which typically implement a search over manually selected machine learning functions and classes, AL learns to identify the relevant classes in an API by analyzing dynamic program traces that use the target machine learning library. AL constructs a conditional probability model from these traces to estimate the likelihood of the generated supervised learning pipelines and uses this model to guide the search to generate pipelines for new datasets. Our evaluation shows that AL can produce successful pipelines for datasets that previous systems fail to process and produces pipelines with comparable predictive performance for datasets that previous systems process successfully.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {175},
numpages = {28},
keywords = {automated machine learning, program analysis for machine learning}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@inproceedings{10.1145/3450267.3452001,
author = {Vardhan, Harsh and Volgyesi, Peter and Sztipanovits, Janos},
title = {Machine learning assisted propeller design},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450267.3452001},
doi = {10.1145/3450267.3452001},
abstract = {Propellers are one of the most widely used propulsive devices for generating thrust from rotational engine motion both in marine vehicles and subsonic air-crafts. Due to their simplicity, robustness and high efficiency, propellers remained the mainstream design choice over the last hundred years. On the other hand, finding the optimal application-specific geometry is still challenging. This work in progress report describes application of modern and rapidly developing Machine Learning (ML) techniques to gain novel designs. We rely on a rich set of preexisting parametric design patterns and accumulated engineering knowledge supplemented by high-fidelity simulation models to formulate the design process as a supervised learning problem.The aim of our work is to develop and evaluate machine learning models for the parametric design of propellers based on application-specific constraints. While the application of ML techniques in optimal propeller design is at a very nascent level, we believe that our early results are promising with a potentially significant impact on the overall design process. The ML-assisted design flow allows for a more automated design space exploration process with less dependency on human intuition and engineering guidance.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {227–228},
numpages = {2},
keywords = {OpenProp, design-space exploration, evolutionary algorithm, machine learning, propeller, random forest regression},
location = {Nashville, Tennessee},
series = {ICCPS '21}
}

@inproceedings{10.1145/3395035.3425191,
author = {Wu, Yujin and Daoudi, Mohamed and Amad, Ali and Sparrow, Laurent and D'Hondt, Fabien},
title = {Unsupervised Learning Method for Exploring Students' Mental Stress in Medical Simulation Training},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425191},
doi = {10.1145/3395035.3425191},
abstract = {So far, stress detection technology usually uses supervised learning methods combined with a series of physiological, physical, or behavioral signals and has achieved promising results. However, the problem of label collection such as the latency of stress response and subjective uncertainty introduced by the questionnaires has not been effectively solved. This paper proposes an unsupervised learning method with K-means clustering for exploring students' autonomic responses to medical simulation training in an ambulant environment. With the use of wearable sensors, features of electrodermal activity and heart rate variability of subjects are extracted to train the K-means model. The Silhouette Score of 0.49 with two clusters was reached, proving the difference in students' mental stress between baseline stage and simulation stage. Besides, with the aid of external ground truth which could be associated with either the baseline phase or simulation phase, four evaluation metrics were calculated and provided comparable results concerning supervised and unsupervised learning methods. The highest classification performance of 70% was reached with the measure of precision. In the future, we will integrate context information or facial image to provide more accurate stress detection.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {165–170},
numpages = {6},
keywords = {EDA, HRV, k-means, mental stress, physiological signal, silhouette score, unsupervised learning},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1016/j.neunet.2021.07.023,
author = {Tonin, Francesco and Patrinos, Panagiotis and Suykens, Johan A.K.},
title = {Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.07.023},
doi = {10.1016/j.neunet.2021.07.023},
journal = {Neural Netw.},
month = oct,
pages = {661–679},
numpages = {19},
keywords = {Kernel methods, Unsupervised learning, Manifold learning, Learning disentangled representations}
}

@inproceedings{10.1007/978-3-030-87897-9_16,
author = {Kalina, Jan and Matonoha, Ctirad},
title = {Robustness of Supervised Learning Based on Combined Centroids},
year = {2021},
isbn = {978-3-030-87896-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87897-9_16},
doi = {10.1007/978-3-030-87897-9_16},
abstract = {Recently, we proposed a novel sparse centroid-based supervised learning method, allowing to optimize a single centroid and its corresponding weights. The method is especially useful for localizing objects in images. Here, we extend the method to the task of joint localization of several objects in a&nbsp;2D-image by means of combining several centroids. The novel approach, i.e. joint optimization of several centroids and a&nbsp;subsequent optimization of their weights, is illustrated on the task of localizing the mouth and both eyes in facial images. Because we are particularly interested in studying the robustness of the method to various modifications of the images, we evaluate the performance of the methods also over images artificially modified by additional noise, occlusion, changed illumination, or rotation. The novel centroid-based method is successful in the localization task, and the optimization turns out to ensure robustness with respect to the presence of noise or occlusion in the images. Moreover, combining the optimized centroids yields more robust results than a method using simple centroids with a highly robust correlation coefficient (with a high breakdown point).},
booktitle = {Artificial Intelligence and Soft Computing: 20th International Conference, ICAISC 2021, Virtual Event, June 21–23, 2021, Proceedings, Part II},
pages = {171–182},
numpages = {12},
keywords = {Machine learning, Sparsity, Regularization, Robust optimization, Outliers}
}

@inproceedings{10.1145/3453800.3453803,
author = {Thanh Trieu, Ngoan and Pottier, Bernard and Rodin, Vincent and Xuan Huynh, Hiep},
title = {Interpretable Machine Learning for Meteorological Data},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453803},
doi = {10.1145/3453800.3453803},
abstract = {Weather forecasting is the task to predict the state of the atmosphere in a given location. In the past, the weather forecast has been done through physical models of the atmosphere as a fluid. It becomes the problem of solving sophisticated equations of fluid dynamics. In recent years, machine learning algorithms have been used to speed up weather data modeling, a computationally intensive task. Machine learning algorithms learn from data and produce relevant predictions. In addition to prediction, there is a need of providing knowledge about domain relationships inside the data. This paper provides a new approach using interpretable machine learning for explaining the characteristic variables of meteorological data. Interpretable machine learning is the use of machine learning models for the extraction of knowledge in the data. An illustration is shown on characteristic variables of meteorological data.},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {11–17},
numpages = {7},
keywords = {BUFR/Express, Environment Simulations, Interpretable Machine Learning, Weather Data},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@inproceedings{10.1145/3380446.3430690,
author = {Khailany, Brucek},
title = {Accelerating Chip Design with Machine Learning},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430690},
doi = {10.1145/3380446.3430690},
abstract = {As Moore's law has provided an exponential increase in chip transistor density, the unique features we can now include in large chips are no longer predominantly limited by area constraints. Instead, new capabilities are increasingly limited by the engineering effort associated with digital design, verification, and implementation. As applications demand more performance and energy efficiency from specialization in the post-Moore's-law era, we expect required complexity and design effort to increase.Historically, these challenges have been met through levels of abstraction and automation. Over the last few decades, Electronic Design Automation (EDA) algorithms and methodologies were developed for all aspects of chip design - design verification and simulation, logic synthesis, place-and-route, and timing and physical signoff analysis. With each increase in automation, total work per chip has increased, but more work has also been offloaded from manual effort to software. Just as machine learning (ML) has transformed software in many domains, we expect advancements in ML will also transform EDA software and as a result, chip design workflows.In this talk, we highlight work from our research group and the community applying ML to various chip design prediction tasks [1]. We show how deep convolutional neural networks [2] and graph-based neural networks [3] can be used in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted chip design workflow to automate optimization tasks. In this future vision, GPU acceleration, neural-network predictors, and deep reinforcement learning techniques combine to automate VLSI design and optimization.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {33},
numpages = {1},
keywords = {VLSI, design methodology, machine learning},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1016/j.jbi.2015.05.014,
author = {Costa, Gabriella Castro B. and Braga, Regina and David, Jos\'{e} Maria N. and Campos, Fernanda},
title = {A Scientific Software Product Line for the Bioinformatics domain},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {56},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2015.05.014},
doi = {10.1016/j.jbi.2015.05.014},
abstract = {Display Omitted An architecture to support a SPL for scientific applications.An approach where the semantics is highlighted.Use of ontologies in conjunction with feature models.The implementation of an SSPL.Case studies in the bioinformatics area (sequencing/genetic alignment). ContextMost specialized users (scientists) that use bioinformatics applications do not have suitable training on software development. Software Product Line (SPL) employs the concept of reuse considering that it is defined as a set of systems that are developed from a common set of base artifacts. In some contexts, such as in bioinformatics applications, it is advantageous to develop a collection of related software products, using SPL approach. If software products are similar enough, there is the possibility of predicting their commonalities, differences and then reuse these common features to support the development of new applications in the bioinformatics area. ObjectivesThis paper presents the PL-Science approach which considers the context of SPL and ontology in order to assist scientists to define a scientific experiment, and to specify a workflow that encompasses bioinformatics applications of a given experiment. This paper also focuses on the use of ontologies to enable the use of Software Product Line in biological domains. MethodIn the context of this paper, Scientific Software Product Line (SSPL) differs from the Software Product Line due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and using this abstract workflow model the products (scientific applications/algorithms) are instantiated. ResultsThrough the use of ontology as a knowledge representation model, we can provide domain restrictions as well as add semantic aspects in order to facilitate the selection and organization of bioinformatics workflows in a Scientific Software Product Line. The use of ontologies enables not only the expression of formal restrictions but also the inferences on these restrictions, considering that a scientific domain needs a formal specification. ConclusionsThis paper presents the development of the PL-Science approach, encompassing a methodology and an infrastructure, and also presents an approach evaluation. This evaluation presents case studies in bioinformatics, which were conducted in two renowned research institutions in Brazil.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {239–264},
numpages = {26},
keywords = {Feature model, Ontology, Scientific workflow, Sequence alignment, Software Product Line}
}

@article{10.1016/j.patrec.2020.12.012,
author = {Zeng, Shaofeng and Liu, Zhiyong and Yang, Xu},
title = {Supervised learning for parameterized Koopmans–Beckmann’s graph matching},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2020.12.012},
doi = {10.1016/j.patrec.2020.12.012},
journal = {Pattern Recogn. Lett.},
month = mar,
pages = {8–13},
numpages = {6},
keywords = {Graph matching, Koopmans–Beckmann, Supervised learning, Structured SVM}
}

@article{10.1007/s10994-021-06030-6,
author = {Watson, David S. and Wright, Marvin N.},
title = {Testing conditional independence in supervised learning algorithms},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {8},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06030-6},
doi = {10.1007/s10994-021-06030-6},
abstract = {We propose the conditional predictive impact (CPI), a consistent and unbiased estimator of the association between one or several features and a given outcome, conditional on a reduced feature set. Building on the knockoff framework of Cand\`{e}s et al. (J R Stat Soc Ser B 80:551–577, 2018), we develop a novel testing procedure that works in conjunction with any valid knockoff sampler, supervised learning algorithm, and loss function. The CPI can be efficiently computed for high-dimensional data without any sparsity constraints. We demonstrate convergence criteria for the CPI and develop statistical inference procedures for evaluating its magnitude, significance, and precision. These tests aid in feature and model selection, extending traditional frequentist and Bayesian techniques to general supervised learning tasks. The CPI may also be applied in causal discovery to identify underlying multivariate graph structures. We test our method using various algorithms, including linear regression, neural networks, random forests, and support vector machines. Empirical results show that the CPI compares favorably to alternative variable importance measures and other nonparametric tests of conditional independence on a diverse array of real and synthetic datasets. Simulations confirm that our inference procedures successfully control Type I error with competitive power in a range of settings. Our method has been implemented in an R package, cpi, which can be downloaded from .},
journal = {Mach. Learn.},
month = aug,
pages = {2107–2129},
numpages = {23},
keywords = {Knockoffs, Machine learning, Conditional independence, Markov blanket, Variable importance}
}

@inproceedings{10.1007/978-3-030-86365-4_47,
author = {Liao, Kaimin and Gan, Ziyu and Yang, Xuan},
title = {Semi-supervised Learning Based Right Ventricle Segmentation Using Deep Convolutional Boltzmann Machine Shape&nbsp;Model},
year = {2021},
isbn = {978-3-030-86364-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86365-4_47},
doi = {10.1007/978-3-030-86365-4_47},
abstract = {Automated Right Ventricle (RV) segmentation is a challenge due to the RV’s variable shape and the lack of labelled data. This paper proposes a semi-supervised learning method based on a convolutional deep Boltzmann machine (CDBM). A CDBM is constructed to learn the complex shape of RV using the short-run MCMC. Next, a semi-supervised learning network composing of a CDBM and two CNNs is proposed. The CNNs and the CDBM have trained alternatively; labelled data are used to train the CNNs, and CDBM reconstructs the predicted results of unlabelled data using the CNNs to guide the training of CNNs further. During this procedure, the CDBM is trained at the same time. Our approach’s main idea is to extract the shape information of RV and use the shape information to improve CNN’s performance. Our approach takes advantage of avoiding overfitting and requiring less labelled data. Besides, our approach does not increase any extra computational cost and parameters during inference. The experiment results show that our approach can improve segmentation accuracy when the labelled training data is small.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part III},
pages = {585–597},
numpages = {13},
keywords = {Deep Boltzmann Machine, Right ventricle segmentation, Semi-supervised learning, Convolutional Neural Network},
location = {Bratislava, Slovakia}
}

@article{10.1007/s11227-018-02737-x,
author = {Lee, Hyeonseo and Lee, Nakyeong and Seo, Harim and Song, Min},
title = {Developing a supervised learning-based social media business sentiment index: Developing a supervised learning-based social media…},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {5},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-02737-x},
doi = {10.1007/s11227-018-02737-x},
abstract = {The fast-growing digital data generation leads to the emergence of the era of big data, which become particularly more valuable because approximately 70% of the collected data in the world comes from social media. Thus, the investigation of online social network services is of paramount importance. In this paper, we use the sentiment analysis, which detects attitudes and emotions toward issues of society posted in social media, to understand the actual economic situation. To this end, two steps are suggested. In the first step, after training the sentiment classifiers with several big data sources of social media datasets, we consider three types of feature sets: feature vector, sequence vector and a combination of dictionary-based feature and sequence vectors. Then, the performance of six classifiers is assessed: MaxEnt-L1, C4.5 decision tree, SVM-kernel, Ada-boost, Na\"{\i}ve Bayes and MaxEnt. In the second step, we collect datasets that are relevant to several economic words that the public use to explicitly express their opinions. Finally, we use a vector auto-regression analysis to confirm our hypothesis. The results show the statistically significant relationship between public sentiment and economic performance. That is, “depression” and “unemployment” lead to KOSPI. Also, it shows that the extracted keywords from the sentiment analysis, such as “price,” “year-end-tax” and “budget deficit,” cause the exchange rates.},
journal = {J. Supercomput.},
month = may,
pages = {3882–3897},
numpages = {16},
keywords = {Sentiment analysis, Social media, Machine learning, Supervised learning}
}

@article{10.1016/j.eswa.2021.114820,
author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
title = {Machine Learning for industrial applications: A comprehensive literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114820},
doi = {10.1016/j.eswa.2021.114820},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {29},
keywords = {Literature review, Industrial applications, Deep Learning, Machine Learning, Operation management}
}

@article{10.1007/s00521-021-05951-6,
author = {Badr, Assem},
title = {Awesome back-propagation machine learning paradigm},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05951-6},
doi = {10.1007/s00521-021-05951-6},
abstract = {For a better future in machine learning (ML), it is necessary to modify our current concepts to get the fastest ML. Many designers had attempted to find the optimal learning rates in their applications through many algorithms over the past decades, but they have not yet achieved their target of highest speed of back-propagation (BP). This research proposes a novel BP rule called the Instant Learning Ratios-Machine Learning (ILR-ML) or (ILRML). Unlike the traditional BP algorithms, the ILR-ML offers its learning without the concepts of the learning rate(s). The ILR-ML has a new concept called the "Learning Ratio" and indicated by a sign (Δℓ). The ILR-ML performs the full BP algorithm with 100% accuracy per each learning iteration. The ILR-ML is more suitable for the online machine learning.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {13225–13249},
numpages = {25},
keywords = {Neural network, Machine learning algorithms, Back-propagation}
}

@article{10.1016/j.datak.2021.101909,
author = {Maass, Wolfgang and Storey, Veda C.},
title = {Pairing conceptual modeling with machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101909},
doi = {10.1016/j.datak.2021.101909},
journal = {Data Knowl. Eng.},
month = jul,
numpages = {35},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@article{10.1016/j.ipm.2021.102642,
author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
title = {Machine learning fairness notions: Bridging the gap with real-world applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102642},
doi = {10.1016/j.ipm.2021.102642},
journal = {Inf. Process. Manage.},
month = sep,
numpages = {32},
keywords = {Fairness, Machine learning, Discrimination, Survey, Systemization of Knowledge (SoK)}
}

@article{10.1145/3459664,
author = {Talbi, El-Ghazali},
title = {Machine Learning into Metaheuristics: A Survey and Taxonomy},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459664},
doi = {10.1145/3459664},
abstract = {During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {129},
numpages = {32},
keywords = {ML-supported metaheuristics, Metaheuristics, machine learning, optimization}
}

@inproceedings{10.1145/3474370.3485662,
author = {Chhabra, Anshuman and Mohapatra, Prasant},
title = {Moving Target Defense against Adversarial Machine Learning},
year = {2021},
isbn = {9781450386586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474370.3485662},
doi = {10.1145/3474370.3485662},
abstract = {As Machine Learning (ML) models are increasingly employed in a number of applications across a multitude of fields, the threat of adversarial attacks against ML models is also increasing. Adversarial samples crafted via specialized attack algorithms have been shown to significantly decrease the performance of ML models. Furthermore, it has also been found that adversarial samples generated for a particular model can transfer across other models, and decrease accuracy and other performance metrics for a model they were not originally crafted for. In recent research, many different defense approaches have been proposed for making ML models robust, ranging from adversarial input re-training to defensive distillation, among others. While these approaches operate at the model-level, we propose an alternate approach to defending ML models against adversarial attacks, using Moving Target Defense (MTD). We formulate the problem and provide preliminary results to showcase the validity of the proposed approach.},
booktitle = {Proceedings of the 8th ACM Workshop on Moving Target Defense},
pages = {29–30},
numpages = {2},
keywords = {adversarial attacks, adversarial machine learning, moving target defense, reinforcement learning},
location = {Virtual Event, Republic of Korea},
series = {MTD '21}
}

@inproceedings{10.5555/3540261.3541456,
author = {Du, Yilun and Li, Shuang and Sharma, Yash and Tenenbaum, Joshua B. and Mordatch, Igor},
title = {Unsupervised learning of compositional energy concepts},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Humans are able to rapidly understand scenes by utilizing concepts extracted from prior experience. Such concepts are diverse, and include global scene descriptors, such as the weather or lighting, as well as local scene descriptors, such as the color or size of a particular object. So far, unsupervised discovery of concepts has focused on either modeling the global scene-level or the local object-level factors of variation, but not both. In this work, we propose COMET, which discovers and represents concepts as separate energy functions, enabling us to represent both global concepts as well as objects under a unified framework. COMET discovers energy functions through recomposing the input image, which we find captures independent factors without additional supervision. Sample generation in COMET is formulated as an optimization process on underlying energy functions, enabling us to generate images with permuted and composed concepts. Finally, discovered visual concepts in COMET generalize well, enabling us to compose concepts between separate modalities of images as well as with other concepts discovered by a separate instance of COMET trained on a different dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1195},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1109/MILCOM52596.2021.9652947,
author = {Abdou, Ahmed and Sheatsley, Ryan and Beugin, Yohan and Shipp, Tyler and McDaniel, Patrick},
title = {HoneyModels: Machine Learning Honeypots},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM52596.2021.9652947},
doi = {10.1109/MILCOM52596.2021.9652947},
abstract = {Machine Learning is becoming a pivotal aspect of many systems today, offering newfound performance on classification and prediction tasks, but this rapid integration also comes with new unforeseen vulnerabilities. To harden these systems the ever-growing field of Adversarial Machine Learning has proposed new attack and defense mechanisms. However, a great asymmetry exists as these defensive methods can only provide security to certain models and lack scalability, computational efficiency, and practicality due to overly restrictive constraints. Moreover, newly introduced attacks can easily bypass defensive strategies by making subtle alterations. In this paper, we study an alternate approach inspired by honeypots to detect adversaries. Our approach yields learned models with an embedded watermark. When an adversary initiates an interaction with our model, attacks are encouraged to add this predetermined watermark stimulating detection of adversarial examples. We show that HoneyModels can reveal 69.5% of adversaries attempting to attack a Neural Network while preserving the original functionality of the model. HoneyModels offer an alternate direction to secure Machine Learning that slightly affects the accuracy while encouraging the creation of watermarked adversarial samples detectable by the HoneyModel but indistinguishable from others for the adversary.},
booktitle = {MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM)},
pages = {886–891},
numpages = {6},
location = {San Diego, CA, USA}
}

@article{10.1016/j.asoc.2021.107280,
author = {Cheng, Chen-Yang and Pourhejazy, Pourya and Ying, Kuo-Ching and Lin, Chen-Fang},
title = {Unsupervised Learning-based Artificial Bee Colony for minimizing non-value-adding operations},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107280},
doi = {10.1016/j.asoc.2021.107280},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {10},
keywords = {Lean manufacturing, Scheduling, Unsupervised learning, Unrelated parallel machines, Metaheuristics}
}

@article{10.1007/s10614-018-9803-z,
author = {Kao, Ying-Fang and Venkatachalam, Ragupathy},
title = {Human and Machine Learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {3},
issn = {0927-7099},
url = {https://doi.org/10.1007/s10614-018-9803-z},
doi = {10.1007/s10614-018-9803-z},
abstract = {In this paper, we consider learning by human beings and machines in the light of Herbert Simon’s pioneering contributions to the theory of Human Problem Solving. Using board games of perfect information as a paradigm, we explore differences in human and machine learning in complex strategic environments. In doing so, we contrast theories of learning in classical game theory with computational game theory proposed by Simon. Among theories that invoke computation, we make a further distinction between computable and computational or machine learning theories. We argue that the modern machine learning algorithms, although impressive in terms of their performance, do not necessarily shed enough light on human learning. Instead, they seem to take us further away from Simon’s lifelong quest to understand the mechanics of actual human behaviour.},
journal = {Comput. Econ.},
month = mar,
pages = {889–909},
numpages = {21},
keywords = {Machine learning, Human problem solving, Herbert Simon, Learning, Artificial intelligence, Go}
}

@article{10.1007/s10489-020-02048-w,
author = {Nerurkar, Pranav and Bhirud, Sunil and Patel, Dhiren and Ludinard, Romaric and Busnel, Yann and Kumari, Saru},
title = {Supervised learning model for identifying illegal activities in Bitcoin},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02048-w},
doi = {10.1007/s10489-020-02048-w},
abstract = {Since its inception in 2009, Bitcoin is mired in controversies for providing a haven for illegal activities. Several types of illicit users hide behind the blanket of anonymity. Uncovering these entities is key for forensic investigations. Current methods utilize machine learning for identifying these illicit entities. However, the existing approaches only focus on a limited category of illicit users. The current paper proposes to address the issue by implementing an ensemble of decision trees for supervised learning. More parameters allow the ensemble model to learn discriminating features that can categorize multiple groups of illicit users from licit users. To evaluate the model, a dataset of 1216 real-life entities on Bitcoin was extracted from the Blockchain. Nine Features were engineered to train the model for segregating 16 different licit-illicit categories of users. The proposed model provided a reliable tool for forensic study. Empirical evaluation of the proposed model vis-a-vis three existing benchmark models was performed to highlight its efficacy. Experiments showed that the specificity and sensitivity of the proposed model were comparable to other models. Due to higher parameters of the ensemble tree model, the classification accuracy was 0.91, with 95% CI - 0.8727, 0.9477. This was better than SVM and Logistic Regression, the two popular models in the literature and comparable to the Random Forest and XGBOOST model. CPU and RAM utilization were also monitored to demonstrate the usefulness of the proposed work for real-world deployment. RAM utilization for the proposed model was higher by 30-45% compared to the other three models. Hence, the proposed model is resource-intensive as it has higher parameters than the other three models. Higher parameters also result in higher accuracy of predictions.},
journal = {Applied Intelligence},
month = jun,
pages = {3824–3843},
numpages = {20},
keywords = {Bitcoin, Fraud detection, Exploratory data analysis}
}

@article{10.1016/j.ins.2021.01.045,
author = {Li, Hao and Wang, Yongli and Li, Yanchao and Xiao, Gang and Hu, Peng and Zhao, Ruxin and Li, Bo},
title = {Learning adaptive criteria weights for active semi-supervised learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {561},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.01.045},
doi = {10.1016/j.ins.2021.01.045},
journal = {Inf. Sci.},
month = jun,
pages = {286–303},
numpages = {18},
keywords = {Batch mode active learning, Adaptive criteria weights, Submodular function, Semi-supervised classification, Semi-supervised clustering}
}

@inproceedings{10.5555/3540261.3542590,
author = {Cabannes, Vivien and Pillaud-Vivien, Loucas and Bach, Francis and Rudi, Alessandro},
title = {Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As annotations of data can be scarce in large-scale practical problems, leveraging unlabelled examples is one of the most important aspects of machine learning. This is the aim of semi-supervised learning. To benefit from the access to unlabelled data, it is natural to diffuse smoothly knowledge of labelled data to unlabelled one. This induces to the use of Laplacian regularization. Yet, current implementations of Laplacian regularization suffer from several drawbacks, notably the well-known curse of dimensionality. In this paper, we provide a statistical analysis to overcome those issues, and unveil a large body of spectral filtering methods that exhibit desirable behaviors. They are implemented through (reproducing) kernel methods, for which we provide realistic computational guidelines in order to make our method usable with large amounts of data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2329},
numpages = {13},
series = {NIPS '21}
}

@article{10.1007/s10772-021-09808-0,
author = {Bhangale, Kishor Barasu and Mohanaprasad, K.},
title = {A review on speech processing using machine learning paradigm},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {2},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-021-09808-0},
doi = {10.1007/s10772-021-09808-0},
abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human–machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.},
journal = {Int. J. Speech Technol.},
month = jun,
pages = {367–388},
numpages = {22},
keywords = {Speech processing, Speech recognition, Machine learning, Speech feature extraction, Speech classification, Speech emotion recognition}
}

@inproceedings{10.1007/978-3-030-61616-8_51,
author = {Lin, Xianghong and Du, Pangao},
title = {Spike-Train Level Unsupervised Learning Algorithm for Deep Spiking Belief Networks},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_51},
doi = {10.1007/978-3-030-61616-8_51},
abstract = {Deep spiking belief network (DSBN) uses unsupervised layer-wise pre-training method to train the network weights, it is stacked with the spike neural machine (SNM) modules. However, the synaptic weights of SNMs are difficult to pre-training through simple and effective approach for spike-train driven networks. This paper proposes a new algorithm that uses unsupervised multi-spike learning rule to train SNMs, which can implement the complex spatio-temporal pattern learning of spike trains. The spike signals first propagate in the forward direction, and then are reconstructed in the reverse direction, and the synaptic weights are adjusted according to the reconstruction error. The algorithm is successfully applied to spike train patterns, the module parameters are analyzed, such as the neuron number and learning rate in the SNMs. In addition, the low reconstruction errors of DSBNs are shown by the experimental results.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {634–645},
numpages = {12},
keywords = {Deep spiking belief networks, Unsupervised learning, Spike neural machines, Reconstruction error},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.patcog.2021.108140,
author = {Liu, Lu and Tan, Robby T.},
title = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108140},
doi = {10.1016/j.patcog.2021.108140},
journal = {Pattern Recogn.},
month = dec,
numpages = {11},
keywords = {Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels}
}

@article{10.1145/3467477,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3467477},
doi = {10.1145/3467477},
abstract = {Evolutionary Computation (EC) approaches are inspired by nature and solve optimization problems in a stochastic manner. They can offer a reliable and effective approach to address complex problems in real-world applications. EC algorithms have recently been used to improve the performance of Machine Learning (ML) models and the quality of their results. Evolutionary approaches can be used in all three parts of ML: preprocessing (e.g., feature selection and resampling), learning (e.g., parameter setting, membership functions, and neural network topology), and postprocessing (e.g., rule optimization, decision tree/support vectors pruning, and ensemble learning). This article investigates the role of EC algorithms in solving different ML challenges. We do not provide a comprehensive review of evolutionary ML approaches here; instead, we discuss how EC algorithms can contribute to ML by addressing conventional challenges of the artificial intelligence and ML communities. We look at the contributions of EC to ML in nine sub-fields: feature selection, resampling, classifiers, neural networks, reinforcement learning, clustering, association rule mining, and ensemble methods. For each category, we discuss evolutionary machine learning in terms of three aspects: problem formulation, search mechanisms, and fitness value computation. We also consider open issues and challenges that should be addressed in future work.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {161},
numpages = {35},
keywords = {Evolutionary computation, learning optimization, swarm intelligence}
}

@article{10.1007/s11227-019-02805-w,
author = {Choi, Hyunseung and Kim, Mintae and Lee, Gyubok and Kim, Wooju},
title = {Unsupervised learning approach for network intrusion detection system using autoencoders},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-02805-w},
doi = {10.1007/s11227-019-02805-w},
abstract = {Network intrusion detection systems are useful tools that support system administrators in detecting various types of intrusions and play an important role in monitoring and analyzing network traffic. In particular, anomaly detection-based network intrusion detection systems are widely used and are mainly implemented in two ways: (1) a supervised learning approach trained using labeled data and (2) an unsupervised learning approach trained using unlabeled data. Most studies related to intrusion detection systems focus on supervised learning. However, the process of acquiring labeled data is expensive, requiring manual labeling by network experts. Therefore, it is worthwhile investigating the development of unsupervised learning approaches for intrusion detection systems. In this study, we developed a network intrusion detection system using an unsupervised learning algorithm autoencoder and verified its performance. As our results show, our model achieved an accuracy of 91.70%, which outperforms previous studies that achieved 80% accuracy using cluster analysis algorithms. Our results provide a practical guideline for developing network intrusion detection systems based on autoencoders and significantly contribute to the exploration of unsupervised learning techniques for various network intrusion detection systems.},
journal = {J. Supercomput.},
month = sep,
pages = {5597–5621},
numpages = {25},
keywords = {Intrusion detection system, Unsupervised learning, Autoencoder, Anomaly detection, NSL-KDD}
}

@inproceedings{10.1007/978-3-030-70866-5_3,
author = {Mohammedi, El-Heithem and Lavinal, Emmanuel and Fleury, Guillaume},
title = {Configuration Faults Detection in IP Virtual Private Networks Based on Machine Learning},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_3},
doi = {10.1007/978-3-030-70866-5_3},
abstract = {Network incidents are largely due to configuration errors, particularly within network service providers who manage large complex networks. Such providers offer virtual private networks to their customers to interconnect their remote sites and provide Internet access. The growing demand for virtual private networks leads service providers to search for novel scalable approaches to locate incidents arising from configuration faults. In this paper, we propose a machine learning approach that aims to locate customer connectivity issues coming from configurations errors, in a BGP/MPLS IP virtual private network architecture. We feed the learning model with valid and faulty configuration data and train it using three algorithms: decision tree, random forest and multi-layer perceptron. Since failures can occur on several routers, we consider the learning problem as a supervised multi-label classification problem, where each customer router is represented by a unique label. We carry out our experiments on three network sizes containing different types of configuration errors. Results show that multi-layer perceptron has a better accuracy in detecting faults than the other algorithms, making it a potential candidate to validate offline network configurations before online deployment.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {40–56},
numpages = {17},
keywords = {Configuration faults detection, Machine learning, Virtual private networks, BGP/MPLS networks},
location = {Paris, France}
}

@inproceedings{10.1145/3461702.3462611,
author = {Perrier, Elija},
title = {Quantum Fair Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462611},
doi = {10.1145/3461702.3462611},
abstract = {In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {843–853},
numpages = {11},
keywords = {fair, learning, machine, quantum},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1016/j.future.2021.06.036,
author = {Janjua, Faisal and Masood, Asif and Abbas, Haider and Rashid, Imran and Khan, Malik M. Zaki Murtaza},
title = {Textual analysis of traitor-based dataset through semi supervised machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.06.036},
doi = {10.1016/j.future.2021.06.036},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {652–660},
numpages = {9},
keywords = {Malicious emails, Insider threat, Machine learning, Enron dataset, TWOS dataset, Text classification}
}

@article{10.1016/j.ijar.2021.06.003,
author = {Cozman, Fabio Gagliardi and Munhoz, Hugo Neri},
title = {Some thoughts on knowledge-enhanced machine learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.06.003},
doi = {10.1016/j.ijar.2021.06.003},
journal = {Int. J. Approx. Reasoning},
month = sep,
pages = {308–324},
numpages = {17},
keywords = {Knowledge representation, Machine learning}
}

@article{10.1007/s10586-021-03313-4,
author = {Lin, Frank Yeong-Sung and Hsiao, Chiu-Han and Zhang, Si-Yuan and Rung, Yi-Ping and Chen, Yu-Xuan},
title = {Cross-device matching approaches: word embedding and supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03313-4},
doi = {10.1007/s10586-021-03313-4},
abstract = {Due to the rapid development of diversified technology, people may use multiple electronic devices, such as personal computers, tablets, and smartphones, to connect to the Internet in their daily lives. Switching between devices enables a user to use e-commerce on various platforms. The complexity of consumer behavior is directly proportional to the number of involved devices. Additionally, since the personal privacy regulations nowadays are getting more strict, the user data on the Internet starts to be anonymous. Thus, determining how the devices are related is an indispensable step in achieving precision marketing or developing customized applications. In this research, the dataset provided by the CIKM Cup 2016 Challenge is used. The representation of a device is created by extracting features from browsing logs. The computation cost is reduced by filtering candidates of a target device instead of comparing them in pairs. Latent semantic indexing representations and techniques of supervised learning are used to accomplish filtering. Performing word embedding can turn literature semantic into vectors through an unsupervised neural ensemble. The addition of feature engineering on the input vectors of supervised classification can enhance the classifier’s discrimination. The classification is used to determine the probability of any two instances belonging to the same user. The significant benefit of the implementation is to form the sequences mentioned above by a cross-device linking mechanism to provide a baseline for aligning with the computation limitation and boosting the performance.},
journal = {Cluster Computing},
month = dec,
pages = {3043–3053},
numpages = {11},
keywords = {Cross-device tracking, Latent semantic indexing, Word embedding, Supervised learning}
}

@article{10.1007/s11042-021-11100-x,
author = {Hsu, Chih-Yu and Wang, Shuai and Qiao, Yu},
title = {Intrusion detection by machine learning for multimedia platform},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {19},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-11100-x},
doi = {10.1007/s11042-021-11100-x},
abstract = {The multimedia service company, Netflix, increased the number of new subscribers during the Coronavirus pandemic age. Intrusion detection systems for multimedia platforms can prevent the platform from network attacks. An intelligent intrusion detection system is proposed for the security IP Multimedia Subsystem (IMS) based on machine learning technology. For increasing the accuracy of the classifiers, it is vital to select the critical features to construct the intrusion detection system. Two-class classifiers, including the Decision Tree, Support Vector Machine, and Naive Bayesian, are selected to evaluate intrusion detection accuracy. According to the three classifiers’ accuracy values, the most critical features are selected based on the features’ ranking orders. Six critical features are selected:Service, dst_host_same_srv_rate, Flag, Protocol Type, Dst_host_rerror_rate, and Count. Numerical comparison with state_of_the_art shows that critical features improve intrusion detection accuracy, which can be better than the deep learning method.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {29643–29656},
numpages = {14},
keywords = {Intrusion detection, Support vector machine, Decision tree, Naive Bayesian classifier, Machine learning, Streaming service, Coronavirus pandemic}
}

@inproceedings{10.1145/3487923.3487938,
author = {Chindove, Hatitye and Brown, Dane},
title = {Adaptive Machine Learning Based Network Intrusion Detection},
year = {2021},
isbn = {9781450385756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487923.3487938},
doi = {10.1145/3487923.3487938},
abstract = {Network intrusion detection system (NIDS) adoption is essential for mitigating computer network attacks in various scenarios. However, the increasing complexity of computer networks and attacks make it challenging to classify network traffic. Machine learning (ML) techniques in a NIDS can be affected by different scenarios, and thus the recency, size and applicability of datasets are vital factors to consider when selecting and tuning a machine learning classifier. The proposed approach evaluates relatively new datasets constructed such that they depict real-world scenarios. It includes analyses of dataset balancing and sampling, feature engineering and systematic ML-based NIDS model tuning focused on the adaptive improvement of intrusion detection. A comparison between machine learning classifiers forms part of the evaluation process. Results on the proposed approach model effectiveness for NIDS are discussed. Recurrent neural networks and random forests models consistently achieved high f1-score results with macro f1-scores of 0.73 and 0.87 for the CICIDS 2017 dataset; and 0.73 and 0.72 against the CICIDS 2018 dataset, respectively.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Its Applications},
articleno = {15},
numpages = {6},
location = {Virtual Event, Mauritius},
series = {icARTi '21}
}

@article{10.1007/s10489-020-01689-1,
author = {Zhu, Qiu-yu and Li, Tian-tian},
title = {Semi-supervised learning method based on predefined evenly-distributed class centroids},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {9},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01689-1},
doi = {10.1007/s10489-020-01689-1},
abstract = {Compared to supervised learning, semi-supervised learning reduces the dependence of deep learning on a large number of labeled samples. In this work, we use a small number of labeled samples and perform data augmentation on unlabeled samples to achieve image classification. Our method constrains all samples to the predefined evenly-distributed class centroids (PEDCC) by the corresponding loss function. Specifically, the PEDCC-Loss for labeled samples, and the maximum mean discrepancy loss for unlabeled samples are used to make the feature distribution closer to the distribution of PEDCC. Our method ensures that the inter-class distance is large and the intra-class distance is small enough to make the classification boundaries between different classes clearer. Meanwhile, for unlabeled samples, we also use KL divergence to constrain the consistency of the network predictions between unlabeled and augmented samples. Our semi-supervised learning method achieves the state-of-the-art results, with 4000 labeled samples on CIFAR10 and 1000 labeled samples on SVHN, and the accuracy is 95.10% and 97.58% respectively. Code is available in .},
journal = {Applied Intelligence},
month = sep,
pages = {2770–2778},
numpages = {9},
keywords = {Semi–supervised learning, Predefined class centroids, PEDCC-loss, Maximum mean discrepancy, Data augmentation}
}

@article{10.1145/3379499,
author = {Suaboot, Jakapan and Fahad, Adil and Tari, Zahir and Grundy, John and Mahmood, Abdun Naser and Almalawi, Abdulmohsen and Zomaya, Albert Y. and Drira, Khalil},
title = {A Taxonomy of Supervised Learning for IDSs in SCADA Environments},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379499},
doi = {10.1145/3379499},
abstract = {Supervisory Control and Data Acquisition (SCADA) systems play an important role in monitoring industrial processes such as electric power distribution, transport systems, water distribution, and wastewater collection systems. Such systems require a particular attention with regards to security aspects, as they deal with critical infrastructures that are crucial to organizations and countries. Protecting SCADA systems from intrusion is a very challenging task because they do not only inherit traditional IT security threats but they also include additional vulnerabilities related to field components (e.g., cyber-physical attacks). Many of the existing intrusion detection techniques rely on supervised learning that consists of algorithms that are first trained with reference inputs to learn specific information, and then tested on unseen inputs for classification purposes. This article surveys supervised learning from a specific security angle, namely SCADA-based intrusion detection. Based on a systematic review process, existing literature is categorized and evaluated according to SCADA-specific requirements. Additionally, this survey reports on well-known SCADA datasets and testbeds used with machine learning methods. Finally, we present key challenges and our recommendations for using specific supervised methods for SCADA systems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {40},
numpages = {37},
keywords = {SCADA security, machine learning, network intrusion, supervised learning}
}

@inproceedings{10.1007/978-3-030-86514-6_25,
author = {Forman, George},
title = {Getting Your Package to the Right Place: Supervised Machine Learning for Geolocation},
year = {2021},
isbn = {978-3-030-86513-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86514-6_25},
doi = {10.1007/978-3-030-86514-6_25},
abstract = {Amazon Last Mile strives to learn an accurate delivery point for each address by using the noisy GPS locations reported from past deliveries. Centroids and other center-finding methods do not serve well, because the noise is consistently biased. The problem calls for supervised machine learning, but how? We addressed it with a novel adaptation of learning to rank from the information retrieval domain. This also enabled information fusion from map layers. Offline experiments show outstanding reduction in error distance, and online experiments estimated millions in annualized savings.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part IV},
pages = {403–419},
numpages = {17},
keywords = {Learning to rank, Geospatial supervised learning},
location = {Bilbao, Spain}
}

@article{10.1007/s00521-020-05030-2,
author = {Punitha, V. and Mala, C.},
title = {Traffic classification in server farm using supervised learning techniques},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05030-2},
doi = {10.1007/s00521-020-05030-2},
abstract = {Server farms used in web hosting and commercial applications connect multiple servers. Edge computing being a realm of cloud technology is orchestrated with server farms to enhance network efficiency. Edge computing increases the availability of cloud resources and Internet services. The higher availability of services and their ease of access deeply affect the user’s requesting behavior. The anomalous requesting behavior is creating malicious traffic, and enormous amount of such traffics at server farm denies the services to the legitimate users. Categorizing the incoming traffic into malicious and non-malicious traffic at server farm is the foremost criteria to eliminate the attacks, which in turn improves the QoS of the server farm. In the light of preventing the biased usage of the server farm, this paper proposes a SVM classifier based on requesting statistics. The proposed classifier discovers the attacks that deny services to legitimate users in two levels, based on the user’s request behavior. The pattern of arrival, its statistical characteristics and security misbehaviors are investigated at both levels. An incremental learning algorithm is proposed to enhance the learning plasticity of the proposed classifier. The experimental results illustrate that the performance of the proposed two-level classifier with respect to classification accuracy is competently improved with incremental learning.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1279–1296},
numpages = {18},
keywords = {Traffic classification, Denial of service, Support vector machine, Machine learning}
}

@inproceedings{10.1007/978-3-030-90370-1_7,
author = {Jin, Kun and Yin, Tongxin and Kamhoua, Charles A. and Liu, Mingyan},
title = {Network Games with Strategic Machine Learning},
year = {2021},
isbn = {978-3-030-90369-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90370-1_7},
doi = {10.1007/978-3-030-90370-1_7},
abstract = {In this paper, we study the strategic machine learning problem with a planner (decision maker) and multiple agents. The planner is the first-mover, who designs, publishes, and commits to a decision rule. The agents then best-respond by manipulating their input features to obtain a desirable decision outcome so as to maximize their utilities. Earlier works in strategic machine learning assume that every agent’s strategic action is independent of others’. By contrast, we consider a different case where agents are connected in a network and can either benefit from their neighbors’ positive decision outcomes from the planner or benefit from their neighbors’ actions. We study the Stackelberg equilibrium in this new setting and highlight the similarities and differences between this model and the literature on network/graphical games and strategic machine learning.},
booktitle = {Decision and Game Theory for Security: 12th International Conference, GameSec 2021, Virtual Event, October 25–27, 2021, Proceedings},
pages = {118–137},
numpages = {20},
keywords = {Stackelberg game, Strategic machine learning, Mechanism design}
}

@inproceedings{10.1007/978-3-030-61377-8_24,
author = {Mello, Claudio D. and Messias, Lucas R. V. and Drews-Jr, Paulo Lilles Jorge and Botelho, Silvia S. C.},
title = {Unsupervised Learning Method for Encoder-Decoder-Based Image Restoration},
year = {2020},
isbn = {978-3-030-61376-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61377-8_24},
doi = {10.1007/978-3-030-61377-8_24},
abstract = {The restoration of a corrupted image is a challenge to computer vision and image processing. In hazy, underwater and medical images, the lack of paired images lead the state of the art to synthesize datasets. The Generative Adversarial Networks (GANs) are widely used in these cases. However, computational cost and training instability are current concerns. We present an unsupervised learning algorithm that does not requires paired dataset to train encoder-decoder-like neural network for image restoration. An encoder-decoder learn to represent its input data in a latent representation and reconstruct then in the output. During the training stage, our algorithm applies the encoder-decoder output image to a degradation block that reinforces its degradation. The degraded and input images are matched using a loss function. After the training process, we obtain a restored image from the decoder. We used ill-exposed images to evaluate and validate our algorithm.},
booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
pages = {348–360},
numpages = {13},
keywords = {Unsupervised learning, Image restoration, Neural network},
location = {Rio Grande, Brazil}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1016/j.imavis.2019.06.011,
author = {Conze, Pierre-Henri and Tilquin, Florian and Lamard, Mathieu and Heitz, Fabrice and Quellec, Gwenol\'{e}},
title = {Unsupervised learning-based long-term superpixel tracking},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {89},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2019.06.011},
doi = {10.1016/j.imavis.2019.06.011},
journal = {Image Vision Comput.},
month = sep,
pages = {289–301},
numpages = {13},
keywords = {Superpixel matching, Unsupervised learning, Superpixel tracking, Multi-step integration, Random forests, Forward-backward consistency}
}

@inproceedings{10.1007/978-3-030-30487-4_30,
author = {Genkin, Alexander and Sengupta, Anirvan M. and Chklovskii, Dmitri},
title = {A Neural Network for Semi-supervised Learning on Manifolds},
year = {2019},
isbn = {978-3-030-30486-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30487-4_30},
doi = {10.1007/978-3-030-30487-4_30},
abstract = {Semi-supervised learning algorithms typically construct a weighted graph of data points to represent a manifold. However, an explicit graph representation is problematic for neural networks operating in the online setting. Here, we propose a feed-forward neural network capable of semi-supervised learning on manifolds without using an explicit graph representation. Our algorithm uses channels that represent localities on the manifold such that correlations between channels represent manifold structure. The proposed neural network has two layers. The first layer learns to build a representation of low-dimensional manifolds in the input data as proposed recently in [8]. The second learns to classify data using both occasional supervision and similarity of the manifold representation of the data. The channel carrying label information for the second layer is assumed to be “silent” most of the time. Learning in both layers is Hebbian, making our network design biologically plausible. We experimentally demonstrate the effect of semi-supervised learning on non-trivial manifolds.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Theoretical Neural Computation: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part I},
pages = {375–386},
numpages = {12},
keywords = {Semi-supervised learning, Online learning, Manifold learning},
location = {Munich, Germany}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s10676-021-09608-9,
author = {Scantamburlo, Teresa},
title = {Non-empirical problems in fair machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-021-09608-9},
doi = {10.1007/s10676-021-09608-9},
abstract = {The problem of fair machine learning has drawn much attention over the last few years and the bulk of offered solutions are, in principle, empirical. However, algorithmic fairness also raises important conceptual issues that would fail to be addressed if one relies entirely on empirical considerations. Herein, I will argue that the current debate has developed an empirical framework that has brought important contributions to the development of algorithmic decision-making, such as new techniques to discover and prevent discrimination, additional assessment criteria, and analyses of the interaction between fairness and predictive accuracy. However, the same framework has also suggested higher-order issues regarding the translation of fairness into metrics and quantifiable trade-offs. Although the (empirical) tools which have been developed so far are essential to address discrimination encoded in data and algorithms, their integration into society elicits key (conceptual) questions such as: What kind of assumptions and decisions underlies the empirical framework? How do the results of the empirical approach penetrate public debate? What kind of reflection and deliberation should stakeholders have over available fairness metrics? I will outline the empirical approach to fair machine learning, i.e. how the problem is framed and addressed, and suggest that there are important non-empirical issues that should be tackled. While this work will focus on the problem of algorithmic fairness, the lesson can extend to other conceptual problems in the analysis of algorithmic decision-making such as privacy and explainability.},
journal = {Ethics and Inf. Technol.},
month = dec,
pages = {703–712},
numpages = {10},
keywords = {Machine learning, Fairness, Empirical approach, Assessment of machine learning}
}

@article{10.1016/j.eswa.2021.115782,
author = {Alhajjar, Elie and Maxwell, Paul and Bastian, Nathaniel},
title = {Adversarial machine learning in Network Intrusion Detection Systems},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115782},
doi = {10.1016/j.eswa.2021.115782},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {13},
keywords = {Network Intrusion Detection Systems, Adversarial machine learning, Evolutionary computation, Deep learning, Monte Carlo simulation}
}

@phdthesis{10.5555/AAI28717041,
author = {Zhu, Pengkai and Brian, Kulis, and Kate, Saenko, and Francesco, Orabona,},
advisor = {Venkatesh, Saligrama,},
title = {Machine Learning Under Limited Resources},
year = {2021},
isbn = {9798460478361},
publisher = {Boston University},
address = {USA},
abstract = {Deep learning methods have led to substantial improvement of performance in many computer vision applications. However, these methods require massive resources, including data collection, label annotation, and computation, which may be insufficient in real-world applications. The constraints of resources limit the deployment of powerful deep models, resulting in degraded performance. Therefore, research topics that address resource limitations, such as few-shot/zero-shot recognition, have recently drawn attention. In this thesis, we develop machine learning methods that reduce the requirement of resources while keeping the prediction accuracy on par with resource-rich models. Specifically, we consider three different settings: label-limited recognition, zero-shot detection, and reducing energy consumption for IoT systems at inference time.We first propose a novel image encoding method that decomposes an image into a few semantic parts and represents each part in a compact vocabulary of a few concepts. Because the concepts learned by our model generalize well to novel objects, this encoding shows competent results in label-limited classification tasks like few-shot/zero-shot recognition and unsupervised domain adaptation. The encoding also demonstrates extraordinary robustness to adversarial image perturbations, and we found the encoding is interpretable by humans through crowd-sourcing evaluations. Next, we propose a statistical model that represents the structural information of an object. Each object is described by the part location and location-independent signatures. They form a latent space on which a structural constraint is imposed. At inference time, the model produces the representations that maximize the posterior probability. We show that the new representation can achieve state-of-the-art performance for few-shot recognition on benchmark datasets.We then study the problem of zero-shot detection. We propose an evaluation protocol and develop two algorithms to address the problem. One algorithm seamlessly integrates semantic attribute predictions into visual features to produce bounding boxes with visual and semantic information. In the second algorithm, we take an approach of data augmentation. First, a conditional variational auto-encoder is employed to produce synthetic features for unseen classes by leveraging the semantic attributes. The confidence predictor is then trained on the real data along with the synthetic features to predict higher confidence scores for unseen objects. Both algorithms show significant improvement in the detection of unseen objects through empirical evaluations on complex datasets.Finally, we present a novel learning framework that associates each edge device in an IoT system with a gating function. The gating function can stop the device from transmitting redundant features to the central inference model for some instances at inference time. This framework can significantly reduce the energy cost by reducing the transmission counts with negligible accuracy degradation in our evaluations on real-world datasets.},
note = {AAI28717041}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inbook{10.1145/3447404.3447414,
author = {Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros},
title = {Machine Learning Basics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447414},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {143–193},
numpages = {51}
}

@article{10.3233/THC-202237,
author = {Choudhury, Avishek},
title = {Predicting cancer using supervised machine learning: Mesothelioma},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-202237},
doi = {10.3233/THC-202237},
journal = {Technol. Health Care},
month = jan,
pages = {45–58},
numpages = {14},
keywords = {Mesothelioma, predictive modeling, decision support system, machine learning, artificial intelligence, lung cancer}
}

@article{10.1016/j.patcog.2018.11.006,
author = {Li, Yanchao and Wang, Yongli and Liu, Qi and Bi, Cheng and Jiang, Xiaohui and Sun, Shurong},
title = {Incremental semi-supervised learning on streaming data},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.006},
doi = {10.1016/j.patcog.2018.11.006},
journal = {Pattern Recogn.},
month = apr,
pages = {383–396},
numpages = {14},
keywords = {Semi-supervised learning, Dynamic feature learning, Streaming data, Classification}
}

@article{10.1007/s00034-019-01173-3,
author = {Geng, Mingyang and Shang, Suning and Ding, Bo and Wang, Huaimin and Zhang, Pengfei},
title = {Unsupervised Learning-Based Depth Estimation-Aided Visual SLAM Approach},
year = {2020},
issue_date = {Feb 2020},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {39},
number = {2},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-019-01173-3},
doi = {10.1007/s00034-019-01173-3},
abstract = {Simultaneous localization and map construction (SLAM) tasks have been proven to benefit greatly from the depth information of the environment. In this paper, we first present an unsupervised end-to-end learning framework for the task of monocular depth and camera motion estimation from video sequences. The difference between our work and the existing unsupervised methods is that we not only use image reconstruction for supervising but also exploit the pose estimation method used in traditional SLAM approaches to enhance the supervised signal and add extra training constraints for the task of monocular depth and camera motion estimation. Furthermore, we successfully exploit our unsupervised learning framework to assist the traditional ORB-SLAM system when the initialization module of ORB-SLAM method could not match enough features. Qualitative and quantitative experiments have shown that our unsupervised learning framework performs the depth estimation task superior to the supervised methods and outperforms the previous state-of-the-art unsupervised approach by 13.5% on KITTI dataset. For the pose estimation task, our method performs comparably to the supervised methods that use ground-truth pose data for training. Besides, our unsupervised learning framework can significantly accelerate the initialization process of the traditional ORB-SLAM system and effectively improve the accuracy of environmental mapping in strong lighting and weak texture scenes.},
journal = {Circuits Syst. Signal Process.},
month = feb,
pages = {543–570},
numpages = {28},
keywords = {Monocular depth estimation, Pose estimation, Unsupervised learning, Visual SLAM system}
}

@inproceedings{10.1145/3461778.3462163,
author = {Scurto, Hugo and Caramiaux, Baptiste and Bevilacqua, Frederic},
title = {Prototyping Machine Learning Through Diffractive Art Practice},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462163},
doi = {10.1145/3461778.3462163},
abstract = {In this paper, we outline a diffractive practice of machine learning (ML) in the frame of material-centered interaction design. To this aim, we review related work in ML, HCI, design, new interfaces for musical expression, and computational art, and introduce two practice-based studies of music performance and robotic art based on interactive machine learning tools, with the hope of revealing the computational materiality of ML, and the potential of embodiment to craft prototypes of ML that reconfigure conceptual or technical approaches to ML. We derive five interference conditions for such art-based ML prototypes—situational whole, small data, shallow model, learnable algorithm, and somaesthetic behaviour—and describe their widening of design and engineering practices of ML prototyping. Finally, we sketch how a process of intra-active machine learning could complement that of interactive machine learning to take materiality as an entry point for ML design within HCI.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {2013–2025},
numpages = {13},
keywords = {Art Practice, Design, Diffractive Methods., Machine Learning},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3417313.3429378,
author = {Disabato, Simone and Roveri, Manuel},
title = {Incremental On-Device Tiny Machine Learning},
year = {2020},
isbn = {9781450381345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417313.3429378},
doi = {10.1145/3417313.3429378},
abstract = {Tiny Machine Learning (TML) is a novel research area aiming at designing and developing Machine Learning (ML) techniques meant to be executed on Embedded Systems and Internet-of-Things (IoT) units. Such techniques, which take into account the constraints on computation, memory, and energy characterizing the hardware platform they operate on, exploit approximation and pruning mechanisms to reduce the computational load and the memory demand of Machine and Deep Learning (DL) algorithms.Despite the advancement of the research, TML solutions present in the literature assume that Embedded Systems and IoT units support only the inference of ML and DL algorithms, whereas their training is confined to more-powerful computing units (due to larger computational load and memory demand). This also prevents such pervasive devices from being able to learn in an incremental way directly from the field to improve the accuracy over time or to adapt to new working conditions.The aim of this paper is to address such an open challenge by introducing an incremental algorithm based on transfer learning and k-nearest neighbor to support the on-device learning (and not only the inference) of ML and DL solutions on embedded systems and IoT units. Moreover, the proposed solution is general and can be applied to different application scenarios. Experimental results on image/audio benchmarks and two off-the-shelf hardware platforms show the feasibility and effectiveness of the proposed solution.},
booktitle = {Proceedings of the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things},
pages = {7–13},
numpages = {7},
keywords = {Deep Learning, Embedded Systems, Incremental Learning, Internet-of-Things, Tiny Machine Learning},
location = {Virtual Event, Japan},
series = {AIChallengeIoT '20}
}

@article{10.1016/j.compeleceng.2021.107527,
author = {Mohammad, Abdul Salam and Pradhan, Manas Ranjan},
title = {Machine learning with big data analytics for cloud security},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {PA},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107527},
doi = {10.1016/j.compeleceng.2021.107527},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {15},
keywords = {Big data, Cloud computing, Cloud security, Data security, Data management, Data storage, Machine learning}
}

@inproceedings{10.1007/978-3-030-59719-1_31,
author = {Song, Youyi and Zhou, Teng and Teoh, Jeremy Yuen-Chun and Zhang, Jing and Qin, Jing},
title = {Unsupervised Learning for CT Image Segmentation via Adversarial Redrawing},
year = {2020},
isbn = {978-3-030-59718-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59719-1_31},
doi = {10.1007/978-3-030-59719-1_31},
abstract = {We propose a novel adversarial learning framework for unsupervised training of CNNs in CT image segmentation. It is motivated by difficulties in collecting voxel-wise annotations, which is laborious, time-consuming and expensive. It is conceptually simple, allowing us to train an effective segmentation network without any human annotation. Specifically, we design the generator with a CNN producing the segmentation results and a decoder redrawing the CT volume based on the segmentation results. The CNN is then implicitly trained in the adversarial learning framework where a discriminator gradually enforcing the generator to generate CT volumes whose distribution well matches the distribution of the training data. We further propose two constrains as regularization schemes for the training procedure to drive the model towards optimal segmentation by avoiding some unreasonable results. We conducted extensive experiments to evaluate the proposed method on a famous publicly available dataset, and the experimental results demonstrate the effectiveness of the proposed method.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV},
pages = {309–320},
numpages = {12},
keywords = {Unsupervised learning, Adversarial redrawing, CNNs, CT image segmentation.},
location = {Lima, Peru}
}

@article{10.1016/j.cosrev.2021.100370,
author = {Garg, Arunim and Mago, Vijay},
title = {Role of machine learning in medical research: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100370},
doi = {10.1016/j.cosrev.2021.100370},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {17},
keywords = {00-01, 99-00, Medical research, Machine learning, Deep learning, Medical data}
}

@article{10.1016/j.neucom.2019.09.039,
author = {Chen, Chuangquan and Gan, Yanfen and Vong, Chi-Man},
title = {Extreme semi-supervised learning for multiclass classification},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {376},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.09.039},
doi = {10.1016/j.neucom.2019.09.039},
journal = {Neurocomput.},
month = feb,
pages = {103–118},
numpages = {16},
keywords = {Multiclass classification, Semi-supervised support vector machine, Extreme learning machine, Approximate empirical kernel map, Alternating optimization}
}

@inproceedings{10.1145/3448016.3459240,
author = {Jiang, Jiawei and Gan, Shaoduo and Liu, Yue and Wang, Fanlin and Alonso, Gustavo and Klimovic, Ana and Singla, Ankit and Wu, Wentao and Zhang, Ce},
title = {Towards Demystifying Serverless Machine Learning Training},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3459240},
doi = {10.1145/3448016.3459240},
abstract = {The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over "serverful" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {857–871},
numpages = {15},
keywords = {machine learning, serverless computing},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@phdthesis{10.5555/AAI28262460,
author = {Lavania, Chandrashekhar and Kannan, Sreeram and Hajishirzi, Hannaneh},
advisor = {Jeffrey, Bilmes,},
title = {Towards Unsupervised Learning of Submodular Functions for Summarization},
year = {2020},
isbn = {9798569995592},
publisher = {University of Washington},
abstract = {In the information age, vast volumes of data are generated daily. There exist a plethora of data sources, including text, videos, and sensor networks. The large size of data can make it difficult to process. Furthermore, the generated data often has considerable redundancy. Therefore, extracting meaningful information from the data can make it easier to process for downstream tasks. Summarization is one way to extract this information. In the past, submodular functions have been successfully used for summarizing data. These functions can be defined based on domain knowledge or can be learned from the data itself. However, supervised learning of submodular functions faces an obstacle as there is often a lack of known good summaries of the data for training. Therefore, it would be beneficial if the functions can be learned in an unsupervised manner. This work proposes an approach towards learning a mixture of submodular functions in an unsupervised manner. It is achieved through a two-part process. First, an autoencoder neural network is trained in a constrained manner. The aim is to produce features such that a larger feature value implies that the input data sample has a larger amount of the corresponding learned property.  It is analogous to bag-of-words features, with the ``words''  learned automatically.   Next, a mixture of submodular functions is instantiated using the learned features. Each component of the mixture consists of a concave composed with a modular function. The mixture weights are learned through an approach that does not directly utilize supervised summary information.  It optimizes a set of meta-objectives, each of which corresponds to a likely necessary condition on what constitutes a good summarization objective.  Empirical results on different data modalities show that the proposed two-part process produces functions that perform significantly better than a variety of baseline methods.This work also explores other applications of the proposed features. The focus application is the summarization of video streams on the fly under a memory budget. The aim is to produce running summaries (within a budget) of incoming video streams at each time step. Any video snippet that is not part of a summary at a given time step is dropped to abide by the memory constraints. To this end, this work proposes two algorithms, one each for single-stream and multi-stream summarization. Empirical evaluations demonstrate that the algorithms, instantiated with the proposed features, can outperform the baselines.In addition to these explorations, it is also shown that the proposed constrained training can be used with different flavors of autoencoder architectures and losses. The influence of these different setups is also demonstrated for the task of summarization.},
note = {AAI28262460}
}

@phdthesis{10.5555/AAI27833863,
author = {Nguyen, Thanh Van and Wong, Raymond and Liu, Jia and Sarkar, Soumik and Vaswani, Namara and Wang, Zhengdao},
advisor = {Chinmay, Hegde,},
title = {Provable Surrogate Gradient-Based Optimization for Unsupervised Learning},
year = {2020},
isbn = {9798662371071},
publisher = {Iowa State University},
address = {USA},
abstract = {Gradient-based optimization lies at the core of modern machine learning and deep learning, with (stochastic) gradient descent algorithms being employed as the main workhorse. The unprecedented success of deep learning over the last decade has arguably been tied with the popularity and mysterious success of these algorithms --- particularly for supervised learning.This success is despite the fact that objective functions in deep learning are extremely non-linear and non-convex.The past few years have witnessed great theoretical advances in analyzing optimization and inductive biases of gradient descent for supervised learning. However, the majority of existing work only applies to settings such as classification and regression. In contrast, the role of gradient descent in the unsupervised setting has gained far less attention. In this work, we make concrete contributions to the understanding of gradient-based optimization in unsupervised learning.We start with dictionary learning, an unsupervised feature learning mechanism widely used in signal processing and machine learning. The primary goal of dictionary learning is to learn sparse, linear representations of the data by minimizing the reconstruction loss. In this problem, the objective function is coupled with an intractable sparse coding step due to the latent representations. Therefore, the gradient of the loss with respect to the model parameters can not be obtained exactly but is only a noisy estimate of the true gradient. However, gradient-based alternating minimization for dictionary learning works surprisingly well in practice while theoretical understanding of the success has lagged behind. We will refer to this method as "surrogate" gradient-based optimization.In Chapter 1 and Chapter 2, we introduce two surrogate gradient descent algorithms for sparse coding. The first algorithm learns a double-sparsity model where the dictionary is the product of a fixed, known basis and a learnable sparse component. The second algorithm provably learns a dictionary from samples with missing entries. In each case, we provide a spectral initialization subroutine that gives a coarse estimate of the true dictionary. Then, starting from this estimate, we prove that the surrogate descent algorithm linearly converges to the true dictionary. We analyze the algorithm and demonstrate superior sample complexity and computational complexity bounds over existing provable approaches.While sparse coding is still widely used, its computational cost is prohibitive for high-dimensional data. Autoencoders have instead emerged as an efficient and flexible alternative for feature learning using neural networks. In Chapter 3, we build upon our theory of surrogate gradient developed in the previous chapters to provide a series of results for autoencoder learning. For several generative models of data, we prove that when trained with gradient descent, two-layer weight-tied autoencoders can successfully recover the ground-truth parameters of the corresponding models. Our analysis establishes theoretical evidence that shallow autoencoder modules can indeed be powerful feature learning mechanisms for a variety of data models. In Chapter 4, we go beyond the local analysis in Chapter 3 and analyze the gradient dynamics of over-parameterized autoencoders. Under a few mild assumptions about the given training dataset, we rigorously prove the linear convergence of gradient descent for randomly initialized autoencoder networks. Our analysis mirrors the recent advances in the emerging theory of neural tangent kernels.Chapter 5 considers a black-box optimization problem where the objective and constraints are specified as solutions to expensive PDE solvers. We pose this optimization as sampling from a Gibbs distribution with a black-box energy function and perform Langevin sampling by using surrogate gradients of the black-box functions learned by deep neural networks. We prove the convergence of the surrogate Langevin dynamics when the target distribution is log-concave and smooth. Finally, in Chapter 6, we lay out several potential directions that merge two lines of research in gradient flow analysis and Langevin dynamics, as well as inverse problems with generative priors.},
note = {AAI27833863}
}

@inproceedings{10.1145/3447548.3470799,
author = {Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong},
title = {Machine Learning Robustness, Fairness, and their Convergence},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470799},
doi = {10.1145/3447548.3470799},
abstract = {Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4046–4047},
numpages = {2},
keywords = {convergence, fairness, machine learning, robustness},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1016/j.ins.2018.12.057,
author = {Li, Yang and Pan, Quan and Wang, Suhang and Peng, Haiyun and Yang, Tao and Cambria, Erik},
title = {Disentangled Variational Auto-Encoder for semi-supervised learning},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {482},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.12.057},
doi = {10.1016/j.ins.2018.12.057},
journal = {Inf. Sci.},
month = may,
pages = {73–85},
numpages = {13},
keywords = {Semi-supervised learning, Variational Auto-encoder, Disentangled representation, Neural networks}
}

@article{10.1145/3453444,
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453444},
doi = {10.1145/3453444},
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {111},
numpages = {39},
keywords = {Machine learning lifecycle, assurance, assurance evidence, machine learning workflow, safety-critical systems}
}

@article{10.1016/j.asoc.2016.08.024,
author = {dos Santos Neto, Pedro de Alcntara and Britto, Ricardo and Rablo, Ricardo de Andrade Lira and Cruz, Jonathas Jivago de Almeida and Lira, Werney Ayala Luz},
title = {A hybrid approach to suggest software product line portfolios},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.024},
doi = {10.1016/j.asoc.2016.08.024},
abstract = {Graphical abstractDisplay Omitted HighlightsThe work proposes a hybrid approach to deal with the Product Portfolio Scope Problem.The approach is composed by a solution to deploy the feature relevance indicated by the customers into code assets of a SPL, based on a systematic method (SQFD).The approach includes a method to estimate the cost of an asset based on common and relevant measures related to source code, together with a fuzzy system to deal with the imprecision to set reference values.The work presents an application of an NSGA-II to search for products minimizing the cost and maximizing the relevance of the candidate products.The approach was evaluated using different scenarios, exploring the mains aspects related to method in the practice: size, granularity of features and products search space.The previous version of our hybrid approach was dependent on the employed technologies and algorithms. Herein we reformulate our approach, detaching it from any particular technique/algorithm.The data collection process associated with our approach was improved to facilitate the hybrid approach's usage and mitigate associated construct validity threats.A more comprehensive evaluation, which focused on show the real word usefulness and scalability of our hybrid approach. To validate the usefulness of our approach, it was used the SPL associated with a tool broadly employed in both industrial and academic contexts (ArgoUML-SPL). The scalability of our approach was evaluated using a synthetic SPL.All the experiments were based on the guidelines defined by Arcuri and Briand in order to evaluate the statistical significance of this kind of work. Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1243–1255},
numpages = {13},
keywords = {Feature model selection problem, Fuzzy inference systems, NSGA-II, Product portfolio scoping, Search based feature model selection, Search based software engineering, Software product lines}
}

@article{10.1007/s00245-019-09637-3,
author = {Calder, Jeff and Slep\v{c}ev, Dejan},
title = {Properly-Weighted Graph Laplacian for Semi-supervised Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {82},
number = {3},
issn = {0095-4616},
url = {https://doi.org/10.1007/s00245-019-09637-3},
doi = {10.1007/s00245-019-09637-3},
abstract = {The performance of traditional graph Laplacian methods for semi-supervised learning degrades substantially as the ratio of labeled to unlabeled data decreases, due to a degeneracy in the graph Laplacian. Several approaches have been proposed recently to address this, however we show that some of them remain ill-posed in the large-data limit. In this paper, we show a way to correctly set the weights in Laplacian regularization so that the estimator remains well posed and stable in the large-sample limit. We prove that our semi-supervised learning algorithm converges, in the infinite sample size limit, to the smooth solution of a continuum variational problem that attains the labeled values continuously. Our method is fast and easy to implement.},
journal = {Appl. Math. Optim.},
month = dec,
pages = {1111–1159},
numpages = {49},
keywords = {Semi-supervised learning, Label propagation, Asymptotic consistency, PDEs on graphs, Gamma-convergence, 49J55, 35J20, 35B65, 62G20, 65N12}
}

@inproceedings{10.1007/978-3-030-87231-1_37,
author = {Hu, Chen and Li, Cheng and Wang, Haifeng and Liu, Qiegen and Zheng, Hairong and Wang, Shanshan},
title = {Self-supervised Learning for MRI Reconstruction with a Parallel Network Training Framework},
year = {2021},
isbn = {978-3-030-87230-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87231-1_37},
doi = {10.1007/978-3-030-87231-1_37},
abstract = {Image reconstruction from undersampled k-space data plays an important role in accelerating the acquisition of MR data, and a lot of deep learning-based methods have been exploited recently. Despite the achieved inspiring results, the optimization of these methods commonly relies on the fully-sampled reference data, which are time-consuming and difficult to collect. To address this issue, we propose a novel self-supervised learning method. Specifically, during model optimization, two subsets are constructed by randomly selecting part of k-space data from the undersampled data and then fed into two parallel reconstruction networks to perform information recovery. Two reconstruction losses are defined on all the scanned data points to enhance the network’s capability of recovering the frequency information. Meanwhile, to constrain the learned unscanned data points of the network, a difference loss is designed to enforce consistency between the two parallel networks. In this way, the reconstruction model can be properly trained with only the undersampled data. During the model evaluation, the undersampled data are treated as the inputs and either of the two trained networks is expected to reconstruct the high-quality results. The proposed method is flexible and can be employed in any existing deep learning-based method. The effectiveness of the method is evaluated on an open brain MRI dataset. Experimental results demonstrate that the proposed self-supervised method can achieve competitive reconstruction performance compared to the corresponding supervised learning method at high acceleration rates (4 and 8). The code is publicly available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VI},
pages = {382–391},
numpages = {10},
keywords = {Image reconstruction, Deep learning, Self-supervised learning, Parallel network},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3447548.3469463,
author = {Wang, Gang and Ciptadi, Arridhana and Ahmadzadeh, Ali},
title = {MLHat: Deployable Machine Learning for Security Defense},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469463},
doi = {10.1145/3447548.3469463},
abstract = {The MLHat workshop aims to bring together academic researchers and industry practitioners to discuss the open challenges, potential solutions, and best practices to deploy machine learning at scale for security defense. The workshop will discuss related topics from both defender perspectives (white-hat) and the attacker perspectives (black-hat). We call the workshop MLHats, to serve as a place for people who are interested in using machine learning to solve practical security problems. The workshop will focus on defining new machine learning paradigms under various security application contexts and identifying exciting new future research directions. At the same time, the workshop will also have a strong industry presence to provide insights into the challenges in deploying and maintaining machine learning models and the much-needed discussion on the capabilities that the state-of-the-arts failed to provide.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4161–4162},
numpages = {2},
keywords = {adversarial machine learning, deployable machine learning, security and privacy},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/1964138.1964139,
author = {Silva, Alan Pedro da and Costa, Evandro and Bittencourt, Ig Ibert and Brito, Patrick H. S. and Holanda, Olavo and Melo, Jean},
title = {Ontology-based software product line for building semantic web applications},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964139},
doi = {10.1145/1964138.1964139},
abstract = {The Software Product Lines (SPL) has proved very effective in building large-scale software. However, few works seek to adjust the approach of software product line to applications in the context of semantic web. This is because applications in this context assume the use of semantic services and intelligent agents. As a result, it is necessary that there are assets that provide adequate interoperability both semantic services and intelligent agents. In this sense, it is proposed in this paper the use of ontologies for the specification of entire a project of a SPL. With this, it can be a sufficiently formal specification that can be interpreted by both software engineers and computational algorithms.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {1},
numpages = {6},
keywords = {ontology, semantic web, software product line},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/3474376.3487276,
author = {Koushanfar, Farinaz},
title = {Machine Learning on Encrypted Data: Hardware to the Rescue},
year = {2021},
isbn = {9781450386623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474376.3487276},
doi = {10.1145/3474376.3487276},
abstract = {Machine Learning on encrypted data is a yet-to-be-addressed challenge. Several recent key advances across different layers of the system, from cryptography and mathematics to logic synthesis and hardware are paving the way for practical realization of privacy preserving computing for certain target applications. This talk highlights the crucial role of hardware and advances in computing architecture in supporting the recent progresses in the field. I outline the main technologies and mixed computing models. I particularly center my talk on the recent progress in synthesis of Garbled Circuits that provide a leap in scalable realization of machine learning on encrypted data. I explore how hardware could pave the way for navigating the complex space of privacy-preserving computing in general, and enabling scalable future mixed protocol solutions. I conclude by briefly discussing the challenges and opportunities moving forward.},
booktitle = {Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security},
pages = {1},
numpages = {1},
keywords = {hardware security, machine learning},
location = {Virtual Event, Republic of Korea},
series = {ASHES '21}
}

@inproceedings{10.1007/978-3-030-59861-7_49,
author = {Wodzinski, Marek and M\"{u}ller, Henning},
title = {Unsupervised Learning-Based Nonrigid Registration of High Resolution Histology Images},
year = {2020},
isbn = {978-3-030-59860-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59861-7_49},
doi = {10.1007/978-3-030-59861-7_49},
abstract = {The use of different dyes during histological sample preparation reveals distinct tissue properties and may improve the diagnosis. Nonetheless, the staining process deforms the tissue slides and registration is necessary before further processing. The importance of this problem led to organizing an open challenge named Automatic Non-rigid Histological Image Registration Challenge (ANHIR), organized jointly with the IEEE ISBI 2019 conference. The challenge organizers provided 481 image pairs and a server-side evaluation platform making it possible to reliably compare the proposed algorithms. The majority of the methods proposed for the challenge were based on the classical, iterative image registration, resulting in high computational load and arguable usefulness in clinical practice due to the long analysis time. In this work, we propose a deep learning-based unsupervised nonrigid registration method, that provides results comparable to the solutions of the best scoring teams, while being significantly faster during the inference. We propose a multi-level, patch-based training and inference scheme that makes it possible to register images of almost any size, up&nbsp;to the highest resolution provided by the challenge organizers. The median target registration error is close to 0.2% of the image diagonal while the average registration time, including the data loading and initial alignment, is below 3&nbsp;s. We freely release both the training and inference code making the results fully reproducible.},
booktitle = {Machine Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {484–493},
numpages = {10},
keywords = {Image registration, Deep learning, Histology, ANHIR},
location = {Lima, Peru}
}

@inproceedings{10.1145/3412841.3442027,
author = {Tsimpourlas, Foivos and Rajan, Ajitha and Allamanis, Miltiadis},
title = {Supervised learning over test executions as a test oracle},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442027},
doi = {10.1145/3412841.3442027},
abstract = {The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is a key remaining issue for automated testing. The paper aims at solving the test oracle problem in a scalable and accurate way. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program.We evaluate our approach using case studies from different application domains - 1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components and 4. Sed stream editor. We found the classification models for all subject programs resulted in high precision, recall and specificity, averaging to 89%, 88% and 92% respectively, while only training with an average 15% of the total traces. Our experiments show that the proposed NN model is promising as a test oracle and is able to learn runtime patterns to distinguish test executions for systems and tests from different application domains.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1521–1531},
numpages = {11},
keywords = {execution trace, neural networks, software testing, test oracle},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1007/978-3-030-78191-0_29,
author = {Chen, Huai and Li, Jieyu and Wang, Renzhen and Huang, Yijie and Meng, Fanrui and Meng, Deyu and Peng, Qing and Wang, Lisheng},
title = {Unsupervised Learning of Local Discriminative Representation for Medical Images},
year = {2021},
isbn = {978-3-030-78190-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78191-0_29},
doi = {10.1007/978-3-030-78191-0_29},
abstract = {Local discriminative representation is needed in many medical image analysis tasks such as identifying sub-types of lesion or segmenting detailed components of anatomical structures. However, the commonly applied supervised representation learning methods require a large amount of annotated data, and unsupervised discriminative representation learning distinguishes different images by learning a global feature, both of which are not suitable for localized medical image analysis tasks. In order to avoid the limitations of these two methods, we introduce local discrimination into unsupervised representation learning in this work. The model contains two branches: one is an embedding branch which learns an embedding function to disperse dissimilar pixels over a low-dimensional hypersphere; and the other is a clustering branch which learns a clustering function to classify similar pixels into the same cluster. These two branches are trained simultaneously in a mutually beneficial pattern, and the learnt local discriminative representations are able to well measure the similarity of local image regions. These representations can be transferred to enhance various downstream tasks. Meanwhile, they can also be applied to cluster anatomical structures from unlabeled medical images under the guidance of topological priors from simulation or other structures with similar topological characteristics. The effectiveness and usefulness of the proposed method are demonstrated by enhancing various downstream tasks and clustering anatomical structures in retinal images and chest X-ray images.},
booktitle = {Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event, June 28–June 30, 2021, Proceedings},
pages = {373–385},
numpages = {13},
keywords = {Unsupervised representation learning, Local discrimination, Topological priors}
}

@article{10.1007/s11063-021-10556-0,
author = {Zhou, Wei and Lian, Cheng and Zeng, Zhigang and Xu, Bingrong and Su, Yixin},
title = {Improve Semi-supervised Learning with Metric Learning Clusters and Auxiliary Fake Samples},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {5},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-021-10556-0},
doi = {10.1007/s11063-021-10556-0},
abstract = {Because it is very expensive to collect a large number of labeled samples to train deep neural networks in certain fields, semi-supervised learning (SSL) researcher has become increasingly important in recent years. There are many consistency regularization-based methods for solving SSL tasks, such as the Π model and mean teacher. In this paper, we first show through an experiment that the traditional consistency-based methods exist the following two problems: (1) as the size of unlabeled samples increases, the accuracy of these methods increases very slowly, which means they cannot make full use of unlabeled samples. (2) When the number of labeled samples is vary small, the performance of these methods will be very low. Based on these two findings, we propose two methods, metric learning clustering (MLC) and auxiliary fake samples, to alleviate these problems. The proposed methods achieve state-of-the-art results on SSL benchmarks. The error rates are 10.20%, 38.44% and 4.24% for CIFAR-10 with 4000 labels, CIFAR-100 with 10,000 labels and SVHN with 1000 labels by using MLC. For MNIST, the auxiliary fake samples method shows great results in cases with the very few labels.},
journal = {Neural Process. Lett.},
month = oct,
pages = {3427–3443},
numpages = {17},
keywords = {Semi-supervised learning, Metric learning, Variational auto-encoders, Very few labeled data}
}

@article{10.3233/IDA-194528,
author = {de Paulo Faleiros, Thiago and Valejo, Alan and de Andrade Lopes, Alneu},
title = {Unsupervised learning of textual pattern based on Propagation in Bipartite Graph},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {3},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-194528},
doi = {10.3233/IDA-194528},
abstract = {Graph-based algorithms have aroused considerable interests in recent years by facilitating pattern recognition and learning via information propagation process through the graph. Here, we propose an unsupervised learning algorithm based on propagation on bipartite graph, referred to as Propagation in Bipartite Graph (PBG) algorithm. The contributions of this approach are threefold: 1) we present an iterative graph-based algorithm and a straight-forward bipartite representation for textual data, in which vertices represent documents and words, and edges between documents and words represent the occurrences of the words in the documents. Additionally, 2) we show that PBG is more flexible and easier to be adapted for different applications than the mathematical formalism of the generative models, and 3) we present a comprehensive evaluation and comparison of PBG to other topic extraction techniques. Here, we describe the strategy employed in PBG algorithm as a problem of maximization of similarity between latent vectors assigned to vertices and edges and demonstrate that the proposed strategy can be improved by assigning good initial values for the vectors. We notice that PBG can be parallelized by a simple adjustment in the algorithm. We also show that the proposed algorithm is competitive with LDA and NMF in the task of textual collection modelling, returning coherent topics, and in the dimensionality reduction task.},
journal = {Intell. Data Anal.},
month = jan,
pages = {543–565},
numpages = {23},
keywords = {Unsupervised learning, topic modelling, bipartite graph representation, dimensionality reduction, text mining}
}

@article{10.1016/j.patrec.2019.08.025,
author = {Happy, S L and Dantcheva, Antitza and Bremond, Francois},
title = {A Weakly Supervised learning technique for classifying facial expressions},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.08.025},
doi = {10.1016/j.patrec.2019.08.025},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {162–168},
numpages = {7},
keywords = {Weakly supervised learning, Facial expression recognition, Label smoothing, 41A05, 41A10, 65D05, 65D17}
}

@article{10.1016/j.knosys.2021.107474,
author = {Che, Feihu and Tao, Jianhua and Yang, Guohua and Liu, Tong and Zhang, Dawei},
title = {Multi-aspect self-supervised learning for heterogeneous information network▪},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107474},
doi = {10.1016/j.knosys.2021.107474},
journal = {Know.-Based Syst.},
month = dec,
numpages = {14},
keywords = {Heterogeneous information network, Self-supervised, Contrastive learning, Graph neural network}
}

@article{10.1016/j.cosrev.2021.100395,
author = {T.K., Balaji and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree},
title = {Machine learning algorithms for social media analysis: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100395},
doi = {10.1016/j.cosrev.2021.100395},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {32},
keywords = {Social Media, Machine learning, Social network analysis, Applications of social media analysis}
}

@phdthesis{10.5555/AAI28498593,
author = {Feng, Zhe and V., Conitzer, and Y., Chen, and S, Kominers,},
advisor = {D, Parkes,},
title = {Machine Learning-Aided Economic Design},
year = {2021},
isbn = {9798534671513},
publisher = {Harvard University},
address = {USA},
abstract = {Nowadays, online markets (e.g. online advertising market and online two-sided markets) grow larger and larger everyday. Designing an efficient and near-optimal market is an intricate task. Market designers are facing challenges not only in regard to scalability, but also coming from the use of data to better understand the behavior of strategic participants. At the same time, these participants are trying to understand how these markets work and to maximize reward. For these reasons, we continue to need improved frameworks for the design of online markets. One challenge for market design is to make effective use of data in order to design better markets. For the players, a central problem is how to optimize their strategy, adaptively learning from feedback and incorporating this along with other side information.To handle these challenges, my thesis focuses on two topics, Economic Design via Machine Learning and Learning in Online Markets. For the first topic, I propose a unified computational framework for data-driven mechanism design that can help a mechanism designer to automatically design a good mechanism to satisfy incentive constraints and achieve a desired objective (e.g. revenue, social welfare). I provide different approaches to guarantee Incentive Compatibility and prove the generalization bounds. This deep-learning framework is very general and can be extended to handle other constraints, e.g., private budget constraints. In addition, I investigate how to transform an approximately incentive compatible mechanism to a fully BIC mechanism without loss of welfare and with only negligible loss of revenue. For the second topic, I analyze the convergence of the outcome achieved by strategic bidders when they adopt mean-based learning algorithms to bid in repeated auctions. I also propose a new online learning algorithm for a bidder to use when bidding in repeated auctions, where the bidder's own value, evolving in an arbitrary manner, and observed only if the bidder wins an auction. This algorithm has exponentially faster convergence in terms of its dependence on the action space than the generic bandit algorithm.},
note = {AAI28498593}
}

@inproceedings{10.1007/978-3-030-89817-5_3,
author = {Guedes, Gustavo Bartz and da Silva, Ana Estela Antunes},
title = {Supervised Learning Approach for Section Title Detection in PDF Scientific Articles},
year = {2021},
isbn = {978-3-030-89816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89817-5_3},
doi = {10.1007/978-3-030-89817-5_3},
abstract = {The majority of scientific articles is available in Portable Document Format (PDF). Although PDF format has the advantage of preserving layout across platforms it does not maintain the original metadata structure, making it difficult further text processing. Despite different layouts, depending on the applied template, articles have a hierarchical structure and are divided into sections, which represent topics of specific subjects, such as methodology and results. Hence, section segmentation serves as an important step for a contextualized text processing of scientific articles. Therefore, this work applies binary classification, a supervised learning task, for section title detection in PDF scientific articles. To train the classifiers, a large dataset (more than 5 millions samples from 7,302 articles) was created through an automated feature extraction approach, comprised by 17 features, where 4 were introduced in this work. Training and testing were made for ten different classifiers for which the best F1 score reached 0.94. Finally, we evaluated our results against CERMINE, an open-source system that extracts metadata from scientific articles, having an absolute improvement in section detection of 0.19 in F1 score.},
booktitle = {Advances in Computational Intelligence: 20th Mexican International Conference on Artificial Intelligence, MICAI 2021, Mexico City, Mexico, October 25–30, 2021, Proceedings, Part I},
pages = {44–54},
numpages = {11},
keywords = {Scientific article segmentation, Section title detection, Text segmentation, Supervised learning}
}

@article{10.1504/ijict.2021.113039,
author = {Haiying, Wang},
title = {Machine learning method based on improved drosophila optimisation algorithm},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {2},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2021.113039},
doi = {10.1504/ijict.2021.113039},
abstract = {Aiming at the problems of poor classification effect and high CPU ratio of traditional machine learning methods, a machine learning method based on improved drosophila optimisation algorithm was proposed. The rank one data mapping and the low order data are established. In low rank support vector set, CP rank organisation of traditional support vector machine is used to improve data security. The traditional drosophila algorithm was improved and optimised to increase the number of data iterations, ensure the compatibility of rank one data, improve the optimal calculation of drosophila, and increase the density clustering. The decomposition process is designed to evaluate the objective function value of the optimal solution. In the evaluation process, support vector machine is used to complete the label classification of learning data. Experimental data show that this method performs well in data classification effect, low-rank data storage dimension characteristic performance and CPU operation proportion performance.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {142–159},
numpages = {17},
keywords = {machine learning, drosophila algorithm, low rank data, support vector machine}
}

@article{10.1016/j.neucom.2021.01.138,
author = {Mihaljevi\'{c}, Bojan and Bielza, Concha and Larra\~{n}aga, Pedro},
title = {Bayesian networks for interpretable machine learning and optimization},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {456},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.01.138},
doi = {10.1016/j.neucom.2021.01.138},
journal = {Neurocomput.},
month = oct,
pages = {648–665},
numpages = {18},
keywords = {Interpretability, Explainable machine learning, Probabilistic graphical models}
}

@article{10.1007/s11063-018-9794-8,
author = {Ngoc, Minh Tran and Park, Dong-Chul},
title = {Centroid Neural Network with Pairwise Constraints for Semi-supervised Learning},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-018-9794-8},
doi = {10.1007/s11063-018-9794-8},
abstract = {A clustering algorithm for datasets with pairwise constraints using the Centroid Neural Network (Cent.NN) is proposed in this paper. The proposed algorithm, referred to as the Centroid Neural Network with Pairwise Constraints (Cent. NN-PC) algorithm, utilizes Cent.NN as its backbone algorithm for data clustering and adopts a semi-supervised learning process for pairwise constraints. A newly formulated energy function is adopted from the original Cent.NN algorithm for the proposed Cent.NN-PC algorithm, introducing penalty terms for violating constraints. The weight update procedure of the proposed Cent.NN-PC algorithm finds optimal prototypes for the given dataset that minimize the quantization error while minimizing the number of violated constraints. In order to evaluate the performance of the proposed Cent.NN-PC algorithm, experiments on six different datasets from the UCI database and two bioinformatics datasets from the KEEL repository are carried out. The performance of the proposed algorithm is compared to that of the the Linear Constrained Vector Quantization Error (LCVQE) algorithm, one of the most commonly used algorithms for data clustering with pairwise constraints. In the experiments, five different numbers of pairwise constraints are utilized to evaluate the clustering performance with constraints of different sizes. The results show that the proposed Cent.NN-PC algorithm outperforms the LCVQE algorithm on most performance criteria, including the total quantization error, the number of violated constraints, and on the three performance metrics of the classification accuracy rate, F-score, and NMI measure outcome. The experiments also show that Cent.NN-PC provides much more stable clustering results at an improved operational speed compared to LCVQE.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1721–1747},
numpages = {27},
keywords = {Classification, Clustering, Learning algorithm, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-88007-1_11,
author = {Lin, Huibin and Wang, Shiping and Liu, Zhanghui and Xiao, Shunxin and Du, Shide and Guo, Wenzhong},
title = {FMixAugment for Semi-supervised Learning with Consistency Regularization},
year = {2021},
isbn = {978-3-030-88006-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88007-1_11},
doi = {10.1007/978-3-030-88007-1_11},
abstract = {Consistency regularization has witnessed tremendous success in the area of semi-supervised deep learning for image classification, which leverages data augmentation on unlabeled examples to encourage the model outputting the invariant predicted class distribution as before augmented. These methods have been made considerable progress in this area, but most of them are at the cost of utilizing more complex models. In this work, we propose a simple and efficient method FMixAugment, which combines the proposed MixAugment with Fourier space-based data masking and applies it on unlabeled examples to generate a strongly-augmented version. Our approach first generates a hard pseudo-label by employing a weakly-augmented version and minimizes the cross-entropy between it and the strongly-augmented version. Furthermore, to improve the robustness and uncertainty measurement of the model, we also enforce consistency constraints between the mixed augmented version and the weakly-augmented version. Ultimately, we introduce a dynamic growth of the confidence threshold for pseudo-labels. Extensive experiments are tested on CIFAR-10/100, SVHN, and STL-10 datasets, which indicate that our method outperforms the previous state-of-the-art methods. Specifically, with 40 labeled examples on CIFAR-10, we achieve 90.21% accuracy, and exceed 95% accuracy with 1000 labeled examples on STL-10.},
booktitle = {Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29 – November 1, 2021, Proceedings, Part II},
pages = {127–139},
numpages = {13},
keywords = {Semi-supervised learning, Image classification, Consistency regularization, Data augmentation},
location = {Beijing, China}
}

@inproceedings{10.1007/978-3-030-70866-5_11,
author = {Phillipson, Frank and Wezeman, Robert S. and Chiscop, Irina},
title = {Three Quantum Machine Learning Approaches for Mobile User Indoor-Outdoor Detection},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_11},
doi = {10.1007/978-3-030-70866-5_11},
abstract = {There is a growing trend in using machine learning techniques for detecting environmental context in communication networks. Machine learning is one of the promising candidate areas where quantum computing can show a quantum advantage over their classical algorithmic counterpart on near term Noisy Intermediate-Scale Quantum (NISQ) devices. The goal of this paper is to give a practical overview of (supervised) quantum machine learning techniques to be used for indoor-outdoor detection. Due to the small number of qubits in current quantum hardware, real application is not yet feasible. Our work is intended to be a starting point for further explorations of quantum machine learning techniques for indoor-outdoor detection.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {167–183},
numpages = {17},
keywords = {Quantum machine learning, Mobile devices, Indoor-outdoor detection, Hybrid quantum-classical, Variational quantum classifier, Quantum classification, Quantum SVM},
location = {Paris, France}
}

@article{10.1007/s00521-020-05609-9,
author = {Zheng, Xiaohan and Zhang, Li and Xu, Zhiqiang},
title = {L1-norm Laplacian support vector machine for data reduction in semi-supervised learning},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05609-9},
doi = {10.1007/s00521-020-05609-9},
abstract = {As a semi-supervised learning method, Laplacian support vector machine (LapSVM) is popular. Unfortunately, the model generated by LapSVM has a poor sparsity. A sparse decision model has always been fascinating because it could implement data reduction and improve performance. To generate a sparse model of LapSVM, we propose an ℓ1-norm Laplacian support vector machine (ℓ1-norm LapSVM), which replaces the ℓ2-norm with the ℓ1-norm in LapSVM. The ℓ1-norm LapSVM has two techniques that can induce sparsity: the ℓ1-norm regularization and the hinge loss function. We discuss two situations for the ℓ1-norm LapSVM, linear and nonlinear ones. In the linear ℓ1-norm LapSVM, the sparse decision model implies that features with nonzero coefficients are contributive. In other words, the linear ℓ1-norm LapSVM can perform feature selection to achieve the goal of data reduction. Moreover, the nonlinear (kernel) ℓ1-norm LapSVM can also implement data reduction in terms of sample selection. In addition, the optimization problem of the ℓ1-norm LapSVM is a convex quadratic programming one. That is, the ℓ1-norm LapSVM has a unique and global solution. Experimental results on semi-supervised classification tasks have shown a comparable performance of our ℓ1-norm LapSVM.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {12343–12360},
numpages = {18},
keywords = {Semi-supervised learning, Support vector machine, ℓ1-norm regularization, Laplacian regularization}
}

@article{10.1007/s10115-020-01439-2,
author = {Lyaqini, Soufiane and Quafafou, Mohamed and Nachaoui, Mourad and Chakib, Abdelkrim},
title = {Supervised learning as an inverse problem based on non-smooth loss function},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {8},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01439-2},
doi = {10.1007/s10115-020-01439-2},
abstract = {This paper is concerned by solving supervised machine learning problem as an inverse problem. Recently, many works have focused on defining a relationship between supervised learning and the well-known inverse problems. However, this connection between the learning problem and the inverse one has been done in the particular case where the inverse problem is reformulated as a minimization problem with a quadratic cost functional (L2 cost functional). Although, it is well known that the cost functional can be L1, L2 or any positive function that measures the gap between the predicted data and the observed one. Indeed, the use of L1 loss function for supervised learning problem gives more consistent results (see Rosasco et al. in Neural Comput 16:1063–1076, 2004). This strengthens the idea of reformulating the inverse problem, associated to machine learning problem, into a minimization problem using L1 functional. However, the L1 loss function is non-differentiable, which precludes the use of standard optimization tools. To overcome this difficulty, we propose in this paper a new technique of approximation based on the reformulation of the associated inverse problem into a minimizing one of a slanting cost functional Chen et al. (MIS Q Manag Inf Syst 36:1165–1188, 2012), which is solved using Tikhonov regularization and Newton’s method. This approach leads to an efficient numerical algorithm allowing us to solve supervised learning problem in the most general framework. To confirm this, we present some numerical results showing the efficiency of the proposed approach. Furthermore, the numerical experiment validation is made through academic and real-life data. Thus, the comparison with existing methods and numerical stability of the algorithm is presented in order to show that our approach is better in terms of convergence speed and quality of predicted models.},
journal = {Knowl. Inf. Syst.},
month = aug,
pages = {3039–3058},
numpages = {20},
keywords = {Inverse problem, Supervised learning, Non-smooth loss function, Optimization, Slanting function, Airfoil self-noise, ECG signals}
}

@inproceedings{10.1145/3380446.3430691,
author = {Khandelwal, Vishal},
title = {Machine-Learning Enabled Next-Generation Physical Design - An EDA Perspective},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430691},
doi = {10.1145/3380446.3430691},
abstract = {Physical design is an ensemble of NP-complete problems that P&amp;R tools attempt to solve in (pseudo) linear time. Advanced process nodes and complex signoff requirements bring in new physical and timing constraints into the implementation flow, making it harder for physical design algorithms to deliver industry-leading power, performance, area (PPA), without giving up design turn-around-time. The relentless pursuit for low-power high-performance designs is putting constant pressure to limit any over-design, creating an acute need to have better models/predictions and advanced analytics to drive implementation flows. Given the advancements in supervised and reinforcement learning, combined with the availability of large-scale compute, Machine Learning (ML) has the potential to become a disruptive paradigm change for EDA tools. In this talk, I would like to share some of the challenges and opportunities for innovation in next-generation physical design using ML.Biography: Vishal leads the physical optimization team for the Digital Implementation products at Synopsys. He has 15 years of R&amp;D experience in building state-of-the-art optimization engines and P&amp;R flows targeting advanced-node low-power high-performance designs. More recently, he has been looking at bringing machine-learning paradigms into digital implementation tools to improve power, performance, area and productivity. Vishal has a B.Tech. from Indian Institute of Technology, Kanpur and a Ph.D. from University of Maryland, College Park. He has won a best paper award at ISPD, co-authored several patents and over 20 IEEE/ACM publications.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {135},
numpages = {1},
keywords = {PPA, machine learning, physical design},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3448016.3457295,
author = {Neutatz, Felix and Biessmann, Felix and Abedjan, Ziawasch},
title = {Enforcing Constraints for Machine Learning Systems via Declarative Feature Selection: An Experimental Study},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457295},
doi = {10.1145/3448016.3457295},
abstract = {Responsible usage of Machine Learning (ML) systems in practice does not only require enforcing high prediction quality, but also accounting for other constraints, such as fairness, privacy, or execution time. One way to address multiple user-specified constraints on ML systems is feature selection. Yet, optimizing feature selection strategies for multiple metrics is difficult to implement and has been underrepresented in previous experimental studies. Here, we propose Declarative Feature Selection (DFS) to simplify the design and validation of ML systems satisfying diverse user-specified constraints. We benchmark and evaluate a representative series of feature selection algorithms. From our extensive experimental results, we derive concrete suggestions on when to use which strategy and show that a meta-learning-driven optimizer can accurately predict the right strategy for an ML task at hand. These results demonstrate that feature selection can help to build ML systems that meet combinations of user-specified constraints, independent of the ML methods used.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1345–1358},
numpages = {14},
keywords = {DFS, bias, declarative feature selection, declarative machine learning, declarative ml, fairness, feature selection, machine learning, meta learning, privacy, robustness},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3383972.3384042,
author = {Pree, Wolfgang and Hoerbinger, Felix},
title = {Applying Machine Learning to a Conventional Data Processing Task: A Quantitative Evaluation},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384042},
doi = {10.1145/3383972.3384042},
abstract = {Though machine learning (ML) can be applied to a wide spectrum of applications, it has been hardly used and evaluated in the context of conventional data processing tasks. Such conventional data processing tasks are characterized by a set of calculations that follow strict rules, such as in accounting or banking applications. This paper quantitatively evaluates how software which is automatically generated by ML methods and tools compares to software programmed by hand. The assessment of poker hands according to Texas Hold'em rules is a representative example for conventional data processing tasks, because of the various exceptions how to assess and compare hands. For some hand values, the rank (two, three, ... king, ace) of the cards is relevant and the suit (club, diamond, heart, spade) irrelevant, and vice versa. This paper shows how an accuracy of 100% can be achieved for assessing poker hands according to Texas Hold'em rules, with a small set of labeled training data compared to the number of possible hands. We also evaluate quantitatively the effect of the labeling quality on accuracy.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {111–115},
numpages = {5},
keywords = {Machine learning, convolutional neural networks, data labeling, feed-forward neural networks, labeling quality, neural networks, robustness, supervised learning},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1007/978-3-030-87240-3_4,
author = {Zheng, Kang and Wang, Yirui and Zhou, Xiao-Yun and Wang, Fakai and Lu, Le and Lin, Chihung and Huang, Lingyun and Xie, Guotong and Xiao, Jing and Kuo, Chang-Fu and Miao, Shun},
title = {Semi-supervised Learning for Bone Mineral Density Estimation in Hip X-Ray Images},
year = {2021},
isbn = {978-3-030-87239-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87240-3_4},
doi = {10.1007/978-3-030-87240-3_4},
abstract = {Bone mineral density (BMD) is a clinically critical indicator of osteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due to the limited accessibility of DEXA machines and examinations, osteoporosis is often under-diagnosed and under-treated, leading to increased fragility fracture risks. Thus it is highly desirable to obtain BMDs with alternative cost-effective and more accessible medical imaging examinations such as X-ray plain films. In this work, we formulate the BMD estimation from plain hip X-ray images as a regression problem. Specifically, we propose a new semi-supervised self-training algorithm to train a BMD regression model using images coupled with DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are generated and refined iteratively for unlabeled images during self-training. We also present a novel adaptive triplet loss to improve the model’s regression accuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD estimation method achieves a high Pearson correlation coefficient of 0.8805 to ground-truth BMDs. It offers good feasibility to use the more accessible and cheaper X-ray imaging for opportunistic osteoporosis screening.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part V},
pages = {33–42},
numpages = {10},
keywords = {Bone mineral density estimation, Hip X-ray, Semi-supervised learning},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-59725-2_37,
author = {Xu, Junshen and Lala, Sayeri and Gagoski, Borjan and Abaci Turk, Esra and Grant, P. Ellen and Golland, Polina and Adalsteinsson, Elfar},
title = {Semi-supervised Learning for Fetal Brain MRI Quality Assessment with ROI Consistency},
year = {2020},
isbn = {978-3-030-59724-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59725-2_37},
doi = {10.1007/978-3-030-59725-2_37},
abstract = {Fetal brain MRI is useful for diagnosing brain abnormalities but is challenged by fetal motion. The current protocol for T2-weighted fetal brain MRI is not robust to motion so image volumes are degraded by inter- and intra- slice motion artifacts. Besides, manual annotation for fetal MR image quality assessment are usually time-consuming. Therefore, in this work, a semi-supervised deep learning method that detects slices with artifacts during the brain volume scan is proposed. Our method is based on the mean teacher model, where we not only enforce consistency between student and teacher models on the whole image, but also adopt an ROI consistency loss to guide the network to focus on the brain region. The proposed method is evaluated on a fetal brain MR dataset with 11,223 labeled images and more than 200,000 unlabeled images. Results show that compared with supervised learning, the proposed method can improve model accuracy by about 6% and outperform other state-of-the-art semi-supervised learning methods. The proposed method is also implemented and evaluated on an MR scanner, which demonstrates the feasibility of online image quality assessment and image reacquisition during fetal MR scans.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI},
pages = {386–395},
numpages = {10},
keywords = {Image quality assessment, Fetal magnetic resonance imaging (MRI), Semi-supervised learning, Convolutional neural network (CNN)},
location = {Lima, Peru}
}

@article{10.1016/j.eswa.2021.114767,
author = {Kim, Misuk},
title = {Adaptive trading system integrating machine learning and back-testing: Korean bond market case},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114767},
doi = {10.1016/j.eswa.2021.114767},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {10},
keywords = {Korean treasury bond market, Treasury bond spread prediction, Treasury futures trading, Back-testing, Machine learning}
}

@inproceedings{10.1007/978-3-030-73280-6_12,
author = {Bregu, Ornela and Zamzami, Nuha and Bouguila, Nizar},
title = {Mixture-Based Unsupervised Learning for Positively Correlated Count Data},
year = {2021},
isbn = {978-3-030-73279-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73280-6_12},
doi = {10.1007/978-3-030-73280-6_12},
abstract = {The Multinomial distribution has been widely used to model count data. However, its Naive Bayes assumption usually degrades clustering performance especially when correlation between features is imminent, i.e., text documents. In this paper, we use the Negative Multinomial distribution to perform clustering based on finite mixture models, where the mixture parameters are to be estimated using a novel minorization-maximization algorithm, thriving in high-dimensionality optimization settings. Furthermore, we integrate a model-based feature selection approach to determine the optimal number of components in the mixture. To evaluate the clustering performance of the proposed model, three real-world applications are considered, namely, COVID-19 analysis, Web page clustering and facial expression recognition.},
booktitle = {Intelligent Information and Database Systems: 13th Asian Conference, ACIIDS 2021, Phuket, Thailand, April 7–10, 2021, Proceedings},
pages = {144–154},
numpages = {11},
keywords = {Negative multinomial distribution, Positive correlation, Minorization-maximization, Minimum message length, Overdispersion},
location = {Phuket, Thailand}
}

@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {bias, fairness, feature selection, machine learning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1177/10943420211029302,
author = {Germann, Tim and Alexander, Francis J and Ang, James and Bilbrey, Jenna A and Balewski, Jan and Casey, Tiernan and Chard, Ryan and Choi, Jong and Choudhury, Sutanay and Debusschere, Bert and DeGennaro, Anthony M and Dryden, Nikoli and Ellis, J Austin and Foster, Ian and Cardona, Cristina Garcia and Ghosh, Sayan and Harrington, Peter and Huang, Yunzhi and Jha, Shantenu and Johnston, Travis and Kagawa, Ai and Kannan, Ramakrishnan and Kumar, Neeraj and Liu, Zhengchun and Maruyama, Naoya and Matsuoka, Satoshi and McCarthy, Erin and Mohd-Yusof, Jamaludin and Nugent, Peter and Oyama, Yosuke and Proffen, Thomas and Pugmire, David and Rajamanickam, Sivasankaran and Ramakrishniah, Vinay and Schram, Malachi and Seal, Sudip K and Sivaraman, Ganesh and Sweeney, Christine and Tan, Li and Thakur, Rajeev and Van Essen, Brian and Ward, Logan and Welch, Paul and Wolf, Michael and Xantheas, Sotiris S and Yager, Kevin G and Yoo, Shinjae and Yoon, Byung-Jun},
title = {Co-design Center for Exascale Machine Learning Technologies (ExaLearn)},
year = {2021},
issue_date = {Nov 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {35},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/10943420211029302},
doi = {10.1177/10943420211029302},
abstract = {Rapid growth in data, computational methods, and computing power is driving a remarkable revolution in what variously is termed machine learning (ML), statistical learning, computational learning, and artificial intelligence. In addition to highly visible successes in machine-based natural language translation, playing the game Go, and self-driving cars, these new technologies also have profound implications for computational and experimental science and engineering, as well as for the exascale computing systems that the Department of Energy (DOE) is developing to support those disciplines. Not only do these learning technologies open up exciting opportunities for scientific discovery on exascale systems, they also appear poised to have important implications for the design and use of exascale computers themselves, including high-performance computing (HPC) for ML and ML for HPC. The overarching goal of the ExaLearn co-design project is to provide exascale ML software for use by Exascale Computing Project (ECP) applications, other ECP co-design centers, and DOE experimental facilities and leadership class computing facilities.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {598–616},
numpages = {19},
keywords = {Machine learning, exascale computing, reinforcement learning, active learning, high-performance computing for machine learning, machine learning for high-performance computing}
}

@article{10.1007/s10994-021-05975-y,
author = {Liang, Jiye and Cui, Junbiao and Wang, Jie and Wei, Wei},
title = {Graph-based semi-supervised learning via improving the quality of the graph dynamically},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {6},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-05975-y},
doi = {10.1007/s10994-021-05975-y},
abstract = {Graph-based semi-supervised learning (GSSL) is an important paradigm among semi-supervised learning approaches and includes the two processes of graph construction and label inference. In most traditional GSSL methods, the two processes are completed independently. Once the graph is constructed, the result of label inference cannot be changed. Therefore, the quality of the graph directly determines the GSSL’s performance. Most traditional graph construction methods make certain assumptions about the data distribution, resulting in the quality of the graph heavily depends on the correctness of these assumptions. Therefore, it is difficult to handle complex and various data distribution for traditional graph construction methods. To overcome such issues, this paper proposes a framework named Graph-based Semi-supervised Learning via Improving the Quality of the Graph Dynamically. In it, the graph construction based on the weighted fusion of multiple clustering results and the label inference are integrated into a unified framework to achieve their mutual guidance and dynamic improvement. Moreover, the proposed framework is a general framework, and most existing GSSL methods can be embedded into it so as to improve their performance. Finally, the working mechanism, the effectiveness in improving the performance of GSSL methods and the advantage compared with other GSSL methods based on dynamic graph construction methods of the proposal are verified through systematic experiments.},
journal = {Mach. Learn.},
month = jun,
pages = {1345–1388},
numpages = {44},
keywords = {Semi-supervised learning, Graph construction, Clustering, Label inference}
}

@inproceedings{10.1007/978-3-030-45231-5_1,
author = {Mehmood, Usama and Roy, Shouvik and Grosu, Radu and Smolka, Scott A. and Stoller, Scott D. and Tiwari, Ashish},
title = {Neural Flocking: MPC-Based Supervised Learning of Flocking Controllers},
year = {2020},
isbn = {978-3-030-45230-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45231-5_1},
doi = {10.1007/978-3-030-45231-5_1},
abstract = {We show how a symmetric and fully distributed flocking controller can be synthesized using Deep Learning from a centralized flocking controller. Our approach is based on Supervised Learning, with the centralized controller providing the training data, in the form of trajectories of state-action pairs. We use Model Predictive Control (MPC) for the centralized controller, an approach that we have successfully demonstrated on flocking problems. MPC-based flocking controllers are high-performing but also computationally expensive. By learning a symmetric and distributed neural flocking controller from a centralized MPC-based one, we achieve the best of both worlds: the neural controllers have high performance (on par with the MPC controllers) and high efficiency. Our experimental results demonstrate the sophisticated nature of the distributed controllers we learn. In particular, the neural controllers are capable of achieving myriad flocking-oriented control objectives, including flocking formation, collision avoidance, obstacle avoidance, predator avoidance, and target seeking. Moreover, they generalize the behavior seen in the training data to achieve these objectives in a significantly broader range of scenarios. In terms of verification of our neural flocking controller, we use a form of statistical model checking to compute confidence intervals for its convergence rate and time to convergence.},
booktitle = {Foundations of Software Science and Computation Structures: 23rd International Conference, FOSSACS 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {1–16},
numpages = {16},
keywords = {Flocking, Model Predictive Control, Distributed Neural Controller, Deep Neural Network, Supervised Learning},
location = {Dublin, Ireland}
}

@article{10.1287/trsc.2021.1045,
author = {Morabit, Mouad and Desaulniers, Guy and Lodi, Andrea},
title = {Machine-Learning–Based Column Selection for Column Generation},
year = {2021},
issue_date = {July-August 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {4},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2021.1045},
doi = {10.1287/trsc.2021.1045},
abstract = {Column generation (CG) is widely used for solving large-scale optimization problems. This article presents a new approach based on a machine learning (ML) technique to accelerate CG. This approach, called column selection, applies a learned model to select a subset of the variables (columns) generated at each iteration of CG. The goal is to reduce the computing time spent reoptimizing the restricted master problem at each iteration by selecting the most promising columns. The effectiveness of the approach is demonstrated on two problems: the vehicle and crew scheduling problem and the vehicle routing problem with time windows. The ML model was able to generalize to instances of different sizes, yielding a gain in computing time of up to 30%.},
journal = {Transportation Science},
month = jul,
pages = {815–831},
numpages = {17},
keywords = {column generation, machine learning, column selection}
}

@article{10.1016/j.knosys.2021.107340,
author = {Zhang, Sheng and Chen, Min and Chen, Jincai and Li, Yuan-Fang and Wu, Yiling and Li, Minglei and Zhu, Chuanbo},
title = {Combining cross-modal knowledge transfer and semi-supervised learning for speech emotion recognition},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {229},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107340},
doi = {10.1016/j.knosys.2021.107340},
journal = {Know.-Based Syst.},
month = oct,
numpages = {10},
keywords = {Semi-supervised learning, Cross-modal knowledge transfer, Speech emotion recognition}
}

@article{10.1007/s42979-021-00592-x,
author = {Sarker, Iqbal H.},
title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00592-x},
doi = {10.1007/s42979-021-00592-x},
abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated&nbsp;applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity&nbsp;systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers&nbsp;in various real-world situations and&nbsp;application areas, particularly from the technical point of view.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {21},
keywords = {Machine learning, Deep learning, Artificial intelligence, Data science, Data-driven decision-making, Predictive analytics, Intelligent applications}
}

@article{10.1016/j.cageo.2021.104696,
author = {Charifi, Rajaa and Es-sbai, Najia and Zennayi, Yahya and Hosni, Taha and Bourzeix, Fran\c{c}ois and Mansouri, Anass},
title = {Sedimentary phosphate classification based on spectral analysis and machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2021.104696},
doi = {10.1016/j.cageo.2021.104696},
journal = {Comput. Geosci.},
month = may,
numpages = {16},
keywords = {Phosphate, Classification, Spectral analysis, Feature selection, Bhattacharyya distance, Machine learning}
}

@article{10.1145/3451179,
author = {Huang, Guyue and Hu, Jingbo and He, Yifan and Liu, Jialong and Ma, Mingyuan and Shen, Zhaoyang and Wu, Juejian and Xu, Yuanfan and Zhang, Hengrui and Zhong, Kai and Ning, Xuefei and Ma, Yuzhe and Yang, Haoyu and Yu, Bei and Yang, Huazhong and Wang, Yu},
title = {Machine Learning for Electronic Design Automation: A Survey},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3451179},
doi = {10.1145/3451179},
abstract = {With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {40},
numpages = {46},
keywords = {Electronic design automation, machine learning, neural networks}
}

@inproceedings{10.5555/3295222.3295276,
author = {Tung, Hsiao-Yu Fish and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina},
title = {Self-supervised learning of motion capture},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5242–5252},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.1145/3479239.3485672,
author = {Garc\'{\i}a Mart\'{\i}, Dolores and Badini, Damiano and De Donno, Danilo and Widmer, Joerg},
title = {Scalable Machine Learning Algorithms to Design Massive MIMO Systems},
year = {2021},
isbn = {9781450390774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479239.3485672},
doi = {10.1145/3479239.3485672},
abstract = {Machine learning is a highly promising tool to design the physical layer of wireless communication systems, but its scaling properties for this purpose have not been widely studied. Machine learning algorithms are typically evaluated to learn SISO communications and low modulation orders, whereas current wireless standards use MIMO and high-order modulation schemes to increase capacity. The memory requirements of current Machine learning algorithms for wireless communications increase exponentially with the number of antennas and thus they cannot be used for advanced physical layers and massive MIMO. In this paper, we study the requirements of end-to-end Machine learning models for large-scale MIMO systems, determine the bottlenecks of the architecture, and design different solutions that vastly reduce overhead and allow training higher MIMO and modulation orders. We show that by training the autoencoder in a bit-wise manner, the memory requirements are reduced by several orders of magnitude, which is a critical step for Machine learning-based physical layer design in practical scenarios. Additionally, our design also improves performance over the classical autoencoder for MIMO.},
booktitle = {Proceedings of the 24th International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {167–171},
numpages = {5},
keywords = {MIMO, machine learning, neural networks, physical layer},
location = {Alicante, Spain},
series = {MSWiM '21}
}

@article{10.1007/s00530-020-00733-x,
author = {Lou, Ranran and Lv, Zhihan and Dang, Shuping and Su, Tianyun and Li, Xinfang},
title = {Application of machine learning in ocean data},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {3},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-020-00733-x},
doi = {10.1007/s00530-020-00733-x},
abstract = {In recent years, machine learning has become a hot research method in various fields and has been applied to every aspect of our life, providing an intelligent solution to problems that could not be solved or difficult to be solved before. Machine learning is driven by data. It learns from a part of the input data and builds a model. The model is used to predict and analyze another part of the data to get the results people want. With the continuous advancement of ocean observation technology, the amount of ocean data and data dimensions are rising sharply. The use of traditional data analysis methods to analyze massive amounts of data has revealed many shortcomings. The development of machine learning has solved these shortcomings. Nowadays, the use of machine learning technology to analyze and apply ocean data becomes the focus of scientific research. This method has important practical and long-term significance for protecting the ocean environment, predicting ocean elements, exploring the unknown, and responding to extreme weather. This paper focuses on the analysis of the state of the art and specific practices of machine learning in ocean data, review the application examples of machine learning in various fields such as ocean sound source identification and positioning, ocean element prediction, ocean biodiversity monitoring, and deep-sea resource monitoring. We also point out some constraints that still exist in the research and put forward the future development direction and application prospects.},
journal = {Multimedia Syst.},
month = feb,
pages = {1815–1824},
numpages = {10},
keywords = {Ocean, Data, Ocean data, Machine learning}
}

@article{10.5555/3455716.3455925,
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R\"{a}tsch, Gunnar and Gelly, Sylvain and Sch\"{o}lkopf, Bernhard and Bachem, Olivier},
title = {A sober look at the unsupervised learning of disentangled representations and their evaluation},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {The idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train over 14 000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on eight data sets. We observe that while the different methods successfully enforce properties "encouraged" by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, different evaluation metrics do not always agree on what should be considered "disentangled" and exhibit systematic differences in the estimation. Finally, increased disentanglement does not seem to necessarily lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {209},
numpages = {62},
keywords = {disentangled representations, impossibility, evaluation, reproducibility, large scale experimental study}
}

@article{10.1145/3451163,
author = {Abououf, Menatalla and Singh, Shakti and Otrok, Hadi and Mizouni, Rabeb and Damiani, Ernesto},
title = {Machine Learning in Mobile Crowd Sourcing: A Behavior-Based Recruitment Model},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3451163},
doi = {10.1145/3451163},
abstract = {With the advent of mobile crowd sourcing (MCS) systems and its applications, the selection of the right crowd is gaining utmost importance. The increasing variability in the context of MCS tasks makes the selection of not only the capable but also the willing workers crucial for a high task completion rate. Most of the existing MCS selection frameworks rely primarily on reputation-based feedback mechanisms to assess the level of commitment of potential workers. Such frameworks select workers having high reputation scores but without any contextual awareness of the workers, at the time of selection, or the task. This may lead to an unfair selection of workers who will not perform the task. Hence, reputation on its own only gives an approximation of workers’ behaviors since it assumes that workers always behave consistently regardless of the situational context. However, following the concept of cross-situational consistency, where people tend to show similar behavior in similar situations and behave differently in disparate ones, this work proposes a novel recruitment system in MCS based on behavioral profiling. The proposed approach uses machine learning to predict the probability of the workers performing a given task, based on their learned behavioral models. Subsequently, a group-based selection mechanism, based on the genetic algorithm, uses these behavioral models in complementation with a reputation-based model to recruit a group of workers that maximizes the quality of recruitment of the tasks. Simulations based on a real-life dataset show that considering human behavior in varying situations improves the quality of recruitment achieved by the tasks and their completion confidence when compared with a benchmark that relies solely on reputation.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {16},
numpages = {28},
keywords = {Machine learning, behavioral profiling, mobile crowd sourcing, selection management, quality of recruitment}
}

@inproceedings{10.1145/3368089.3418780,
author = {Gisi, Joshua},
title = {Synthesizing correct code for machine learning programs},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418780},
doi = {10.1145/3368089.3418780},
abstract = {Success using machine learning (ML) in numerous fields has created a new class of users, who are not experts in the data science domain but want to use ML as a means to solve their inference problems. Various automatic machine learning (AutoML) approaches attempt to make ML solutions accessible to such users. In this work, we present a system that automatically synthesizes correct code within the context of the user’s data using sketching. In sketching, insight is determined through a partial program; a sketch expresses the high-level structure of implementation but leaves holes in place of the low-level details. We use meta-learning on meta-features to approximately solve holes. We observe that the sketch-based approach is more expressive, easier to implement, and easier to optimize than existing AutoML frameworks. Our initial results are very promising. Our approach uses fewer resources and still produces comparable results to existing techniques.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1701–1703},
numpages = {3},
keywords = {Machine Learning, Program Synthesis, Sketch, autoML},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inbook{10.5555/3454287.3455069,
author = {Stretcu, Otilia and Viswanathan, Krishnamurthy and Movshovitz-Attias, Dana and Platanios, Emmanouil Antonios and Tomkins, Andrew and Ravi, Sujith},
title = {Graph agreement models for semi-supervised learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Graph-based algorithms are among the most successful paradigms for solving semi-supervised learning tasks. Recent work on graph convolutional networks and neural graph learning methods has successfully combined the expressiveness of neural networks with graph structures. We propose a technique that, when applied to these methods, achieves state-of-the-art results on semi-supervised learning datasets. Traditional graph-based algorithms, such as label propagation, were designed with the underlying assumption that the label of a node can be imputed from that of the neighboring nodes. However, real-world graphs are either noisy or have edges that do not correspond to label agreement. To address this, we propose Graph Agreement Models (GAM), which introduces an auxiliary model that predicts the probability of two nodes sharing the same label as a learned function of their features. The agreement model is used when training a node classification model by encouraging agreement only for the pairs of nodes it deems likely to have the same label, thus guiding its parameters to better local optima. The classification and agreement models are trained jointly in a co-training fashion. Moreover, GAM can also be applied to any semi-supervised classification problem, by inducing a graph whenever one is not provided. We demonstrate that our method achieves a relative improvement of up to 72% for various node classification models, and obtains state-of-the-art results on multiple established datasets.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {782},
numpages = {11}
}

@phdthesis{10.5555/AAI28722564,
author = {Lim, Robert and Boyana, Norris, and Dejing, Dou, and Camille, Coti, and William, Cresko,},
advisor = {Allen, Malony,},
title = {Accelerating Machine Learning via Multi-Objective Optimization},
year = {2021},
isbn = {9798492727826},
publisher = {University of Oregon},
address = {USA},
abstract = {This dissertation work presents various approaches toward accelerating training of deep neural networks with the use of high-performance computing resources, while balancing learning and systems utilization objectives.  Acceleration of machine learning is formulated as a multi-objective optimization problem that seeks to satisfy multiple objectives, based on its respective constraints.  In machine learning, the objective is to strive for a model that has high accuracy, while eliminating false positives and generalizing beyond the training set.  For systems execution performance, maximizing utilization of the underlying hardware resources within compute and power budgets are constraints that bound the problem.  In both scenarios, the search space is combinatorial and contains multiple local minima that in many cases satisfies the global optimum.  This dissertation work addresses the search of solutions in both performance tuning and neural network training.  Specifically, subgraph matching is proposed to bound the search problem and provide heuristics that guide the solver toward the optimal solution.  Mixed precision operations is also proposed for solving systems of linear equations and for training neural networks for image classification for evaluating the stability and robustness of the operations.  Use cases are presented with CUDA performance tuning and neural network training, demonstrating the effectiveness of the proposed technique.  The experiments were carried out on single and multi-node GPU clusters, and reveals opportunities for further exploration in this critical hardware/software co-design space of accelerated machine learning.},
note = {AAI28722564}
}

@phdthesis{10.5555/AAI28644688,
author = {Zhang, Clark and Pratik, Chaudhari, and Dinesh, Jayaraman, and Szymon, Jakubczak,},
advisor = {Alejandro, Ribeiro,},
title = {Machine Learning for Robot Motion Planning},
year = {2021},
isbn = {9798535569383},
publisher = {University of Pennsylvania},
address = {USA},
abstract = {Robot motion planning is a field that encompasses many different problems and algorithms. From the traditional piano mover's problem to more complicated kinodynamic planning problems, motion planning requires a broad breadth of human expertise and time to design well functioning algorithms. A traditional motion planning pipeline consists of modeling a system and then designing a planner and planning heuristics. Each part of this pipeline can incorporate machine learning. Planners and planning heuristics can benefit from machine learned heuristics, while system modeling can benefit from model learning. Each aspect of the motion planning pipeline comes with trade offs between computational effort and human effort. This work explores algorithms that allow motion planning algorithms and frameworks to find a compromise between the two. First, a framework for learning heuristics for sampling-based planners is presented. The efficacy of the framework depends on human designed features and policy architecture. Next, a framework for learning system models is presented that incorporates human knowledge as constraints. The amount of human effort can be modulated by the quality of the constraints given. Lastly, semi-automatic constraint generation is explored to enable a larger range of trade-offs between human expert constraint generation and data driven constraint generation. We apply these techniques and show results in a variety of robotic systems.},
note = {AAI28644688}
}

@inproceedings{10.1145/3461702.3462521,
author = {Dai, Jessica and Fazelpour, Sina and Lipton, Zachary},
title = {Fair Machine Learning Under Partial Compliance},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462521},
doi = {10.1145/3461702.3462521},
abstract = {Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {55–65},
numpages = {11},
keywords = {distributive justice, fair machine learning, fairness, hiring, regulation, segregation, simulations},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1145/3418034,
author = {Cummings, Mary L. and Li, Songpo},
title = {Subjectivity in the Creation of Machine Learning Models},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3418034},
doi = {10.1145/3418034},
abstract = {Transportation analysts are inundated with requests to apply popular machine learning modeling techniques to datasets to uncover never-before-seen relationships that could potentially revolutionize safety, congestion, and mobility. However, the results from such models can be influenced not just by biases in underlying data, but also through practitioner-induced biases. To demonstrate the significant number of subjective judgments made in the development and interpretation of machine learning models, we developed Logistic Regression and Neural Network models for transportation-focused datasets including those looking at driving injury/fatalities and pedestrian fatalities. We then developed five different representations of feature importance for each dataset, including different feature interpretations commonly used in the machine learning community. Twelve distinct judgments were highlighted in the development and interpretation of these models, which produced inconsistent results. Such inconsistencies can lead to very different interpretations of the results, which can lead to errors of commission and omission, with significant cost and safety implications if policies are erroneously adapted from such outcomes.},
journal = {J. Data and Information Quality},
month = may,
articleno = {7},
numpages = {19},
keywords = {Bias, interpretable machine learning, transportation, logistic regression, subjectivity}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3447548.3469451,
author = {Katariya, Sumeet and Rao, Nikhil and Reddy, Chandan K.},
title = {Workshop on Data-Efficient Machine Learning (DeMaL)},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469451},
doi = {10.1145/3447548.3469451},
abstract = {The recent increase in the size of neural networks has led to a proportional increase in the demands for high-quality human-annotated data. Labeling data is a costly and time-consuming endeavor, and the need for large data is often satiated through creative techniques such as data augmentation, transfer learning, self-supervised learning, active learning, to name a few. Many of these techniques are designed for specific data types such as images, text, and speech. The data in many data-mining applications however is multi-modal in nature, has implicit signals from user-interactions, and involves multiple agents. Given the uniqueness, importance, and growing interest in these problems, we feel that the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD) 2021 is an appropriate venue for running a workshop on Data-efficient Machine Learning. In this proposal, we discuss our vision for this workshop.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4135–4136},
numpages = {2},
keywords = {active learning, crowdsourcing, data augmentation, self-supervised, semi-supervised, unsupervised},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1007/978-3-030-26142-9_6,
author = {Wu, William},
title = {Weakly Supervised Learning by a Confusion Matrix of Contexts},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_6},
doi = {10.1007/978-3-030-26142-9_6},
abstract = {Context consideration can help provide more background and related information for weakly supervised learning. The inclusion of less documented historical and environmental context in researching diabetes amongst Pima Indians uncovered reasons which were more likely to explain why some Pima Indians had much higher rates of diabetes than Caucasians, primarily due to historical, environmental and social causes rather than their specific genetic patterns or ethnicity as suggested by many medical studies.If historical and environmental factors are considered as external contexts when not included as part of a dataset for research, some forms of internal contexts may also exist inside the dataset without being declared. This paper discusses a context construction model that transforms a confusion matrix into a matrix of categorical, incremental and correlational context to emulate a kind of internal context to search for more informative patterns in order to improve weakly supervised learning from limited labeled samples for unlabeled data.When the negative and positive labeled samples and misclassification errors are compared to “happy families” and “unhappy families”, the contexts constructed by this model in the classification experiments reflected the Anna Karenina principle well - “Happy families are all alike; every unhappy family is unhappy in its own way”, an encouraging sign to further explore contexts associated with harmonizing patterns and divisive causes for knowledge discovery in a world of uncertainty.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {59–64},
numpages = {6},
keywords = {Weakly supervised learning, Context, Confusion matrix, Context construction, Contextual analysis},
location = {Macau, China}
}

@article{10.1145/3436755,
author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
title = {When Machine Learning Meets Privacy: A Survey and Outlook},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436755},
doi = {10.1145/3436755},
abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {31},
numpages = {36},
keywords = {Machine learning, deep learning, differential privacy, privacy}
}

@inproceedings{10.1007/978-3-030-87231-1_12,
author = {Cheng, Kai and Ma, Yiting and Sun, Bin and Li, Yang and Chen, Xuejin},
title = {Depth Estimation for Colonoscopy Images with Self-supervised Learning from Videos},
year = {2021},
isbn = {978-3-030-87230-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87231-1_12},
doi = {10.1007/978-3-030-87231-1_12},
abstract = {Depth estimation in colonoscopy images provides geometric clues for downstream medical analysis tasks, such as polyp detection, 3D reconstruction, and diagnosis. Recently, deep learning technology has made significant progress in monocular depth estimation for natural scenes. However, without sufficient ground truth of dense depth maps for colonoscopy images, it is significantly challenging to train deep neural networks for colonoscopy depth estimation. In this paper, we propose a novel approach that makes full use of both synthetic data and real colonoscopy videos. We use synthetic data with ground truth depth maps to train a depth estimation network with a generative adversarial network model. Despite the lack of ground truth depth, real colonoscopy videos are used to train the network in a self-supervision manner by exploiting temporal consistency between neighboring frames. Furthermore, we design a masked gradient warping loss in order to ensure temporal consistency with more reliable correspondences. We conducted both quantitative and qualitative analysis on an existing synthetic dataset and a set of real colonoscopy videos, demonstrating the superiority of our method on more accurate and consistent depth estimation for colonoscopy images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VI},
pages = {119–128},
numpages = {10},
keywords = {Colonoscopy, Depth estimation, Self-supervised learning, Videos, Temporal consistency},
location = {Strasbourg, France}
}

@article{10.1504/ijwgs.2021.118395,
author = {Mousavi, Mitra and Rezazadeh, Javad and Sianaki, Omid Ameri},
title = {Machine learning applications for fog computing in IoT: a survey},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {17},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2021.118395},
doi = {10.1504/ijwgs.2021.118395},
abstract = {Today, internet of things (IoT) has become an important paradigm. Everyday increasing number of IoT applications and services emerge. Smart devices connected by the IoT generate significant amounts of data. Analysis IoT sensor data using machine learning algorithms is a key to achieve useful information for prediction, classification, data association and data conceptualisation. Offloading input data to cloud servers leads to increased communication costs. Undertaking data analytics at the network edge using fog computing enables the rapid processing of incoming data for real-time response. In this paper, we examine the results of using different machine learning algorithms on fog nodes based on existing research. These results are low latency, high accuracy and low bandwidth. Also, this work presents the current fog computing architecture which consists of different layers that distribute computing, storage, control and networking and finally we investigate the challenges and open issues related to the deployment of machine learning on fog nodes.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {293–320},
numpages = {27},
keywords = {internet of things, IoT, fog computing, machine learning, fog-based machine learning}
}

@inproceedings{10.1007/978-3-030-88052-1_2,
author = {Wang, Gao and Wang, Gaoli},
title = {Improved Differential-ML Distinguisher: Machine Learning Based Generic Extension for Differential Analysis},
year = {2021},
isbn = {978-3-030-88051-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88052-1_2},
doi = {10.1007/978-3-030-88052-1_2},
abstract = {At CRYPTO 2019, Gohr first proposes a deep learning based differential analysis on round-reduced Speck32/64. Then Yadav etal. present a framework to construct the differential-ML (machine learning) distinguisher by combining the traditional differential distinguisher and the machine learning based differential distinguisher, which breaks the limit of the ML differential distinguisher on the number of attack rounds. However, the results obtained based on this method are not necessarily better than the results gained by traditional analysis. In this paper, we offer three novel greedy strategies (M1, M2 and M3) to solve this problem. The strategy M1 provides better differential-ML distinguishers by considering all combinations of classical differential distinguishers and ML differential distinguishers. And the strategy M2 uses the best ML differential distinguishers to splice classical differential distinguishers forward, while the strategy M3 adopts the best classical differential distinguishers to splice ML differential distinguishers. As proof of works, we apply our methods to round-reduced Speck32/64, Speck48/72 and Speck64/96 and get some improved cryptanalysis results. For the construction of differential-ML distinguishers, we can reach 11-round Speck32/64, 14-round Speck48/72 and 18-round Speck64/96 with 227, 245, 262 data respectively.},
booktitle = {Information and Communications Security: 23rd International Conference, ICICS 2021, Chongqing, China, November 19-21, 2021, Proceedings, Part II},
pages = {21–38},
numpages = {18},
keywords = {Differential analysis, Machine learning, Lightweight ciphers, Speck},
location = {Chongqing, China}
}

@inproceedings{10.5555/3507788.3507807,
author = {Ara\'{u}jo, Rodrigo and Holmes, Reid},
title = {Lightweight self-adaptive configuration using machine learning},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Modern distributed systems are comprised of many components that often have complex configuration parameters to allow them to be tuned to differing runtime requirements. Engineers must manually adjust many of these parameters to achieve their desired runtime behaviours. Unfortunately, static configurations are often insufficient, but ad hoc configuration modifications can unexpectedly degrade overall system quality.In this work, we describe Finch, a tool for injecting a machine learning-based MAPE-K feedback loop into existing REST-based systems to automate configuration tuning. Finch configures and optimizes systems according to service-level agreements under uncertain workloads and usage patterns. Rather than changing the core infrastructure of a target system to fit the feedback loop, Finch asks the user to perform a small set of actions: adding limited instrumentation to the code and configuration parameters and defining service-level objectives and agreements. With these changes, Finch learns how to dynamically configure the system at runtime to self-adapt to dynamic workloads.We provide a proof-of-concept evaluation to demonstrate how Finch can provide an automated self-adaptive system that replaces the trial-and-error engineering effort that otherwise would be spent manually optimizing a system's wide array of configuration parameters.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {machine learning, self adaptation, software configuration},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.patrec.2018.08.008,
author = {He, Fang and Wang, Rong and Jia, Weimin},
title = {Fast semi-supervised learning with anchor graph for large hyperspectral images},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.08.008},
doi = {10.1016/j.patrec.2018.08.008},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {319–326},
numpages = {8},
keywords = {Hyperspectral images (HSI) classification, Graph-based semi-supervised learning (SSL), Anchor graph}
}

@inproceedings{10.1145/3447548.3470804,
author = {Wang, Xin and Zhu, Wenwu},
title = {Automated Machine Learning on Graph},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470804},
doi = {10.1145/3447548.3470804},
abstract = {Machine learning on graphs has been extensively studiedin both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attentions from the research community. In this tutorial, we discuss AutoML on graphs, primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. To the best of our knowledge, this tutorial is the first to systematically and comprehensively review automated machine learning on graphs, possessing a great potential to draw a large amount of interests in the community.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4082–4083},
numpages = {2},
keywords = {automl, graph representation learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3468264.3468614,
author = {Cito, J\"{u}rgen and Dillig, Isil and Kim, Seohyun and Murali, Vijayaraghavan and Chandra, Satish},
title = {Explaining mispredictions of machine learning models using rule induction},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468614},
doi = {10.1145/3468264.3468614},
abstract = {While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {716–727},
numpages = {12},
keywords = {explainability, machine learning, rule induction},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@inproceedings{10.1145/3474085.3475340,
author = {Wang, Li and Fan, Baoyu and Guo, Zhenhua and Zhao, Yaqian and Zhang, Runze and Li, Rengang and Gong, Weifeng and Wang, Endong},
title = {Knowledge-Supervised Learning: Knowledge Consensus Constraints for Person Re-Identification},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475340},
doi = {10.1145/3474085.3475340},
abstract = {The consensus of multiple views on the same data will provide extra regularization, thereby improving accuracy. Based on this idea, we proposed a novel Knowledge-Supervised Learning (KSL) method for person re-identification (Re-ID), which can improve the performance without introducing extra inference cost. Firstly, we introduce isomorphic auxiliary training strategy to conduct basic multiple views that simultaneously train multiple classifier heads of the same network on the same training data. The consensus constraints aim to maximize the agreement among multiple views. To introduce this regular constraint, inspired by knowledge distillation that paired branches can be trained collaboratively through mutual imitation learning. Three novel constraints losses are proposed to distill the knowledge that needs to be transferred across different branches: similarity of predicted classification probability for cosine space constraints, distance of embedding features for euclidean space constraints, hard sample mutual mining for hard sample space constraints. From different perspectives, these losses complement each other. Experiments on four mainstream Re-ID datasets show that a standard model with KSL method trained from scratch outperforms its ImageNet pre-training results by a clear margin. With KSL method, a lightweight model without ImageNet pre-training outperforms most large models. We expect that these discoveries can attract some attention from the current de facto paradigm of "pre-training and fine-tuning" in Re-ID task to the knowledge discovery during model training.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1866–1874},
numpages = {9},
keywords = {consensus constraints, isomorphic auxiliary training, knowledge distillation, person retrieval},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1007/978-3-030-88081-1_58,
author = {Ta\c{s}delen, Osman and \c{C}arkacioglu, Levent and T\"{o}reyin, Beh\c{c}et U\u{g}ur},
title = {Anomaly Detection on ADS-B Flight Data Using Machine Learning Techniques},
year = {2021},
isbn = {978-3-030-88080-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88081-1_58},
doi = {10.1007/978-3-030-88081-1_58},
abstract = {With the rapid increase in the number of flights all over the world, the management and control of flight operations has become difficult in recent years. Moreover, the expectations for the aviation sector indicate that this increase will continue in the upcoming years. Therefore, safer and systematic monitoring systems by eliminating the requirement of human-dependent tracking during the air travel of an aircraft and automating the detection of abnormal situations has become a major problem in aviation sector. With the recent advances in artificial intelligence, a safer and systematic tracking system for controlling the airspace by eliminating the need for human-dependent tracking during the flight of aircraft in the air has become possible.In this study, we aimed to create a system that detects and predicts movements to indicate abnormal, dangerous situations in the airspace by monitoring radar flight data using machine learning and deep learning techniques. We applied two different methods, i.e., Proximity Based kNN and Auto Encoder We used real-life historical radar flight data set which consists of Flight Radar 24 data were converted from ADS-B messages for learning. We created simulation data and used this data for testing and validation for our trained model. Within the scope of this project, we also developed a system to monitor air traffic through radar tracks with our model and present the abnormal situations to the user through a visual interface for decision support. In this visualization, we present the abnormal situations if one of the algorithms labeled as anomaly. Results for both methods have shown that our findings were similar to the real-life predictions.},
booktitle = {Computational Collective Intelligence: 13th International Conference, ICCCI 2021, Rhodes, Greece, September 29 – October 1, 2021, Proceedings},
pages = {771–783},
numpages = {13},
keywords = {Anomaly detection, Deep learning, Machine learning, Proximity based kNN, Auto encoder, Flight control, ADS-B, Air traffic management},
location = {Rhodos, Greece}
}

@article{10.1109/MM.2020.3016551,
author = {Litz, Heiner and Hashemi, Milad},
title = {Machine Learning for Systems},
year = {2020},
issue_date = {Sept.-Oct. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {40},
number = {5},
issn = {0272-1732},
url = {https://doi.org/10.1109/MM.2020.3016551},
doi = {10.1109/MM.2020.3016551},
abstract = {The six papers in this special section focus on machine learning for computer systems. Specialized computer systems have driven the performance and capability of deep learning over the past decade.1 However, as machine learning models and systems improve, there is a growing opportunity to also use these models to improve how we design, architect, optimize, and automate computer systems and software. This is a challenging area, both from a learning and a systems perspective. Systems often impose tight size, latency, or reliability constraints on learning mechanisms that do not arise in other applications of machine learning, such as computer vision or natural language processing. From a learning perspective, systems is a challenging application, where input features are often large and sparse, action spaces are gigantic, and generalization is a key attribute.},
journal = {IEEE Micro},
month = sep,
pages = {6–7},
numpages = {2}
}

@inproceedings{10.1145/3439706.3446896,
author = {Dhar, Tonmoy and Kunal, Kishor and Li, Yaguang and Lin, Yishuang and Madhusudan, Meghna and Poojary, Jitesh and Sharma, Arvind K. and Burns, Steven M. and Harjani, Ramesh and Hu, Jiang and Mukherjee, Parijat and Yaldiz, Soner and Sapatnekar, Sachin S.},
title = {Machine Learning Techniques in Analog Layout Automation},
year = {2021},
isbn = {9781450383004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439706.3446896},
doi = {10.1145/3439706.3446896},
abstract = {The quality of layouts generated by automated analog design have traditionally not been able to match those from human designers over a wide range of analog designs. The ALIGN (Analog Layout, Intelligently Generated from Netlists) project [2, 3, 6] aims to build an open-source analog layout engine [1] that overcomes these challenges, using a variety of approaches. An important part of the toolbox is the use of machine learning (ML) methods, combined with traditional methods, and this talk overviews our efforts. The input to ALIGN is a SPICE-like netlist and a set of perfor- mance specifications, and the output is a GDSII layout. ALIGN automatically recognizes hierarchies in the input netlist. To detect variations of known blocks in the netlist, approximate subgraph iso- morphism methods based on graph convolutional networks can be used [5]. Repeated structures in a netlist are typically constrained by layout requirements related to symmetry or matching. In [7], we use a mix of graph methods and ML to detect symmetric and array structures, including the use of neural network based approximate matching through the use of the notion of graph edit distances. Once the circuit is annotated, ALIGN generates the layout, going from the lowest level cells to higher levels of the netlist hierarchy. Based on an abstraction of the process design rules, ALIGN builds parameterized cell layouts for each structure, accounting for the need for common centroid layouts where necessary [11]. These cells then undergo placement and routing that honors the geomet- ric constraints (symmetry, common-centroid). The chief parameter that changes during layout is the set of interconnect RC parasitics: excessively large RCs could result in an inability to meet perfor- mance. These values can be controlled by reducing the distance between blocks, or, in the case of R, by using larger effective wire widths (using multiple parallel connections in FinFET technologies where wire widths are quantized) to reduce the effective resistance. ALIGN has developed several approaches based on ML for this purpose [4, 8, 9] that rapidly predict whether a layout will meet the performance constraints that are imposed at the circuit level, and these can be deployed together with conventional algorithmic methods [10] to rapidly prune out infeasible layouts. This presentation overviews our experience in the use of ML- based methods in conjunction with conventional algorithmic ap- proaches for analog design. We will show (a) results from our efforts so far, (b) appropriate methods for mixing ML methods with tra- ditional algorithmic techniques for solving the larger problem of analog layout, (c) limitations of ML methods, and (d) techniques for overcoming these limitations to deliver workable solutions for analog layout automation.},
booktitle = {Proceedings of the 2021 International Symposium on Physical Design},
pages = {71–72},
numpages = {2},
keywords = {analog layout automation, machine learning},
location = {Virtual Event, USA},
series = {ISPD '21}
}

@phdthesis{10.5555/AAI29248048,
author = {Liu, Fred},
advisor = {Lars, Stentoft, and Tim, Conley, and Charles, Saunders,},
title = {Essays in Financial Econometrics and Machine Learning},
year = {2021},
isbn = {9798845497796},
publisher = {The University of Western Ontario (Canada)},
abstract = {Financial econometrics is a highly interdisciplinary field that integrates finance, economics, probability, statistics, and applied mathematics. Machine learning is a growing area in finance that is particularly suitable for studying problems with many variables. My thesis contains three chapters that explore financial econometrics and machine learning in the fields of asset pricing and risk management.  Chapter 2 studies the implications of the new Basel 3 regulations. In 2019, the BCBS finalized the Basel 3 regulatory regime, which changes the regulatory measure of market risk and adds new complex calculations based on liquidity and risk factors. This chapter is motivated by these changes and seeks to answer the question of how regulation affects banks' choice of risk-management models, whether it incentivizes them to use correctly specified models, and if it results in more stable capital requirements.  Chapter 3 conducts, to our knowledge, the largest study ever of five-minute equity market returns using state-of-the-art machine learning models trained on the cross-section of lagged market index constituent returns, where we show that regularized linear models and nonlinear tree-based models yield significant market return predictability. Ensemble models perform the best across time and their predictability translates into economically significant Sharpe ratios of 0.98 after transaction costs. These results provide strong evidence that intraday market returns are predictable during short time horizons.  Chapter 4 studies the idiosyncratic tail risk premium and common factor. Stocks in the highest idiosyncratic tail risk decile earn 8% higher average annualized returns than in the lowest. I propose a risk-based explanation for this premium, in which shocks to intermediary funding cause idiosyncratic tail risk to follow a strong factor structure, and the factor, common idiosyncratic tail risk (CITR), comoves with intermediary funding. Consequently, firms with high idiosyncratic tail risk have high exposure to CITR shocks, and command a risk premium due to their low returns when intermediary constraints tighten. To test my explanation, I create a novel measure of idiosyncratic tail risk. Consistent with my explanation, CITR shocks are procyclical, are correlated to intermediary factors, are priced in assets, and explain the idiosyncratic tail risk premium.},
note = {AAI29248048}
}

@article{10.1016/j.ijcci.2021.100281,
author = {Vartiainen, Henriikka and Toivonen, Tapani and Jormanainen, Ilkka and Kahila, Juho and Tedre, Matti and Valtonen, Teemu},
title = {Machine learning for middle schoolers: Learning through data-driven design},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {C},
issn = {2212-8689},
url = {https://doi.org/10.1016/j.ijcci.2021.100281},
doi = {10.1016/j.ijcci.2021.100281},
journal = {Int. J. Child-Comp. Interact.},
month = sep,
numpages = {12},
keywords = {AI, Machine learning, K-12, Computational thinking, Design-oriented pedagogy, Design-based research}
}

@inproceedings{10.1007/978-3-030-86514-6_26,
author = {Zigrand, Louis and Alizadeh, Pegah and Traversi, Emiliano and Wolfler Calvo, Roberto},
title = {Machine Learning Guided Optimization for Demand Responsive Transport Systems},
year = {2021},
isbn = {978-3-030-86513-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86514-6_26},
doi = {10.1007/978-3-030-86514-6_26},
abstract = {Most of the time, objective functions used for solving static combinatorial optimization problems cannot deal efficiently with their real-time counterparts. It is notably the case of Shared Mobility Systems where the dispatching framework must adapt itself dynamically to the demand. More precisely, in the context of Demand Responsive Transport (DRT) services, various objective functions have been proposed in the literature to optimize the vehicles routes. However, these objective functions are limited in practice because they discard the dynamic evolution of the demand. To overcome such a limitation, we propose a Machine Learning Guided Optimization methodology to build a new objective function based on simulations and historical data. This way, we are able to take the demand’s dynamic evolution into account. We also present how to design the main components of the proposed framework to fit a DRT application: data generation and evaluation, training process and model optimization. We show the efficiency of our proposed methodology on real-world instances, obtained in a collaboration with Padam Mobility, an international company developing Shared Mobility Systems.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part IV},
pages = {420–436},
numpages = {17},
keywords = {Demand responsive transport, Surrogate modeling, Combinatorial optimization},
location = {Bilbao, Spain}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1016/j.patcog.2019.03.022,
author = {Ma, Qing and Bai, Cong and Zhang, Jinglin and Liu, Zhi and Chen, Shengyong},
title = {Supervised learning based discrete hashing for image retrieval},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {92},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.03.022},
doi = {10.1016/j.patcog.2019.03.022},
journal = {Pattern Recogn.},
month = aug,
pages = {156–164},
numpages = {9},
keywords = {Hashing, Supervised learning, Neural network, Optimization}
}

@article{10.1016/j.eswa.2021.115102,
author = {Mancuso, Paolo and Piccialli, Veronica and Sudoso, Antonio M.},
title = {A machine learning approach for forecasting hierarchical time series},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115102},
doi = {10.1016/j.eswa.2021.115102},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {17},
keywords = {Hierarchical time series, Forecast, Machine learning, Deep neural network}
}

@article{10.1137/20M1332827,
author = {Bar, Leah and Sochen, Nir},
title = {Strong Solutions for PDE-Based Tomography by Unsupervised Learning},
year = {2021},
issue_date = {January 2021},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {14},
number = {1},
url = {https://doi.org/10.1137/20M1332827},
doi = {10.1137/20M1332827},
abstract = {We introduce a novel neural network-based PDEs solver for forward and inverse problems. The solver is grid free, mesh free, and shape free, and the solution is approximated by a neural network.
We employ an unsupervised approach such that the input to the network is a point set in an arbitrary domain, and the output is the
set of the corresponding function values.  The network is trained to minimize deviations of the learned function from the PDE solution and
satisfy the boundary conditions.
The resulting solution in turn is an explicit, smooth, differentiable function with a known analytical form. We solve the forward problem (observations given the underlying model's parameters), semi-inverse problem (model's parameters given the observations in the whole domain), and full tomography inverse problem (model's parameters given the observations on the boundary) by solving the forward and semi-inverse problems at the same time.
The optimized loss function consists of few elements: fidelity term of $L_2$ norm that enforces the PDE in the weak sense, an $L_infty$ norm term that enforces pointwise fidelity and thus promotes a strong solution, and boundary and initial conditions constraints. It further accommodates regularizers for the solution and/or the model's parameters of the differential operator. This setting is flexible in the sense that regularizers can be tailored to specific  problems. We demonstrate our method on several free shape two dimensional (2D) second order systems with application to electrical impedance tomography (EIT) and diffusion equation. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework enables,  in principle, the solution of high order and high dimensional nonlinear PDEs.},
journal = {SIAM J. Img. Sci.},
month = jan,
pages = {128–155},
numpages = {28},
keywords = {PDEs, forward problems, inverse problems, unsupervised learning, deep networks, EIT, 35C99, 65M32, 65N21}
}

@article{10.1016/j.jnca.2020.102576,
author = {Gu, Rentao and Yang, Zeyuan and Ji, Yuefeng},
title = {Machine learning for intelligent optical networks: A comprehensive survey},
year = {2020},
issue_date = {May 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {157},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2020.102576},
doi = {10.1016/j.jnca.2020.102576},
journal = {J. Netw. Comput. Appl.},
month = may,
numpages = {22},
keywords = {Optical networks, Machine learning, Resource management, Optical performance monitoring, Neural networks, Reinforcement learning}
}

@article{10.1016/j.csl.2019.04.004,
author = {Campos, Victor de Abreu and Pedronette, Daniel Carlos Guimar\~{a}es},
title = {A framework for speaker retrieval and identification through unsupervised learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {58},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2019.04.004},
doi = {10.1016/j.csl.2019.04.004},
journal = {Comput. Speech Lang.},
month = nov,
pages = {153–174},
numpages = {22},
keywords = {Speaker recognition, Speaker retrieval, Unsupervised learning, Vector quantization, Gaussian mixture model, i-vector}
}

@article{10.1007/s10506-020-09270-4,
author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Fr\'{e}nay, Beno\^{\i}t},
title = {Legal requirements on explainability in machine learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0924-8463},
url = {https://doi.org/10.1007/s10506-020-09270-4},
doi = {10.1007/s10506-020-09270-4},
abstract = {Deep learning and other black-box models are becoming more and more popular today. Despite their high performance, they may not be accepted ethically or legally because of their lack of explainability. This paper presents the increasing number of legal requirements on machine learning model interpretability and explainability in the context of private and public decision making. It then explains how those legal requirements can be implemented into machine-learning models and concludes with a call for more inter-disciplinary research on explainability.},
journal = {Artif. Intell. Law},
month = jun,
pages = {149–169},
numpages = {21},
keywords = {Interpretability, Explainability, Machine learning, Law}
}

@inproceedings{10.1145/3240508.3240699,
author = {Wang, Lingjing and Qian, Cheng and Wang, Jifei and Fang, Yi},
title = {Unsupervised Learning of 3D Model Reconstruction from Hand-Drawn Sketches},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240699},
doi = {10.1145/3240508.3240699},
abstract = {3D objects modeling has gained considerable attention in the visual computing community. We propose a low-cost unsupervised learning model for 3D objects reconstruction from hand-drawn sketches. Recent advancements in deep learning opened new opportunities to learn high-quality 3D objects from 2D sketches via supervised networks. However, the limited availability of labeled 2D hand-drawn sketches data (i.e. sketches and its corresponding 3D ground truth models) hinders the training process of supervised methods. In this paper, driven by a novel design of combination of retrieval and reconstruction process, we developed a learning paradigm to reconstruct 3D objects from hand-drawn sketches, without the use of well-labeled hand-drawn sketch data during the entire training process. Specifically, the paradigm begins with the training of an adaption network via autoencoder with adversarial loss, embedding the unpaired 2D rendered image domain with the hand-drawn sketch domain to a shared latent vector space. Then from the embedding latent space, for each testing sketch image, we retrieve a few (e.g. five) nearest neighbors from the training 3D data set as prior knowledge for a 3D Generative Adversarial Network. Our experiments verify our network's robust and superior performance in handling 3D volumetric object generation from single hand-drawn sketch without requiring any 3D ground truth labels.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1820–1828},
numpages = {9},
keywords = {generative model, sketch modeling, unsupervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1016/j.knosys.2019.104982,
author = {Castellanos-Garz\'{o}n, Jos\'{e} A. and Costa, Ernesto and Jaimes S., Jos\'{e} Luis and Corchado, Juan M.},
title = {An evolutionary framework for machine learning applied to medical data},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {185},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104982},
doi = {10.1016/j.knosys.2019.104982},
journal = {Know.-Based Syst.},
month = dec,
numpages = {14},
keywords = {Machine learning, Logical rule induction, Data mining, Supervised learning, Evolutionary computation, Genetic programming, Ensemble classifier, Medical data}
}

@article{10.1016/j.patcog.2019.107076,
author = {Huang, Bin and Chen, Renwen and Zhou, Qinbang and Xu, Wang},
title = {Eye landmarks detection via weakly supervised learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107076},
doi = {10.1016/j.patcog.2019.107076},
journal = {Pattern Recogn.},
month = feb,
numpages = {11},
keywords = {Eye landmarks detection, Special format data, Weakly supervised learning, Object detection, Recurrent learning module}
}

@inproceedings{10.1007/978-3-030-66843-3_24,
author = {Anjum, Sadia and Hussain, Lal and Ali, Mushtaq and Abbasi, Adeel Ahmed},
title = {Automated Multi-class Brain Tumor Types Detection by Extracting RICA Based Features and Employing Machine Learning Techniques},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_24},
doi = {10.1007/978-3-030-66843-3_24},
abstract = {Brain tumor is the leading reason of mortality across the globe. It is obvious that the chances of survival can be increased if the tumor is identified and properly classified at an initial stage. Several factors such as type, texture and location help to categorize the brain tumor. In this study, we extracted reconstruction independent component analysis (RICA) base features from brain tumor types such as glioma, meningioma, pituitary and applied robust machine learning algorithms such as linear discriminant analysis (LDA) and support vector machine (SVM) with linear and quadratic kernels. The jackknife 10-fold cross validation was used for training and testing data validation. The SVM with quadratic kernel gives the highest multiclass detection performance. To detect pituitary, the highest detection performance was obtained with sensitivity (93.85%), specificity (100%), PPV (100%), NPV (97.27%), accuracy (98.07%) and AUC (96.92). To detect glioma, the highest detection performance was obtained with accuracy (94.35%), AUC (0.9508). To detect the meningioma, the highest was obtained with accuracy (96.18%), AUC (0.9095). The findings reveal that proposed methodology based on RICA features to detect multiclass brain tumor types will be very useful for treatment modification to achieve better clinical outcomes.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {249–258},
numpages = {10},
keywords = {Feature extraction, Machine learning, Glioma, Meningioma, Pituitary, Image analysis},
location = {Lima, Peru}
}

@article{10.1145/3399595,
author = {Agnesina, Anthony and Lim, Sung Kyu and Lepercq, Etienne and Cid, Jose Escobedo Del},
title = {Improving FPGA-Based Logic Emulation Systems through Machine Learning},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3399595},
doi = {10.1145/3399595},
abstract = {We present a machine learning (ML) framework to improve the use of computing resources in the FPGA compilation step of a commercial FPGA-based logic emulation flow. Our ML models enable highly accurate predictability of the final place and route design qualities, runtime, and optimal mapping parameters. We identify key compilation features that may require aggressive compilation efforts using our ML models. Experiments based on our large-scale database from an industry’s emulation system show that our ML models help reduce the total number of jobs required for a given netlist by 33%. Moreover, our job scheduling algorithm based on our ML model reduces the overall time to completion of concurrent compilation runs by 24%. In addition, we propose a new method to compute “recommendations” from our ML model to perform re-partitioning of difficult partitions. Tested on a large-scale industry system on chip design, our recommendation flow provides additional 15% compile time savings for the entire system on chip. To exploit our ML model inside the time-critical multi-FPGA partitioning step, we implement it in an optimized multi-threaded representation.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jul,
articleno = {46},
numpages = {20},
keywords = {Field programmable gate array, SoC verification, emulation flow optimization with machine learning}
}

@inproceedings{10.1007/978-3-030-77977-1_26,
author = {Torres, Marcella},
title = {A Machine Learning Method for Parameter Estimation and Sensitivity Analysis},
year = {2021},
isbn = {978-3-030-77976-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77977-1_26},
doi = {10.1007/978-3-030-77977-1_26},
abstract = {We discuss the application of a supervised machine learning method, random forest algorithm (RF), to perform parameter space exploration and sensitivity analysis on ordinary differential equation models. Decision trees can provide complex decision boundaries and can help visualize decision rules in an easily digested format that can aid in understanding the predictive structure of a dynamic model and the relationship between input parameters and model output. We study a simplified process for model parameter tuning and sensitivity analysis that can be used in the early stages of model development.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part V},
pages = {330–343},
numpages = {14},
keywords = {Parameter estimation, Machine learning, Sensitivity analysis, Ordinary differential equations, Random forest},
location = {Krakow, Poland}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {14},
keywords = {Global development, developing countries}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {50},
numpages = {47},
keywords = {Data exfiltration, advanced persistent threat, data breach, data leakage, machine learning}
}

@article{10.1007/s10586-021-03359-4,
author = {Yadav, Mahendra Pratap and Rohit and Yadav, Dharmendra Kumar},
title = {Maintaining container sustainability through machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03359-4},
doi = {10.1007/s10586-021-03359-4},
abstract = {Container-based virtualization is a new technology used by cloud providers to provide cloud services to end-user. This technology has various advantages (e.g. lightweight, quickly deployable, and efficient for resource utilization) for executing an application. It reduces the operating cost, carbon emission, and allocates the resources dynamically. Different cloud applications have different requirements. Deploying resources according to peak requirements always can be costly. On the other hand, always having minimum computing resources may not meet workload’s peak requirements, and may cause degraded system performance, less throughput, more response time and service level agreement violations. Hence, it becomes a challenge to maintain optimal level of resources to fulfill the SLA requirements for the applications. To address the above issues, we propose an auto-scaler which uses proactive approach (Support Vector Regression) to perform horizontal elasticity for Docker containers in response to fluctuating workload for real-time applications. As the workload increases, additional resources will be allocated dynamically supporting elasticity. The increase in capacity of a machine dynamically is termed as elasticity. The effective mechanism of elasticity avoids the violation of SLA and penalties in terms of user’s loss. The proposed auto-scaler uses the IBM computing model, MAPE-K principle to perform elasticity using the workload predictions made by the SVR model. The predicted workload helps auto-scaler to find out the minimum numbers of replicas needed for a container of a cluster so that it handles the future workload. The experimental results show that the results of SVM prediction keep the performance of the system sustainable with fluctuating workload.},
journal = {Cluster Computing},
month = dec,
pages = {3725–3750},
numpages = {26},
keywords = {Cloud computing, Elasticity, Container, Auto-scaling, MAPE model, Prediction models, Support vector machine, Machine learning}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@book{10.5555/3278388,
author = {Gori, Marco},
title = {Machine Learning: A Constraint-Based Approach},
year = {2017},
isbn = {0081006594},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Machine Learning: A Constraint-Based Approach provides readers with a refreshing look at the basic models and algorithms of machine learning, with an emphasis on current topics of interest that includes neural networks and kernel machines. The book presents the information in a truly unified manner that is based on the notion of learning from environmental constraints. While regarding symbolic knowledge bases as a collection of constraints, the book draws a path towards a deep integration with machine learning that relies on the idea of adopting multivalued logic formalisms, like in fuzzy systems. A special attention is reserved to deep learning, which nicely fits the constrained- based approach followed in this book. This book presents a simpler unified notion of regularization, which is strictly connected with the parsimony principle, and includes many solved exercises that are classified according to the Donald Knuth ranking of difficulty, which essentially consists of a mix of warm-up exercises that lead to deeper research problems. A software simulator is also included. Presents fundamental machine learning concepts, such as neural networks and kernel machines in a unified manner Provides in-depth coverage of unsupervised and semi-supervised learning Includes a software simulator for kernel machines and learning from constraints that also includes exercises to facilitate learning Contains 250 solved examples and exercises chosen particularly for their progression of difficulty from simple to complex}
}

@article{10.1016/j.infsof.2012.06.014,
author = {Andersson, Henric and Herzog, Erik and \"{O}Lvander, Johan},
title = {Experience from model and software reuse in aircraft simulator product line engineering},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.014},
doi = {10.1016/j.infsof.2012.06.014},
abstract = {Context: ''Reuse'' and ''Model Based Development'' are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft. Objective: The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method: The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results: A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics. Conclusion: The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {595–606},
numpages = {12},
keywords = {Configurator, Knowledge Based Engineering, Model Based Development, PDM, SPL, Software Product Line}
}

@inproceedings{10.1007/978-3-031-15116-3_7,
author = {Casimiro, Maria and Romano, Paolo and Garlan, David and Moreno, Gabriel A. and Kang, Eunsuk and Klein, Mark},
title = {Self-adaptive Machine Learning Systems: Research Challenges and&nbsp;Opportunities},
year = {2021},
isbn = {978-3-031-15115-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-15116-3_7},
doi = {10.1007/978-3-031-15116-3_7},
abstract = {Today’s world is witnessing a shift from human-written software to machine-learned software, with the rise of systems that rely on machine learning. These systems typically operate in non-static environments, which are prone to unexpected changes, as is the case of self-driving cars and enterprise systems. In this context, machine-learned software can misbehave. Thus, it is paramount that these systems are capable of detecting problems with their machined-learned components and adapting themselves to maintain desired qualities. For instance, a fraud detection system that cannot adapt its machine-learned model to efficiently cope with emerging fraud patterns or changes in the volume of transactions is subject to losses of millions of dollars. In this paper, we take a first step towards the development of a framework for self-adaptation of systems that rely on machine-learned components. We describe: (i) a set of causes of machine-learned component misbehavior and a set of adaptation tactics inspired by the literature on machine learning, motivating them with the aid of two running examples from the enterprise systems and cyber-physical systems domains; (ii) the required changes to the MAPE-K loop, a popular control loop for self-adaptive systems; and (iii) the challenges associated with developing this framework. We conclude with a set of research questions to guide future work.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021 Tracks and Workshops; V\"{a}xj\"{o}, Sweden, September 13–17, 2021, Revised Selected Papers},
pages = {133–155},
numpages = {23},
keywords = {Self-adaptive systems, Machine learning, Model degradation, Learning-enabled systems, Learning-enabled components},
location = {V\"{a}xj\"{o}, Sweden}
}

@article{10.1287/mnsc.2018.3255,
author = {Yoganarasimhan, Hema},
title = {Search Personalization Using Machine Learning},
year = {2020},
issue_date = {March 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {66},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2018.3255},
doi = {10.1287/mnsc.2018.3255},
abstract = {Firms typically use query-based search to help consumers find information/products on their websites. We consider the problem of optimally ranking a set of results shown in response to a query. We propose a personalized ranking mechanism based on a user’s search and click history. Our machine-learning framework consists of three modules: (a) feature generation, (b) normalized discounted cumulative gain–based LambdaMART algorithm, and (c) feature selection wrapper. We deploy our framework on large-scale data from a leading search engine using Amazon EC2 servers and present results from a series of counterfactual analyses. We find that personalization improves clicks to the top position by 3.5% and reduces the average error in rank of a click by 9.43% over the baseline. Personalization based on short-term history or within-session behavior is shown to be less valuable than long-term or across-session personalization. We find that there is significant heterogeneity in returns to personalization as a function of user history and query type. The quality of personalized results increases monotonically with the length of a user’s history. Queries can be classified based on user intent as transactional, informational, or navigational, and the former two benefit more from personalization. We also find that returns to personalization are negatively correlated with a query’s past average performance. Finally, we demonstrate the scalability of our framework and derive the set of optimal features that maximizes accuracy while minimizing computing time.This paper was accepted by Juanjuan Zhang, marketing.},
journal = {Manage. Sci.},
month = mar,
pages = {1045–1070},
numpages = {26},
keywords = {marketing, online search, personalization, machine learning, search engines}
}

@article{10.1007/s00521-021-06485-7,
author = {Phamtoan, Dinh and Vovan, Tai},
title = {Building fuzzy time series model from unsupervised learning technique and genetic algorithm},
year = {2021},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06485-7},
doi = {10.1007/s00521-021-06485-7},
abstract = {This paper proposes a new model to interpolate time series and forecast it effectively for the future. The important contribution of this study is the combination of optimal techniques for fuzzy clustering problem using genetic algorithm and forecasting model for fuzzy time series. Firstly, the proposed model finds the suitable number of clusters for a series and optimizes the clustering problem by the genetic algorithm using the improved Davies and Bouldin index as the objective function. Secondly, the study gives the method to establish the fuzzy relationship of each element to the established clusters. Finally, the developed model establishes the rule to forecast for the future. The steps of the proposed model are presented clearly and illustrated by the numerical example. Furthermore, it has been realized positively by the established MATLAB procedure. Performing for a lot of series (3007 series) with the differences about characteristics and areas, the new model has shown the significant performance in comparison with the existing models via some parameters to evaluate the built model. In addition, we also present an application of the proposed model in forecasting the COVID-19 victims in Vietnam that it can perform similarly for other countries. The numerical examples and application show potential in the forecasting area of this research.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {7235–7252},
numpages = {18},
keywords = {Cluster analysis, Forecast, Fuzzy time series, Interpolate}
}

@phdthesis{10.5555/AAI28864695,
author = {Rahgooy, Taher and Jason, Harman, and Arash, Mahyari, and Nicholas, Mattei,},
advisor = {Brent, Venable, K.},
title = {Machine Learning Guided by Linguistic and Behavioral Knowledge},
year = {2021},
isbn = {9798762100731},
publisher = {The University of West Florida},
abstract = {The recent success of AI has been primarily driven by the extraordinary progress in the field of machine learning. The ultimate goal of machine learning is to develop algorithms capable of making accurate predictions in an explainable way by learning efficiently from a small amount of training data. Despite an exceptionally fast-paced growth, machine learning has been exceedingly successful in achieving accurate predictions, at the cost of sacrificing most of, if not all, explainability and by relying on huge amount of training data. Recent work has, on the other hand, shown that domain knowledge, when properly incorporated in learning algorithms, can facilitate learning from small data sets and provide various forms of explainability. In this dissertation, I propose novel ways of incorporating linguistic and behavioral knowledge into machine learning models for achieving different goals such as improving prediction accuracy, using less data, increase explainability, and evaluating cognitive biases.  We exemplify our novel approaches on some challenging tasks that require special treatment either due to lack of data and/or need for explainable predictions.  We first consider extracting spatial relations from language, which is a complex task due to the ambiguity of spatial relations and scarcity of available training data. To this end, we use linguistic knowledge to define various constraints imposed on classifiers to infer the correct classifications holistically.  Human choice prediction is the other domain that we consider because of the fundamental role it plays in the understanding of human behavior and in the design of intelligent systems. We propose novel methods to leverage procedural knowledge, in the form of psychological models of decision making, in combination with machine learning, to achieve better predictions, understand the underlying deliberation processes, and elicit user preferences.  Finally, we extend our work to the domain of sequential decision making by designing agents that learn constraints from demonstrations and then use cognitive models as orchestrators to exploit these learned constraints for making choices between conflicting goals.  We use various real world and synthetic data to evaluate our proposed methods throughout this dissertation. Our experimental results show the efficacy of our methods which significantly improves upon the state-of-the-art in all of the considered tasks.},
note = {AAI28864695}
}

@inproceedings{10.1007/978-3-030-30446-1_16,
author = {Kawamoto, Yusuke},
title = {Towards Logical Specification of Statistical Machine Learning},
year = {2019},
isbn = {978-3-030-30445-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30446-1_16},
doi = {10.1007/978-3-030-30446-1_16},
abstract = {We introduce a logical approach to formalizing statistical properties of machine learning. Specifically, we propose a formal model for statistical classification based on a Kripke model, and formalize various notions of classification performance, robustness, and fairness of classifiers by using epistemic logic. Then we show some relationships among properties of classifiers and those between classification performance and robustness, which suggests robustness-related properties that have not been formalized in the literature as far as we know. To formalize fairness properties, we define a notion of counterfactual knowledge and show techniques to formalize conditional indistinguishability by using counterfactual epistemic operators. As far as we know, this is the first work that uses logical formulas to express statistical properties of machine learning, and that provides epistemic (resp. counterfactually epistemic) views on robustness (resp. fairness) of classifiers.},
booktitle = {Software Engineering and Formal Methods: 17th International Conference, SEFM 2019, Oslo, Norway, September 18–20, 2019, Proceedings},
pages = {293–311},
numpages = {19},
keywords = {Epistemic logic, Possible world semantics, Divergence, Machine learning, Statistical classification, Robustness, Fairness},
location = {Oslo, Norway}
}

@inproceedings{10.1145/3411501.3419432,
author = {Haralampieva, Veneta and Rueckert, Daniel and Passerat-Palmbach, Jonathan},
title = {A Systematic Comparison of Encrypted Machine Learning Solutions for Image Classification},
year = {2020},
isbn = {9781450380881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411501.3419432},
doi = {10.1145/3411501.3419432},
abstract = {This work provides a comprehensive review of existing frameworks based on secure computing techniques in the context of private image classification. The in-depth analysis of these approaches is followed by careful examination of their performance costs, in particular runtime and communication overhead.To further illustrate the practical considerations when using different privacy-preserving technologies, experiments were conducted using four state-of-the-art libraries implementing secure computing at the heart of the data science stack: PySyft and CrypTen supporting private inference via Secure Multi-Party Computation, TF-Trusted utilising Trusted Execution Environments and HE-Transformer relying on Homomorphic encryption.Our work aims to evaluate the suitability of these frameworks from a usability, runtime requirements and accuracy point of view. In order to better understand the gap between state-of-the-art protocols and what is currently available in practice for a data scientist, we designed three neural network architecture to obtain secure predictions via each of the four aforementioned frameworks. Two networks were evaluated on the MNIST dataset and one on the Malaria Cell image dataset. We observed satisfying performances for TF-Trusted and CrypTen and noted that all frameworks perfectly preserved the accuracy of the corresponding plaintext model.},
booktitle = {Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice},
pages = {55–59},
numpages = {5},
keywords = {benchmark, image classification, machine learning, privacy-preserving inference},
location = {Virtual Event, USA},
series = {PPMLP'20}
}

@inproceedings{10.1007/978-3-030-66096-3_28,
author = {Tokmakov, Pavel and Hebert, Martial and Schmid, Cordelia},
title = {Unsupervised Learning of Video Representations via Dense Trajectory Clustering},
year = {2020},
isbn = {978-3-030-66095-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66096-3_28},
doi = {10.1007/978-3-030-66096-3_28},
abstract = {This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part II},
pages = {404–421},
numpages = {18},
keywords = {Unsupervised representation learning, Action recognition},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.1007/978-3-030-44584-3_43,
author = {von Rueden, Laura and Mayer, Sebastian and Sifa, Rafet and Bauckhage, Christian and Garcke, Jochen},
title = {Combining Machine Learning and Simulation to a Hybrid Modelling Approach: Current and Future Directions},
year = {2020},
isbn = {978-3-030-44583-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-44584-3_43},
doi = {10.1007/978-3-030-44584-3_43},
abstract = {In this paper, we describe the combination of machine learning and simulation towards a hybrid modelling approach. Such a combination of data-based and knowledge-based modelling is motivated by applications that are partly based on causal relationships, while other effects result from hidden dependencies that are represented in huge amounts of data. Our aim is to bridge the knowledge gap between the two individual communities from machine learning and simulation to promote the development of hybrid systems. We present a conceptual framework that helps to identify potential combined approaches and employ it to give a structured overview of different types of combinations using exemplary approaches of simulation-assisted machine learning and machine-learning assisted simulation. We also discuss an advanced pairing in the context of Industry 4.0 where we see particular further potential for hybrid systems.},
booktitle = {Advances in Intelligent Data Analysis XVIII: 18th International Symposium on Intelligent Data Analysis, IDA 2020, Konstanz, Germany, April 27–29, 2020, Proceedings},
pages = {548–560},
numpages = {13},
keywords = {Machine learning, Simulation, Hybrid approaches},
location = {Konstanz, Germany}
}

@article{10.1007/s11042-020-09512-2,
author = {Khan, Ayaz H. and Zubair, Muhammad},
title = {Classification of multi-lingual tweets, into multi-class model using Na\"{\i}ve Bayes and semi-supervised learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09512-2},
doi = {10.1007/s11042-020-09512-2},
abstract = {Twitter is a social media platform which has been proven to be a great tool for insights of emotions about products, policies etc. through a 280-character message called tweet, containing direct and unfiltered emotions by a large amount of user population. Twitter has attracted the attention of many researchers owing to the fact that every tweet is by default, public in nature which is not the case with Facebook. This paper proposes a model for multi-lingual (English and Roman Urdu) classification of tweets over diversely ranged classes (non-hierarchical architecture). Previous work in tweet classification is narrowly focused either on single language or either on uniform set of classes at most (Positive, Extremely Positive, Negative and Extremely Negative). The proposed model is based on semi-supervised learning and proposed feature selection approach makes it less dependent and highly adaptive for grabbing trending terms. This makes it a strong contender of choice for streaming data. In the methodology, using Na\"{\i}ve Bayes learning algorithm for each phase, obtained remarkable accuracy of up to 87.16% leading from both KNN and SVM models which are popular for NLP and Text classification domains.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {32749–32767},
numpages = {19},
keywords = {Twitter, Sentiment analysis, Sentiment classification, Semi-supervised learning}
}

@inbook{10.5555/3454287.3455036,
author = {G\"{o}lz, Paul and Kahng, Anson and Procaccia, Ariel D.},
title = {Paradoxes in fair machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {749},
numpages = {11}
}

@phdthesis{10.5555/AAI28712992,
author = {Dick, Sebastian and Michael, Zingale, and Thomas, Allison, and J, Harrison, Robert},
advisor = {Marivi, Fern\'{a}ndez-Serra,},
title = {Improving Density Functional Theory with Machine Learning},
year = {2021},
isbn = {9798460433902},
publisher = {State University of New York at Stony Brook},
address = {USA},
abstract = {In times of increasing threats from climate change and pandemics, the need for efficient and accurate tools for the in silico design of new therapeutics and materials is more pressing than ever. Electronic structure calculations based on quantum mechanical methods provide a way to generate scientific insights from first principles and thus form a fundamental pillar of modern molecular sciences. In principle, any atomic system is described to an extremely high degree of accuracy by the equations of quantum mechanics. Although these equations are well understood, solving them for systems larger than a few atoms is still prohibitively expensive.Advanced methods from quantum chemistry offer reliable approximate solutions and are routinely used to simulate small to medium-sized molecules. Unfortunately, their computational cost and poor scaling rule out their use in simulating larger molecules, which are often relevant to understanding biochemical processes and the properties of materials. A different tool, density functional theory (DFT), in particular Kohn-Sham DFT, initially created to investigate properties of solids, has seen an increase in popularity due to its favorable cubic scaling. The core idea of DFT is as simple as it is appealing. Rather than treating interactions between any two electrons explicitly, electrons merely interact with a mean-field density. This approximation makes the efficient implementation of DFT possible, enabling researchers to simulate systems with up to thousands of atoms.Despite its attractiveness, DFT comes with its own set of problems. The theory resembles an incomplete jigsaw puzzle with all pieces known except for one: the so-called exchange-correlation (XC) functional. Finding the true XC functional would render DFT exact, and chemical processes could be accurately described. However, the true form of this elusive functional is so far unknown, and there is little hope that it can ever be written down in a closed-form expression. For practical applications, this missing piece to the puzzle has to be approximated. Many approximations, varying in complexity and accuracy, exist, and researchers have to decide on a case-by-case basis which functional to use. Doing so, however, is far from ideal, as the added degree of freedom can introduce hard-to-control systematic errors.In this work, we outline avenues for creating new XC functionals with the help of neural networks, a machine learning method. Neural networks are considered universal approximators, which means they can fit any function with arbitrary accuracy. For this reason, some people believe machine learning might hold the key to achieving something close to an exact functional.We introduce the concept of physically informed machine learning and propose two approaches to fitting density functionals. In one approach, prior physical knowledge is injected into the training procedure by learning to add small corrections to physically motivated calculations. Our second approach demonstrates how physical information can be directly incorporated into the optimization algorithm in the form of differential equations. We show that both approaches lead to machine learning models that are significantly more data-efficient and reliable than those without physical priors. Trained automatically, the thus created models routinely outperform carefully hand-designed functionals. However, we also find that caution needs to be exercised when using machine-learned models, as they lack some of the safety-nets that traditional functionals are designed with and therefore run the risk of failing in unexpected scenarios.We conclude that machine-learned models have become a valuable addition to a researcher's toolbox while not replacing hand-designed functionals. In particular, we conjecture that a future path towards an exact functional will depend on a strong symbiosis between the two seemingly divergent approaches. Physical insights will lead the way to new functionals by posing strong constraints on their form, while flexible machine-learning methods will realize these functionals within the given bounds. Our efforts are complemented by a collection of novel software libraries and toolkits that make machine-learned functionals available to the broader research community.},
note = {AAI28712992}
}

@inproceedings{10.1145/3351095.3375624,
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter},
title = {Explainable machine learning in deployment},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375624},
doi = {10.1145/3351095.3375624},
abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {648–657},
numpages = {10},
keywords = {deployed systems, explainability, machine learning, qualitative study, transparency},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1007/s00521-021-05749-6,
author = {Karlos, Stamatis and Aridas, Christos and Kanas, Vasileios G. and Kotsiantis, Sotiris},
title = {Classification of acoustical signals by combining active learning strategies with semi-supervised learning schemes},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05749-6},
doi = {10.1007/s00521-021-05749-6},
abstract = {In real-world cases, handling both labeled and unlabeled data has raised the interest of several Data Scientists and Machine Learning engineers, leading to several demonstrations that apply data-augmenting approaches in order to obtain a robust and, at the same time, accurate enough learning behavior. The main reason is the existence of much unlabeled data that are ignored by conventional supervised approaches, reducing the chance of enriching the final formatted hypothesis. However, the majority of the proposed methods that operate using both kinds of these data are oriented toward exploiting only one category of these algorithms, without combining their strategies. Since the most popular of them regarding the classification task are Active and Semi-supervised Learning approaches, we aim to design a framework that combines both of them trying to fuse their advantages during the main core of the learning process. Thus, we conduct an empirical evaluation of such a combinatory approach over three problems, which stem from various fields but are all tackled through the use of acoustical signals, operating under the pool-based scenario: gender identification, emotion detection and automatic speaker recognition. Into the proposed combinatory framework, which operates under training sets with small cardinality, our results prove the benefits of adopting such kind of semi-automated approaches regarding both the achieved predictive correctness when reduced consumption of resources takes place, as well as the smoothness of the learning convergence. Several learners have been examined for reaching to more general conclusions, and a variant of self-training scheme has been also examined.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {3–20},
numpages = {18},
keywords = {Combined learning framework, Self-training scheme, Active learning queries, Acoustical signal classification, Data augmentation techniques, Semi-automated approaches}
}

@article{10.14778/3450980.3450989,
author = {Liu, Tongyu and Fan, Ju and Luo, Yinqing and Tang, Nan and Li, Guoliang and Du, Xiaoyong},
title = {Adaptive data augmentation for supervised learning over missing data},
year = {2021},
issue_date = {March 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3450980.3450989},
doi = {10.14778/3450980.3450989},
abstract = {Real-world data is dirty, which causes serious problems in (supervised) machine learning (ML). The widely used practice in such scenario is to first repair the labeled source (a.k.a. train) data using rule-, statistical- or ML-based methods and then use the "repaired" source to train an ML model. During production, unlabeled target (a.k.a. test) data will also be repaired, and is then fed in the trained ML model for prediction. However, this process often causes a performance degradation when the source and target datasets are dirty with different noise patterns, which is common in practice.In this paper, we propose an adaptive data augmentation approach, for handling missing data in supervised ML. The approach extracts noise patterns from target data, and adapts the source data with the extracted target noise patterns while still preserving supervision signals in the source. Then, it patches the ML model by retraining it on the adapted data, in order to better serve the target. To effectively support adaptive data augmentation, we propose a novel generative adversarial network (GAN) based framework, called DAGAN, which works in an unsupervised fashion. DAGAN consists of two connected GAN networks. The first GAN learns the noise pattern from the target, for target mask generation. The second GAN uses the learned target mask to augment the source data, for source data adaptation. The augmented source data is used to retrain the ML model. Extensive experiments show that our method significantly improves the ML model performance and is more robust than the state-of-the-art missing data imputation solutions for handling datasets with different missing value patterns.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1202–1214},
numpages = {13}
}

@article{10.1016/j.cor.2021.105504,
author = {Xu, Xiang and Zhu, Daoli},
title = {New method for solving Ivanov regularization-based support vector machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {136},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2021.105504},
doi = {10.1016/j.cor.2021.105504},
journal = {Comput. Oper. Res.},
month = dec,
numpages = {10},
keywords = {Support vector machine, Ivanov regularization, Randomized primal–dual coordinate method}
}

@article{10.1016/j.knosys.2021.107358,
author = {Tang, Mengzi and P\'{e}rez-Fern\'{a}ndez, Ra\'{u}l and De Baets, Bernard},
title = {A comparative study of machine learning methods for ordinal classification with absolute and relative information},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {230},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107358},
doi = {10.1016/j.knosys.2021.107358},
journal = {Know.-Based Syst.},
month = oct,
numpages = {15},
keywords = {Machine learning, Absolute information, Relative information, Ordinal classification}
}

@inproceedings{10.1007/978-3-030-88238-9_10,
author = {Yadav, Tarun and Kumar, Manoj},
title = {Differential-ML Distinguisher: Machine Learning Based Generic Extension for&nbsp;Differential Cryptanalysis},
year = {2021},
isbn = {978-3-030-88237-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88238-9_10},
doi = {10.1007/978-3-030-88238-9_10},
abstract = {The differential attack is a basic cryptanalytic technique for block ciphers. Application of machine learning shows promising results for the differential cryptanalysis. In this paper, we present a new technique to extend the classical differential distinguisher using machine learning (ML). We use r-round classical differential distinguisher to build an s-round ML based differential distinguisher. This s-round ML distinguisher is used to construct an (r+s)-round differential-ML distinguisher with the reduced data complexity. We demonstrate this technique on the lightweight block ciphers SPECK32, SIMON32, and GIFT64 by constructing the differential-ML distinguishers. The data complexities of distinguishers for 9-round SPECK32, 12-round SIMON32, and 8-round GIFT64 are reduced from 230 to 220, 234 to 222, and 238 to 220 respectively. Moreover, the differential-ML distinguisher for SIMON32 is the first 12-round distinguisher with the data complexity less than 232.},
booktitle = {Progress in Cryptology – LATINCRYPT 2021: 7th International Conference on Cryptology and Information Security in Latin America, Bogot\'{a}, Colombia, October 6–8, 2021, Proceedings},
pages = {191–212},
numpages = {22},
keywords = {Block cipher, Differential cryptanalysis, Machine learning},
location = {Bogot\'{a}, Colombia}
}

@inproceedings{10.1109/MILCOM52596.2021.9652909,
author = {Chin, Peter and Do, Emily and Doucette, Cody and Kalashian, Brandon and Last, David and Lenz, Nathan and Lu, Edward and Minor, Devon and Noyes, Elias and Rock, Colleenn and Soule, Nathaniel and Walczak, Nicholas and Canestrare, Dave},
title = {TAK-ML: Applying Machine Learning at the Tactical Edge},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM52596.2021.9652909},
doi = {10.1109/MILCOM52596.2021.9652909},
abstract = {The “Every Soldier is a Sensor” (ES2) concept employs warfighters' proximity to unfolding events in order to provide better situational awareness and decision-making capabilities. However, today's ES2 practices put the burden of data collection on warfighters themselves, and the burden of interpretation (across potentially many inputs) on commanders. This leads to a situation where data collection is limited by the capacity of the warfighter (who is busy executing their core objectives), and data fusion, interpretation, and analysis are limited by the cognitive constraints of the human commanders and analysts interpreting the potentially massive amounts of data. The TAK-ML framework transitions these burdens to machines, allowing collection, fusion, and learning to operate at machine speed and scale. To accomplish this, TAK-ML takes recent advancements in mobile device capabilities and machine learning techniques and applies them to the Tactical Assault Kit (TAK) ecosystem, e.g., ATAK mobile devices and TAK servers, to facilitate the easy application of ML to real mission sets. This paper describes the TAK-ML framework which supports data collection, model building, and model execution/employment in tactical environments, as well as a set of initial applications of this framework. The framework and applications are described and evaluated, showing the capabilities available, the ease of use of the system, and initial insights into the efficacy of the resulting models and applications.},
booktitle = {MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM)},
pages = {108–114},
numpages = {7},
location = {San Diego, CA, USA}
}

@inproceedings{10.1145/3394486.3406461,
author = {Ahmad, Muhammad Aurangzeb and Patel, Arpit and Eckert, Carly and Kumar, Vikas and Teredesai, Ankur},
title = {Fairness in Machine Learning for Healthcare},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406461},
doi = {10.1145/3394486.3406461},
abstract = {The issue of bias and fairness in healthcare has been around for centuries. With the integration of AI in healthcare the potential to discriminate and perpetuate unfair and biased practices in healthcare increases many folds The tutorial focuses on the challenges, requirements and opportunities in the area of fairness in healthcare AI and the various nuances associated with it. The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3529–3530},
numpages = {2},
keywords = {fate ml, fatml, healthcare ai, machine learning in healthcare, fairness},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3430665.3456326,
author = {Wunderlich, Linus and Higgins, Allen and Lichtenstein, Yossi},
title = {Machine Learning for Business Students: An Experiential Learning Approach},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456326},
doi = {10.1145/3430665.3456326},
abstract = {This paper reports on the design and teaching of a course on Machine Learning (ML) for business students, focusing on the interaction between content and teaching approach. We demonstrate that the nature of ML technology is amenable to experiential learning and is well suited for business students as users of the technology. The community-based tools, documentation and datasets enable non-programmers to use and adapt open, public-domain ML examples. Students learn how to select algorithms for specific data and tasks, experiment with hyper-parameters and neural-network structures, evaluate results and interpret their business implications. We conclude that business students, at least in our school, often have good abstract understanding of computing and may be ready for deeper learning of digital technology. The general-purpose nature of ML makes it suitable for real-world business problems, making it business-relevant technology that should be introduced into business schools.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {512–518},
numpages = {7},
keywords = {business analytics and big data, experiential learning, higher education, machine learning, non-CS majors},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{10.5555/3172077.3172089,
author = {Bing, Lidong and Cohen, William W. and Dhingra, Bhuwan},
title = {Using graphs of classifiers to impose declarative constraints on semi-supervised learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1454–1460},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3472749.3474734,
author = {Fran\c{c}oise, Jules and Caramiaux, Baptiste and Sanchez, T\'{e}o},
title = {Marcelle: Composing Interactive Machine Learning Workflows and Interfaces},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474734},
doi = {10.1145/3472749.3474734},
abstract = {Human-centered approaches to machine learning have established theoretical foundations, design principles and interaction techniques to facilitate end-user interaction with machine learning systems. Yet, general-purpose toolkits supporting the design of interactive machine learning systems are still missing, despite their potential to foster reuse, appropriation and collaboration between different stakeholders including developers, machine learning experts, designers and end users. In this paper, we present an architectural model for toolkits dedicated to the design of human interactions with machine learning. The architecture is built upon a modular collection of interactive components that can be composed to build interactive machine learning workflows, using reactive pipelines and composable user interfaces. We introduce Marcelle, a toolkit for the design of human interactions with machine learning that implements this model. We illustrate Marcelle with two implemented case studies: (1) a HCI researcher conducts user studies to understand novice interaction with machine learning, and (2) a machine learning expert and a clinician collaborate to develop a skin cancer diagnosis system. Finally, we discuss our experience with the toolkit, along with its limitation and perspectives.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–53},
numpages = {15},
keywords = {Architectural Model, Interactive Machine Learning, Machine Teaching, Toolkit.},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3475992.3475998,
author = {Amelin, Vladislav and Romanov, Nikita and Vasilyev, Robert and Shvets, Rostyslav and Yanovich, Yury and Zhygulin, Viacheslav},
title = {Machine Learning View on Blockchain Parameter Adjustment},
year = {2021},
isbn = {9781450389518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475992.3475998},
doi = {10.1145/3475992.3475998},
abstract = {A fundamental problem in distributed computing is achieving agreement among many parties for a single data value in the presence of faulty processes–to get consensus. The consensus mechanism is an underlying part of blockchain design and commits new blocks and changes protocol itself. In addition to classic correctness requirements, blockchains need specific ones: high performance regarding transactions per second, fast transaction confirmation,&nbsp;etc. Blockchains control the requirements with parameters. But how to meet qualitative and optimize quantitative requirements? Typically we have the main blockchain network without access to try different parameters and the test network to do whatever we want. In the paper, we provide a machine learning view on the blockchain parameter adjustment. We list the blockchain parameters for Solana blockchain and apply feature importance to select the most significant parameters during the forthcoming optimization.},
booktitle = {Proceedings of the 2021 3rd Blockchain and Internet of Things Conference},
pages = {38–43},
numpages = {6},
keywords = {blockchain, consensus, machine learning, optimization, simulation},
location = {Ho Chi Minh City, Vietnam},
series = {BIOTC '21}
}

@inproceedings{10.1145/3318299.3318363,
author = {Zheng, Heci and Wu, Chunhua},
title = {Predicting Personality Using Facebook Status Based on Semi-supervised Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318363},
doi = {10.1145/3318299.3318363},
abstract = {Personality analysis on social media is a research hotspot due to the importance of personality research in psychology as well as the rapid development of social media. Many studies have used social media status to analyze user's personality, but most of them are conducted on inadequate label data and linguistic features. In this paper, to explore the usage of unlabeled data on personality analysis, a personality analysis framework based on semi-supervised learning is introduced. Besides, for making full use of the language information in social media status, the well-known n-gram model is adopted to extract linguistic features. The experimental results demonstrate the semi-supervised learning can take advantage of unlabeled data and improve the accuracy of prediction model.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {59–64},
numpages = {6},
keywords = {Personality, semi-surpervised learning, social media status},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1007/s00766-013-0165-8,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Dynamic decision models for staged software product line configuration},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0165-8},
doi = {10.1007/s00766-013-0165-8},
abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.},
journal = {Requir. Eng.},
month = jun,
pages = {187–212},
numpages = {26},
keywords = {Feature models, Software product lines, Stakeholder preferences, Utility elicitation}
}

@article{10.1007/s10664-021-09994-0,
author = {Vitui, Arthur and Chen, Tse-Hsun (Peter)},
title = {MLASP: Machine learning assisted capacity planning: An industrial experience report},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09994-0},
doi = {10.1007/s10664-021-09994-0},
abstract = {In industrial environments it is critical to find out the capacity of a system and plan for a deployment layout that meets the production traffic demands. The system capacity is influenced by both the performance of the system’s constituting components and the physical environment setup. In a large system, the configuration parameters of individual components give the flexibility to developers and load test engineers to tune system performance without changing the source code. However, due to the large search space, estimating the capacity of the system given different configuration values is a challenging and costly process. In this paper, we propose an approach, called MLASP, that uses machine learning models to predict the system key performance indicators (i.e., KPIs), such as throughput, given a set of features made off configuration parameter values, including server cluster setup, to help engineers in capacity planning for production environments. Under the same load, we evaluate MLASP on two large-scale mission-critical enterprise systems developed by Ericsson and on one open-source system. We find that: 1) MLASP can predict the system throughput with a very high accuracy. The difference between the predicted and the actual throughput is less than 1%; and 2) By using only a small subset of the training data (e.g., 3% of the entire data for the open-source system), MLASP can still predict the throughput accurately. We also document our experience of successfully integrating the approach into an industrial setting. In summary, this paper highlights the benefits and potential of using machine learning models to assist load test engineers in capacity planning.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {27},
keywords = {Load testing, Capacity testing, Performance testing, Machine learning, Deep learning}
}

@article{10.1016/j.inffus.2019.12.001,
author = {Meng, Tong and Jing, Xuyang and Yan, Zheng and Pedrycz, Witold},
title = {A survey on machine learning for data fusion},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2019.12.001},
doi = {10.1016/j.inffus.2019.12.001},
journal = {Inf. Fusion},
month = may,
pages = {115–129},
numpages = {15},
keywords = {Data fusion, Machine learning, Fusion methods, Fusion criteria}
}

@article{10.1137/20M1317992,
author = {Sim, Byeongsu and Oh, Gyutaek and Kim, Jeongsol and Jung, Chanyong and Ye, Jong Chul},
title = {Optimal Transport Driven CycleGAN for Unsupervised Learning in Inverse Problems},
year = {2020},
issue_date = {January 2020},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1137/20M1317992},
doi = {10.1137/20M1317992},
abstract = {To improve the performance of  classical generative adversarial networks (GANs), Wasserstein generative adversarial networks (WGANs) were developed as a Kantorovich dual formulation of the optimal transport (OT) problem using Wasserstein-1 distance. However, it was not clear how CycleGAN-type generative models can be derived from the OT theory. Here we show that a novel  CycleGAN architecture can be derived as a Kantorovich dual OT formulation if a penalized least squares (PLS) cost with deep learning--based inverse path penalty is used as a transportation cost. One of the most important advantages of this formulation is that depending on the knowledge of the forward problem, distinct variations of CycleGAN architecture can be derived: for example,  one with two pairs of generators and discriminators, and the other with only a single pair of generator and discriminator. Even for the two generator cases, we show that the structural knowledge of the forward operator can lead to a simpler generator architecture which significantly simplifies the neural network training. The new CycleGAN formulation, which we call the OT-CycleGAN, has been applied for various biomedical imaging problems,  such as
accelerated magnetic resonance imaging (MRI), super-resolution microscopy, and low-dose X-ray computed tomography (CT). Experimental results confirm the efficacy and flexibility of the theory.},
journal = {SIAM J. Img. Sci.},
month = jan,
pages = {2281–2306},
numpages = {26},
keywords = {unsupervised learning, optimal transport, CycleGAN, penalized least squares, inverse problems, 68Q32, 68T05, 68T45, 65L09, 68U10}
}

@inproceedings{10.1007/978-3-030-77870-5_28,
author = {Benamira, Adrien and Gerault, David and Peyrin, Thomas and Tan, Quan Quan},
title = {A Deeper Look at Machine Learning-Based Cryptanalysis},
year = {2021},
isbn = {978-3-030-77869-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77870-5_28},
doi = {10.1007/978-3-030-77870-5_28},
abstract = {At CRYPTO’19, Gohr proposed a new cryptanalysis strategy based on the utilisation of machine learning algorithms. Using deep neural networks, he managed to build a neural based distinguisher that surprisingly surpassed state-of-the-art cryptanalysis efforts on one of the versions of the well studied NSA block cipher SPECK (this distinguisher could in turn be placed in a larger key recovery attack). While this work opens new possibilities for machine learning-aided cryptanalysis, it remains unclear how this distinguisher actually works and what information is the machine learning algorithm deducing. The attacker is left with a black-box that does not tell much about the nature of the possible weaknesses of the algorithm tested, while hope is thin as interpretability of deep neural networks is a well-known difficult task.In this article, we propose a detailed analysis and thorough explanations of the inherent workings of this new neural distinguisher. First, we studied the classified sets and tried to find some patterns that could guide us to better understand Gohr’s results. We show with experiments that the neural distinguisher generally relies on the differential distribution on the ciphertext pairs, but also on the differential distribution in penultimate and antepenultimate rounds. In order to validate our findings, we construct a distinguisher for SPECK cipher based on pure cryptanalysis, without using any neural network, that achieves basically the same accuracy as Gohr’s neural distinguisher and with the same efficiency (therefore improving over previous non-neural based distinguishers).Moreover, as another approach, we provide a machine learning-based distinguisher that strips down Gohr’s deep neural network to a bare minimum. We are able to remain very close to Gohr’s distinguishers’ accuracy using simple standard machine learning tools. In particular, we show that Gohr’s neural distinguisher is in fact inherently building a very good approximation of the Differential Distribution Table (DDT) of the cipher during the learning phase, and using that information to directly classify ciphertext pairs. This result allows a full interpretability of the distinguisher and represents on its own an interesting contribution towards interpretability of deep neural networks.Finally, we propose some method to improve over Gohr’s work and possible new neural distinguishers settings. All our results are confirmed with experiments we have been conducted on SPECK block cipher (source code available online).},
booktitle = {Advances in Cryptology – EUROCRYPT 2021: 40th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, October 17–21, 2021, Proceedings, Part I},
pages = {805–835},
numpages = {31},
keywords = {Differential cryptanalysis, SPECK, Machine learning, Deep neural networks, Interpretability},
location = {Zagreb, Croatia}
}

@article{10.1007/s11128-019-2470-8,
author = {Hou, S. C. and Yi, X. X.},
title = {Quantum Lyapunov control with machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1570-0755},
url = {https://doi.org/10.1007/s11128-019-2470-8},
doi = {10.1007/s11128-019-2470-8},
abstract = {Quantum state engineering is a central task in Lyapunov-based quantum control. Given different initial states, better performance may be achieved if the control parameters, such as the Lyapunov function, are individually optimized for each initial state, however, at the expense of computing resources. To tackle this issue, we propose an initial-state-adaptive Lyapunov control strategy with machine learning. Specifically, artificial neural networks are used to learn the relationship between the optimal control parameters and initial states through supervised learning with samples. Two designs are presented where the feedforward neural network and the general regression neural network are used to select control schemes and design Lyapunov functions, respectively. We demonstrate the performance of the designs with a three-level quantum system for an eigenstate control problem. Since the sample generation and the training of neural networks are carried out in advance, the initial-state-adaptive Lyapunov control can be implemented for new initial states without much increase of computational resources.},
journal = {Quantum Information Processing},
month = nov,
numpages = {20},
keywords = {Lyapunov control, Machine learning, Neural network, Quantum state preparation}
}

@inproceedings{10.1007/978-3-030-64881-7_16,
author = {Sharma, Arnab and Wehrheim, Heike},
title = {Automatic Fairness Testing of Machine Learning Models},
year = {2020},
isbn = {978-3-030-64880-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64881-7_16},
doi = {10.1007/978-3-030-64881-7_16},
abstract = {In recent years, there has been an increased application of machine learning (ML) to decision making systems. This has prompted an urgent need for validating requirements on ML models. Fairness is one such requirement to be ensured in numerous application domains. It specifies a software as “learned” by an ML algorithm to not be biased in the sense of discriminating against some attributes (like gender or age), giving different decisions upon flipping the values of these attributes.In this work, we apply verification-based testing (VBT) to the fairness checking of ML models. Verification-based testing employs verification technology to generate test cases potentially violating the property under interest. For fairness testing, we additionally provide a specification language for the formalization of different fairness requirements. From the ML model under test and fairness specification VBT automatically generates test inputs specific to the specified fairness requirement. The empirical evaluation on several benchmark ML models shows verification-based testing to perform better than existing fairness testing techniques with respect to effectiveness.},
booktitle = {Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings},
pages = {255–271},
numpages = {17},
keywords = {Fairness, Machine learning testing, SMT solving},
location = {Naples, Italy}
}

@article{10.1145/3424660,
author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua},
title = {From Server-Based to Client-Based Machine Learning: A Comprehensive Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3424660},
doi = {10.1145/3424660},
abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users’ local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users’ sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {6},
numpages = {36},
keywords = {Mobile intelligence, decentralized training, distributed system, federated learning, machine learning}
}

@inproceedings{10.1007/978-3-030-79379-1_1,
author = {Meinke, Karl and Khosrowjerdi, Hojat},
title = {Use Case Testing: A Constrained Active Machine Learning Approach},
year = {2021},
isbn = {978-3-030-79378-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79379-1_1},
doi = {10.1007/978-3-030-79379-1_1},
abstract = {As a methodology for system design and testing, use cases are well-known and widely used. While current active machine learning (ML) algorithms can effectively automate unit testing, they do not scale up&nbsp;to use case testing of complex systems in an efficient way.We present a new parallel distributed processing (PDP) architecture for a constrained active machine learning (CAML) approach to use case testing. To exploit CAML we introduce a use case modeling language with: (i) compile-time constraints on query generation, and (ii) run-time constraints using dynamic constraint checking. We evaluate this approach by applying a prototype implementation of CAML to use case testing of simulated multi-vehicle autonomous driving scenarios.},
booktitle = {Tests and Proofs: 15th International Conference, TAP 2021, Held as Part of STAF 2021, Virtual Event, June 21–22, 2021, Proceedings},
pages = {3–21},
numpages = {19},
keywords = {Autonomous driving, Constraint solving, Learning-based testing, Machine learning, Model checking, Requirements testing, Use case testing}
}

@inproceedings{10.1145/3233547.3233667,
author = {Ahmad, Muhammad Aurangzeb and Eckert, Carly and Teredesai, Ankur},
title = {Interpretable Machine Learning in Healthcare},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233667},
doi = {10.1145/3233547.3233667},
abstract = {This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {559–560},
numpages = {2},
keywords = {explainable ai, explainable machine learning, interpretable machine learning, machine learning in healthcare},
location = {Washington, DC, USA},
series = {BCB '18}
}

@phdthesis{10.5555/AAI28643572,
author = {Lu, Jie and Prateek, Mittal, and Mengdi, Wang, and Sun-Yuan, Kung,},
advisor = {K, Jha, Niraj and Naveen, Verma,},
title = {Energy-Efficient Implementation of Machine Learning Algorithms},
year = {2021},
isbn = {9798471106895},
publisher = {Princeton University},
address = {USA},
abstract = {Pattern-recognition algorithms from the domain of machine learning play a prominent role in embedded sensing systems, in order to derive inferences from sensor data. Very often, such systems face severe energy constraints. The focus of this thesis is on mitigating the energy required for computation, communication, and storage by exploiting various forms of computation algorithms. In the first part of our work, we focus on reducing the computations necessary during linear signal-processing. In order to achieve computation reduction, we consider random projection. Random projection is a form of compression that preserves a similarity metric widely used for pattern recognition is used for computational energy reduction. The form of compression is random projection, and the similarity metric is inner product between source vectors. Given the prominence of random projections within compressive sensing, previous research has explored this idea for application to compressively-sensed signals. We show that random projections can be exploited more generally without compressive sensing, enabling significant reduction in computational energy and avoiding a significant source of error. The approach is referred to as compressed signal processing (CSP). It applies to Nyquist-sampled signals.The second part of our work focuses on dealing with signal processing that may not be linear. We look into approximate computing and its potential as an algorithmic approach to reducing energy. Approximate computing is a broad approach that has recently received considerable attention in the context of inference systems. This stems from the observation that many inference systems exhibit various forms of tolerance to data noise. While some systems have demonstrated significant approximation-vs.-energy knobs to exploit this, they have been applicable to specific kernels and architectures; the more generally available knobs have been relatively weak, resulting in large data noise for relatively modest energy savings (e.g., voltage overscaling, bit precision scaling). In this work, we explore the use of genetic programming (GP) to compute approximate features. Further, we leverage a method that enhances tolerance to feature-data noise through directed retraining of the inference stage. Previous work in GP has shown that it generalizes well to enable approximation of a broad range of computations, raising the potential for broad applicability of the proposed approach. The focus on feature extraction is deliberate because it involves diverse, often highly nonlinear, operations, challenging general applicability of energy-reducing approaches.The third part of our work takes into consideration multi-task algorithms. By exploiting the concept of transfer learning and energy-efficient data show accelerators, we show that the use of convolutional autoencoders can enable various levels of reduction in computational energy and avoid a significant reduction in inference performance when multiple task categories are targeted for obtaining an inference. In order to minimize inference computational energy, a convolutional autoencoder is used for learning a generalized representation of inputs. We consider three scenarios: transferring layers using convolutional autoencoders, transferring layers using convolutional neural networks trained on different tasks, and no layer transfer. We also take into account the performance when transferring only convolutional layers versus when transferring convolutional layers and a fully connected layer.We study our methodologies through validations of their generalizability and through applications using clinical and image data.},
note = {AAI28643572}
}

@article{10.1007/s42979-020-00374-x,
author = {Ugraiah, Purushotham and Shivabasave Gowda, Chethan Kanakapura},
title = {Semi-Supervised Learning to Enhance Speech Signal for Mobile Communication},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00374-x},
doi = {10.1007/s42979-020-00374-x},
abstract = {Machine learning algorithm to enhance the complex speech signal for mobile communication is one of the research problems in signal processing. The objective of this research paper is to develop a learning algorithm that improves the quality and intelligibility of voice signals that gets are corrupted by real world noise while they are transmitted through the channel. In this paper, we consider a semi-supervised machine learning algorithm for mobile phones that comes with system software to improve SNR of speech signal which is corrupted by manmade disturbance. Most of the disturbances are non-stationary where the effect of noise is non-uniform for all spectral components. In the projected algorithm training, the system is completed with a set of speech and noise data base. System parameters are derived during training process; these parameters are updated as per the disturbance present in the signal. These parameters are used to remove the noise present in speech signal. The obtained results show a substantial progress in SNR by 5–8% as compared to traditional methods.},
journal = {SN Comput. Sci.},
month = nov,
numpages = {10},
keywords = {Speech enhancement, Semi-supervised, Non-stationary, Machine learning, SNR}
}

@inproceedings{10.1145/3461702.3462556,
author = {Simons, Joshua and Adams Bhatti, Sophia and Weller, Adrian},
title = {Machine Learning and the Meaning of Equal Treatment},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462556},
doi = {10.1145/3461702.3462556},
abstract = {Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {956–966},
numpages = {11},
keywords = {equal treatment, fairness, machine learning, philosophy, politics},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1145/3433987,
author = {Kristiansen, Stein and Nikolaidis, Konstantinos and Plagemann, Thomas and Goebel, Vera and Traaen, Gunn Marit and \O{}verland, Britt and Aaker\o{}y, Lars and Hunt, Tove-Elizabeth and Loennechen, Jan P\r{a}l and Steinshamn, Sigurd Loe and Bendz, Christina Holt and Anfinsen, Ole-Gunnar and Gullestad, Lars and Akre, Harriet},
title = {Machine Learning for Sleep Apnea Detection with Unattended Sleep Monitoring at Home},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3433987},
doi = {10.1145/3433987},
abstract = {Sleep apnea is a common and strongly under-diagnosed severe sleep-related respiratory disorder with periods of disrupted or reduced breathing during sleep. To diagnose sleep apnea, sleep data are collected with either polysomnography or polygraphy and scored by a sleep expert. We investigate in this work the use of supervised machine learning to automate the analysis of polygraphy data from the A3 study containing more than 7,400 hours of sleep monitoring data from 579 patients. We conduct a systematic comparative study of classification performance and resource use with different combinations of 27 classifiers and four sleep signals. The classifiers achieve up to 0.8941 accuracy (kappa: 0.7877) when using all four signal types simultaneously and up to 0.8543 accuracy (kappa: 0.7080) with only one signal, i.e., oxygen saturation. Methods based on deep learning outperform other methods by a large margin. All deep learning methods achieve nearly the same maximum classification performance even when they have very different architectures and sizes. When jointly accounting for classification performance, resource consumption and the ability to achieve with less training data high classification performance, we find that convolutional neural networks substantially outperform the other classifiers.},
journal = {ACM Trans. Comput. Healthcare},
month = feb,
articleno = {14},
numpages = {25},
keywords = {Sleep apnea, machine learning, polygraphy, portable sleep monitor, unattended sleep monitoring}
}

@inproceedings{10.5555/3367471.3367546,
author = {Verma, Vikas and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and Lopez-Paz, David},
title = {Interpolation consistency training for semi-supervised learning},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {3635–3641},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1016/j.procs.2021.09.132,
author = {Khang, Pham Quoc and Kaczmarczyk, Klaudia and Tutak, Piotr and Golec, Pawe\l{} and Kuziak, Katarzyna and Depczy\'{n}ski, Rados\l{}aw and Hernes, Marcin and Rot, Artur},
title = {Machine learning for liquidity prediction on Vietnamese stock market},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.132},
doi = {10.1016/j.procs.2021.09.132},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {3590–3597},
numpages = {8},
keywords = {stock market, liquidity, machine learning, prediction}
}

@inproceedings{10.1145/3394486.3403214,
author = {Guo, Lan-Zhe and Zhou, Zhi and Li, Yu-Feng},
title = {RECORD: Resource Constrained Semi-Supervised Learning under Distribution Shift},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403214},
doi = {10.1145/3394486.3403214},
abstract = {Semi-supervised learning (SSL) tries to improve performance with the use of massive unlabeled data, which typically works in an offline manner with two assumptions. i) Data distribution is static; ii) Data storage overhead is unlimited. In many online tasks, however, none of the above assumptions is valid. For example, in online image classification, a large amount of unlabeled images increases sharply, which makes it difficult to store them in full; meanwhile, the content of unlabeled images changes constantly, and it is no longer suitable to assume a fixed distribution. We call such a novel setting Resource Constrained SSL under Distribution Shift (or Record for short) and to our best knowledge, it has not been thoroughly studied yet. This paper presents a systemic solution Record consisting of three sub-steps, that is, distribution tracking, sample selection and model updating. Specifically, we propose an effective method to track the distribution changes and locate distribution shifted samples. A novel influence-based approach is used to select the most influential samples for the distribution change based on resource constraints. Finally, we free up memory to put the latest unlabeled data with its pseudo-label for the next distribution tracking. Extensive empirical results confirm the effectiveness of our scheme. In the case of diverse and unknown distribution shifts, our solution is consistently and clearly better than many baseline and SOTA methods along with the memory budget and in some cases it can even approximate the performance of oracle.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1636–1644},
numpages = {9},
keywords = {distribution shift, resource constraint, semi-supervised learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1613/jair.1.12228,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {245–317},
numpages = {73}
}

@article{10.1016/j.neucom.2015.11.042,
author = {Li, Jianqiang and Wang, Fei},
title = {Semi-supervised learning via mean field methods},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {177},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.11.042},
doi = {10.1016/j.neucom.2015.11.042},
abstract = {The recent years have witnessed a surge of interest in semi-supervised learning methods. Numerous methods have been proposed for learning from partially labeled data. In this paper, a novel semi-supervised learning approach based on statistical physics is proposed. We treat each data point as an Ising spin and the interaction between pairwise spins is captured by the similarity between the pairwise points. The labels of the data points are treated as the directions of the corresponding spins. In semi-supervised setting, some of the spins have fixed directions (which corresponds to the labeled data), and our task is to determine the directions of other spins. An approach based on the Mean Field theory is proposed to achieve this goal. Finally the experimental results on both toy and real world data sets are provided to show the effectiveness of our method.},
journal = {Neurocomput.},
month = feb,
pages = {385–393},
numpages = {9},
keywords = {Mean field, Semi-supervised learning}
}

@inproceedings{10.1145/3386263.3407599,
author = {Zhang, Jiliang and Li, Chen and Ye, Jing and Qu, Gang},
title = {Privacy Threats and Protection in Machine Learning},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3407599},
doi = {10.1145/3386263.3407599},
abstract = {With the improvement of computing power and storage level, Machine Learning (ML), especially Deep Learning (DL), has shown its capabilities beyond humans in areas such as image recognition, speech processing, and content recommendation. However, the data collected to build ML models often contains sensitive information, and models may have high commercial value. Compared with the security problem of model prediction errors caused by malicious external influences, privacy threats have not attracted widespread attention, and they have characteristics that are difficult to define and detect. This article reviews recent research progress on ML privacy. First, the privacy threats on data and models in different scenarios are described in detail. Then, typical privacy protection methods are introduced. Finally, the limitations and future development trends of ML privacy research are discussed.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {531–536},
numpages = {6},
keywords = {machine learning, privacy protection, privacy threats},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.1007/978-3-030-58601-0_26,
author = {Ke, Zhanghan and Qiu, Di and Li, Kaican and Yan, Qiong and Lau, Rynson W. H.},
title = {Guided Collaborative Training for Pixel-Wise Semi-Supervised Learning},
year = {2020},
isbn = {978-3-030-58600-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58601-0_26},
doi = {10.1007/978-3-030-58601-0_26},
abstract = {We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collaborative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outperforms state-of-the-art SSL methods by a large margin.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII},
pages = {429–445},
numpages = {17},
keywords = {Semi-supervised learning, Pixel-wise vision tasks},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.5555/3524938.3525766,
author = {Sim, Rachael Hwee Ling and Zhang, Yehong and Chan, Mun Choon and Low, Bryan Kian Hsiang},
title = {Collaborative machine learning with incentive-aware model rewards},
year = {2020},
publisher = {JMLR.org},
abstract = {Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {828},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-319-29817-7_11,
author = {Saydali, Sajad and Parvin, Hamid and Safaei, Ali A.},
title = {Classifier Ensemble by Semi-supervised Learning: Local Aggregation Methodology},
year = {2015},
isbn = {9783319298160},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29817-7_11},
doi = {10.1007/978-3-319-29817-7_11},
abstract = {A novel approach for automatic mine detection using SONAR data is proposed in this paper relying on a probabilistic based fusion method to classify SONAR instances as mine or mine-like object. The proposed semi-supervised algorithm minimizes some target functions, which fuse context identification, multi-algorithm fusion criteria and a semi-supervised learning term. Our optimization purpose is to learn contexts as compact clusters in subspaces of the high-dimensional feature space through probabilistic feature discrimination and semi-supervised learning. The semi-supervised clustering component appoints degree of typicality to each data sample in order to identify and reduce the influence of noise points and outliers. Then, the approach yields optimal fusion parameters for each context. The experiments on synthetic datasets and standard SONAR dataset illustrate that our semi-supervised local fusion outperforms individual classifiers and unsupervised local fusion.},
booktitle = {Revised Selected Papers of the 10th International Doctoral Workshop on Mathematical and Engineering Methods in Computer Science - Volume 9548},
pages = {119–132},
numpages = {14},
keywords = {Classifier fusion, Ensemble learning, Supervised learning},
location = {Tel\u{a}\'{z}, Czech Republic},
series = {MEMICS 2015}
}

@article{10.1109/TASLP.2021.3133189,
author = {Wu, Haibin and Li, Xu and Liu, Andy T. and Wu, Zhiyong and Meng, Helen and Lee, Hung-Yi},
title = {Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3133189},
doi = {10.1109/TASLP.2021.3133189},
abstract = {Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. The purification module aims at alleviating the adversarial perturbations in the samples and pulling the contaminated adversarial inputs back towards the decision boundary. Experimental results show that our proposed purification module effectively counters adversarial attacks and outperforms traditional filters from both alleviating the adversarial noise and maintaining the performance of genuine samples. The detection module aims at detecting adversarial samples from genuine ones based on the statistical properties of ASV scores derived by a unique ASV integrating with different number of SSLMs. Experimental results show that our detection module helps shield the ASV by detecting adversarial samples. Both purification and detection methods are helpful for defending against different kinds of attack algorithms. Moreover, since there is no common metric for evaluating the ASV performance under adversarial attacks, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {202–217},
numpages = {16}
}

@article{10.1007/s10796-016-9724-0,
author = {Liu, Jun and Timsina, Prem and El-Gayar, Omar},
title = {A comparative analysis of semi-supervised learning: The case of article selection for medical systematic reviews},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-016-9724-0},
doi = {10.1007/s10796-016-9724-0},
abstract = {While systematic reviews are positioned as an essential element of modern evidence-based medical practice, the creation of these reviews is resource intensive. To mitigate this problem, there have been some attempts to leverage supervised machine learning to automate the article triage procedure. This approach has been proved to be helpful for updating existing systematic reviews. However, this technique holds very little promise for creating new reviews because training data is rarely available when it comes to systematic creation. In this research we assess and compare the applicability of semi-supervised learning to overcome this labeling bottleneck and support the creation of systematic reviews. The results indicated that semi-supervised learning could significantly reduce the human effort and is a viable technique for automating medical systematic review creation with a small-sized training dataset.},
journal = {Information Systems Frontiers},
month = apr,
pages = {195–207},
numpages = {13},
keywords = {Active learning, Medical systematic reviews, Self-training, Semi-supervised learning, Text analytics, Text mining}
}

@inproceedings{10.1145/3382734.3405695,
author = {El-Mhamdi, El-Mahdi and Guerraoui, Rachid and Guirguis, Arsany and Hoang, L\^{e} Nguy\^{e}n and Rouault, S\'{e}bastien},
title = {Genuinely Distributed Byzantine Machine Learning},
year = {2020},
isbn = {9781450375825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382734.3405695},
doi = {10.1145/3382734.3405695},
abstract = {Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the "general" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes.We show that this problem can be solved in an asynchronous system, despite the presence of ⅓ Byzantine parameter servers and ⅓ Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony.We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24--176% in the synchronous case and 28--220% in the asynchronous case.},
booktitle = {Proceedings of the 39th Symposium on Principles of Distributed Computing},
pages = {355–364},
numpages = {10},
keywords = {byzantine fault tolerance, byzantine parameter servers, distributed machine learning},
location = {Virtual Event, Italy},
series = {PODC '20}
}

@article{10.1016/j.asoc.2019.105871,
author = {Sarkar, Jnanendra Prasad and Saha, Indrajit and Chakraborty, Sinjan and Maulik, Ujjwal},
title = {Machine learning integrated credibilistic semi supervised clustering for categorical data},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105871},
doi = {10.1016/j.asoc.2019.105871},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {14},
keywords = {Categorical data, Credibilistic clustering, Friedman test, Fuzzy set, Machine learning, Possibilistic measure, Semi supervised clustering, Statistical significance}
}

@inproceedings{10.1007/978-3-030-61470-6_26,
author = {Bure\v{s}, Tom\'{a}\v{s} and Gerostathopoulos, Ilias and Hn\v{e}tynka, Petr and Pacovsk\'{y}, Jan},
title = {Forming Ensembles at Runtime: A Machine Learning Approach},
year = {2020},
isbn = {978-3-030-61469-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61470-6_26},
doi = {10.1007/978-3-030-61470-6_26},
abstract = {Smart system applications (SSAs) built on top of cyber-physical and socio-technical systems are increasingly composed of components that can work both autonomously and by cooperating with each other. Cooperating robots, fleets of cars and fleets of drones, emergency coordination systems are examples of SSAs. One approach to enable cooperation of SSAs is to form dynamic cooperation groups—ensembles—between components at runtime. Ensembles can be formed based on predefined rules that determine which components should be part of an ensemble based on their current state and the state of the environment (e.g., “group together 3 robots that are closer to the obstacle, their battery is sufficient and they would not be better used in another ensemble”). This is a computationally hard problem since all components are potential members of all possible ensembles at runtime. In our experience working with ensembles in several case studies the past years, using constraint programming to decide which ensembles should be formed does not scale for more than a limited number of components and ensembles. Also, the strict formulation in terms of hard/soft constraints does not easily permit for runtime self-adaptation via learning. This poses a serious limitation to the use of ensembles in large-scale and partially uncertain SSAs. To tackle this problem, in this paper we propose to recast the ensemble formation problem as a classification problem and use machine learning to efficiently form ensembles at scale.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Engineering Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part II},
pages = {440–456},
numpages = {17},
keywords = {Adaptation, Ensembles, Cooperative systems, Machine learning},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3473856.3474027,
author = {Foerste, Markus and Nadj, Mario and Knaeble, Merlin and Maedche, Alexander and Gehrmann, Leonie and Stahl, Florian},
title = {An Interactive Machine Learning System for Image Advertisements},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3474027},
doi = {10.1145/3473856.3474027},
abstract = {Advertising is omnipresent in all countries around the world and has a strong influence on consumer behavior. Given that advertisements aim to be memorable, attract attention and convey the intended information in a limited space, it seems striking that previous research in economics and management has mostly neglected the content and style of actual advertisements and their evolution over time. With this in mind, we collected more than one million print advertisements from the English-language weekly news magazine “The Economist” from 1843 to 2014. However, there is a lack of interactive intelligent systems capable of processing such a vast amount of image data and allowing users to automatically and manually add metadata, explore images, find and test assertions, and use machine learning techniques they did not have access to before. Inspired by the research field of interactive machine learning, we propose such a system that enables domain experts like marketing scholars to process and analyze this huge collection of image advertisements.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {574–577},
numpages = {4},
keywords = {advertising, image ads, interactive machine learning},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1145/3313831.3376275,
author = {Dove, Graham and Fayard, Anne-Laure},
title = {Monsters, Metaphors, and Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376275},
doi = {10.1145/3313831.3376275},
abstract = {Machine learning (ML) poses complex challenges for user experience (UX) designers. Typically unpredictable and opaque, it may produce unforeseen outcomes detrimental to particular groups or individuals, yet simultaneously promise amazing breakthroughs in areas as diverse as medical diagnosis and universal translation. This results in a polarized view of ML, which is often manifested through a technology-as-monster metaphor. In this paper, we acknowledge the power and potential of this metaphor by resurfacing historic complexities in human-monster relations. We (re)introduce these liminal and ambiguous creatures, and discuss their relation to ML. We offer a background to designers' use of metaphor, and show how the technology-as-monster metaphor can generatively probe and (re)frame the questions ML poses. We illustrate the effectiveness of this approach through a detailed discussion of an early-stage generative design workshop inquiring into ML approaches to supporting student mental health and well-being.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {generative metaphor, machine learning, monster theory, ux design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1007/978-3-031-21671-8_11,
author = {Rahgooy, Taher and Venable, K. Brent and Trueblood, Jennifer S.},
title = {Integrating Machine Learning and Cognitive Modeling of Decision Making},
year = {2021},
isbn = {978-3-031-21670-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21671-8_11},
doi = {10.1007/978-3-031-21671-8_11},
abstract = {Modeling human decision making plays a fundamental role in the design of intelligent systems capable of rich interactions and effective teamwork. In this paper we consider the task of choice prediction in settings with multiple alternatives. Cognitive models of decision making can successfully replicate and explain behavioral effects involving uncertainty and interactions among alternatives but are computationally intensive to train. ML approaches excel in terms of choice prediction accuracy, but fail to provide insights on the underlying preference reasoning. We study different degrees of integration of ML and cognitive models for this task. We show, via testing on behavioral data, that our hybrid approach, based on the integration of a neural network and the Multi-alternative Linear Ballistic Accumulator cognitive model, requires significantly less time to train, and allows to capture important cognitive parameters while maintaining similar accuracy to the pure ML approach.},
booktitle = {Computational Theory of Mind for Human-Machine Teams: First International Symposium, ToM for Teams 2021, Virtual Event, November 4–6, 2021, Revised Selected Papers},
pages = {173–193},
numpages = {21},
keywords = {Cognitive models, Decision making, Machine learning, Preferential choice prediction, Artificial neural networks, Behavioral effects}
}

@article{10.1016/j.patcog.2017.10.022,
author = {Chang, Jianlong and Wang, Lingfeng and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
title = {Deep unsupervised learning with consistent inference of latent representations},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {77},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.022},
doi = {10.1016/j.patcog.2017.10.022},
abstract = {An end-to-end unsupervised method is proposed to pre-train deep neural networks.The presented model is optimized under the EM algorithm framework.A series of variants of the proposed method are obtained.The convergence of our approach and its variants is theoretically verified. Utilizing unlabeled data to train deep neural networks (DNNs) is a crucial but challenging task. In this paper, we propose an end-to-end approach to tackle this problem with consistent inference of latent representations. Specifically, each unlabeled data point is considered as a seed to generate a set of latent labeled data points by adding various random disturbances or transformations. Under the expectation maximization framework, DNNs can be trained in an unsupervised way by minimizing the distances between the data points with the same latent representations. Furthermore, several variants of our approach can be derived by applying regularized and sparse constraints during optimization. Theoretically, the convergence of the proposed method and its variants are fully analyzed. Experimental results show that the proposed approach can significantly improve the performance on various tasks, including image classification and clustering. Such results also indicate that our method can guide DNNs to learn more invariant feature representations in comparison with traditional unsupervised methods.},
journal = {Pattern Recogn.},
month = may,
pages = {438–453},
numpages = {16},
keywords = {Consistent inference of latent representations, Deep unsupervised learning}
}

@book{10.5555/3278343,
author = {Ghatak, Abhijit},
title = {Machine Learning with R},
year = {2017},
isbn = {9789811068072},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book helps readers understand the mathematics of machine learning, and apply them in different situations. It is divided into two basic parts, the first of which introduces readers to the theory of linear algebra, probability, and data distributions and its applications to machine learning. It also includes a detailed introduction to the concepts and constraints of machine learning and what is involved in designing a learning algorithm. This part helps readers understand the mathematical and statistical aspects of machine learning. In turn, the second part discusses the algorithms used in supervised and unsupervised learning. It works out each learning algorithm mathematically and encodes it in R to produce customized learning applications. In the process, it touches upon the specifics of each algorithm and the science behind its formulation. The book includes a wealth of worked-out examples along with R codes. It explains the code for each algorithm, and readers can modify the code to suit their own needs. The book will be of interest to all researchers who intend to use R for machine learning, and those who are interested in the practical aspects of implementing learning algorithms for data analysis. Further, it will be particularly useful and informative for anyone who has struggled to relate the concepts of mathematics and statistics to machine learning.}
}

@inproceedings{10.1145/3313831.3376488,
author = {Cryan, Jenna and Tang, Shiliang and Zhang, Xinyi and Metzger, Miriam and Zheng, Haitao and Zhao, Ben Y.},
title = {Detecting Gender Stereotypes: Lexicon vs. Supervised Learning Methods},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376488},
doi = {10.1145/3313831.3376488},
abstract = {Biases in language influence how we interact with each other and society at large. Language affirming gender stereotypes is often observed in various contexts today, from recommendation letters and Wikipedia entries to fiction novels and movie dialogue. Yet to date, there is little agreement on the methodology to quantify gender stereotypes in natural language (specifically the English language). Common methodology (including those adopted by companies tasked with detecting gender bias) rely on a lexicon approach largely based on the original BSRI study from 1974.In this paper, we reexamine the role of gender stereotype detection in the context of modern tools, by comparatively analyzing efficacy of lexicon-based approaches and end-to-end, ML-based approaches prevalent in state-of-the-art natural language processing systems. Our efforts using a large dataset show that even compared to an updated lexicon-based approach, end-to-end classification approaches are significantly more robust and accurate, even when trained by moderately sized corpora.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {gender bias, gender stereotypes, lexicon, machine learning, natural language processing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1007/978-3-031-21517-9_12,
author = {Sujatha, G. and Sankareswari, K.},
title = {A Comparative Study on Machine Learning Based Classifier Model for Wheat Seed Classification},
year = {2021},
isbn = {978-3-031-21516-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21517-9_12},
doi = {10.1007/978-3-031-21517-9_12},
abstract = {Seed classification is a process of categorizing different varieties of seeds into different classes on the basis of their morphological features. Seed identification is further complicated due to common object recognition constraints such as light, pose and orientation. Wheat has always been one of the globally common consumed foods in India. A large number of wheat varieties have been cultivated, exported and imported all around the world. Enormous studies have been done on identifying crop diseases and classifying the crop types. In the present work, wheat seed classification is performed to distinguish the three different Indian wheat varieties by their collected morphological features and applied machine learning models to develop wheat variety classification system. The seed features used here are length of kernel, compactness, asymmetry coefficient, width of kernel, length of kernel groove, area and perimeter. The present work carried out with different classifiers such as Decision Tree, Random Forest, Neural Net, Nearest Neighbors, Gaussian Process, AdaBoost, Naive Bayes, Support Vector Machine(SVM) Linear, SVM RBF(SVM with the Radial Basis Function) and SVM Sigmoid with 2&nbsp;K-fold cross validation. Also obtained the results using 5 fold and 10-fold Cross Validation.},
booktitle = {Mining Intelligence and Knowledge Exploration: 9th International Conference, MIKE 2021, Hammamet, Tunisia, November 1–3, 2021, Proceedings},
pages = {120–127},
numpages = {8},
keywords = {Agriculture, Seed classification, SVM, Neural net, Gaussian, K-fold cross validation, Random forest, Decision tree, Classifiers},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1145/3433210.3437513,
author = {Li, Jiangnan and Yang, Yingyuan and Sun, Jinyuan Stella and Tomsovic, Kevin and Qi, Hairong},
title = {ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3437513},
doi = {10.1145/3433210.3437513},
abstract = {Recent research demonstrated that the superficially well-trained machine learning (ML) models are highly vulnerable to adversarial examples. As ML techniques are becoming a popular solution for cyber-physical systems (CPSs) applications in research literatures, the security of these applications is of concern. However, current studies on adversarial machine learning (AML) mainly focus on pure cyberspace domains. The risks the adversarial examples can bring to the CPS applications have not been well investigated. In particular, due to the distributed property of data sources and the inherent physical constraints imposed by CPSs, the widely-used threat models and the state-of-the-art AML algorithms in previous cyberspace research become infeasible.We study the potential vulnerabilities of ML applied in CPSs by proposing Constrained Adversarial Machine Learning (ConAML), which generates adversarial examples that satisfy the intrinsic constraints of the physical systems. We first summarize the difference between AML in CPSs and AML in existing cyberspace systems and propose a general threat model for ConAML. We then design a best-effort search algorithm to iteratively generate adversarial examples with linear physical constraints. We evaluate our algorithms with simulations of two typical CPSs, the power grids and the water treatment system. The results show that our ConAML algorithms can effectively generate adversarial examples which significantly decrease the performance of the ML models even under practical constraints.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {52–66},
numpages = {15},
keywords = {adversarial machine learning, cyber-physical system, intrusion detection},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3368089.3417043,
author = {Ahmed, Md Sohel and Ishikawa, Fuyuki and Sugiyama, Mahito},
title = {Testing machine learning code using polyhedral region},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417043},
doi = {10.1145/3368089.3417043},
abstract = {To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1533–1536},
numpages = {4},
keywords = {Lasso, Machine learning code, Mutation Analysis, Polyhedral region, Testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3425577.3425581,
author = {Chen, Bin and Zhao, Congcong},
title = {Weakly Supervised Learning with Discrimination Mechanism for Object Detection},
year = {2021},
isbn = {9781450388023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425577.3425581},
doi = {10.1145/3425577.3425581},
abstract = {In order to reduce the time consuming and expensive process of manually annotating data, and achieve the purpose of lightweight deployment. In this paper, an object detection method for weakly supervised learning with discrimination mechanism is proposed. We introduce the classification branch and the location branch based on the Darknet-53 backbone network of YOLO model, utilize Global Average Pooling (GAP) and Softmax to complete classification on selected areas, and adopt classification activation map for location. In addition, we use a model compression mechanism for model pruning operations, which reduces the size of the model and achieves the lightweight goal. These can effectively solve the problems of object detection to a certain extent. The results show that the improved model achieves good performance in terms of robustness and stability while maintaining the accuracy and efficiency of object detection, further improving the effectiveness of object detection tasks in practical application scenarios.},
booktitle = {Proceedings of the 3rd International Conference on Control and Computer Vision},
pages = {17–21},
numpages = {5},
keywords = {discrimination mechanism, model compression mechanism, object detection, weakly supervised learning},
location = {Macau, China},
series = {ICCCV '20}
}

@article{10.1007/s10270-020-00825-2,
author = {Kawamoto, Yusuke},
title = {An epistemic approach to the formal specification of statistical machine learning},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00825-2},
doi = {10.1007/s10270-020-00825-2},
abstract = {We propose an epistemic approach to formalizing statistical properties of machine learning. Specifically, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then, we formalize various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic. In this formalization, we show relationships among properties of classifiers, and relevance between classification performance and robustness. As far as we know, this is the first work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {293–310},
numpages = {18},
keywords = {Modal logic, Possible world semantics, Machine learning, Classification performance, Robustness, Fairness}
}

@article{10.1016/j.compbiomed.2021.104450,
author = {Sharma, Samriti and Singh, Gurvinder and Sharma, Manik},
title = {A comprehensive review and analysis of supervised-learning and soft computing techniques for stress diagnosis in humans},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104450},
doi = {10.1016/j.compbiomed.2021.104450},
journal = {Comput. Biol. Med.},
month = jul,
numpages = {19},
keywords = {Stress, Supervised learning, Soft computing, Nature-inspired methods, Fuzzy logic, Deep learning techniques}
}

@inproceedings{10.1007/978-3-031-08421-8_34,
author = {Gaglio, Salvatore and Giammanco, Andrea and Lo Re, Giuseppe and Morana, Marco},
title = {Adversarial Machine Learning in&nbsp;e-Health: Attacking a&nbsp;Smart Prescription System},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_34},
doi = {10.1007/978-3-031-08421-8_34},
abstract = {Machine learning (ML) algorithms are the basis of many services we rely on in our everyday life. For this reason, a new research line has recently emerged with the aim of investigating how ML can be misled by adversarial examples. In this paper we address an e-health scenario in which an automatic system for prescriptions can be deceived by inputs forged to subvert the model’s prediction. In particular, we present an algorithm capable of generating a precise sequence of moves that the adversary has to take in order to elude the automatic prescription service. Experimental analyses performed on a real dataset of patients’ clinical records show that a minimal alteration of the clinical records can subvert predictions with high probability.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {490–502},
numpages = {13},
keywords = {Adversarial Machine Learning, Healthcare, Evasion attacks}
}

@article{10.1007/s00521-020-05109-w,
author = {Jirak, Doreen and Biertimpel, David and Kerzel, Matthias and Wermter, Stefan},
title = {Solving visual object ambiguities when pointing: an unsupervised learning approach},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {7},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05109-w},
doi = {10.1007/s00521-020-05109-w},
abstract = {Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Particularly, pointing gestures are necessary to dissolve ambiguities in a scene and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domestic domains, the development of gesture interfaces complementing human–robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the corresponding object have to be correctly recognized in real time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be captured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects and use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised learning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define the so-called ambiguity classes. Our implementation for the hand and object detection is independent of any markers or skeleton models; thus, it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities resulting from close object proximity.},
journal = {Neural Comput. Appl.},
month = apr,
pages = {2297–2319},
numpages = {23},
keywords = {Pointing gestures, Pointing intention, Object ambiguities, Grow-when-required networks, Human–robot interaction}
}

@inproceedings{10.1145/3373509.3373558,
author = {Zhao, Xuan and Li, Yali and Wang, Shengjin},
title = {Face Quality Assessment via Semi-supervised Learning},
year = {2020},
isbn = {9781450376570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373509.3373558},
doi = {10.1145/3373509.3373558},
abstract = {Face quality assessment, used for selecting a "good" subset from face images captured over multiple frames in uncontrolled conditions, plays a significant role in video-based face recognition. By removing the poor quality images, it can not only improve recognition performance but also reduce the computation cost. This paper proposes an end-to-end face quality assessment algorithm based on a semi-supervised learning framework. The contributions of the proposed method are threefold. (i) Making use of unlabeled data from target domain to fine-tune a neural network by a strategy of automatically updating labels. (ii) Combining prior knowledge with feature learning by using a set of characteristics as binary constraints. (iii) Proposing a light neural network model for training and predicting. Experiments demonstrate that our model can get much higher accuracy in face quality assessment task than the models trained with the same amount of labeled faces, meanwhile the complexity is lower. Experimental results also show that our method can improve the performance of face recognition by face selection.},
booktitle = {Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition},
pages = {288–293},
numpages = {6},
keywords = {Face quality assessment, light CNN, prior knowledge, semi-supervised learning},
location = {Beijing, China},
series = {ICCPR '19}
}

@article{10.1016/j.eswa.2021.114774,
author = {Rasheed, Fareeha and Wahid, Abdul},
title = {Learning style detection in E-learning systems using machine learning techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114774},
doi = {10.1016/j.eswa.2021.114774},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Machine learning, Classification, Learning style, E-learning}
}

@article{10.1007/s11042-020-09079-y,
author = {Umer, Saiyed and Mohanta, Partha Pratim and Rout, Ranjeet Kumar and Pandey, Hari Mohan},
title = {Machine learning method for cosmetic product recognition: a visual searching approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {28–29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09079-y},
doi = {10.1007/s11042-020-09079-y},
abstract = {A cosmetic product recognition system is proposed in this paper. For this recognition system, we have proposed a cosmetic product database that contains image samples of forty different cosmetic items. The purpose of this recognition system is to recognize Cosmetic products with there types, brands and retailers such that to analyze a customer experience what kind of products and brands they need. This system has various applications in such as brand recognition, product recognition and also the availability of the products to the vendors. The implementation of the proposed system is divided into three components: preprocessing, feature extraction and classification. During preprocessing we have scaled and transformed the color images into gray-scaled images to speed up the process. During feature extraction, several different feature representation schemes: transformed, structural and statistical texture analysis approaches have been employed and investigated by employing the global and local feature representation schemes. Various machine learning supervised classification methods such as Logistic Regression, Linear Support Vector Machine, Adaptive k-Nearest Neighbor, Artificial Neural Network and Decision Tree classifiers have been employed to perform the classification tasks. Apart from this, we have also performed some data analytic tasks for Brand Recognition as well as Retailer Recognition and for these experimentation, we have employed some datasets from the ‘Kaggle’ website and have obtained the performance due to the above-mentioned classifiers. Finally, the performance of the cosmetic product recognition system, Brand Recognition and Retailer Recognition have been aggregated for the customer decision process in the form of the state-of-the-art for the proposed system.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {34997–35023},
numpages = {27},
keywords = {Cosmetic products, E-commerce application, Feature extraction, Machine learning, Visual search}
}

@inproceedings{10.1145/3404835.3462814,
author = {Li, Yunqi and Ge, Yingqiang and Zhang, Yongfeng},
title = {Tutorial on Fairness of Machine Learning in Recommender Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462814},
doi = {10.1145/3404835.3462814},
abstract = {Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users' or providers' satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2654–2657},
numpages = {4},
keywords = {AI ethics, fairness, machine learning, recommender systems},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1007/978-3-030-77385-4_37,
author = {Bloem, Peter and Wilcke, Xander and van Berkel, Lucas and de Boer, Victor},
title = {kgbench: A Collection of Knowledge Graph Datasets for Evaluating Relational and Multimodal Machine Learning},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_37},
doi = {10.1007/978-3-030-77385-4_37},
abstract = {Graph neural networks and other machine learning models offer a promising direction for machine learning on relational and multimodal data. Until now, however, progress in this area is difficult to gauge. This is primarily due to a limited number of datasets with (a) a high enough number of labeled nodes in the test set for precise measurement of performance, and (b) a rich enough variety of multimodal information to learn from. We introduce a set of new benchmark tasks for node classification on RDF-encoded knowledge graphs. We focus primarily on node classification, since this setting cannot be solved purely by node embedding models. For each dataset, we provide test and validation sets of at least 1000 instances, with some over 10000. Each task can be performed in a purely relational manner, or with multimodal information. All datasets are packaged in a CSV format that is easily consumable in any machine learning environment, together with the original source data in RDF and pre-processing code for full provenance. We provide code for loading the data into numpy and pytorch. We compute performance for several baseline models.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {614–630},
numpages = {17},
keywords = {Knowledge graphs, Machine learning, Message passing models, Multimodal learning}
}

@article{10.1145/3445812,
author = {Jesus, Gon\c{c}alo and Casimiro, Ant\'{o}nio and Oliveira, Anabela},
title = {Using Machine Learning for Dependable Outlier Detection in Environmental Monitoring Systems},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3445812},
doi = {10.1145/3445812},
abstract = {Sensor platforms used in environmental monitoring applications are often subject to harsh environmental conditions while monitoring complex phenomena. Therefore, designing dependable monitoring systems is challenging given the external disturbances affecting sensor measurements. Even the apparently simple task of outlier detection in sensor data becomes a hard problem, amplified by the difficulty in distinguishing true data errors due to sensor faults from deviations due to natural phenomenon, which look like data errors. Existing solutions for runtime outlier detection typically assume that the physical processes can be accurately modeled, or that outliers consist in large deviations that are easily detected and filtered by appropriate thresholds. Other solutions assume that it is possible to deploy multiple sensors providing redundant data to support voting-based techniques. In this article, we propose a new methodology for dependable runtime detection of outliers in environmental monitoring systems, aiming to increase data quality by treating them. We propose the use of machine learning techniques to model each sensor behavior, exploiting the existence of correlated data provided by other related sensors. Using these models, along with knowledge of processed past measurements, it is possible to obtain accurate estimations of the observed environment parameters and build failure detectors that use these estimations. When a failure is detected, these estimations also allow one to correct the erroneous measurements and hence improve the overall data quality. Our methodology not only allows one to distinguish truly abnormal measurements from deviations due to complex natural phenomena, but also allows the quantification of each measurement quality, which is relevant from a dependability perspective.We apply the methodology to real datasets from a complex aquatic monitoring system, measuring temperature and salinity parameters, through which we illustrate the process for building the machine learning prediction models using a technique based on Artificial Neural Networks, denoted ANNODE (ANN Outlier Detection). From this application, we also observe the effectiveness of our ANNODE approach for accurate outlier detection in harsh environments. Then we validate these positive results by comparing ANNODE with state-of-the-art solutions for outlier detection. The results show that ANNODE improves existing solutions regarding accuracy of outlier detection.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jul,
articleno = {29},
numpages = {30},
keywords = {Dependability, aquatic monitoring, data quality, machine learning, neural networks, outlier detection}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1007/s10994-020-05941-0,
author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, C\'{e}line and Gromowski, Mark and Schmid, Ute},
title = {Beneficial and harmful explanatory machine learning},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {4},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-020-05941-0},
doi = {10.1007/s10994-020-05941-0},
abstract = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.},
journal = {Mach. Learn.},
month = apr,
pages = {695–721},
numpages = {27},
keywords = {Inductive logic programming, Comprehensibility, Ultra-strong machine learning, Explainable AI}
}

@article{10.1016/j.compeleceng.2021.107574,
author = {Rajpoot, Vikram and Garg, Lalit and Alam, M. Zahid and Sangeeta and Parashar, Vivek and Tapashetti, Pratibhadevi and Arjariya, Tripti},
title = {Analysis of machine learning based LEACH robust routing in the Edge Computing systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {PB},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107574},
doi = {10.1016/j.compeleceng.2021.107574},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {17},
keywords = {Wireless sensor networks, Edge Computing, Machine learning, Data Fusion Method, LEACH routing protocol, Independent RNN}
}

@inproceedings{10.1007/978-3-030-58548-8_30,
author = {Taherkhani, Fariborz and Dabouei, Ali and Soleymani, Sobhan and Dawson, Jeremy and Nasrabadi, Nasser M.},
title = {Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning},
year = {2020},
isbn = {978-3-030-58547-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58548-8_30},
doi = {10.1007/978-3-030-58548-8_30},
abstract = {Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {509–526},
numpages = {18},
keywords = {Semi-Supervised Learning, Hierarchical optimal transport},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/ISLPED52811.2021.9502472,
author = {Marculescu, Diana},
title = {When climate meets machine learning: edge to cloud ML energy efficiency},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISLPED52811.2021.9502472},
doi = {10.1109/ISLPED52811.2021.9502472},
abstract = {A large portion of current cloud and edge workloads feature Machine Learning (ML) tasks, thereby requiring a deep understanding of their energy efficiency. While the holy grail for judging the quality of a ML model has largely been testing accuracy, and only recently its resource usage, neither of these metrics translate directly to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers the need for building accurate, platform-specific power and latency models for ML and efficient hardware-aware ML design methodologies, thus allowing machine learners and hardware designers to identify not just the best accuracy ML model configuration, but also those that satisfy given hardware constraints.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {37},
numpages = {1},
keywords = {hardware-aware ML, model compression, neural architecture search, quantization},
location = {Boston, Massachusetts},
series = {ISLPED '21}
}

@article{10.1007/s00607-021-00902-4,
author = {Le Thi, Thuy and Phan Thi, Tuoi and Quan Thanh, Tho},
title = {Machine learning using context vectors for object coreference resolution},
year = {2021},
issue_date = {Mar 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {105},
number = {3},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-00902-4},
doi = {10.1007/s00607-021-00902-4},
abstract = {Object coreference resolution is used in sentiment analysis to identify sentiment words referring to an aspect of an object in a document. However, this poses a challenge in natural language processing and is consequently an area of ongoing research. Further, to the best of our knowledge, object coreference resolution with more than one object has not been given much attention. To effectively address object coreference resolution, this paper proposes a method in which machine learning is applied to a large volume of textual data represented by context vectors, constituting a new form of language representation. The proposed machine learning model uses these vectors to achieve state-of-the-art performance in object coreference resolution. In addition, a combination of dependency grammar, sentiment ontology, and coreference graphs is used to obtain triplets of object, aspect, and sentiment. In experiments conducted on sentiment textual data obtained from Amazon.com, the proposed method achieved an average coreference resolution of object, aspect, and sentiment precision value of approximately 90%. This result suggests that the proposed method can contribute considerably to the field of object coreference resolution, and further research is therefore warranted.},
journal = {Computing},
month = jan,
pages = {539–558},
numpages = {20},
keywords = {Object coreference resolution, Object aspect, Sentiment analysis, Sentiment ontology, CROAS, 68T50}
}

@article{10.1613/jair.1.11854,
author = {Z\"{o}ller, Marc-Andr\'{e} and Huber, Marco F.},
title = {Benchmark and Survey of Automated Machine Learning Frameworks},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11854},
doi = {10.1613/jair.1.11854},
abstract = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {409–472},
numpages = {64}
}

@article{10.1007/s11063-017-9724-1,
author = {Kim, Jonghong and Bukhari, Waqas and Lee, Minho},
title = {Feature Analysis of Unsupervised Learning for Multi-task Classification Using Convolutional Neural Network},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-017-9724-1},
doi = {10.1007/s11063-017-9724-1},
abstract = {This study analyzes the characteristics of unsupervised feature learning using a convolutional neural network (CNN) to investigate its efficiency for multi-task classification and compare it to supervised learning features. We keep the conventional CNN structure and introduce modifications into the convolutional auto-encoder design to accommodate a subsampling layer and make a fair comparison. Moreover, we introduce non-maximum suppression and dropout for a better feature extraction and to impose sparsity constraints. The experimental results indicate the effectiveness of our sparsity constraints. We also analyze the efficiency of unsupervised learning features using the t-SNE and variance ratio. The experimental results show that the feature representation obtained in unsupervised learning is more advantageous for multi-task learning than that obtained in supervised learning.},
journal = {Neural Process. Lett.},
month = jun,
pages = {783–797},
numpages = {15},
keywords = {Auto-encoder, Convolutional neural networks, Deep learning, Multi-task learning, Unsupervised learning}
}

@article{10.1016/j.compeleceng.2021.107362,
author = {P, Gouthaman and Sankaranarayanan, Suresh},
title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107362},
doi = {10.1016/j.compeleceng.2021.107362},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {9},
keywords = {Software model, Agile, Waterfall, Evolutionary, Incremental, Machine learning, Risk prediction}
}

@article{10.1007/s00521-020-05058-4,
author = {Borg, Anton and Boldt, Martin and Rosander, Oliver and Ahlstrand, Jim},
title = {E-mail classification with machine learning and word embeddings for improved customer support},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05058-4},
doi = {10.1007/s00521-020-05058-4},
abstract = {Classifying e-mails into distinct labels can have a great impact on customer support. By using machine learning to label e-mails, the system can set up queues containing e-mails of a specific category. This enables support personnel to handle request quicker and more easily by selecting a queue that match their expertise. This study aims to improve a manually defined rule-based algorithm, currently implemented at a large telecom company, by using machine learning. The proposed model should have higher F1-score and classification rate. Integrating or migrating from a manually defined rule-based model to a machine learning model should also reduce the administrative and maintenance work. It should also make the model more flexible. By using the frameworks, TensorFlow, Scikit-learn and Gensim, the authors conduct a number of experiments to test the performance of several common machine learning algorithms, text-representations, word embeddings to investigate how they work together. A long short-term memory network showed best classification performance with an F1-score of 0.91. The authors conclude that long short-term memory networks outperform other non-sequential models such as support vector machines and AdaBoost when predicting labels for e-mails. Further, the study also presents a Web-based interface that were implemented around the LSTM network, which can classify e-mails into 33 different labels.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1881–1902},
numpages = {22},
keywords = {E-mail classification, Machine learning, Long short-term memory, Natural language processing}
}

@article{10.1007/s42979-021-00967-0,
author = {Kilaskar, Mohini and Saindane, Neha and Ansari, Nabeel and Doshi, Dhaval and Kulkarni, Mayuri},
title = {Machine Learning Algorithms for Analysis and Prediction of Depression},
year = {2021},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {2},
url = {https://doi.org/10.1007/s42979-021-00967-0},
doi = {10.1007/s42979-021-00967-0},
abstract = {Today, depression is one of the critical mental health problems faced by humans of all ages and gender. In this era of increasing technology, it causes a life of less physical work, continuous pressure on one's life, which creates a risk of intellectual disturbance. The work culture, peer pressure, stressful life, emotional imbalance, family disturbances, and social life are resulting in depression. Depression may also sometimes lead to a heart attack. Depression causes adverse effects and becomes a serious medical problem in how individuals feel and act in everyday life. This psychological state causes feelings of sadness, anxiety, loss of interest in things and jobs, and could barely result in suicide. In this paper, the analysis of different Machine Learning Algorithms has been done and compared them by selecting various parameters and then showing which algorithm is more accurate for predicting depression.},
journal = {SN Comput. Sci.},
month = dec,
numpages = {6},
keywords = {Machine learning, Depression, XGBoost, SVM, Logistic regression, Random forest}
}

@inproceedings{10.1007/978-3-030-29726-8_25,
author = {Siebert, Sophie and Schon, Claudia and Stolzenburg, Frieder},
title = {Commonsense Reasoning Using Theorem Proving and Machine Learning},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_25},
doi = {10.1007/978-3-030-29726-8_25},
abstract = {Commonsense reasoning is a difficult task for a computer to handle. Current algorithms score around 80% on benchmarks. Usually these approaches use machine learning which lacks explainability, however. Therefore, we propose a combination with automated theorem proving here. Automated theorem proving allows us to derive new knowledge in an explainable way, but suffers from the inevitable incompleteness of existing background knowledge. We alleviate this problem by using machine learning. In this paper, we present our approach which uses an automatic theorem prover, large existing ontologies with background knowledge, and machine learning. We present first experimental results and identify an insufficient amount of training data and lack of background knowledge as causes for our system not to stand out much from the baseline.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {395–413},
numpages = {19},
keywords = {Commonsense reasoning, Causal reasoning, Machine learning, Theorem proving, Large background knowledge},
location = {Canterbury, United Kingdom}
}

@inproceedings{10.1007/978-3-030-67670-4_39,
author = {Gijsbers, Pieter and Vanschoren, Joaquin},
title = {GAMA: A General Automated Machine Learning Assistant},
year = {2020},
isbn = {978-3-030-67669-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67670-4_39},
doi = {10.1007/978-3-030-67670-4_39},
abstract = {The General Automated Machine learning Assistant (GAMA) is a modular AutoML system developed to empower users to track and control how AutoML algorithms search for optimal machine learning pipelines, and facilitate AutoML research itself. In contrast to current, often black-box systems, GAMA allows users to plug in different AutoML and post-processing techniques, logs and visualizes the search process, and supports easy benchmarking. It currently features three AutoML search algorithms, two model post-processing steps, and is designed to allow for more components to be added.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part V},
pages = {560–564},
numpages = {5},
location = {Ghent, Belgium}
}

@inproceedings{10.1007/978-3-030-71158-0_14,
author = {Ferreira, Lu\'{\i}s and Pilastri, Andr\'{e} and Martins, Carlos and Santos, Pedro and Cortez, Paulo},
title = {A Scalable and Automated Machine Learning Framework to Support Risk Management},
year = {2020},
isbn = {978-3-030-71157-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71158-0_14},
doi = {10.1007/978-3-030-71158-0_14},
abstract = {Due to the growth of data and widespread usage of Machine Learning (ML) by non-experts, automation and scalability are becoming key issues for ML. This paper presents an automated and scalable framework for ML that requires minimum human input. We designed the framework for the domain of telecommunications risk management. This domain often requires non-ML-experts to continuously update supervised learning models that are trained on huge amounts of data. Thus, the framework uses Automated Machine Learning (AutoML), to select and tune the ML models, and distributed ML, to deal with Big Data. The modules included in the framework are task detection (to detect classification or regression), data preprocessing, feature selection, model training, and deployment. In this paper, we focus the experiments on the model training module. We first analyze the capabilities of eight AutoML tools: Auto-Gluon, Auto-Keras, Auto-Sklearn, Auto-Weka, H2O AutoML, Rminer, TPOT, and TransmogrifAI. Then, to select the tool for model training, we performed a benchmark with the only two tools that address a distributed ML (H2O AutoML and TransmogrifAI). The experiments used three real-world datasets from the telecommunications domain (churn, event forecasting, and fraud detection), as provided by an analytics company. The experiments allowed us to measure the computational effort and predictive capability of the AutoML tools. Both tools obtained high-quality results and did not present substantial predictive differences. Nevertheless, H2O AutoML was selected by the analytics company for the model training module, since it was considered a more mature technology that presented a more interesting set of features (e.g., integration with more platforms). After choosing H2O AutoML for the ML training, we selected the technologies for the remaining components of the architecture (e.g., data preprocessing and web interface).},
booktitle = {Agents and Artificial Intelligence: 12th International Conference, ICAART 2020, Valletta, Malta, February 22–24, 2020, Revised Selected Papers},
pages = {291–307},
numpages = {17},
keywords = {Automated machine learning, Distributed machine learning, Supervised learning, Risk management},
location = {Valletta, Malta}
}

@article{10.1007/s00521-020-05457-7,
author = {Liu, Dunnan and Xu, Xiaofeng and Liu, Mingguang and Liu, Yaling},
title = {Dynamic traffic classification algorithm and simulation of energy Internet of things based on machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05457-7},
doi = {10.1007/s00521-020-05457-7},
abstract = {With the rapid development of information technology, a large amount of traffic generated by various Internet applications occupies a large amount of network resources. It poses a huge challenge to service quality and has a negative impact on Internet security. In order to utilize network resources effectively and provide effective management and control measures for network administrators, network traffic classification technologies is a hot topic for scientists to identify application layer protocols. Today, there are more and more applications based on TCP/IP. With the emergence of various anti-surveillance applications, traditional port and application-based identification methods are difficult to meet current or future traffic identification requirements. It has become a very challenging problem to require more efficient, accurate, intelligent and real-time Internet traffic identification. The Internet of Things is a new network concept proposed by people who based on Internet prototypes. It enables the end user of the system can carry out communication and exchange of information and data between any project. In recent years, with the continuous advancement of Internet of Things technology, the coverage of the Internet of Things has become very wide, and the number of different types of networks that make up the Internet of Things is also increasing. This paper aims to find the dynamic network traffic classification problem of hybrid fixed in dynamic network and dynamic network in mobile network, and gives a reasonable mapping scheme. The dynamics of network traffic for Internet of Things are reflected fully and will not cause route flapping. The simulation results show that the decision tree classification algorithm in machine learning has higher efficiency, and improves the utilization of network resources.},
journal = {Neural Comput. Appl.},
month = may,
pages = {3967–3976},
numpages = {10},
keywords = {Machine Learning, Internet of Things, Network Traffic, Traffic Classification}
}

@article{10.1016/j.ipm.2021.102555,
author = {\.{Z}bikowski, Kamil and Antosiuk, Piotr},
title = {A machine learning, bias-free approach for predicting business success using Crunchbase data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {4},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102555},
doi = {10.1016/j.ipm.2021.102555},
journal = {Inf. Process. Manage.},
month = jul,
numpages = {18},
keywords = {Startups, Supervised learning, XGBoost, Crunchbase, Look-ahead bias}
}

@inproceedings{10.1007/978-3-030-60450-9_30,
author = {Lin, Pingping and Luo, Xudong},
title = {A Survey of Sentiment Analysis Based on Machine Learning},
year = {2020},
isbn = {978-3-030-60449-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60450-9_30},
doi = {10.1007/978-3-030-60450-9_30},
abstract = {Every day, Facebook, Twitter, Weibo and other social network sites and major e-commerce sites generate a large number of online reviews with emotions. The analysing people’s opinions from these reviews can assist a variety of decision-making processes in organisations, products, and administrations. Therefore, it is practically and theoretically important to study how to analyse online reviews with emotions. To help researchers study sentiment analysis, in this paper, we survey the machine learning based method for sentiment analysis of online reviews. These methods are main based on Support Vector Machine, Neural Networks, Na\"{\i}ve Bayes, Bayesian network, Maximum entropy, and some hybrid methods. In particular, we point out the main problems in the machine learning based methods for sentiment analysis and the problems to be solved in the future.},
booktitle = {Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part I},
pages = {372–387},
numpages = {16},
keywords = {Sentiment analysis, Machine learning, Integrated learning, Transfer learning},
location = {Zhengzhou, China}
}

@inproceedings{10.1007/978-3-030-80599-9_20,
author = {Sazzed, Salim},
title = {Improving Sentiment Classification in Low-Resource Bengali Language Utilizing Cross-Lingual Self-supervised Learning},
year = {2021},
isbn = {978-3-030-80598-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-80599-9_20},
doi = {10.1007/978-3-030-80599-9_20},
abstract = {One of the barriers of sentiment analysis research in low-resource languages such as Bengali is the lack of annotated data. Manual annotation requires resources, which are scarcely available in low-resource languages. We present a cross-lingual hybrid methodology that utilizes machine translation and prior sentiment information to generate accurate pseudo-labels. By leveraging the pseudo-labels, a supervised ML classifier is trained for sentiment classification. We contrast the performance of the proposed self-supervised methodology with the Bengali and English sentiment classification methods (i.e., methods which do not require labeled data). We observe that the self-supervised hybrid methodology improves the macro F1 scores by 15%–25%. The results infer that the proposed framework can improve the performance of sentiment classification in low-resource languages that lack labeled data.},
booktitle = {Natural Language Processing and Information Systems: 26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021, Saarbr\"{u}cken, Germany, June 23–25, 2021, Proceedings},
pages = {218–230},
numpages = {13},
keywords = {Bangla sentiment analysis, Pseudo-label generation, Cross-lingual sentiment analysis},
location = {Saarbr\"{u}cken, Germany}
}

@inproceedings{10.1145/2957276.2957280,
author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
title = {Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957280},
doi = {10.1145/2957276.2957280},
abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
booktitle = {Proceedings of the 2016 ACM International Conference on Supporting Group Work},
pages = {3–8},
numpages = {6},
keywords = {axial coding, coding families, grounded theory, machine learning, supervised learning, unsupervised learning},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1007/978-3-030-81242-3_10,
author = {Heaps, John and Krishnan, Ram and Huang, Yufei and Niu, Jianwei and Sandhu, Ravi},
title = {Access Control Policy Generation from&nbsp;User Stories Using Machine Learning},
year = {2021},
isbn = {978-3-030-81241-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-81242-3_10},
doi = {10.1007/978-3-030-81242-3_10},
abstract = {Agile software development methodology involves developing code incrementally and iteratively from a set of evolving user stories. Since software developers use user stories to write code, these user stories are better representations of the actual code than that of the high-level product documentation. In this paper, we develop an automated approach using machine learning to generate access control information from a set of user stories that describe the behavior of the software product in question. This is an initial step to automatically produce access control specifications and perform automated security review of a system with minimal human involvement. Our approach takes a set of user stories as input to a transformers-based deep learning model, which classifies if each user story contains access control information. It then identifies the actors, data objects, and operations the user story contains in a named entity recognition task. Finally, it determines the type of access between the identified actors, data objects, and operations through a classification prediction. This information can then be used to construct access control documentation and information useful to stakeholders for assistance during access control engineering, development, and review.},
booktitle = {Data and Applications Security and Privacy XXXV: 35th Annual IFIP WG 11.3 Conference, DBSec 2021, Calgary, Canada, July 19–20, 2021, Proceedings},
pages = {171–188},
numpages = {18},
keywords = {Access control, Software engineering, Agile development, User stories, Machine learning, Deep learning},
location = {Calgary, AB, Canada}
}

@article{10.1134/S0005117919090078,
author = {Popkov, Yu. S.},
title = {Randomized Machine Learning Procedures},
year = {2019},
issue_date = {Sep 2019},
publisher = {Plenum Press},
address = {USA},
volume = {80},
number = {9},
issn = {0005-1179},
url = {https://doi.org/10.1134/S0005117919090078},
doi = {10.1134/S0005117919090078},
abstract = {A new concept of machine learning based on the computer simulation of entropy-optimal randomized models is proposed. The procedures of randomized machine learning (RML) with “hard” and “soft” randomization are considered; the former imply the exact reproduction of empirical balances while the latter their rough reproduction with an accepted approximation criterion. RML algorithms are formulated as functional entropy-linear programming problems. Applications of RML procedures to text classification and the randomized forecasting of migratory interaction of regional systems are presented.},
journal = {Autom. Remote Control},
month = sep,
pages = {1653–1670},
numpages = {18},
keywords = {randomization, hard and soft randomization procedures, uncertainty, entropy, matrix norms, empirical balances, text classification, dynamic regression}
}

@inproceedings{10.1145/3472673.3473966,
author = {Chakraborty, Suranjan and Deng, Lin and Dehlinger, Josh},
title = {Towards authentic undergraduate research experiences in software engineering and machine learning},
year = {2021},
isbn = {9781450386241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472673.3473966},
doi = {10.1145/3472673.3473966},
abstract = {Authentic undergraduate research experiences have been shown to be very effective at sustaining students’ learning motivation and enhancing students’ theoretical knowledge and practical skills. However, there still exists some common challenges in undergraduate research. In this paper, we describe an approach that offers undergraduate students authentic and immersive research experience focusing on applied machine learning for software engineering and discuss our experiences with example undergraduate research projects and outcomes. A survey was designed to assess students’ overall experience of participating in authentic undergraduate research projects in machine learning for software engineering. Preliminary results from this survey are provided.},
booktitle = {Proceedings of the 3rd International Workshop on Education through Advanced Software Engineering and Artificial Intelligence},
pages = {54–57},
numpages = {4},
keywords = {Machine learning, computing education, software engineering, undergraduate research},
location = {Athens, Greece},
series = {EASEAI 2021}
}

@article{10.1007/s11042-020-10067-5,
author = {\"{O}zer, \c{C}a\u{g}da\c{s} and \c{C}evik, Taner and G\"{u}rhanl\i{}, Ahmet},
title = {A machine learning-based framework for predicting game server load},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10067-5},
doi = {10.1007/s11042-020-10067-5},
abstract = {Server load prediction can be utilized for load-balancing and load-sharing in distributed systems. The use of machine learning (ML) algorithms for load estimation in distributed system applications can increase the availability and performance of servers. Hence, a number of machine learning algorithms have been applied thus far for server load estimation. This study focuses on increasing the performance of game servers by accurately predicting the workload of game servers in short, medium and long term prediction situations. While doing this, various machine learning techniques have been applied and the algorithms that give the best results are presented. In terms of implementation, companies using their servers and data centers can try to increase their level of satisfaction by using these algorithms. A prediction model is developed and the estimation performances of a number of fundamental ML methods i.e., Na\"{\i}ve Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), Support Vector Machine (SVM), Fast Large Margin (FLM), Convolutional Neural Network CNN are analyzed. The data used during the training stage is obtained by listening to the TCP/IP packet traffic and the real-data is extracted by performing an extensive analysis of the total transferred-data that includes also the payload. In the analysis phase, the goodput is considered in order to reveal exact resource requirements. Comprehensive simulations are performed under various conditions for high accuracy performance analysis. Experimental results indicate that the proposed ML-based prediction shows promising performance in terms of load prediction when compared to the common approaches present in the literature.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {9527–9546},
numpages = {20},
keywords = {Machine learning, Load prediction, Game server}
}

@article{10.1007/s00607-021-00958-2,
author = {Khalid, Yasir Noman and Aleem, Muhammad and Ahmed, Usman and Prodan, Radu and Islam, Muhammad Arshad and Iqbal, Muhammad Azhar},
title = {FusionCL: a machine-learning based approach for OpenCL kernel fusion to increase system performance},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {10},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-00958-2},
doi = {10.1007/s00607-021-00958-2},
abstract = {Employing general-purpose graphics processing units (GPGPU) with the help of OpenCL has resulted in greatly reducing the execution time of data-parallel applications by taking advantage of the massive available parallelism. However, when a small data size application is executed on GPU there is a wastage of GPU resources as the application cannot fully utilize GPU compute-cores. There is no mechanism to share a GPU between two kernels due to the lack of operating system support on GPU. In this paper, we propose the provision of a GPU sharing mechanism between two kernels that will lead to increasing GPU occupancy, and as a result, reduce execution time of a job pool. However, if a pair of the kernel is competing for the same set of resources (i.e., both applications are compute-intensive or memory-intensive), kernel fusion may also result in a significant increase in execution time of fused kernels. Therefore, it is pertinent to select an optimal pair of kernels for fusion that will result in significant speedup over their serial execution. This research presents FusionCL, a machine learning-based GPU sharing mechanism between a pair of OpenCL kernels. FusionCL identifies each pair of kernels (from the job pool), which are suitable candidates for fusion using a machine learning-based fusion suitability classifier. Thereafter, from all the candidates, it selects a pair of candidate kernels that will produce maximum speedup after fusion over their serial execution using a fusion speedup predictor. The experimental evaluation shows that the proposed kernel fusion mechanism reduces execution time by 2.83\texttimes{} when compared to a baseline scheduling scheme. When compared to state-of-the-art, the reduction in execution time is up to 8%.},
journal = {Computing},
month = oct,
pages = {2171–2202},
numpages = {32},
keywords = {Scheduling, Kernel fusion, High-performance computing, Machine learning, 68M20}
}

@inproceedings{10.5555/3540261.3541639,
author = {Richards, Dominic and Negahban, Sahand N and Rebeschini, Patrick},
title = {Distributed machine learning with sparse heterogeneous data},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by distributed machine learning settings such as Federated Learning, we consider the problem of fitting a statistical model across a distributed collection of heterogeneous data sets whose similarity structure is encoded by a graph topology. Precisely, we analyse the case where each node is associated with fitting a sparse linear model, and edges join two nodes if the difference of their solutions is also sparse. We propose a method based on Basis Pursuit Denoising with a total variation penalty, and provide finite sample guarantees for sub-Gaussian design matrices. Taking the root of the tree as a reference node, we show that if the sparsity of the differences across nodes is smaller than the sparsity at the root, then recovery is successful with fewer samples than by solving the problems independently, or by using methods that rely on a large overlap in the signal supports, such as the group Lasso. We consider both the noiseless and noisy setting, and numerically investigate the performance of distributed methods based on Distributed Alternating Direction Methods of Multipliers (ADMM) and hyperspectral unmixing.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1378},
numpages = {13},
series = {NIPS '21}
}

@article{10.4018/ijkss.2014100104,
author = {Ripon, Shamim H and Hossain, Sk. Jahir and Piash, Moshiur Mahamud},
title = {Logic-Based Analysis and Verification of Software Product Line Variant Requirement Model},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100104},
doi = {10.4018/ijkss.2014100104},
abstract = {Software Product Line SPL provides the facility to systematically reuse of software improving the efficiency of software development regarding time, cost and quality. The main idea of SPL is to identify the common core functionality that can be implemented once and reused afterwards. A variant model has also to be developed to manage the variants of the SPL. Usually, a domain model consisting of the common and variant requirements is developed during domain engineering phase to alleviate the reuse opportunity. The authors present a product line model comprising of a variant part for the management of variant and a decision table to depict the customization of decision regarding each variant. Feature diagrams are widely used to model SPL variants. Both feature diagram and our variant model, which is based on tabular method, lacks logically sound formal representation and hence, not amenable to formal verification. Formal representation and verification of SPL has gained much interest in recent years. This chapter presents a logical representation of the variant model by using first order logic. With this representation, the table based variant model as well as the graphical feature diagram can now be verified logically. Besides applying first-order-logic to model the features, the authors also present an approach to model and analyze SPL model by using semantic web approach using OWL-DL. The OWL-DL representation also facilitates the search and maintenance of feature models and support knowledge sharing within a reusable engineering context. Reasoning tools are used to verify the consistency of the feature configuration for both logic-based and semantic web-based approaches.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {52–76},
numpages = {25},
keywords = {Domain Model, Feature Diagrams, OWL-DL, Software Product Line SPL, Web-Based Approaches}
}

@inproceedings{10.1007/978-3-319-13365-2_20,
author = {Rahman, Musfiqur and Ripon, Shamim},
title = {Using Bayesian Networks to Model and Analyze Software Product Line Feature Model},
year = {2014},
isbn = {9783319133645},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-13365-2_20},
doi = {10.1007/978-3-319-13365-2_20},
abstract = {Proper management of requirements plays a significant role in the successful development of any software product family. Application of AI, Bayesian Network (BN) in particular, is gaining much interest in Software Engineering, mainly in predicting software defects and software reliability. Feature analysis and its associated decision making is a suitable target area where BN can make remarkable effect. In SPL, a feature tree portrays various types of features as well as captures the relationships among them. This paper applies BN in modeling and analyzing features in a feature tree. Various feature analysis rules are first modeled and then verified in BN. The verification confirms the definition of the rules and thus these rules can be used in various decision making stages in SPL.},
booktitle = {Proceedings of the 8th International Workshop on Multi-Disciplinary Trends in Artificial Intelligence - Volume 8875},
pages = {220–231},
numpages = {12},
keywords = {Bayesian Networks, Dead feature, False Optional, Software Product Line},
location = {Bangalore, India},
series = {MIWAI 2014}
}

@inproceedings{10.1007/978-3-030-67731-2_37,
author = {Bayram, Firas and Garbarino, Davide and Barla, Annalisa},
title = {Predicting Tennis Match Outcomes with Network Analysis and Machine Learning},
year = {2021},
isbn = {978-3-030-67730-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67731-2_37},
doi = {10.1007/978-3-030-67731-2_37},
abstract = {Singles tennis is one of the most popular individual sports in the world. Many researchers have embarked on a wide range of approaches to model a tennis match, using probabilistic modeling, or applying machine learning models to predict the outcome of matches. In this paper, we propose a novel approach based on network analysis to infer a surface-specific and time-varying score for professional tennis players and use it in addition to players’ statistics of previous matches to represent tennis match data. Using the resulting features, we apply advanced machine learning paradigms such as Multi-Output Regression and Learning Using Privileged Information, and compare the results with standard machine learning approaches. The models are trained and tested on more than 83,000 men’s singles tennis matches between the years 1991 and 2020. Evaluating the results shows the proposed methods provide more accurate predictions of tennis match outcome than classical approaches and outperform the existing methods in the literature and the current state-of-the-art models in tennis.},
booktitle = {SOFSEM 2021: Theory and Practice of Computer Science: 47th International Conference on Current Trends in Theory and Practice of Computer Science, SOFSEM 2021, Bolzano-Bozen, Italy, January 25–29, 2021, Proceedings},
pages = {505–518},
numpages = {14},
keywords = {Machine learning, Network analysis, Learning Using Privileged Information, Multi-Output Regression, Tennis outcome prediction},
location = {Bolzano-Bozen, Italy}
}

@inproceedings{10.5555/3305381.3305435,
author = {Bojanowski, Piotr and Joulin, Armand},
title = {Unsupervised learning by predicting Noise},
year = {2017},
publisher = {JMLR.org},
abstract = {Convolutional neural networks provide visual features that perform well in many computer vision applications. However, training these networks requires large amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {517–526},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@phdthesis{10.5555/AAI28495393,
author = {Nguyen, Anthony T. and Shiqian, Ma, and Luis, Rademacher,},
advisor = {Krishnakumar, Balasubramanian,},
title = {Advances in Stochastic Optimization for Machine Learning},
year = {2021},
isbn = {9798538100583},
publisher = {University of California, Davis},
abstract = {We discuss two advances made in Stochastic Optimization where they arise out of a general problem, namely minimizing an objective function of the form $f(x) = mathbb{E}_{xi}[F(x, xi)]$ for $x in X subseteq mathbb{R}.n$, where $F(x, xi)$ is a stochastic function with some random variable $xi$.  ewlineindent The first project, in extbf{Chapter} $2$, deals with minimizing an objective function of the form $f_1 circ cdots circ f_T(x)$ where $f_i(x) = mathbb{E}_{xi_i}[G_i(x,xi_i)]$. In this setting, we assume that each component $f_i$ is smooth, and in addition, we assume the access of a first-order oracle that outputs noisy estimates of the components and their derivatives. We introduce two algorithms that utilize moving average updates, and we prove that they converge to an $epsilon$-stationary point. The difference between these two algorithms is the first uses a mini-batch of samples in each iteration while the second uses linearized stochastic estimates of the function values. The sample complexities of the mini-batches and the stochastic linearized approaches for obtaining an $epsilon$-stationary point are $mathcal{O}(frac{1}{epsilon.6})$ and $mathcal{O}(frac{1}{epsilon.4})$, respectively. indent The second project, in extbf{Chapter} $3$, discusses minimizing a convex function $f_0(x) = mathbb{E}_{xi_0}[F_0(x, xi_0)]$ with functional inequality constraints $f_i(x) = mathbb{E}_{xi_i}[F_i(x, xi_i)] leqslant 0$ ($i in {1, dots, m}$) using a zeroth-order oracle. We assume that we have access to noisy function value evaluations.  The algorithm performs an extrapolation and numerically solves the dual optimization problem by performing a gradient ascent and descent at each iteration. Finally, the numerical solution is the weighted average of the iterates from the gradient descents. The number of calls to the oracle to find an $epsilon$-approximate optimal solution is $mathcal{O}(frac{(m+1)n}{epsilon.2})$. Next, we present an algorithm in the non-convex setting based on cite{boob2019proximal}; utilizing our algorithm for the convex setting, the non-convex algorithm has sample complexity $mathcal{O}(frac{(m+1)n}{epsilon.3})$.},
note = {AAI28495393}
}

@inproceedings{10.1007/978-3-030-58475-7_49,
author = {Ignatiev, Alexey and Cooper, Martin C. and Siala, Mohamed and Hebrard, Emmanuel and Marques-Silva, Joao},
title = {Towards Formal Fairness in Machine Learning},
year = {2020},
isbn = {978-3-030-58474-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58475-7_49},
doi = {10.1007/978-3-030-58475-7_49},
abstract = {One of the challenges of deploying machine learning (ML) systems is fairness. Datasets often include sensitive features, which ML algorithms may unwittingly use to create models that exhibit unfairness. Past work on fairness offers no formal guarantees in their results. This paper proposes to exploit formal reasoning methods to tackle fairness. Starting from an intuitive criterion for fairness of an ML model, the paper formalises it, and shows how fairness can be represented as a decision problem, given some logic representation of an ML model. The same criterion can also be applied to assessing bias in training data. Moreover, we propose a reasonable set of axiomatic properties which no other definition of dataset bias can satisfy. The paper also investigates the relationship between fairness and explainability, and shows that approaches for computing explanations can serve to assess fairness of particular predictions. Finally, the paper proposes SAT-based approaches for learning fair ML models, even when the training data exhibits bias, and reports experimental trials.},
booktitle = {Principles and Practice of Constraint Programming: 26th International Conference, CP 2020, Louvain-La-Neuve, Belgium, September 7–11, 2020, Proceedings},
pages = {846–867},
numpages = {22},
location = {Louvain-la-Neuve, Belgium}
}

@article{10.1016/j.jbi.2021.103762,
author = {Jia, Yan and Lawton, Tom and Burden, John and McDermid, John and Habli, Ibrahim},
title = {Safety-driven design of machine learning for sepsis treatment},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103762},
doi = {10.1016/j.jbi.2021.103762},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {16},
keywords = {Machine learning, Sepsis treatment, Safety assurance}
}

@article{10.1016/j.patrec.2021.07.004,
author = {Briguglio, William and Moghaddam, Parisa and Yousef, Waleed A. and Traor\'{e}, Issa and Mamun, Mohammad},
title = {Machine learning in precision medicine to preserve privacy via encryption},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.07.004},
doi = {10.1016/j.patrec.2021.07.004},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {148–154},
numpages = {7},
keywords = {Machine learning, Encryption, Homomorphic encryption, Precision medicine, Privacy}
}

@article{10.1007/s00500-020-05226-7,
author = {Khuat, Thanh Tung and Ruta, Dymitr and Gabrys, Bogdan},
title = {Hyperbox-based machine learning algorithms: a comprehensive survey},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05226-7},
doi = {10.1007/s00500-020-05226-7},
abstract = {With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.},
journal = {Soft Comput.},
month = jan,
pages = {1325–1363},
numpages = {39},
keywords = {Hyperboxes, Membership function, Fuzzy min–max neural network, Hybrid classifiers, Data classification, Clustering, Online learning}
}

@article{10.1016/j.neucom.2019.01.083,
author = {Zhang, Xiao-Yu and Shi, Haichao and Zhu, Xiaobin and Li, Peng},
title = {Active semi-supervised learning based on self-expressive correlation with generative adversarial networks},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {345},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.01.083},
doi = {10.1016/j.neucom.2019.01.083},
journal = {Neurocomput.},
month = jun,
pages = {103–113},
numpages = {11},
keywords = {Active learning, Semi-supervised learning, Generative adversarial networks, Batch selection, Representation learning}
}

@phdthesis{10.5555/AAI28715432,
author = {Wadekar, Digvijay and Glennys, Farrar, and Yacine, Ali-Haimoud, and Ken, Van Tilburg, and Shirley, Ho,},
advisor = {Roman, Scoccimarro,},
title = {Accelerating Cosmological Inference with Novel Analytic Methods and Machine Learning},
year = {2021},
isbn = {9798496509169},
publisher = {New York University},
address = {USA},
abstract = {Astronomical observations of millions of galaxies in the Universe help us shed light upon questions such as: What is the Universe made up of? What is its origin and what will be its ultimate fate? In order to answer these questions, we need to infer cosmological parameters from galaxy survey data. We typically use summary statistics such as the power spectrum and we need an accurate estimate of their covariance matrix. The traditional process of obtaining the covariance for spectroscopic surveys involves simulating thousands of mocks. In Chapter 1, I developed an analytic approach for the covariance matrix which is more than four orders of magnitude faster. In Chapter 2, I validated our method with an analysis of the BOSS DR12 data. Furthermore, our analytic approach is free of sampling noise which makes it useful for upcoming surveys like DESI, Euclid and many others.In order to extract the wealth of cosmological information that lies beyond the linear scales, one needs to account for baryonic effects which are captured by hydrodynamic simulations. However, such simulations have a huge computational cost (~10 million CPU hours for 0.001 Gpc3 volume) and cannot therefore be directly used in predictions for upcoming surveys which will probe ~100 Gpc3 volumes. Focusing on neutral hydrogen (HI), I trained neural networks on hydro simulations in Chapter 3 to quickly generate accurate HI maps from gravity-only dark matter simulations. I took the initiative of interpreting the neural network and we learnt useful additions in Chapter 4 to theoretical models like HOD. In particular, a basic HOD model relies on a widely used assumption that the baryonic properties of a dark matter halo depend only on its mass. However, I inferred from the neural network that the halo environment also has a crucial dependence on the HI content of the halo. I also inferred novel symbolic expressions for encoding the effect of halo environment on the clustering of HI using a newly developed tool called symbolic regression. These help in better understanding the galaxy-halo connection and marginalizing over baryonic effects to extract cosmology from non-linear scales.I also explored the direction of using observations of individual astrophysical systems like dwarf galaxies to probe alternatives to the standard cold, collisionless dark matter (CDM) paradigm. Due to a lack of baryonic feedback, dwarf galaxies are pristine objects for such analyses and therefore are a subject of interest for numerous upcoming surveys like the Rubin observatory. In Chapter 5, I used the optical and 21cm observations of a gas-rich dwarf galaxy of the Milky Way called Leo T to set strong constraints on DM-baryon interactions. Such constraints are complementary to the early-universe constraints as they are not affected by assumptions about cosmology. Furthermore, for the popular dark photon DM model, we obtained constraints stronger than all the previous literature.},
note = {AAI28715432}
}

@inproceedings{10.5555/3524938.3525346,
author = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip B.},
title = {The non-IID data quagmire of decentralized machine learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {408},
numpages = {12},
series = {ICML'20}
}

@inproceedings{10.1145/3460319.3464844,
author = {Dutta, Saikat and Selvam, Jeeva and Jain, Aryaman and Misailovic, Sasa},
title = {TERA: optimizing stochastic regression tests in machine learning projects},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464844},
doi = {10.1145/3460319.3464844},
abstract = {The stochastic nature of many Machine Learning (ML) algorithms makes testing of ML tools and libraries challenging. ML algorithms allow a developer to control their accuracy and run-time through a set of hyper-parameters, which are typically manually selected in tests. This choice is often too conservative and leads to slow test executions, thereby increasing the cost of regression testing.  We propose TERA, the first automated technique for reducing the cost of regression testing in Machine Learning tools and libraries(jointly referred to as projects) without making the tests more flaky. TERA solves the problem of exploring the trade-off space between execution time of the test and its flakiness as an instance of Stochastic Optimization over the space of algorithm hyper-parameters. TERA presents how to leverage statistical convergence-testing techniques to estimate the level of flakiness of the test for a specific choice of hyper-parameters during optimization.  We evaluate TERA on a corpus of 160 tests selected from 15 popular machine learning projects. Overall, TERA obtains a geo-mean speedup of 2.23x over the original tests, for the minimum passing probability threshold of 99%. We also show that the new tests did not reduce fault detection ability through a mutation study and a study on a set of 12 historical build failures in studied projects.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {413–426},
numpages = {14},
keywords = {Bayesian Optimization, Machine Learning, Software Testing, Test Optimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1007/s10922-020-09583-4,
author = {Awad, Mohamad Khattar and Ahmed, Marwa Hassan Hafez and Almutairi, Ali F. and Ahmad, Imtiaz},
title = {Machine Learning-Based Multipath Routing for Software Defined Networks},
year = {2021},
issue_date = {Apr 2021},
publisher = {Plenum Press},
address = {USA},
volume = {29},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-020-09583-4},
doi = {10.1007/s10922-020-09583-4},
abstract = {Network softwarization has recently been enabled via the software-defined networking (SDN) paradigm, which separates the data plane from control plane allowing for a flexible and centralized control of networks. This separation facilitates implementation of machine learning techniques for network management and optimization. In this work, a machine learning-based multipath routing (MLMR) framework is proposed for software-defined networks with quality-of-service (QoS) constraints and flow rules space constraints. The QoS-aware multipath routing problem in SDN is modeled as multicommodity network flow problem with side constraints, that is known to be NP-hard. The proposed framework utilizes network status estimates, and their corresponding routing configurations available at the network central controller to learn a mapping function between them. Once the mapping function is learned, it is applied on live-inputs of network status and routing requests to predict a multipath routing solutions in real-time. Performance evaluations of the MLMR framework on real traces of network traffic verify its accuracy and resilience to noise in training data. Furthermore, the MLMR framework demonstrates more than 98.99% improvement in computational efficiency.},
journal = {J. Netw. Syst. Manage.},
month = apr,
numpages = {30},
keywords = {Machine learning, Software defined networks, Software defined networking, Routing}
}

@inproceedings{10.1007/978-3-030-79457-6_42,
author = {Quang, Do Nguyet and Selamat, Ali and Krejcar, Ondrej},
title = {Recent Research on Phishing Detection Through Machine Learning Algorithm},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_42},
doi = {10.1007/978-3-030-79457-6_42},
abstract = {The rapid growth of emerging technologies, smart devices, 5G communication, etc. have contributed to the accumulation of data, hence introducing the big data era. Big data imposes a variety of challenges associated with machine learning, especially in phishing detection. Therefore, this paper aims to provide an analysis and summary of current research in phishing detection through machine learning for big data. To achieve this goal, this study adopted a systematic literature review (SLR) technique and critically analyzed a total of 30 papers from various journals and conference proceedings. These papers were selected from previous studies in five different databases on content published between 2018 and January 2021. The results obtained from this study reveal a limited number of research works that comprehensively reviewed the feasibility of applying both machine learning and big data technologies in the context of phishing detection.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {495–508},
numpages = {14},
keywords = {Cybersecurity, Phishing detection, Machine learning (ML), Big data},
location = {Kuala Lumpur, Malaysia}
}

@phdthesis{10.5555/AAI30013420,
author = {Nandi, Anupama and Atanas, Rountev, and Wei-Lun, Chao,},
advisor = {Raef, Bassily,},
title = {Addressing Fundamental Limitations in Differentially Private Machine Learning},
year = {2021},
isbn = {9798351442815},
publisher = {The Ohio State University},
abstract = {Differential privacy has become a widely accepted data privacy model because of the strong formal guarantee it offers, namely no individual's data has a significant impact on the outcome of analyses on the data set. Unfortunately, this strict guarantee leads to fundamental limitations pertaining to the utility of different machine learning algorithms. These limitations manifest in various problem settings.For instance, learning simple classes of functions (e.g. one-dimensional thresholds over R) is impossible under differential privacy even though it can be easily learned without privacy constraints. As another example, the resulting error in privately solving fundamental problems such as stochastic convex optimization (SCO) or empirical risk minimization (ERM) is known to incur a necessary dependence on the dimension of the problem, which limits the applicability of such private algorithms in practical scenarios where the dimension is often very large.The goal of this dissertation is to tackle some of these fundamental limitations. To circumvent these challenges, we pursue two directions: (1) relaxing the model of differentially private learning, and (2) exploiting the geometry of the learning problem.In the first direction, the goal is to exploit a limited amount of public data to achieve substantial gains in accuracy (or, equivalently savings in sample complexity) of differentially private algorithms. In particular,We study the problem of differentially private release of classification queries. In this problem, the algorithm is given a private training dataset drawn from some unknown distribution and a stream of classification queries given by a sequence unlabeled feature vectors. Here, the feature-vectors defining the set of queries are assumed to be public (i.e., they do not involve any privacy constraints), and drawn from the same distribution as the feature vectors of the private dataset. We formally study this problem in the agnostic probably approximately correct (PAC) learning model and give a construction with formal guarantees on the sample complexity. In particular, given any hypothesis class with VC-dimension d, we show that our construction can privately answer up to m classification queries with average excess error α using a private sample of size [formula omitted] (assuming the privacy parameter [formula omitted]). We also extend our construction to show that one can privately answer any number of classification queries with average excess error α using a private sample of size [formula omitted]. When [formula omitted] and the privacy parameter [formula omitted], our private sample complexity bound is essentially optimal.Next, we examine a new model of supervised learning under privacy constraints in which a learning algorithm has access to a mixed dataset of private and public examples that arise from possibly different populations (distributions). In particular, the target population D is a mixture of two sub-populations: a private sub-population Dpriv of private and sensitive data, and a public sub-population Dpub of data with no privacy concerns. Each example drawn from D is assumed to contain a privacy-status bit that indicates whether the example is private or public. Here the goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. Prior works that studied utilizing public data in differentially private learning assumed that the private and public data come from the same distribution. We give a construction for learning linear classifiers in Rd and show that such an assumption can be circumvented. We show that in the case where the privacy status is correlated with the target label, linear classifiers in Rd can be learned, in the agnostic as well as the realizable setting, with sample complexity which is comparable to that of the classical (non-private) PAC-learning.The second direction we explore aims at exploiting the geometry of the learning problem to provide better accuracy guarantees for differentially private stochastic convex optimization (DP-SCO). In DP-SCO, the goal is to minimize the population risk [formula omitted] for convex loss functions of x over some feasible set given access to i.i.d. samples z1,...,zn from a data distribution D, under the constraint of differential privacy. Prior works in DP-SCO have focused only on the Euclidean setting (ℓ setting), where both the feasible set and the subgradients of the loss are bounded in the ℓ2 norm. For this setting, existing results show that the optimal rate for the population risk of DP-SCO has a necessary dependence on √d, where d is the dimensionality of the problem. In this dissertation, we initiate a systematic study of DP-SCO in non-Euclidean settings, where the geometry of the feasible set and the space of the sub-gradients of the loss are described in non-Euclidean norms (e.g. ℓp norm). Particularly for the polyhedral setup, where the feasible set is polyhedral, and the losses are convex and smooth w.r.t. a polyhedral norm (e.g. the ℓ1 norm), we give the first linear-time DP-SCO algorithm. Our excess risk guarantee is nearly optimal and nearly independent of the dimension. We extend our study to DP-SCO over non-Euclidean settings, particularly, over ℓp-normed spaces for [formula omitted]. For p ∈ (1,∞), we derive a lower bound on the excess risk for this range of p showing a necessary dependence on √d, where d is the dimensionality of the ℓp space. For p ∈ (1,2), under standard smoothness assumption, we give the first linear-time algorithm with nearly optimal excess risk. For p ∈ (2, ∞), existing noisy stochastic gradient methods attain optimal excess risk in the low-dimensional regime.},
note = {AAI30013420}
}

@article{10.1016/j.patcog.2016.04.020,
author = {Amorim, Willian P. and Falc\~{a}o, Alexandre X. and Papa, Jo\~{a}o P. and Carvalho, Marcelo H.},
title = {Improving semi-supervised learning through optimum connectivity},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {60},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.04.020},
doi = {10.1016/j.patcog.2016.04.020},
abstract = {The annotation of large data sets by a classifier is a problem whose challenge increases as the number of labeled samples used to train the classifier reduces in comparison to the number of unlabeled samples. In this context, semi-supervised learning methods aim at discovering and labeling informative samples among the unlabeled ones, such that their addition to the correct class in the training set can improve classification performance. We present a semi-supervised learning approach that connects unlabeled and labeled samples as nodes of a minimum-spanning tree and partitions the tree into an optimum-path forest rooted at the labeled nodes. It is suitable when most samples from a same class are more closely connected through sequences of nearby samples than samples from distinct classes, which is usually the case in data sets with a reasonable relation between number of samples and feature space dimension. The proposed solution is validated by using several data sets and state-of-the-art methods as baselines. HighlightsA new algorithm for semi-supervised learning based on optimum-path forest.The algorithm provides significant improvements in accuracy and efficiency.Labels are propagated from labeled to unlabeled training samples with less errors.The novel classifier can be more accurate than other state-of-the-art methods.A fast and effective algorithm suitable for developing active learning methods.},
journal = {Pattern Recogn.},
month = dec,
pages = {72–85},
numpages = {14},
keywords = {Optimum-path forest classifiers, Semi-supervised learning}
}

@inproceedings{10.5555/3507788.3507808,
author = {Tan, Waikeat and Alhamid, Mohammed and Kalil, Mohamad and Yang, Ronghao and Corvinelli, Vincent and Zuzarte, Calisto and Finnie, Liam},
title = {Query predicate selectivity using machine learning in Db2®},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The accuracy of cardinality estimation or the number of rows flowing through the query execution plan operators plays an important role in SQL query optimization. Cost-based optimizers depend on cardinality estimation to evaluate execution costs to select an optimal access plan. Achieving accurate cardinality estimation is difficult or expensive on tables that have correlated or skewed columns. Inaccurate cardinality estimation can lead to slow or unstable query performance. Although collecting statistics on multiple column combinations can minimize estimation errors with multiple predicates, it is hard to cover all column combinations. This paper presents a novel integrated approach using Machine Learning (ML) to learn and approximate the multivariate Cumulative Frequency Function (CFF) of column values, which is used to estimate cardinality for predicates with various relational operators. The key idea is that a model can learn the distribution of the data in the relation and can be used to predict cardinality for the query predicates accurately. The CFF model is also extended to estimate join cardinalities between tables. Experimental results demonstrate a significant improvement of cardinality estimation accuracy, computation efficiency, and amount of input required to train the model. Integration with the traditional optimizer is key to a smooth transition towards use in a production environment. This paper covers earlier technology previews shipped with Db2®.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {143–152},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3306618.3314293,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {active learning, explainable artificial intelligence, interpretability, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3447555.3466566,
author = {Herzog, Benedict and Reif, Stefan and H\"{u}gel, Fabian and H\"{o}nig, Timo and Schr\"{o}der-Preikschat, Wolfgang},
title = {Towards Automated System-Level Energy-Efficiency Optimisation using Machine Learning: Poster},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3466566},
doi = {10.1145/3447555.3466566},
abstract = {Modern computing systems need to execute applications in an energy-efficient manner. To this end, operating systems, middleware, and run-time systems offer plenty of parameters that support fine-tuning their behaviour. However, their individual and combined impact on performance and power draw is so complex that this optimisation potential is often ignored in practice. This paper therefore discusses a cross-layer system design that uses machine learning internally to enable fine-tuning run-time systems to their current workload. Our approach includes all layers, from the hardware to the application, considering both performance and power draw.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {274–275},
numpages = {2},
keywords = {Energy Efficiency, Machine Learning, System Configuration},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@article{10.1016/j.asoc.2021.107269,
author = {Bayliss, Christopher},
title = {Machine learning based simulation optimisation for urban routing problems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107269},
doi = {10.1016/j.asoc.2021.107269},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {17},
keywords = {Team orienteering problem, Learnheuristic, Traffic simulation, Machine learning, Metaheuristics}
}

@article{10.1504/ijaip.2021.113782,
author = {Dey, Barnali and Hossain, Ashraf and Bera, Rabindranath},
title = {Possible adoption of various machine learning techniques in cognitive radio - a survey},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.113782},
doi = {10.1504/ijaip.2021.113782},
abstract = {The concept of cognitive radio (CR) system is the need for next generation wireless communication technology in terms of providing intelligence and superior performance to a wireless device. The CR is mainly an intelligent system which is aware of its environment and is well capable to adapt in accordance with the changing environment and user needs. The concept of adaptation of the communication system can be realised well with machine learning capability inculcated within the system. It is a well-known fact that, the key strengths of any machine learning paradigm is its ability to adapt with respect to the dynamic changing system parameters. In this paper, an attempt has been made to compile various applications of machine learning techniques for different activities of CR cycle. Further, this note reviews the work on development of machine learning techniques for spectrum sensing of CR in order to make the CR system as a whole practically feasible and robust, thus mitigating its existing computational limitations due to the use of conventional techniques.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {439–463},
numpages = {24},
keywords = {cognitive radio, machine learning, spectrum sensing, energy detection}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@article{10.1145/3359786,
author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
title = {Techniques for interpretable machine learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3359786},
doi = {10.1145/3359786},
abstract = {Uncovering the mysterious ways machine learning models make decisions.},
journal = {Commun. ACM},
month = dec,
pages = {68–77},
numpages = {10}
}

@article{10.1016/j.ins.2016.04.040,
author = {Forestier, Germain and Wemmert, C\'{e}dric},
title = {Semi-supervised learning using multiple clusterings with limited labeled data},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {361},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.04.040},
doi = {10.1016/j.ins.2016.04.040},
abstract = {Supervised classification consists in learning a predictive model using a set of labeled samples. It is accepted that predictive models accuracy usually increases as more labeled samples are available. Labeled samples are generally difficult to obtain as the labeling step if often performed manually. On the contrary, unlabeled samples are easily available. As the labeling task is tedious and time consuming, users generally provide a very limited number of labeled objects. However, designing approaches able to work efficiently with a very limited number of labeled samples is highly challenging. In this context, semi-supervised approaches have been proposed to leverage from both labeled and unlabeled data.In this paper, we focus on cases where the number of labeled samples is very limited. We review and formalize eight semi-supervised learning algorithms and introduce a new method that combine supervised and unsupervised learning in order to use both labeled and unlabeled data. The main idea of this method is to produce new features derived from a first step of data clustering. These features are then used to enrich the description of the input data leading to a better use of the data distribution. The efficiency of all the methods is compared on various artificial, UCI datasets, and on the classification of a very high resolution remote sensing image. The experiments reveal that our method shows good results, especially when the number of labeled sample is very limited. It also confirms that combining labeled and unlabeled data is very useful in pattern recognition.},
journal = {Inf. Sci.},
month = sep,
pages = {48–65},
numpages = {18},
keywords = {Classification, Pattern recognition, Remote sensing, Semi-supervised learning}
}

@inproceedings{10.1145/3340482.3342743,
author = {Foidl, Harald and Felderer, Michael},
title = {Risk-based data validation in machine learning-based software systems},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342743},
doi = {10.1145/3340482.3342743},
abstract = {Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Data Validation, Machine Learning, Risk-based Testing},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3278721.3278722,
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
title = {Non-Discriminatory Machine Learning through Convex Fairness Criteria},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278722},
doi = {10.1145/3278721.3278722},
abstract = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {116},
numpages = {1},
keywords = {machine learning, non-discrimination, proportional fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3409334.3452059,
author = {Qiu, Jiabao and Moh, Melody and Moh, Teng-Sheng},
title = {Fast streaming translation using machine learning with transformer},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452059},
doi = {10.1145/3409334.3452059},
abstract = {Machine Translation is the usage of machine learning techniques in translation from one language to another. It has recently been applied to streaming translation, also known as automatic subtitling. The most common challenge in this area is the trade-off between correctness and speed. Due to its real-time feature, streaming translation needs high speed as it has strict playtime constraints. This paper proposes an enhanced Transformer model for fast streaming translation. The proposed machine-learning method is described, implemented, and evaluated based on a common German-English bilingual dataset. The evaluation results have shown that the proposed system successfully achieved a good speed in the training phase, and a high speed in the actual translating phrase that is fast enough for real-time applications, while also maintaining robust correctness. We believe the proposed Transformer model is a significant contribution to natural-language processing, and would be useful for other real-time translation applications.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {9–16},
numpages = {8},
keywords = {machine learning, machine translation, natural language processing, neural networks},
location = {Virtual Event, USA},
series = {ACMSE '21}
}

@article{10.1016/j.jpdc.2019.07.007,
author = {Garc\'{\i}a-Mart\'{\i}n, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, H\r{a}kan},
title = {Estimation of energy consumption in machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.07.007},
doi = {10.1016/j.jpdc.2019.07.007},
journal = {J. Parallel Distrib. Comput.},
month = dec,
pages = {75–88},
numpages = {14},
keywords = {Machine learning, GreenAI, Energy consumption, Deep learning, High performance computing}
}

@article{10.1007/s00357-019-09352-2,
author = {Lu, Chengbo and Mei, Ying},
title = {An Optimal Weight Semi-Supervised Learning Machine for Neural Networks with Time Delay},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {3},
issn = {0176-4268},
url = {https://doi.org/10.1007/s00357-019-09352-2},
doi = {10.1007/s00357-019-09352-2},
abstract = {In this paper, an optimal weight semi-supervised learning machine for a single-hidden layer feedforward network (SLFN) with time delay is developed. Both input weights and output weights of the SLFN are globally optimized with manifold regularization. By feature mapping, input vectors can be placed at the prescribed positions in the feature space in the sense that the separability of all nonlinearly separable patterns can be maximized, unlabeled data can be leveraged to improve the classification accuracy when labeled data are scarce, and a high degree of recognition accuracy can be achieved with a small number of hidden nodes in the SLFN. Some simulation examples are presented to show the excellent performance of the proposed algorithm.},
journal = {J. Classif.},
month = oct,
pages = {656–670},
numpages = {15},
keywords = {Neural networks, Optimal weight learning, Semi-supervised learning, Manifold regularization, Time delay}
}

@article{10.3233/JIFS-189498,
author = {Wang, Lejie and Ramachandran, Varatharajan},
title = {Improving the performance of precision poverty alleviation based on big data mining and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189498},
doi = {10.3233/JIFS-189498},
abstract = {Since the reform began in our country, with the rapid economic growth in recent years, the income level has grown extremely unequal, and it is difficult for the low-income poor to benefit from the rapid economic growth. The most important prerequisite for the fight against poverty is the accurate identification of the causes of poverty. To date, our country has not reached the level of maturity required to accurately study the causes of poverty in various households. However, with the rapid development of Internet technology and big data technology in recent years, the application of large-scale data technology and data extraction algorithms to poverty reduction can identify truly poor households faster and more accurately. Compared with traditional machine learning algorithms, there are no machine storage and technical constraints, can use a large amount of data and rely on multiple data samples.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6617–6628},
numpages = {12},
keywords = {Big data mining, machine learning, precision poverty alleviation, performance improvement}
}

@article{10.3233/IP-200264,
author = {Keen, Justin and Ruddle, Roy and Palczewski, Jan and Aivaliotis, Georgios and Palczewska, Anna and Megone, Christopher and Macnish, Kevin},
title = {Machine learning, materiality and governance: A health and social care case study},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {1},
issn = {1570-1255},
url = {https://doi.org/10.3233/IP-200264},
doi = {10.3233/IP-200264},
abstract = {There is a widespread belief that machine learning tools can be used to improve decision-making in health and social care. At the same time, there are concerns that they pose threats to privacy and confidentiality. Policy makers therefore need to develop governance arrangements that balance benefits and risks associated with the new tools. This article traces the history of developments of information infrastructures for secondary uses of personal datasets, including routine reporting of activity and service planning, in health and social care. The developments provide broad context for a study of the governance implications of new tools for the analysis of health and social care datasets. We find that machine learning tools can increase the capacity to make inferences about the people represented in datasets, although the potential is limited by the poor quality of routine data, and the methods and results are difficult to explain to other stakeholders. We argue that current local governance arrangements are piecemeal, but at the same time reinforce centralisation of the capacity to make inferences about individuals and populations. They do not provide adequate oversight, or accountability to the patients and clients represented in datasets.},
journal = {Info. Pol.},
month = jan,
pages = {57–69},
numpages = {13},
keywords = {Machine learning, governance, accountability, information infrastructure, health care, social care}
}

@article{10.1016/j.jnca.2021.103186,
author = {Cheng, Qiumei and Wu, Chunming and Zhou, Haifeng and Kong, Dezhang and Zhang, Dong and Xing, Junchi and Ruan, Wei},
title = {Machine learning based malicious payload identification in software-defined networking},
year = {2021},
issue_date = {Oct 2021},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {192},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2021.103186},
doi = {10.1016/j.jnca.2021.103186},
journal = {J. Netw. Comput. Appl.},
month = oct,
numpages = {12},
keywords = {Software-defined networking, Deep packet inspection, Machine learning, Linear prediction}
}

@inproceedings{10.1145/3368089.3418538,
author = {\v{C}egi\v{n}, J\'{a}n},
title = {Machine learning based test data generation for safety-critical software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418538},
doi = {10.1145/3368089.3418538},
abstract = {Unit testing focused on Modified Condition/Decision Coverage (MC/DC) criterion is essential in development safety-critical systems. However, design of test data that meets the MC/DC criterion currently needs detailed manual analysis of branching conditions in units under test by test engineers. Multiple state-of-art approaches exist with proven usage even in industrial projects. However, these approaches have multiple shortcomings, one of them being the Path explosion problem which has not been fully solved yet. Machine learning methods as meta-heuristic approximations can model behaviour of programs that are hard to test using traditional approaches, where the Path explosion problem does occur and thus could solve the limitations of the current state-of-art approaches. I believe, motivated by an ongoing collaboration with an industrial partner, that the machine learning methods could be combined with existing approaches to produce an approach suitable for testing of safety-critical projects.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1678–1681},
numpages = {4},
keywords = {MC/DC criterion, machine learning, test data generation, unit testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/3540261.3541519,
author = {von K\"{u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch\"{o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
title = {Self-supervised learning with data augmentations provably isolates content from style},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1258},
numpages = {17},
series = {NIPS '21}
}

@inproceedings{10.1145/3461002.3473068,
author = {Santos, Edilton Lima dos},
title = {STARS: software technology for adaptable and reusable systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473068},
doi = {10.1145/3461002.3473068},
abstract = {Dynamic Software Product Lines (DSPLs) engineering implements self-adaptive systems by dynamically binding or unbinding features at runtime according to a feature model. However, these features may interact in unexpected and undesired ways leading to critical consequences for the DSPL. Moreover, (re)configurations may negatively affect the runtime system's architectural qualities, manifesting architectural bad smells. These issues are challenging to detect due to the combinatorial explosion of the number of interactions amongst features. As some of them may appear at runtime, we need a runtime approach to their analysis and mitigation. This thesis introduces the Behavioral Map (BM) formalism that captures information from different sources (feature model, code) to automatically detect these issues. We provide behavioral map inference algorithms. Using the Smart Home Environment (SHE) as a case study, we describe how a BM is helpful to identify critical feature interactions and architectural smells. Our preliminary results already show promising progress for both feature interactions and architectural bad smells identification at runtime.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {13–17},
numpages = {5},
keywords = {MAPE-K loop, dynamic software product lines engineering, self-adapting system, software architecture, software product line engineering, software testing},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3318299.3318300,
author = {Wu, Qingyang},
title = {A Review of Methods Used in Machine Learning and Data Analysis},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318300},
doi = {10.1145/3318299.3318300},
abstract = {This report provides an overview of machine learning and data analysis with explanation of the advantages and disadvantages of different methods. I also demonstrate a practical implementation of the described methods on a dataset of real estate prices.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {43–51},
numpages = {9},
keywords = {Data exploration, machine learning, principal component analysis},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.3233/SW-200388,
author = {Hitzler, Pascal and Janowicz, Krzysztof and d’Amato, Claudia},
title = {Machine Learning for the Semantic Web: Lessons learnt and next research directions},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {11},
number = {1},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-200388},
doi = {10.3233/SW-200388},
abstract = {Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed, jointly with the analysis of their main peculiarities and drawbacks. Afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.},
journal = {Semant. Web},
month = jan,
pages = {195–203},
numpages = {9},
keywords = {Machine Learning, symbol-based methods, numeric-based methods}
}

@inproceedings{10.1145/3469968.3469973,
author = {Li, Haomiao and Jin, Zian and Krishnamoorthy, Sujatha},
title = {E-Waste Management Using Machine Learning},
year = {2021},
isbn = {9781450389808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469968.3469973},
doi = {10.1145/3469968.3469973},
abstract = {In the realm of innovation, as individuals begin adjusting to new development, advancement begins clearing its way into individuals' lives. Each adjustment in innovation gets better than ever, with old gadgets being supplanted and deserted. Such Electronic and Electrical Equipments (EEEs) that are disposed of by clients are named e-waste. EEEs are comprised of a large number of segments, some containing poisonous substances that affect human wellbeing and climate, if not handled appropriately. The volume of EEEs that is delivered all through the world, has driven governments in different nations to make severe arrangements, to guarantee effective removal of the created e-waste. This paper presents the utilization of ML (Machine Learning) for E-Waste Management framework with a center to build up a prescient model by contrasting the presentation of gradient boosting regression tree (GBRT) and Neural Network (NN) ML calculations to gauge week by week e-wastage for every Urban sub-segments. Various information-driven strategies incorporate the current designed model and its alteration alongside regular machine learning calculations. The utilization of Machine Learning calculation gives improved arrangement exactness of 99.1 % when utilizing the best performing algorithm. Notwithstanding the prominent quantitative upgrades, the proposed plan can likewise help in enhancing long-haul e-waste management in smart-city ambient utilizing the recorded insights.},
booktitle = {Proceedings of the 6th International Conference on Big Data and Computing},
pages = {30–35},
numpages = {6},
keywords = {E-waste management, Random Forest algorithm, data mining},
location = {Shenzhen, China},
series = {ICBDC '21}
}

@inproceedings{10.1145/3460112.3471966,
author = {Beery, Sara and Cole, Elijah and Parker, Joseph and Perona, Pietro and Winner, Kevin},
title = {Species Distribution Modeling for Machine Learning Practitioners: A Review},
year = {2021},
isbn = {9781450384537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460112.3471966},
doi = {10.1145/3460112.3471966},
abstract = {Conservation science depends on an accurate understanding of what’s happening in a given ecosystem. How many species live there? What is the makeup of the population? How is that changing over time? Species Distribution Modeling (SDM) seeks to predict the spatial (and sometimes temporal) patterns of species occurrence, i.e. where a species is likely to be found. The last few years have seen a surge of interest in applying powerful machine learning tools to challenging problems in ecology [2, 5, 8]. Despite its considerable importance, SDM has received relatively little attention from the computer science community. Our goal in this work is to provide computer scientists with the necessary background to read the SDM literature and develop ecologically useful ML-based SDM algorithms. In particular, we introduce key SDM concepts and terminology, review standard models, discuss data availability, and highlight technical challenges and pitfalls.},
booktitle = {Proceedings of the 4th ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {329–348},
numpages = {20},
keywords = {ecological niche modeling, machine learning, species distribution modeling},
location = {Virtual Event, Australia},
series = {COMPASS '21}
}

@inproceedings{10.1145/3292500.3330667,
author = {Ahmed, Zeeshan and Amizadeh, Saeed and Bilenko, Mikhail and Carr, Rogan and Chin, Wei-Sheng and Dekel, Yael and Dupre, Xavier and Eksarevskiy, Vadim and Filipi, Senja and Finley, Tom and Goswami, Abhishek and Hoover, Monte and Inglis, Scott and Interlandi, Matteo and Kazmi, Najeeb and Krivosheev, Gleb and Luferenko, Pete and Matantsev, Ivan and Matusevych, Sergiy and Moradi, Shahab and Nazirov, Gani and Ormont, Justin and Oshri, Gal and Pagnoni, Artidoro and Parmar, Jignesh and Roy, Prabhat and Siddiqui, Mohammad Zeeshan and Weimer, Markus and Zahirazami, Shauheen and Zhu, Yiwen},
title = {Machine Learning at Microsoft with ML.NET},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330667},
doi = {10.1145/3292500.3330667},
abstract = {Machine Learning is transitioning from an art and science into a technology available to every developer. In the near future, every application on every platform will incorporate trained models to encode data-based decisions that would be impossible for developers to author. This presents a significant engineering challenge, since currently data science and modeling are largely decoupled from standard software development processes. This separation makes incorporating machine learning capabilities inside applications unnecessarily costly and difficult, and furthermore discourage developers from embracing ML in first place. In this paper we present ML.NET, a framework developed at Microsoft over the last decade in response to the challenge of making it easy to ship machine learning models in large software applications. We present its architecture, and illuminate the application demands that shaped it. Specifically, we introduce DataView, the core data abstraction of ML.NET which allows it to capture full predictive pipelines efficiently and consistently across training and inference lifecycles. We close the paper with a surprisingly favorable performance study of ML.NET compared to more recent entrants, and a discussion of some lessons learned.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2448–2458},
numpages = {11},
keywords = {data view, machine learning, model deployment, pipelines},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1007/978-3-030-86044-8_5,
author = {Caporuscio, Mauro and De Toma, Marco and Muccini, Henry and Vaidhyanathan, Karthik},
title = {A Machine Learning Approach to Service Discovery for Microservice Architectures},
year = {2021},
isbn = {978-3-030-86043-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86044-8_5},
doi = {10.1007/978-3-030-86044-8_5},
abstract = {Service discovery mechanisms have continuously evolved during the last years to support the effective and efficient service composition in large-scale microservice applications. Still, the dynamic nature of services (and of their contexts) are being rarely taken into account for maximizing the desired quality of service. This paper proposes using machine learning techniques, as part of the service discovery process, to select microservice instances in a given context, maximize QoS, and take into account the continuous changes in the execution environment. Both deep neural networks and reinforcement learning techniques are used. Experimental results show how the proposed approach outperforms traditional service discovery mechanisms.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021, Virtual Event, Sweden, September 13-17, 2021, Proceedings},
pages = {66–82},
numpages = {17},
keywords = {Service discovery, Machine learning, Microservices architecture}
}

@article{10.14778/3476311.3476408,
author = {Jindal, Alekh and Interlandi, Matteo},
title = {Machine learning for cloud data systems: the progress so far and the path forward},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476408},
doi = {10.14778/3476311.3476408},
abstract = {The goal of this tutorial is to educate the audience about the state of the art in ML for cloud data systems, both in research and in practice. The tutorial is divided in two parts: the progress, and the path forward.Part I covers the recent successes in deploying machine learning solutions for cloud data systems. We will discuss the practical considerations taken into account and the progress made at various levels. The goal is to compare and contrast the promise of ML for systems with the ground actually covered in industry.Finally, Part II discusses practical issues of machine learning in the enterprise covering the generation of explanations, model debugging, model deployment, model management, constraints on eyes-on data usage and anonymization, and a discussion of the technical debt that can accrue through machine learning and models in the enterprise.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3202–3205},
numpages = {4}
}

@inproceedings{10.1145/3329486.3329489,
author = {Louren\c{c}o, Raoni and Freire, Juliana and Shasha, Dennis},
title = {Debugging Machine Learning Pipelines},
year = {2019},
isbn = {9781450367974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329486.3329489},
doi = {10.1145/3329486.3329489},
abstract = {Machine learning tasks entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous or uninformative outputs, the pipeline may fail or produce incorrect results. Inferring the root cause of failures and unexpected behavior is challenging, usually requiring much human thought, and is both time consuming and error prone. We propose a new approach that makes use of iteration and provenance to automatically infer the root causes and derive succinct explanations of failures. Through a detailed experimental evaluation, we assess the cost, precision, and recall of our approach compared to the state of the art. Our source code and experimental data will be available for reproducibility and enhancement.},
booktitle = {Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning},
articleno = {3},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {DEEM'19}
}

@inproceedings{10.1145/3459637.3483280,
author = {Li, Yunqi and Ge, Yingqiang and Zhang, Yongfeng},
title = {CIKM 2021 Tutorial on Fairness of Machine Learning in Recommender Systems},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3483280},
doi = {10.1145/3459637.3483280},
abstract = {Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users' or providers' satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4857–4860},
numpages = {4},
keywords = {AI ethics, fairness, machine learning, recommender systems},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1007/s10915-019-00908-3,
author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier},
title = {Machine Learning for Semi Linear PDEs},
year = {2019},
issue_date = {June      2019},
publisher = {Plenum Press},
address = {USA},
volume = {79},
number = {3},
issn = {0885-7474},
url = {https://doi.org/10.1007/s10915-019-00908-3},
doi = {10.1007/s10915-019-00908-3},
abstract = {Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.},
journal = {J. Sci. Comput.},
month = jun,
pages = {1667–1712},
numpages = {46},
keywords = {Deep learning, Machine learning, Monte-Carlo methods, Semilinear PDEs}
}

@inproceedings{10.1609/aaai.v33i01.33018868,
author = {Sam, Deepak Babu and Sajjan, Neeraj N. and Maurya, Himanshu and Babu, R. Venkatesh},
title = {Almost unsupervised learning for dense crowd counting},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33018868},
doi = {10.1609/aaai.v33i01.33018868},
abstract = {We present an unsupervised learning method for dense crowd count estimation. Marred by large variability in appearance of people and extreme overlap in crowds, enumerating people proves to be a difficult task even for humans. This implies creating large-scale annotated crowd data is expensive and directly takes a toll on the performance of existing CNN based counting models on account of small datasets. Motivated by these challenges, we develop Grid Winner-Take-All (GWTA) autoencoder to learn several layers of useful filters from unlabeled crowd images. Our GWTA approach divides a convolution layer spatially into a grid of cells. Within each cell, only the maximally activated neuron is allowed to update the filter. Almost 99.9% of the parameters of the proposed model are trained without any labeled data while the rest 0.1% are tuned with supervision. The model achieves superior results compared to other unsupervised methods and stays reasonably close to the accuracy of supervised baseline. Furthermore, we present comparisons and analyses regarding the quality of learned features across various models.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1088},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1007/978-3-030-29959-0_2,
author = {Zheng, Yifeng and Duan, Huayi and Wang, Cong},
title = {Towards Secure and Efficient Outsourcing of Machine Learning Classification},
year = {2019},
isbn = {978-3-030-29958-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29959-0_2},
doi = {10.1007/978-3-030-29959-0_2},
abstract = {Machine learning classification has been successfully applied in numerous applications, such as healthcare, finance, and more. Outsourcing classification services to the cloud has become an intriguing practice as this brings many prominent benefits like ease of management and scalability. Such outsourcing, however, raises critical privacy concerns to both the machine learning model provider and the client interested in using the classification service. In this paper, we focus on classification outsourcing with decision trees, one of the most popular classifiers. We propose for the first time a secure framework allowing decision tree based classification outsourcing while maintaining the confidentiality of the provider’s model (parameters) and the client’s input feature vector. Our framework requires no interaction from the provider and the client—they can go offline after the initial submission of their respective encrypted inputs to the cloud. This is a distinct advantage over prior art for practical deployment, as they all work under the client-provider setting where synchronous online interactions between the provider and client is required. Leveraging the lightweight additive secret sharing technique, we build our protocol from the ground up&nbsp;to enable secure and efficient outsourcing of decision tree evaluation, tailored to address the challenges posed by secure in-the-cloud dealing with versatile components including input feature selection, decision node evaluation, path evaluation, and classification generation. Through evaluation we show the practical performance of our design, and the substantial client-side savings over prior art, say up&nbsp;to four orders of magnitude in computation and 163\texttimes{} in communication.},
booktitle = {Computer Security – ESORICS 2019: 24th European Symposium on Research in Computer Security, Luxembourg, September 23–27, 2019, Proceedings, Part I},
pages = {22–40},
numpages = {19},
keywords = {Cloud security, Machine learning, Secure outsourcing},
location = {Luxembourg, Luxembourg}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous defect prediction, Machine learning, DevOps, Continuous integration}
}

@inproceedings{10.1145/3457682.3457690,
author = {Lasri, Imane and RiadSolh, Anouar and El Belkacemi, Mourad},
title = {Toward an Effective Analysis of COVID-19 Moroccan Business Survey Data using Machine Learning Techniques},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457690},
doi = {10.1145/3457682.3457690},
abstract = {COVID-19 pandemic has gravely affected our societies and economies with severe consequences. To contain the spread of the disease, most governments around the world authorized unprecedented measures, including Morocco, which has closed the borders and adopted full lockdown between March and June 2020. However, these measures have resulted in economic loss and have led to dramatic changes in how businesses act and consumers behave. The main focus of this study was to examine the impact of the full lockdown on Moroccan enterprises based on the COVID-19 Moroccan business survey carried out by the High Commission for Planning (HCP). A three-stage analysis method was employed. First, multiple correspondence analysis (MCA) was used to reduce the dimensionality of the categorical variables, and k-means clustering algorithm was used to cluster the data, then decision tree algorithm was performed in order to interpret each cluster and the maximum accuracy achieved is 84.45%. Compared with the decision tree algorithm, an artificial neural network (ANN) with stratified 10-fold cross-validation was applied to the dataset and has reached an accuracy of 83.4%. The simulation results confirm the effectiveness of the proposed techniques for analyzing survey data.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {50–58},
numpages = {9},
keywords = {ANN, Categorical surveys, Decision tree, Factor analysis, K-Means, Machine learning},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1016/j.eswa.2021.114942,
author = {Li, Hao and Misra, Siddharth},
title = {Robust machine-learning workflow for subsurface geomechanical characterization and comparison against popular empirical correlations},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {177},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114942},
doi = {10.1016/j.eswa.2021.114942},
journal = {Expert Syst. Appl.},
month = sep,
numpages = {16},
keywords = {Machine Learning, Geomechanical, Sonic, Oil and Gas, Neural Network}
}

@inproceedings{10.1007/978-3-030-79457-6_52,
author = {Dedabrishvili, Mariam and Dundua, Besik and Mamaiashvili, Natia},
title = {Smartphone Sensor-Based Fall Detection Using Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_52},
doi = {10.1007/978-3-030-79457-6_52},
abstract = {Human Activity Recognition and particularly detection of abnormal activities such as falls have become a point of interest to many researchers worldwide since falls are considered to be one of the leading causes of injury and death, especially in the elderly population. The prompt intervention of caregivers in critical situations can significantly improve the autonomy and well-being of individuals living alone and those who require remote monitoring. This paper presents a study of accelerometer and gyroscope data retrieved from smartphone embedded sensors, using iOS-based devices. In the project framework there was developed a mobile application for data collection with the following fall type and fall-like activities: Falling Right, Falling Left, Falling Forward, Falling Backward, Sitting Fast, and Jumping. The collected dataset has passed the preprocessing phase and afterward was classified using different Machine Learning algorithms, namely, by Decision Trees, Random Forest, Logistic Regression, k-Nearest Neighbour, XGBoost, LightGBM, and Pytorch Neural Network. Unlike other similar studies, during the experimental setting, volunteers were asked to have smartphones freely in their pockets without tightening and fixing them on the body. This natural way of keeping a mobile device is quite challenging in terms of noisiness however it is more comfortable to wearers and causes fewer constraints. The obtained results are promising that encourages us to continue working with the aim to reach sufficient accuracy along with building a real-time application for potential users.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {609–620},
numpages = {12},
keywords = {Fall detection, Data preprocessing, Data classification, Smartphone embedded sensors, Mobile applications},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1002/smr.2238,
author = {Naeem, Muhammad Rashid and Lin, Tao and Naeem, Hamad and Liu, Hailu},
title = {A machine learning approach for classification of equivalent mutants},
year = {2020},
issue_date = {May 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {5},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2238},
doi = {10.1002/smr.2238},
abstract = {Mutation testing is a fault‐based technique to test the quality of test suites by inducing artificial syntactic faults or mutants in a source program. However, some mutants have the same semantics as original program and cannot be detected by any test suite input known as equivalent mutants. Equivalent mutant problem (EMP) is undecidable as it requires manual human effort to identify a mutant as equivalent or killable. The constraint‐based testing (CBT) theory suggests the use of mathematical constraints which can help reveal some equivalent mutants using mutant features. In this paper, we consider three metrics of CBT theory, ie, reachability, necessity, and sufficiency to extract feature constraints from mutant programs. Constraints are extracted using program dependency graphs. Other features such as degree of significance, semantic distance, and information entropy of mutants are also extracted to build a binary classification model. Machine learning algorithms such as Random Forest, GBT, and SVM are applied under two application scenarios (split‐project and cross‐project) on ten Java programs to predict equivalent mutants. The analysis of the study demonstrates that that the proposed techniques not only improves the efficiency of the equivalent mutant detection but also reduces the effort required to perform it with small accuracy loss.},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {32},
keywords = {equivalent mutants, machine learning, mutation testing, program semantics, static analysis}
}

@article{10.1007/s11276-021-02781-1,
author = {Kamboj, Anil Kumar and Jindal, Poonam and Verma, Pankaj},
title = {Machine learning-based physical layer security: techniques, open challenges, and applications},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02781-1},
doi = {10.1007/s11276-021-02781-1},
abstract = {Wireless physical layer security (WPLS) is a powerful technology for current and emerging mobile networks. Physical layer authentication (PLA), antenna selection (AS), and relay node selection are the main elements that add diversity and strength to the paradigm of WPLS. However, heterogeneity, ultra-density, and high mobility requirements make the work difficult for the security of wireless networks. Recently, machine learning has emerged as a promising tool to alleviate the increasing complexity of wireless networks. Hence, this paper introduces intelligent WPLS by concentration on PLA, AS, and relay node selection. First, it presents the background and types of WPLS and ML. Then, revisit the three basic methods of WPLS enhancement, i.e., relay node selection, AS, and authentication, and their integration with ML. Furthermore, several key challenges faced by intelligent WPLS were&nbsp;discussed along with the comprehensive investigation of its different applications in the wireless networks such as the internet of things, device-to-device communication, cognitive radio, non-orthogonal multiple access, and unmanned aerial vehicles. Finally, the appendix includes a detailed survey of ML techniques for WPLS. This article proposes to motivate and help interested readers to easily and rapidly understand the state-of-the-art of WPLS and intelligent WPLS.},
journal = {Wirel. Netw.},
month = nov,
pages = {5351–5383},
numpages = {33},
keywords = {Physical layer security, Cooperative communication, Physical layer authentication, Antenna selection, Relay selection, Wireless networks, Machine learning}
}

@article{10.1177/02783649211045736,
author = {Nanayakkara, Thrishantha and Barfoot, Tim and Howard, Thomas and Tang, Tim Y. and De Martini, Daniele and Wu, Shangzhe and Newman, Paul},
title = {Self-supervised learning for using overhead imagery as maps in outdoor range sensor localization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {40},
number = {12–14},
issn = {0278-3649},
url = {https://doi.org/10.1177/02783649211045736},
doi = {10.1177/02783649211045736},
abstract = {Traditional approaches to outdoor vehicle localization assume a reliable, prior map is available, typically built using the same sensor suite as the on-board sensors used during localization. This work makes a different assumption. It assumes that an overhead image of the workspace is available and utilizes that as a map for use for range-based sensor localization by a vehicle. Here, range-based sensors are radars and lidars. Our motivation is simple, off-the-shelf, publicly available overhead imagery such as Google satellite images can be a ubiquitous, cheap, and powerful tool for vehicle localization when a usable prior sensor map is unavailable, inconvenient, or expensive. The challenge to be addressed is that overhead images are clearly not directly comparable to data from ground range sensors because of their starkly different modalities. We present a learned metric localization method that not only handles the modality difference, but is also cheap to train, learning in a self-supervised fashion without requiring metrically accurate ground truth. By evaluating across multiple real-world datasets, we demonstrate the robustness and versatility of our method for various sensor configurations in cross-modality localization, achieving localization errors on-par with a prior supervised approach while requiring no pixel-wise aligned ground truth for supervision at training. We pay particular attention to the use of millimeter-wave radar, which, owing to its complex interaction with the scene and its immunity to weather and lighting conditions, makes for a compelling and valuable use case.},
journal = {Int. J. Rob. Res.},
month = dec,
pages = {1488–1509},
numpages = {22},
keywords = {Localization, cross-modality localization, deep learning, self-supervised learning}
}

@inproceedings{10.1145/3318299.3318340,
author = {Zhenning, Guo},
title = {Distributed Machine Learning over Directed Network with Fixed Communication Delays},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318340},
doi = {10.1145/3318299.3318340},
abstract = {In this paper, we present a distributed machine learning algorithm over a network with fixed-delay tolerance. The network is directed and strongly connected. The training dataset is distributed to all agents in the network. We combine the distributed convex optimization (which utilizes double linear iterations) and corresponding machine learning algorithm. Each agent can only access its own local dataset. Suppose the delay between any pair of agents is time-invariant. The simulation shows that our algorithm is able to work under delayed transmission, in the sense that over time at each agent t the ratio of the estimate value xi(t) and scaling variable yi(t) can converge to the optimal point of the global cost function corresponding to the machine learning problem.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {22–26},
numpages = {5},
keywords = {Distributed system, convex optimization, machine learning, network},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1016/j.asoc.2021.107948,
author = {Min, Liangyu and Dong, Jiawei and Liu, Jiangwei and Gong, Xiaomin},
title = {Robust mean-risk portfolio optimization using machine learning-based trade-off parameter},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107948},
doi = {10.1016/j.asoc.2021.107948},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {22},
keywords = {Portfolio selection, Robust optimization, Hybrid models, Machine learning, Risk measures}
}

@article{10.1007/s42979-021-00856-6,
author = {El Mezouari, Asmae and El Fazziki, Abdelaziz and Sadgal, Mohammed},
title = {Hadoop–Spark Framework for Machine Learning-Based Smart Irrigation Planning},
year = {2021},
issue_date = {Jan 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {1},
url = {https://doi.org/10.1007/s42979-021-00856-6},
doi = {10.1007/s42979-021-00856-6},
abstract = {Up-to-date, given the expanding increase of the population and the development of human daily lifestyles, the expenditure of freshwater resources increments progressively. It appears that there is a need to optimize at least the consumption of fresh water in agriculture. For this reason, novel various irrigation technologies have been deployed in this context like drip irrigation, flood irrigation, and decision support systems to come up with the constraints of climate changes that decrease the water availability but it is still limited. Therefore, the majority of researchers are working until today on automating the irrigation systems. These smart systems rely mainly on the advances of information technologies like the internet of things, big data, and machine learning for aligning irrigations with climatic changes. Besides, integrating the predictive process helps in anticipating and adapting to the climatic constraints in agriculture, using meticulous soil and environment dependencies analysis based on features’ prediction. In this paper, we enriched our proposed flexible online learning (OL) framework designed for promoting irrigation decisions based on soil characteristics analysis and prediction. We shed the light on a comparative study of four predictive methods, in particular, the auto-regressive moving average, the eXtreme Gradient Boosting, the random forest, and the deep artificial neural networks implemented inside the Hadoop/Spark environment to predict the humidity of the soil, relying on soil temperature and time in several depths. In the end, we discussed the precision of these models in various conditions.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {10},
keywords = {Irrigation planning, Time series, Hadoop, Spark, Machine learning, Big data}
}

@inproceedings{10.1145/3394171.3413547,
author = {Yin, Shi and Wang, Shangfei and Chen, Xiaoping and Chen, Enhong},
title = {Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413547},
doi = {10.1145/3394171.3413547},
abstract = {Current work of facial landmark tracking usually requires large amounts of fully annotated facial videos to train a landmark tracker. To relieve the burden of manual annotations, we propose a novel facial landmark tracking method that makes full use of unlabeled facial videos by exploiting both self-supervised and semi-supervised learning mechanisms. First, self-supervised learning is adopted for representation learning from unlabeled facial videos. Specifically, a facial video and its shuffled version are fed into a feature encoder and a classifier. The feature encoder is used to learn visual representations, and the classifier distinguishes the input videos as the original or the shuffled ones. The feature encoder and the classifier are trained jointly. Through self-supervised learning, the spatial and temporal patterns of a facial video are captured at representation level. After that, the facial landmark tracker, consisting of the pre-trained feature encoder and a regressor, is trained semi-supervisedly. The consistencies among the tracking results of the original, the inverse and the disturbed facial sequences are exploited as the constraints on the unlabeled facial videos, and the supervised loss is adopted for the labeled videos. Through semi-supervised end-to-end training, the tracker captures sequential patterns inherent in facial videos despite small amount of manual annotations. Experiments on two benchmark datasets show that the proposed framework outperforms state-of-the-art semi-supervised facial landmark tracking methods, and also achieves advanced performance compared to fully supervised facial landmark tracking methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2991–2998},
numpages = {8},
keywords = {facial landmark tracking, self-supervised learning, semi-supervised learning},
location = {Seattle, WA, USA},
series = {MM '20}
}

@phdthesis{10.5555/AAI28764024,
author = {Shi, Zhan and Don, Fussell, and Qiang, Liu, and Milad, Hashemi,},
advisor = {Calvin, Lin,},
title = {Machine Learning for Prediction Problems in Computer Architecture},
year = {2020},
isbn = {9798538144778},
publisher = {The University of Texas at Austin},
abstract = {The solutions to many problems in computer architecture involve predictions, which are often based on heuristics. Given the success of machine learning in solving prediction problems, it is natural to wonder if machine learning can better solve architectural prediction problems. Unfortunately, despite vastly outperforming traditional heuristics in various fields, machine learning has seen limited impact on prediction problems in computer architecture. The main challenge is that each architectural prediction problem exhibits unique constraints that prevent off-the-shelf machine learning algorithms from being more effective than heuristics. For example, hardware prediction problems, such as branch prediction and cache replacement, impose severe latency and area constraints that make multi-layer neural networks largely infeasible.In this thesis, we propose machine learning solutions to three important problems in computer architecture, namely cache replacement, data prefetching, and the automatic design of neural network accelerators. In our solutions, we focus on not only the design of learning algorithms, but also the use of learning algorithms under the unique constraints of each problem. In particular, to deal with the extremely tight area and latency constraints of replacement policies and data prefetchers, we propose to first design powerful yet impractical neural network models, from which we derive important insights that can be used to design practical predictors. To deal with the highly constrained search space in the automated design of neural network accelerators, we propose a new constrained Bayesian optimization framework to effectively explore the search space where over 90% of designs are infeasible.},
note = {AAI28764024}
}

@article{10.1016/j.inffus.2018.09.013,
author = {Praveen Kumar, D. and Amgoth, Tarachand and Annavarapu, Chandra Sekhara Rao},
title = {Machine learning algorithms for wireless sensor networks: A survey},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.09.013},
doi = {10.1016/j.inffus.2018.09.013},
journal = {Inf. Fusion},
month = sep,
pages = {1–25},
numpages = {25},
keywords = {Wireless sensor networks, Machine learning, Energy efficiency, Network lifetime, Data aggregation}
}

@inproceedings{10.1007/978-3-030-64793-3_25,
author = {Belavadi, Vibha and Zhou, Yan and Kantarcioglu, Murat and Thuriasingham, Bhavani},
title = {Attacking Machine Learning Models for Social Good},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_25},
doi = {10.1007/978-3-030-64793-3_25},
abstract = {As machine learning (ML) techniques are becoming widely used, awareness of the harmful effect of automation is growing. Especially, in problem domains where critical decisions are made, machine learning-based applications may raise ethical issues with respect to fairness and privacy. Existing research on fairness and privacy in the ML community mainly focuses on providing remedies during the ML model training phase. Unfortunately, such remedies may not be voluntarily adopted by the industry that is concerned about the profits. In this paper, we propose to apply, from the user’s end, a fair and legitimate technique to “game” the ML system to ameliorate its social accountability issues. We show that although adversarial attacks can be exploited to tamper with ML systems, they can also be used for social good. We demonstrate the effectiveness of our proposed technique on real world image and credit data.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {457–471},
numpages = {15},
keywords = {Adversarial machine learning, Adversarial attacks, Artificial intelligence fairness, Data privacy},
location = {College Park, MD, USA}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@article{10.1145/3418207,
author = {Rodi\'{c}, Lea Duji\'{c} and \v{Z}upanovi\'{c}, Tomislav and Perkovi\'{c}, Toni and \v{S}oli\'{c}, Petar and Rodrigues, Joel J. P. C.},
title = {Machine Learning and Soil Humidity Sensing: Signal Strength Approach},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418207},
doi = {10.1145/3418207},
abstract = {The Internet-of-Things vision of ubiquitous and pervasive computing gives rise to future smart irrigation systems comprising the physical and digital worlds. A smart irrigation ecosystem combined with Machine Learning can provide solutions that successfully solve the soil humidity sensing task in order to ensure optimal water usage. Existing solutions are based on data received from the power hungry/expensive sensors that are transmitting the sensed data over the wireless channel. Over time, the systems become difficult to maintain, especially in remote areas due to the battery replacement issues with a large number of devices. Therefore, a novel solution must provide an alternative, cost- and energy-effective device that has unique advantage over the existing solutions. This work explores the concept of a novel, low-power, LoRa-based, cost-effective system that achieves humidity sensing using Deep Learning techniques that can be employed to sense soil humidity with high accuracy simply by measuring the signal strength of the given underground beacon device.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {39},
numpages = {21},
keywords = {Soil humidity, RSSI, LoRa, Deep learning, SVR, LSTM}
}

@inproceedings{10.5555/3432601.3432605,
author = {Krishnakumar, Sanjena and Abdou, Tamer},
title = {Towards interpretable and maintainable supervised learning using shapley values in arrhythmia},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\"{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {23–32},
numpages = {10},
keywords = {LIME, SHAP, arrhythmia, bellwether, healthcare, local model-agnostic interpretation, machine learning, multi-class classification, shapley value},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1007/s00521-020-04912-9,
author = {Li, Daming and Deng, Lianbing and Cai, Zhiming},
title = {RETRACTED ARTICLE: Design of traffic object recognition system based on machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {14},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04912-9},
doi = {10.1007/s00521-020-04912-9},
abstract = {In recent years, researchers have proposed many methods to solve the problem of obstacle detection. However, computer vision-based vehicle detection and recognition technology is still not mature enough. This research combines machine learning technology to construct a traffic object recognition system and applies innovative technology to the computer vision recognition system to construct an automatic identification system suitable for current traffic demand and improve the stability of the traffic system. Moreover, this study uses a combination of a monocular camera and a binocular camera to sense the traffic environment and obtain vehicle position and velocity information. In addition, this study is based on the binocular stereo camera to find the obstacle space and obtain the obstacle relative to the position and speed of the vehicle and combine the obstacle space information to optimize the obstacle frame of the target vehicle. Through experimental research and analysis, it can be seen that the algorithm proposed in this study has certain recognition effect and can be applied to traffic object recognition.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {8143–8156},
numpages = {14},
keywords = {Machine learning, Image recognition, Traffic, Vehicle identification system}
}

@inproceedings{10.5555/3408352.3408705,
author = {Xun, Lei and Tran-Thanh, Long and Al-Hashimi, Bashir M and Merrett, Geoff V.},
title = {Optimising resource management for embedded machine learning},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Machine learning inference is increasingly being executed locally on mobile and embedded platforms, due to the clear advantages in latency, privacy and connectivity. In this paper, we present approaches for online resource management in heterogeneous multi-core systems and show how they can be applied to optimise the performance of machine learning workloads. Performance can be defined using platform-dependent (e.g. speed, energy) and platform-independent (accuracy, confidence) metrics. In particular, we show how a Deep Neural Network (DNN) can be dynamically scalable to trade-off these various performance metrics. Achieving consistent performance when executing on different platforms is necessary yet challenging, due to the different resources provided and their capability, and their time-varying availability when executing alongside other workloads. Managing the interface between available hardware resources (often numerous and heterogeneous in nature), software requirements, and user experience is increasingly complex.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1556–1561},
numpages = {6},
keywords = {dynamic deep neural network, embedded machine learning, runtime resource management},
location = {Grenoble, France},
series = {DATE '20}
}

@article{10.1007/s00180-020-00970-8,
author = {Sambasivan, Rajiv and Das, Sourish and Sahu, Sujit K.},
title = {A Bayesian perspective of statistical machine learning for big data},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {3},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-020-00970-8},
doi = {10.1007/s00180-020-00970-8},
abstract = {Statistical Machine Learning (SML) refers to a body of algorithms and methods by which computers are allowed to discover important features of input data sets which are often very large in size. The very task of feature discovery from data is essentially the meaning of the keyword ‘learning’ in SML. Theoretical justifications for the effectiveness of the SML algorithms are underpinned by sound principles from different disciplines, such as Computer Science and Statistics. The theoretical underpinnings particularly justified by statistical inference methods are together termed as statistical learning theory. This paper provides a review of SML from a Bayesian decision theoretic point of view—where we argue that many SML techniques are closely connected to making inference by using the so called Bayesian paradigm. We discuss many important SML techniques such as supervised and unsupervised learning, deep learning, online learning and Gaussian processes especially in the context of very large data sets where these are often employed. We present a dictionary which maps the key concepts of SML from Computer Science and Statistics. We illustrate the SML techniques with three moderately large data sets where we also discuss many practical implementation issues. Thus the review is especially targeted at statisticians and computer scientists who are aspiring to understand and apply SML for moderately large to big data sets.},
journal = {Comput. Stat.},
month = sep,
pages = {893–930},
numpages = {38},
keywords = {Bayesian methods, Big data, Machine learning, Statistical learning}
}

@inproceedings{10.1145/3457682.3457686,
author = {Wang, Yunzhuo and Sun, Hao and Sun, Guangzhong},
title = {DSP-PIGAN: A Precision-Consistency Machine Learning Algorithm for Solving Partial Differential Equations},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457686},
doi = {10.1145/3457682.3457686},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {21–26},
numpages = {6},
keywords = {Generative Adversarial Network, Physics-informed Machine Learning, Precision-Consistency},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1016/j.ins.2019.07.096,
author = {Zhang, Yun and Kwong, Sam and Wang, Shiqi},
title = {Machine learning based video coding optimizations: A survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {506},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.07.096},
doi = {10.1016/j.ins.2019.07.096},
journal = {Inf. Sci.},
month = jan,
pages = {395–423},
numpages = {29},
keywords = {Video coding, High efficiency video coding, Machine learning, Mode decision, Visual quality assessment, Convolutional neural network, Deep learning, Versatile video coding}
}

@article{10.1007/s13748-020-00217-z,
author = {Sawaqed, Laith S. and Alrayes, Ayman M.},
title = {Bearing fault diagnostic using machine learning algorithms},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {4},
url = {https://doi.org/10.1007/s13748-020-00217-z},
doi = {10.1007/s13748-020-00217-z},
abstract = {This study aims to enhance the condition monitoring of external ball bearings using the raw data provided by Paderborn University which provided sufficient data for motor current signal MCS. Three classes of bearings have been used: healthy bearings, bearings with an inner race defect, and bearings with outer race defect. Online data at different operating conditions, bearings, and faults extent of artificial and real damages have been chosen to provide the generalization and robustness of the model. After proper preprocessing to the raw data of vibration and MCS, time, frequency, and time–frequency domain features have been extracted. Then, optimal features have been selected using genetic algorithm. Artificial neural network with optimized structure using genetic algorithm has been implemented. A comparison between the performance of vibration and motor current signal has been presented. Moreover, our results are compared to previous work by using the same raw data. Results showed the potential of motor current signal in bearing fault diagnosis with high classification accuracy. Moreover, the results showed the possibility to provide a promised diagnostic model that can diagnose bearings of real faults with different fault severities using MCS.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {341–350},
numpages = {10},
keywords = {Bearing damage detection, Machine fault diagnostic, Vibration, Motor current signal, Machine learning algorithm, Neural networks, Genetic algorithm}
}

@article{10.1016/j.comnet.2021.108474,
author = {Ara\'{u}jo, Samuel M.A. and de Souza, Fernanda S.H. and Mateus, Geraldo R.},
title = {A hybrid optimization-Machine Learning approach for the VNF placement and chaining problem},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {199},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108474},
doi = {10.1016/j.comnet.2021.108474},
journal = {Comput. Netw.},
month = nov,
numpages = {18},
keywords = {VNF, NFV, Optimization, Machine Learning, Placement, Chaining, Online}
}

@inproceedings{10.1007/978-3-030-58465-8_3,
author = {Je\v{z}ek, Bruno and Ouhrabka, Adam and Slab\'{y}, Antonin},
title = {Procedural Content Generation via Machine Learning in 2D Indoor Scene},
year = {2020},
isbn = {978-3-030-58464-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58465-8_3},
doi = {10.1007/978-3-030-58465-8_3},
abstract = {The article proposes a method of combining multiple deep forward neural networks to generate a distribution of objects in a 2D scene. The main concepts of machine learning, neural networks and procedural content generation concerning this intention are presented here. Additionally, these concepts are put into the context of computer graphics and used in a practical example of generating an indoor 2D scene. A method of vectorization of input datasets for training forward neural networks is proposed. Scene generation is based on the consequent placement of objects of different classes into the free space defining a room of a certain shape. Several evaluate methods have been proposed for testing the correctness of generation.},
booktitle = {Augmented Reality, Virtual Reality, and Computer Graphics: 7th International Conference, AVR 2020, Lecce, Italy, September 7–10, 2020, Proceedings, Part I},
pages = {34–49},
numpages = {16},
keywords = {Computer graphics, Machine learning, Procedural content generation (PCG), Procedural content generation via machine learning (PCGML)},
location = {Lecce, Italy}
}

@article{10.1007/s00607-020-00845-2,
author = {Gupta, Manjari and Bhargava, Lava and Indu, S.},
title = {Dynamic workload-aware DVFS for multicore systems using machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-020-00845-2},
doi = {10.1007/s00607-020-00845-2},
abstract = {With growing heterogeneity and complexity in applications, demand to design an energy-efficient and fast computing system in multi-core architecture has heightened. This paper presents a regression-based dynamic voltage frequency scaling model which studies and utilizes workload characteristics to obtain optimal voltage–frequency (v–f) settings. The proposed framework leverages the workload profile information together with power constraints to compute the best-suited voltage–frequency (v–f) settings to (a) maintain global power budget at chip-level, (b) maximize performance while enforcing power constraints at the per-core level. The presented algorithm works in conjunction with the workload characterizer and senses change in application requirements and apply the knowledge to select the next setting for the core. Our results when compared with two state-of-the-art algorithms MaxBIPS and TPEq achieve the average power reduction of 33% and 25% respectively across 32-core architecture for PARSEC benchmarks.},
journal = {Computing},
month = aug,
pages = {1747–1769},
numpages = {23},
keywords = {Dynamic voltage frequency scaling, Workload decomposition, Multicore processors, Energy-performance tradeoff, Machine learning, 62J05, 68M20}
}

@article{10.5555/3322706.3361994,
author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
title = {Tunability: importance of hyperparameters of machine learning algorithms},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define databased defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1934–1965},
numpages = {32},
keywords = {classification, hyperparameters, machine learning, meta-learning, supervised learning, tuning}
}

@inproceedings{10.1007/978-3-030-80599-9_1,
author = {Veres, Csaba and Sampson, Jennifer},
title = {You Can’t Learn What’s Not There: Self Supervised Learning and the Poverty of the Stimulus},
year = {2021},
isbn = {978-3-030-80598-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-80599-9_1},
doi = {10.1007/978-3-030-80599-9_1},
abstract = {Diathesis alternation describes the property of language that individual verbs can be used in different subcategorization frames. However, seemingly similar verbs such as drizzle and spray can behave differently in terms of the alternations they can participate in (drizzle/spray water on the plant; *drizzle/spray the plant with water). By hypothesis, primary linguistic data is not sufficient to learn which verbs alternate and which do not. We tested two state-of-the-art machine learning models trained by self supervision, and found little evidence that they could learn the correct pattern of acceptability judgement in the locative alternation. This is consistent with a poverty of stimulus argument that primary linguistic data does not provide sufficient information to learn aspects of linguistic knowledge. The finding has important consequences for machine learning models trained by self supervision, since they depend on the evidence present in the raw training input.},
booktitle = {Natural Language Processing and Information Systems: 26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021, Saarbr\"{u}cken, Germany, June 23–25, 2021, Proceedings},
pages = {3–14},
numpages = {12},
location = {Saarbr\"{u}cken, Germany}
}

@inproceedings{10.1145/3440094.3440389,
author = {Kabanda, Gabriel},
title = {A bayesian network model for machine learning and cyber security},
year = {2021},
isbn = {9781450387675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440094.3440389},
doi = {10.1145/3440094.3440389},
abstract = {The phenomenal growth in the use of internet-based technologies has resulted in complexities in cyber security subjecting organizations to cyber-attacks. This research is purposed to develop a cyber-security system that uses the Bayesian Network structure and Machine Learning. The research determined the cyber-security framework appropriate for a developing nation; evaluated network detection and prevention systems that use Artificial Intelligence paradigms such as finite automata, neural networks, genetic algorithms, fuzzy logic, support vector machines, or diverse data-mining-based approaches; analyzed Bayesian Networks that can be represented as graphical models and are directional to represent cause-effect relationships; and developed a Bayesian Network model that can handle complexity in cybersecurity. The Pragmatism paradigm used in this research, as a philosophy is intricately related to the mixed-method approach, which is largely quantitative with the research design being a survey and an experiment, but supported by qualitative approaches where Focus Group discussions were held. The Artificial Intelligence paradigms evaluated include machine learning methods, autonomous robotic vehicles, artificial neural networks, and fuzzy logic. Alternative improved solutions discussed include the use of machine learning algorithms specifically Artificial Neural Networks (ANN), Decision Tree C4.5, Random Forests, and Support Vector Machines (SVM).},
booktitle = {Proceedings of the 2nd Africa-Asia Dialogue Network (AADN) International Conference on Advances in Business Management and Electronic Commerce Research},
articleno = {9},
numpages = {7},
keywords = {Bayesian network model, artificial intelligence (AI), artificial neural networks (ANN) and decision tree, cybersecurity, machine learning (ML)},
location = {Ganzhou, China},
series = {AADNIC-ABMECR '20}
}

@article{10.1016/j.ijar.2019.07.009,
author = {Denundefinedux, Thierry and Kanjanatarakul, Orakanya and Sriboonchitta, Songsak},
title = {A new evidential K-nearest neighbor rule based on contextual discounting with partially supervised learning},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2019.07.009},
doi = {10.1016/j.ijar.2019.07.009},
journal = {Int. J. Approx. Reasoning},
month = oct,
pages = {287–302},
numpages = {16},
keywords = {Belief functions, Dempster-Shafer theory, Classification, Machine learning, Soft labels, Uncertain data}
}

@article{10.1007/s11276-020-02398-w,
author = {Sarmah, Rupam and Taggu, Amar and Marchang, Ningrinla},
title = {Detecting Byzantine attack in cognitive radio networks using machine learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02398-w},
doi = {10.1007/s11276-020-02398-w},
abstract = {One primary function in a cognitive radio network (CRN) is spectrum sensing. In an infrastructure-based CRN, instead of individual nodes independently sensing the presence of the incumbent signal and taking decisions thereon, a fusion center (FC) aggregates the sensing reports from the individual nodes and makes the final decision. Such collaborative spectrum sensing (CSS) is known to result in better sensing accuracy. On the other hand, CSS is vulnerable to Spectrum Sensing Data Falsification (SSDF) attack (a.k.a. Byzantine attack) wherein a node maliciously falsifies the sensing report prior to sending it to the FC, with the intention of disrupting the spectrum sensing process. This paper investigates the use of machine learning techniques, viz., SVM, Neural Network, Naive Bayes and Ensemble classifiers for detection of SSDF attacks in a CRN where the sensing reports are binary (i.e., either 1 or 0). The learning techniques are studied under two experimental scenarios: (a) when the training and test data are drawn from the same data-set, and (b) when separate data-sets are used for training and testing. Under the first scenario, of all the techniques, NN and Ensemble are the most robust showing consistently very good performance across varying presence of attackers in the system. Moreover performance comparison with an existing non-machine learning technique shows that the learning techniques are generally more robust than the existing algorithm under high presence of attackers. Under the second scenario, in a limited environment, Ensemble is the most robust method showing good overall performance.},
journal = {Wirel. Netw.},
month = nov,
pages = {5939–5950},
numpages = {12},
keywords = {Vulnerability detection, Spectrum sensing, Machine learning, Cognitive radio network, Frequency property, Data science}
}

@article{10.1007/s00778-021-00665-6,
author = {Wang, Jin and Wu, Jiacheng and Li, Mingda and Gu, Jiaqi and Das, Ariyam and Zaniolo, Carlo},
title = {Formal semantics and high performance in declarative machine learning using Datalog},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00665-6},
doi = {10.1007/s00778-021-00665-6},
abstract = {With an escalating arms race to adopt machine learning (ML) in diverse application domains, there is an urgent need to support declarative machine learning over distributed data platforms. Toward this goal, a new framework is needed where users can specify ML tasks in a manner where programming is decoupled from the underlying algorithmic and system concerns. In this paper, we argue that declarative abstractions based on Datalog are natural fits for machine learning and propose a purely declarative ML framework with a Datalog query interface. We show that using aggregates in recursive Datalog programs entails a concise expression of ML applications, while providing a strictly declarative formal semantics. This is achieved by introducing simple conditions under which the semantics of recursive programs is guaranteed to be equivalent to that of aggregate-stratified ones. We further provide specialized compilation and planning techniques for semi-naive fixpoint computation in the presence of aggregates and optimization strategies that are effective on diverse recursive programs and distributed data platforms. To test and demonstrate these research advances, we have developed a powerful and user-friendly system on top of Apache Spark. Extensive evaluations on large-scale datasets illustrate that this approach will achieve promising performance gains while improving both programming flexibility and ease of development and deployment for ML applications.},
journal = {The VLDB Journal},
month = may,
pages = {859–881},
numpages = {23},
keywords = {Datalog, Declarative machine learning, Apache spark, Scalability}
}

@phdthesis{10.5555/AAI28770522,
author = {Sun, Mengzhen},
advisor = {A, Friesner, Richard},
title = {Machine Learning Applications in Proteins: Interaction Prediction and Structure Prediction},
year = {2021},
isbn = {9798460487943},
publisher = {Columbia University},
address = {USA},
abstract = {This thesis focuses on the two research projects which have applied machine learning techniques to the protein-related topics. The first project is to use protein sequences and the interaction graph to address the protein-protein interaction prediction problem. The second project is to leverage the sequences of protein loops within and beyond homologs to predict the protein loop structures.In the protein-protein interaction prediction project, we applied the pretrained language models, which were trained on large sets of protein sequences, as one of the protein feature extraction methods. Another feature extraction method is the graph learning on the protein interaction graph. The graph learning embeddings and the language model embeddings were fed into classification models to predict if two proteins are interacting or not. We trained and tested our methods on the S. cerevisiae dataset and the human dataset. Our results are comparable to or better than other state-of-art methods, with the advantages that our method is faster at the sample preparation step and has a larger application scope for requiring only protein sequences. We also did experiments with datasets from different similarity cutoffs between the train and test set of the human dataset, and our method has shown an effective prediction ability even with a strict similarity cutoff.In the protein loop prediction project, we utilized the attention-based encoder-decoder language models to predict the protein loop inter-residue distances from the protein loop sequences. We fed the model with the loop sequences and received arrays of numbers representing the distances between each Cα pair in the loops. We utilized two different strategies to reconstruct the loops from the predicted distances. One was firstly to calculate the Cα coordinates from the predicted distances, and then apply a fast full-atom reconstruction method starting from Cα coordinates to build the local loop structures. Our local loop structure prediction results of this method are very competitive with low local RMSDs, especially with the lowest standard deviations. The second method was to integrate the predicted inter-residue distances as constraints to the de novo loop prediction method PLOP (Jacobson et al. 2004). We tested the loop reconstruction process on the 8-res and 12-res loop benchmark sets. This method has the best performance compared to other state-of-art methods, and the incorporation of such machine learning step decreased the computing time of the standalone PLOP program.},
note = {AAI28770522}
}

@article{10.15388/21-INFOR468,
author = {Kovalev, Maxim and Utkin, Lev and Coolen, Frank and Konstantinov, Andrei},
title = {Counterfactual Explanation of Machine Learning Survival Models},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {4},
issn = {0868-4952},
url = {https://doi.org/10.15388/21-INFOR468},
doi = {10.15388/21-INFOR468},
abstract = {A method for counterfactual explanation of machine learning survival models is proposed. One of the difficulties of solving the counterfactual explanation problem is that the classes of examples are implicitly defined through outcomes of a machine learning survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the counterfactual is introduced. This condition is based on using a distance between mean times to event. It is shown that the counterfactual explanation problem can be reduced to a standard convex optimization problem with linear constraints when the explained black-box model is the Cox model. For other black-box models, it is proposed to apply the well-known Particle Swarm Optimization algorithm. Numerical experiments with real and synthetic data demonstrate the proposed method.},
journal = {Informatica},
month = jan,
pages = {817–847},
numpages = {31},
keywords = {interpretable model, explainable AI, survival analysis, censored data, convex optimization, counterfactual explanation, Cox model, Particle Swarm Optimization}
}

@article{10.1007/s10462-020-09814-9,
author = {Gangavarapu, Tushaar and Jaidhar, C. D. and Chanduka, Bhabesh},
title = {Applicability of machine learning in spam and phishing email filtering: review and approaches},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {7},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09814-9},
doi = {10.1007/s10462-020-09814-9},
abstract = {With the influx of technological advancements and the increased simplicity in communication, especially through emails, the upsurge in the volume of unsolicited bulk emails (UBEs) has become a severe threat to global security and economy. Spam emails not only waste users’ time, but also consume a lot of network bandwidth, and may also include malware as executable files. Alternatively, phishing emails falsely claim users’ personal information to facilitate identity theft and are comparatively more dangerous. Thus, there is an intrinsic need for the development of more robust and dependable UBE filters that facilitate automatic detection of such emails. There are several countermeasures to spam and phishing, including blacklisting and content-based filtering. However, in addition to content-based features, behavior-based features are well-suited in the detection of UBEs. Machine learning models are being extensively used by leading internet service providers like Yahoo, Gmail, and Outlook, to filter and classify UBEs successfully. There are far too many options to consider, owing to the need to facilitate UBE detection and the recent advances in this domain. In this paper, we aim at elucidating on the way of extracting email content and behavior-based features, what features are appropriate in the detection of UBEs, and the selection of the most discriminating feature set. Furthermore, to accurately handle the menace of UBEs, we facilitate an exhaustive comparative study using several state-of-the-art machine learning algorithms. Our proposed models resulted in an overall accuracy of 99% in the classification of UBEs. The text is accompanied by snippets of Python code, to enable the reader to implement the approaches elucidated in this paper.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {5019–5081},
numpages = {63},
keywords = {Feature engineering, Machine learning, Phishing, Python, Spam}
}

@article{10.1016/j.jbi.2021.103751,
author = {Shahid, Osama and Nasajpour, Mohammad and Pouriyeh, Seyedamin and Parizi, Reza M. and Han, Meng and Valero, Maria and Li, Fangyu and Aledhari, Mohammed and Sheng, Quan Z.},
title = {Machine learning research towards combating COVID-19: Virus detection, spread prevention, and medical assistance},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103751},
doi = {10.1016/j.jbi.2021.103751},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {16},
keywords = {COVID-19, Machine learning, Artificial intelligence, Healthcare, Drug development, Predictive analysis}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@inproceedings{10.1145/3400302.3415770,
author = {Klemme, Florian and Prinz, Jannik and van Santen, Victor M. and Henkel, J\"{o}rg and Amrouch, Hussam},
title = {Modeling emerging technologies using machine learning: challenges and opportunities},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415770},
doi = {10.1145/3400302.3415770},
abstract = {Compact models of transistors act as the link between semiconductor technology and circuit design via circuit simulations. Unfortunately, compact model development and calibration is a challenging and time-intensive task, hindering rapid prototyping of a circuit (via circuit simulations) in emerging technologies. Moreover, foundries want to protect their confidential technology details to prevent reverse engineering. Hence, they limit access to compact transistor models of commercial technologies (e.g., with Non-Disclosure-Agreements). In this work, we propose Machine Learning (ML) to bridge the gap between early device measurements and later occurring compact model development. Our approach employs a Neural Network (NN) that captures the electrical response of a conventional FinFET transistor without knowledge of semiconductor physics. Additionally, our approach can be applied to emerging technologies, using Negative Capacitance FinFET (NC-FinFET) as an example for a (challenging to model) emerging technology. Inherently, the black-box nature of ML approaches keeps technology manufacturing details confidential. Furthermore, we show how using solely R2 score as our fitness function is insufficient and instead propose fitness based on key electrical characteristics or transistors like threshold voltage. Our NN-based transistor modeling can infer FinFET and NC-FinFET with an R2 score larger than 0.99 and transistor characteristics within 5% of experimental data.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {15},
numpages = {9},
keywords = {FinFET, compact model, machine learning, negative capacitance FinFET, neural network, transistor model},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1007/s00521-019-04222-9,
author = {Faigl, Jan},
title = {Unsupervised learning-based solution of the Close Enough Dubins Orienteering Problem},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04222-9},
doi = {10.1007/s00521-019-04222-9},
abstract = {This paper reports on the application of novel unsupervised learning-based method called the Growing Self-Organizing Array (GSOA) to data collection planning with curvature-constrained paths that is motivated by surveillance missions with aerial vehicles. The planning problem is formulated as the Close Enough Dubins Orienteering Problem which combines combinatorial optimization with continuous optimization to determine the most rewarding data collection path that does not exceed the given travel budget and satisfies the motion constraints of the vehicle. The combinatorial optimization consists of selecting a subset of the most rewarding data to be collected and the schedule of data collection. On the other hand, the continuous optimization stands to determine the most suitable waypoint locations from which selected data can be collected together with the determination of the headings at the waypoints for the used Dubins vehicle model. The existing purely combinatorial approaches need to discretize the possible waypoint locations and headings into some finite sets, and the solution is computationally very demanding because the problem size is quickly increased. On the contrary, the employed GSOA performs online sampling of the waypoints and headings during the adaptation of the growing structure that represents the requested curvature-constrained data collection path. Regarding the presented results, the proposed approach provides solutions to orienteering problems with competitive quality, but it is significantly less computationally demanding.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {18193–18211},
numpages = {19},
keywords = {Data collection planning, Surveillance missions and aerial vehicles, Growing Self-Organizing Array, GSOA}
}

@inproceedings{10.1109/INFOCOM42981.2021.9488920,
author = {Li, Weiting and Xiang, Liyao and Zhou, Zhou and Peng, Feng},
title = {Privacy Budgeting for Growing Machine Learning Datasets},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM42981.2021.9488920},
doi = {10.1109/INFOCOM42981.2021.9488920},
abstract = {The wide deployment of machine learning (ML) models and service APIs exposes the sensitive training data to untrusted and unknown parties, such as end-users and corporations. It is important to preserve data privacy in the released ML models. An essential issue with today’s privacy-preserving ML platforms is a lack of concern on the tradeoff between data privacy and model utility: a private datablock can only be accessed a finite number of times as each access is privacy-leaking. However, it has never been interrogated whether such privacy leaked in the training brings good utility. We propose a differentially-private access control mechanism on the ML platform to assign datablocks to queries. Each datablock arrives at the platform with a privacy budget, which would be consumed at each query access. We aim to make the most use of the data under the privacy budget constraints. In practice, both datablocks and queries arrive continuously so that each access decision has to be made without knowledge about the future. Hence we propose online algorithms with a worst-case performance guarantee. Experiments on a variety of settings show our privacy budgeting scheme yields high utility on ML platforms.},
booktitle = {IEEE INFOCOM 2021 - IEEE Conference on Computer Communications},
pages = {1–10},
numpages = {10},
location = {Vancouver, BC, Canada}
}

@article{10.1007/s00521-020-05035-x,
author = {Hamdia, Khader M. and Zhuang, Xiaoying and Rabczuk, Timon},
title = {An efficient optimization approach for designing machine learning models based on genetic algorithm},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05035-x},
doi = {10.1007/s00521-020-05035-x},
abstract = {Machine learning (ML) methods have shown powerful performance in different application. Nonetheless, designing ML models remains a challenge and requires further research as most procedures adopt a trial and error strategy. In this study, we present a methodology to optimize the architecture and the feature configurations of ML models considering a supervised learning process. The proposed approach employs genetic algorithm (GA)-based integer-valued optimization for two ML models, namely deep neural networks (DNN) and adaptive neuro-fuzzy inference system (ANFIS). The selected variables in the DNN optimization problems are the number of hidden layers, their number of neurons and their activation function, while the type and the number of membership functions are the design variables in the ANFIS optimization problem. The mean squared error (MSE) between the predictions and the target outputs is minimized as the optimization fitness function. The proposed scheme is validated through a case study of computational material design. We apply the method to predict the fracture energy of polymer/nanoparticles composites (PNCs) with a database gathered from the literature. The optimized DNN model shows superior prediction accuracy compared to the classical one-hidden layer network. Also, it outperforms ANFIS with significantly lower number of generations in GA. The proposed method can be easily extended to optimize similar architecture properties of ML models in various complex systems.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1923–1933},
numpages = {11},
keywords = {Machine learning, Deep neural networks, Optimization, Genetic algorithm, Polymer nanocomposites, Fracture energy.}
}

@inproceedings{10.1007/978-3-030-82136-4_25,
author = {Kerestely, Arpad and Baicoianu, Alexandra and Bocu, Razvan},
title = {A Research Study on Running Machine Learning Algorithms on Big Data with Spark},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_25},
doi = {10.1007/978-3-030-82136-4_25},
abstract = {The design and implementation of proactive fault diagnosis systems concerning the bearings during their manufacturing process requires the selection of robust representation learning techniques, which belong to the broader scope of the machine learning techniques. Particular systems, such as those that are based on machine learning libraries like Scikit-learn, favor the actual processing of the data, while essentially disregarding relevant computational parameters, such as the speed of the data processing, or the consideration of scalability as an important design and implementation feature. This paper describes an integrated machine learning-based data analytics system, which processes the large amounts of data that are generated by the bearings manufacturing processes using a multinode cluster infrastructure. The data analytics system uses an optimally configured and deployed Spark environment. The proposed data analytics system is thoroughly assessed using a large dataset that stores real manufacturing data, which is generated by the respective bearings manufacturing processes. The performance assessment demonstrates that the described approach ensures the timely and scalable processing of the data. This achievement is relevant, as it exceeds the processing capabilities of significant existing data analytics systems.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {307–318},
numpages = {12},
keywords = {Big data, High performance computing, Spark, Fault detection, Representation techniques, Machine learning},
location = {Tokyo, Japan}
}

@inproceedings{10.1109/ISIT45174.2021.9517751,
author = {Wang, Ye and Aeron, Shuchin and Rakin, Adnan Siraj and Koike-Akino, Toshiaki and Moulin, Pierre},
title = {Robust Machine Learning via Privacy/ Rate-Distortion Theory},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISIT45174.2021.9517751},
doi = {10.1109/ISIT45174.2021.9517751},
abstract = {Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Our work draws the connection between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.},
booktitle = {2021 IEEE International Symposium on Information Theory (ISIT)},
pages = {1320–1325},
numpages = {6},
location = {Melbourne, Australia}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1007/s10845-020-01706-7,
author = {Cui, Lu-jun and Sun, Man-ying and Cao, Yan-long and Zhao, Qi-jian and Zeng, Wen-han and Guo, Shi-rui},
title = {A novel tolerance geometric method based on machine learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01706-7},
doi = {10.1007/s10845-020-01706-7},
abstract = {In most cases, designers must manually specify geometric tolerance types and values when designing mechanical products. For the same nominal geometry, different designers may specify different types and values of geometric tolerances. To reduce the uncertainty and realize the tolerance specification automatically, a tolerance specification method based on machine learning is proposed. The innovation of this paper is to find out the information that affects geometric tolerances selection and use machine learning methods to generate tolerance specifications. The realization of tolerance specifications is changed from rule-driven to data-driven. In this paper, feature engineering is performed on the data for the application scenarios of tolerance specifications, which improves the performance of the machine learning model. This approach firstly considers the past tolerance specification schemes as cases and sets up the cases to the tolerance specification database which contains information such as datum reference frame, positional relationship, spatial relationship, and product cost. Then perform feature engineering on the data and established machine learning algorithm to convert the tolerance specification problem into an optimization problem. Finally, a gear reducer as a case study is given to verify the method. The results are evaluated with three different machine learning evaluation indicators and made a comparison with the tolerance specification method in the industry. The final results show that the machine learning algorithm can automatically generate tolerance specifications, and after feature engineering, the accuracy of the tolerance specification results is improved.},
journal = {J. Intell. Manuf.},
month = mar,
pages = {799–821},
numpages = {23},
keywords = {Computer-aided tolerancing (CAT), Tolerance specification, Machine learning, Feature engineering, Optimization problem}
}

@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3298981},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {19},
keywords = {Federated learning, GDPR, transfer learning}
}

@article{10.5555/3122009.3122047,
author = {Popovici, Elena},
title = {Bridging supervised learning and test-based co-optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1255–1293},
numpages = {39},
keywords = {active learning, co-optimization, free lunch, optimal algorithms, supervised learning}
}

@article{10.1016/j.future.2019.04.017,
author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J.P.C. and Hassan, Suhaidi and Korotaev, Valery V.},
title = {Machine learning in the Internet of Things: Designed techniques for smart cities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.017},
doi = {10.1016/j.future.2019.04.017},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {826–843},
numpages = {18},
keywords = {Internet of Things, Machine learning, Medical, Smart grid, VANET}
}

@inproceedings{10.1145/3338840.3355691,
author = {Kim, Heejin and Kim, Younggwan and Hong, Jiman},
title = {Cluster management framework for autonomic machine learning platform},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355691},
doi = {10.1145/3338840.3355691},
abstract = {Autonomic machine learning platforms must provide the necessary management tasks while monitoring the execution status of remotely running machine learning tasks and the performance of the model being trained. In this paper, we design a cluster management framework. The proposed cluster management framework monitors distributed computing resources so that it helps the autonomic machine learning platform to select the proper machine learning algorithm and to execute the proper machine learning model.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {128–130},
numpages = {3},
keywords = {autonomic machine learning platform, cluster management framework, machine learning},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1145/3318464.3386146,
author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
title = {The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386146},
doi = {10.1145/3318464.3386146},
abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of "pipeline jungles" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {785–800},
numpages = {16},
keywords = {AutoML, ML pipelines, ML primitives, machine learning, software development},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.5555/3171837.3172033,
author = {Steeg, Greg Ver},
title = {Unsupervised learning via total correlation explanation},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Learning by children and animals occurs effortlessly and largely without obvious supervision. Successes in automating supervised learning have not translated to the more ambiguous realm of unsupervised learning where goals and labels are not provided. Barlow (1961) suggested that the signal that brains leverage for unsupervised learning is dependence, or redundancy, in the sensory environment. Dependence can be characterized using the information-theoretic multivariate mutual information measure called total correlation. The principle of Total Correlation Ex-planation (CorEx) is to learn representations of data that "explain" as much dependence in the data as possible. We review some manifestations of this principle along with successes in unsupervised learning problems across diverse domains including human behavior, biology, and language.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {5151–5155},
numpages = {5},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3351095.3372836,
author = {Hancox-Li, Leif},
title = {Robustness in machine learning explanations: does it matter?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372836},
doi = {10.1145/3351095.3372836},
abstract = {The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {640–647},
numpages = {8},
keywords = {artificial intelligence, epistemology, ethics, explanation, machine learning, methodology, objectivity, philosophy, robustness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3474085.3475435,
author = {Hu, Bingyu and Zha, Zheng-Jun and Liu, Jiawei and Zhu, Xierong and Xie, Hongtao},
title = {Cluster and Scatter: A Multi-grained Active Semi-supervised Learning Framework for Scalable Person Re-identification},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475435},
doi = {10.1145/3474085.3475435},
abstract = {Active learning has recently attracted increasing attention in the task of person re-identification, due to its unique scalability that not only maximally reduces the annotation cost but also retains the satisfying performance. Although some preliminary active learning methods have been explored in scalable person re-identification task, they have the following two problems: 1) the inefficiency in the selection process of image pairs due to the huge search space, and 2) the ineffectiveness caused by ignoring the impact of unlabeled data in model training. Considering that, we propose a Multi-grained Active Semi-Supervised learning framework, named MASS, to address the scalable person re-identification problem existing in the practical scenarios. Specifically, we firstly design a cluster-scatter procedure to alleviate the inefficiency problem, which consists of two components: cluster step and scatter step. The cluster step shrinks the search space into individual small clusters by a coarse-grained clustering method, and the subsequent scatter step further mines the hard distinguished image pairs from unlabelled set to purify the learned clusters by a novel centrality-based adaptive purification strategy. Afterward, we introduce a customized purification loss for the purified clustering, which utilizes the complementary information in both labeled and unlabeled data to optimize the model for solving the ineffectiveness problem. The cluster-scatter procedure and the model optimization are performed in an iterative fashion to achieve the promising performance while greatly reducing the annotation cost. Extensive experimental results have demonstrated that MASS can even achieve a competitive performance with fully supervised methods in the case of extremely less annotation requirements.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2605–2614},
numpages = {10},
keywords = {active learning, person re-identification, semi-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1145/3470974,
author = {Park, Jurn-Gyu and Dutt, Nikil and Lim, Sung-Soo},
title = {An Interpretable Machine Learning Model Enhanced Integrated CPU-GPU DVFS Governor},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3470974},
doi = {10.1145/3470974},
abstract = {Modern heterogeneous CPU-GPU-based mobile architectures, which execute intensive mobile gaming/graphics applications, use software governors to achieve high performance with energy-efficiency. However, existing governors typically utilize simple statistical or heuristic models, assuming linear relationships using a small unbalanced dataset of mobile games; and the limitations result in high prediction errors for dynamic and diverse gaming workloads on heterogeneous platforms. To overcome these limitations, we propose an interpretable machine learning (ML) model enhanced integrated CPU-GPU governor: (1) It builds tree-based piecewise linear models (i.e., model trees) offline considering both high accuracy (low error) and interpretable ML models based on mathematical formulas using a simulatability operation counts quantitative metric. And then (2) it deploys the selected models for online estimation into an integrated CPU-GPU Dynamic Voltage Frequency Scaling governor. Our experiments on a test set of 20 mobile games exhibiting diverse characteristics show that our governor achieved significant energy efficiency gains of over 10% (up to 38%) improvements on average in energy-per-frame with a surprising-but-modest 3% improvement in Frames-per-Second performance, compared to a typical state-of-the-art governor that employs simple linear regression models.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {108},
numpages = {28},
keywords = {Machine learning techniques, power management policies, dynamic voltage and frequency scaling (DVFS), integrated GPU, model-based design, interpretable machine learning models}
}

@inproceedings{10.1145/3340531.3411860,
author = {Ding, Jiahao and Wang, Jingyi and Liang, Guannan and Bi, Jinbo and Pan, Miao},
title = {Towards Plausible Differentially Private ADMM Based Distributed Machine Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411860},
doi = {10.1145/3340531.3411860},
abstract = {The Alternating Direction Method of Multipliers (ADMM) and its distributed version have been widely used in machine learning. In the iterations of ADMM, model updates using local private data and model exchanges among agents impose critical privacy concerns. Despite some pioneering works to relieve such concerns, differentially private ADMM still confronts many research challenges. For example, the guarantee of differential privacy (DP) relies on the premise that the optimality of each local problem can be perfectly attained in each ADMM iteration, which may never happen in practice. The model trained by DP ADMM may have low prediction accuracy. In this paper, we address these concerns by proposing a novel (Improved) Plausible differentially Private ADMM algorithm, called PP-ADMM and IPP-ADMM. In PP-ADMM, each agent approximately solves a perturbed optimization problem that is formulated from its local private data in an iteration, and then perturbs the approximate solution with Gaussian noise to provide the DP guarantee. To further improve the model accuracy and convergence, an improved version IPP-ADMM adopts sparse vector technique (SVT) to determine if an agent should update its neighbors with the current perturbed solution. The agent calculates the difference of the current solution from that in the last iteration, and if the difference is larger than a threshold, it passes the solution to neighbors; or otherwise the solution will be discarded. Moreover, we propose to track the total privacy loss under the zero-concentrated DP (zCDP) and provide a generalization performance analysis. Experiments on real-world datasets demonstrate that under the same privacy guarantee, the proposed algorithms are superior to the state of the art in terms of model accuracy and convergence rate.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {285–294},
numpages = {10},
keywords = {ADMM, decentralized optimization, differential privacy, distributed machine learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1007/978-3-030-60334-2_33,
author = {Cheng, Jieyu and Dalca, Adrian V. and Z\"{o}llei, Lilla},
title = {Unbiased Atlas Construction for Neonatal Cortical Surfaces via Unsupervised Learning},
year = {2020},
isbn = {978-3-030-60333-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60334-2_33},
doi = {10.1007/978-3-030-60334-2_33},
abstract = {Due to the dynamic cortical development of neonates after birth, existing cortical surface atlases for adults are not suitable for representing neonatal brains. It has been proposed that pediatric spatio-temporal atlases are more appropriate to characterize the neural development. We present a novel network comprised of an atlas inference module and a non-linear surface registration module, SphereMorph, to construct a continuous neonatal cortical surface atlas with respect to post-menstrual age. We explicitly aim to diminish bias in the constructed atlas by regularizing the mean displacement field. We trained the network on 445 neonatal cortical surfaces from the developing Human Connectome Project (dHCP). We assessed the quality of the constructed atlas by evaluating the accuracy of the spatial normalization of another 100 dHCP surfaces as well as the parcellation accuracy of 10 subjects from an independent dataset that included manual parcellations. We also compared the network’s performance to that of existing spatio-temporal cortical surface atlases, i.e. the 4D University of North Carolina (UNC) neonatal atlases. The proposed network provides continuous spatial-temporal atlases rather than other 4D atlases at discrete time points and we demonstrate that our representation preserves better alignment in cortical folding patterns across subjects than the 4D UNC neonatal atlases.},
booktitle = {Medical Ultrasound, and Preterm, Perinatal and Paediatric Image Analysis: First International Workshop, ASMUS 2020, and 5th International Workshop, PIPPI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4-8, 2020, Proceedings},
pages = {334–342},
numpages = {9},
location = {Lima, Peru}
}

@article{10.1016/j.cose.2020.102092,
author = {Nowroozi, Ehsan and Dehghantanha, Ali and Parizi, Reza M. and Choo, Kim-Kwang Raymond},
title = {A survey of machine learning techniques in adversarial image forensics},
year = {2021},
issue_date = {Jan 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {100},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102092},
doi = {10.1016/j.cose.2020.102092},
journal = {Comput. Secur.},
month = jan,
numpages = {25},
keywords = {Image forensics, Adversarial machine learning, Adversarial learning, Adversarial setting, Image manipulation detection, Cyber security}
}

@article{10.1016/j.cviu.2019.102879,
author = {Ahmad, Touqeer and Bebis, George and Nicolescu, Monica and Nefian, Ara and Fong, Terry},
title = {Horizon line detection using supervised learning and edge cues},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2019.102879},
doi = {10.1016/j.cviu.2019.102879},
journal = {Comput. Vis. Image Underst.},
month = feb,
numpages = {16},
keywords = {41A05, 41A10, 65D05, 65D17}
}

@inproceedings{10.1007/978-3-030-91100-3_16,
author = {Kamaleson, Nishanthan and Chu, Dominique and Otero, Fernando E. B.},
title = {Automatic Information Extraction from Electronic Documents Using Machine Learning},
year = {2021},
isbn = {978-3-030-91099-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91100-3_16},
doi = {10.1007/978-3-030-91100-3_16},
abstract = {The digital processing of electronic documents is widely exploited across many domains to improve the efficiency of information extraction. However, paper documents are still largely being used in practice. In order to process such documents, a manual procedure is used to inspect them and extract the values of interest. As this task is monotonous and time consuming, it is prone to introduce human errors during the process. In this paper, we present an efficient and robust system that automates the aforementioned task by using a combination of machine learning techniques: optical character recognition, object detection and image processing techniques. This not only speeds up the process but also improves the accuracy of extracted information compared to a manual procedure.},
booktitle = {Artificial Intelligence XXXVIII: 41st SGAI International Conference on Artificial Intelligence, AI 2021, Cambridge, UK, December 14–16, 2021, Proceedings},
pages = {183–194},
numpages = {12},
keywords = {OCR, Layout analysis, Image detection, Information extraction},
location = {Cambridge, United Kingdom}
}

@inproceedings{10.1007/978-3-030-27520-4_17,
author = {Prasad, Bakshi Rohit and Agarwal, Sonali},
title = {Scalable Least Square Twin Support Vector Machine Learning},
year = {2019},
isbn = {978-3-030-27519-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27520-4_17},
doi = {10.1007/978-3-030-27520-4_17},
abstract = {Machine Learning (ML) on massive scale datasets, called Big Data, has become a challenge for traditional computing and storage technologies. Henceforth, massive scale ML is an emerging domain of research. Least Square Twin Support Vector Machine (LSTSVM) is a faster variant of Support Vector Machine (SVM). However, it suffers from scalability issues and shows computational and/or storage bottlenecks on massive datasets. Proposed work designs a scalable solution to LSTSVM called Distributed LSTSVM (DLSTSVM). DLSTSVM is designed using distributed parallel computing on top of cluster of multiple machines. After applying horizontal partitioning on massive datasets, DLSTSVM trains it in distributed parallel fashion and finds two non-parallel hyper-planes as decision boundaries for two different classes. MapReduce paradigm is utilized to execute parallel computation on partitioned data in a way that averts memory constraints. Proposed technique achieves computational and storage scalability without losing prediction accuracy.},
booktitle = {Big Data Analytics and Knowledge Discovery: 21st International Conference, DaWaK 2019, Linz, Austria, August 26–29, 2019, Proceedings},
pages = {239–249},
numpages = {11},
keywords = {Big Data, MapReduce, Cluster computing, Distributed machine learning, Supervised learning, LSTSVM, Parallel processing},
location = {Linz, Austria}
}

@article{10.1007/s11704-020-9441-1,
author = {Sun, Xiaobing and Zhou, Tianchi and Wang, Rongcun and Duan, Yucong and Bo, Lili and Chang, Jianming},
title = {Experience report: investigating bug fixes in machine learning frameworks/libraries},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9441-1},
doi = {10.1007/s11704-020-9441-1},
abstract = {Machine learning (ML) techniques and algorithms have been successfully and widely used in various areas including software engineering tasks. Like other software projects, bugs are also common in ML projects and libraries. In order to more deeply understand the features related to bug fixing in ML projects, we conduct an empirical study with 939 bugs from five ML projects by manually examining the bug categories, fixing patterns, fixing scale, fixing duration, and types of maintenance. The results show that (1) there are commonly seven types of bugs in ML programs; (2) twelve fixing patterns are typically used to fix the bugs in ML programs; (3) 68.80% of the patches belong to micro-scale-fix and small-scale-fix; (4) 66.77% of the bugs in ML programs can be fixed within one month; (5) 45.90% of the bug fixes belong to corrective activity from the perspective of software maintenance. Moreover, we perform a questionnaire survey and send them to developers or users of ML projects to validate the results in our empirical study. The results of our empirical study are basically consistent with the feedback from developers. The findings from the empirical study provide useful guidance and insights for developers and users to effectively detect and fix bugs in ML projects.},
journal = {Front. Comput. Sci.},
month = dec,
numpages = {16},
keywords = {bug fixing, machine learning project, empirical study, questionnaire survey}
}

@article{10.1007/s11265-019-01505-1,
author = {Asgari, Bahar and Mukhopadhyay, Saibal and Yalamanchili, Sudhakar},
title = {MAHASIM: Machine-Learning Hardware Acceleration Using a Software-Defined Intelligent Memory System},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {6},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-019-01505-1},
doi = {10.1007/s11265-019-01505-1},
abstract = {As computations in machine-learning applications are increasing simultaneously along the size of datasets, the energy and performance costs of data movement dominate that of compute. This issue is more pronounced in embedded systems with limited resources and energy. Although near-data-processing (NDP) is pursued as an architectural solution, comparatively less attention has been focused on how to scale NDP for larger-scale embedded machine learning applications (e.g., speech and motion processing). We propose machine-learning hardware acceleration using a software-defined intelligent memory system (Mahasim). Mahasim is a scalable NDP-based memory system, in which application performance scales with the size of data. The building blocks of Mahasim are the programable memory slices, supported by data partitioning, compute-aware memory allocation, and an independent in-memory execution model. For recurrent neural networks, Mahasim shows up to 537.95 GFLOPS/W energy efficiency and 3.9x speedup, when the size of the system increases from 2 to 256 memory slices, which indicates that Mahasim favors larger problems.},
journal = {J. Signal Process. Syst.},
month = jun,
pages = {659–675},
numpages = {17},
keywords = {Machine learning, Neural networks, Near-data-processing, Memory system}
}

@inproceedings{10.1145/3468218.3469042,
author = {Jiao, Long and Sun, Guohua and Le, Junqing and Zeng, Kai},
title = {Machine Learning-Assisted Wireless PHY Key Generation with Reconfigurable Intelligent Surfaces},
year = {2021},
isbn = {9781450385619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468218.3469042},
doi = {10.1145/3468218.3469042},
abstract = {The key generation rate (KGR) performance of wireless physical layer (PHY) key generation can be limited by the quasi-static slow fading environment. In this work, we aim to exploit the radio environment reconfiguration ability enabled by reconfigurable intelligent surface (RIS) to improve KGR of PHY key generation. By rapidly changing the RIS configurations, the randomness or entropy rate of the wireless channel can be significantly increased, thus improving the KGR. To achieve high KGR while keeping low bit disagreement ratio (BDR), for the first time, we propose a machine learning (ML) based adaptive quantization level prediction scheme to decide an optimal quantization level based on channel state information (CSI). Simulation results show that with a prediction accuracy as high as 98.2%, the proposed ML-based prediction model tends to assign high quantization levels in the high SNR regime to reduce BDR, while adopting low quantization levels under low SNRs to maintain a low BDR.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning},
pages = {61–66},
numpages = {6},
keywords = {Physical Layer Key Generation, Physical Layer Security, Reconfigurable Intelligent Surface, Smart Environment Reconfiguration},
location = {Abu Dhabi, United Arab Emirates},
series = {WiseML '21}
}

@inproceedings{10.1145/3394885.3431629,
author = {Jiang, Weiwen and Xiong, Jinjun and Shi, Yiyu},
title = {When Machine Learning Meets Quantum Computers: A Case Study},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431629},
doi = {10.1145/3394885.3431629},
abstract = {Along with the development of AI democratization, the machine learning approach, in particular neural networks, has been applied to wide-range applications. In different application scenarios, the neural network will be accelerated on the tailored computing platform. The acceleration of neural networks on classical computing platforms, such as CPU, GPU, FPGA, ASIC, has been widely studied; however, when the scale of the application consistently grows up, the memory bottleneck becomes obvious, widely known as memory-wall. In response to such a challenge, advanced quantum computing, which can represent 2N states with N quantum bits (qubits), is regarded as a promising solution. It is imminent to know how to design the quantum circuit for accelerating neural networks. Most recently, there are initial works studying how to map neural networks to actual quantum processors. To better understand the state-of-the-art design and inspire new design methodology, this paper carries out a case study to demonstrate an end-to-end implementation. On the neural network side, we employ the multilayer perceptron to complete image classification tasks using the standard and widely used MNIST dataset. On the quantum computing side, we target IBM Quantum processors, which can be programmed and simulated by using IBM Qiskit. This work targets the acceleration of the inference phase of a trained neural network on the quantum processor. Along with the case study, we will demonstrate the typical procedure for mapping neural networks to quantum circuits.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {593–598},
numpages = {6},
keywords = {IBM Qiskit, IBM Quantum, MNIST dataset, neural networks, quantum computing},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.3233/JIFS-189490,
author = {Qing, Yang and Zejun, Wang and Ramachandran, Varatharajan},
title = {Research on the impact of entrepreneurship policy on employment based on improved machine learning algorithms},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189490},
doi = {10.3233/JIFS-189490},
abstract = {After my country’s economy has entered a new normal, in terms of employment, which has led to the coexistence of the old and new contradictions in employment in our country and the coexistence of employment expansion and stabilization of employment. In this context, it is impossible to achieve full employment and completely eliminate unemployment by relying solely on economic growth. This paper improves traditional machine learning algorithms and builds an entrepreneurial policy analysis model based on improved machine learning to analyze the impact of entrepreneurial policies on employment. Moreover, this paper uses a projection pursuit comprehensive evaluation model optimized by genetic algorithm to conduct empirical research on entrepreneurial environment conditions. In addition, this paper verifies its rationality by regression analysis of empirical results and TEA (Entrepreneurial Activity of All Employees) index, and deeply explores the inherent laws and development characteristics of entrepreneurial environmental conditions from multiple perspectives such as time series and spatial distribution. The research results show that the method proposed in this paper is effective.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6517–6528},
numpages = {12},
keywords = {Machine learning, improved algorithm, entrepreneurial policy, employment impact}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {application security, classifiers ensemble, machine learning, open-source software, self-training},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1007/s00500-015-1892-1,
author = {Chen, Zhenxiang and Liu, Zhusong and Peng, Lizhi and Wang, Lin and Zhang, Lei},
title = {A novel semi-supervised learning method for Internet application identification},
year = {2017},
issue_date = {April     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {8},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1892-1},
doi = {10.1007/s00500-015-1892-1},
abstract = {Several methods based on port, payload, and transport layer features have been proposed to detect, identify, and manage Internet traffic. The diminished effectiveness of port-based identification and overheads of deep packet inspection methods motivated us to identify Internet traffic by combining distinctive flow characteristics with the machine learning method. However, the abundant ground truth Internet traffic, which is important for building a supervised classifier, is difficult to be obtained in real conditions. In this study, we propose a semi-supervised learning method that combines further division of recognition space technique with data gravitation theory. The further division of recognition space classifier is a powerful multi-classification tool that can be helpful for multi-application identification. The data gravitation may reveal the underlying data space structure from unlabeled data, and thus, it is integrated into the classification to develop a better classifier. The experimental results on the real Internet application traffic datasets demonstrate the advantages of our proposed work.},
journal = {Soft Comput.},
month = apr,
pages = {1963–1975},
numpages = {13},
keywords = {Data gravitation, Internet traffic classification, Recognition space, Semi-supervised learning}
}

@article{10.1145/3457607,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {115},
numpages = {35},
keywords = {Fairness and bias in artificial intelligence, deep learning, machine learning, natural language processing, representation learning}
}

@article{10.1145/3459666,
author = {Pl\"{O}tz, Thomas},
title = {Applying Machine Learning for Sensor Data Analysis in Interactive Systems: Common Pitfalls of Pragmatic Use and Ways to Avoid Them},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459666},
doi = {10.1145/3459666},
abstract = {With the widespread proliferation of (miniaturized) sensing facilities and the massive growth and popularity of the field of machine learning (ML) research, new frontiers in automated sensor data analysis have been explored that lead to paradigm shifts in many application domains. In fact, many practitioners now employ and rely more and more on ML methods as integral part of their sensor data analysis workflows—thereby not necessarily being ML experts or having an interest in becoming one. The availability of toolkits that can readily be used by practitioners has led to immense popularity and widespread adoption and, in essence, pragmatic use of ML methods. ML having become mainstream helps pushing the core agenda of practitioners, yet it comes with the danger of misusing methods and as such running the risk of leading to misguiding if not flawed results.Based on years of observations in the ubiquitous and interactive computing domain that extensively relies on sensors and automated sensor data analysis, and on having taught and worked with numerous students in the field, in this article I advocate a considerate use of ML methods by practitioners, i.e., non-ML experts, and elaborate on pitfalls of an overly pragmatic use of ML techniques. The article not only identifies and illustrates the most common issues, it also offers ways and practical guidelines to avoid these, which shall help practitioners to benefit from employing ML in their core research domains and applications.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {134},
numpages = {25},
keywords = {Sensor data analysis, machine learning applications}
}

@inproceedings{10.1145/3442442.3452301,
author = {Sun, Haipei and Yang, Yiding and Li, Yanying and Liu, Huihui and Wang, Xinchao and Wang, Wendy Hui},
title = {Automating Fairness Configurations for Machine Learning},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452301},
doi = {10.1145/3442442.3452301},
abstract = {Recent years have witnessed substantial efforts devoted to ensuring algorithmic fairness for machine learning (ML), spanning from formalizing fairness metrics to designing fairness-enhancing methods. These efforts lead to numerous possible choices in terms of fairness definitions and fairness-enhancing algorithms. However, finding the best fairness configuration (including both fairness definition and fairness-enhancing algorithms) for a specific ML task is extremely challenging in practice. The large design space of fairness configurations combined with the tremendous cost required for fairness deployment poses a major obstacle to this endeavor. This raises an important issue: can we enable automated fairness configurations for a new ML task on a potentially unseen dataset? To this point, we design Auto-Fair, a system that provides recommendations of fairness configurations by ranking all fairness configuration candidates based on their evaluations on prior ML tasks. At the core of Auto-Fair lies a meta-learning model that ranks all fairness configuration candidates by utilizing: (1) a set of meta-features that are derived from both datasets and fairness configurations that were used in prior evaluations; and (2) the knowledge accumulated from previous evaluations of fairness configurations on related ML tasks and datasets. The experimental results on 350 different fairness configurations and 1,500 data samples demonstrate the effectiveness of Auto-Fair.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {193–201},
numpages = {9},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3477543.3477551,
author = {Yao, Honglei and Zhu, Guangjie and Yang, Yijie},
title = {Primary User Emulation Detection in Wireless Networks with Machine Learning Approach},
year = {2021},
isbn = {9781450390101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477543.3477551},
doi = {10.1145/3477543.3477551},
abstract = {A novel method based on machine learning approach to detect primary user emulation in cognitive radio networks is proposed. The states of wireless channels are collected. And using the locally weighted linear regression algorithm (LWLR), the number of the available channels in next cycle is predicted in advance. In the paper, the error distribution is estimated and calculated. With a given error, the primary user emulation can be detected in the system. Simulation results demonstrate the prediction results performance with the different thresholds of the prediction error.},
booktitle = {2021 8th International Conference on Automation and Logistics (ICAL)},
pages = {49–53},
numpages = {5},
keywords = {LWLR, available channels, machine learning, prediction, primary user emulation detection},
location = {Chongqing, China},
series = {ICAL 2021}
}

@inproceedings{10.1145/3368089.3409737,
author = {Gaaloul, Khouloud and Menghi, Claudio and Nejati, Shiva and Briand, Lionel C. and Wolfe, David},
title = {Mining assumptions for software components using machine learning},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409737},
doi = {10.1145/3368089.3409737},
abstract = {Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {159–171},
numpages = {13},
keywords = {Decision trees, Environment assumptions, Machine learning, Model checking, Search-based software testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3292500.3330744,
author = {Bernardi, Lucas and Mavridis, Themistoklis and Estevez, Pablo},
title = {150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330744},
doi = {10.1145/3292500.3330744},
abstract = {Booking.com is the world's largest online travel agent where millions of guests find their accommodation and millions of accommodation providers list their properties including hotels, apartments, bed and breakfasts, guest houses, and more. During the last years we have applied Machine Learning to improve the experience of our customers and our business. While most of the Machine Learning literature focuses on the algorithmic or mathematical aspects of the field, not much has been published about how Machine Learning can deliver meaningful impact in an industrial environment where commercial gains are paramount. We conducted an analysis on about 150 successful customer facing applications of Machine Learning, developed by dozens of teams in Booking.com, exposed to hundreds of millions of users worldwide and validated through rigorous Randomized Controlled Trials. Following the phases of a Machine Learning project we describe our approach, the many challenges we found, and the lessons we learned while scaling up such a complex technology across our organization. Our main conclusion is that an iterative, hypothesis driven process, integrated with other disciplines was fundamental to build 150 successful products enabled by Machine Learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1743–1751},
numpages = {9},
keywords = {business impact, data science, e-commerce, experimentation, machine learning, product development},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1007/s10664-020-09881-0,
author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
title = {Testing machine learning based systems: a systematic mapping},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09881-0},
doi = {10.1007/s10664-020-09881-0},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5193–5254},
numpages = {62},
keywords = {Systematic mapping, Systematic review, Software testing, Machine learning}
}

@article{10.3233/JIFS-189575,
author = {Xu, Xiaoying and Zeng, Zhijian and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Analysis of regional economic evaluation based on machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189575},
doi = {10.3233/JIFS-189575},
abstract = {The regional economic evaluation and analysis has guiding significance for the subsequent economic strategy formulation. Due to the influence of various factors, the volatility of some current economic evaluation models is relatively large. According to the needs of regional economic evaluation, this study uses computer technology combined with regional economic development to build an economic development evaluation model to evaluate and analyze the regional economy. Through comparative analysis, this study selects the entropy weight-TOPSIS model as the comprehensive evaluation model of regional economy, uses the entropy weight method to determine the weight of each index, and then uses the TOPSIS method to conduct comprehensive evaluation. In addition, this study designs a control experiment to analyze the performance of this study model. Moreover, this study uses the model proposed in this study to conduct regional economic evaluation in recent years, and compares it with real data, and observes the test results with statistical charts and table data. The research results show that this research model has a certain effect, which can provide analytical tools for the follow-up economic strategy research and analysis.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7543–7553},
numpages = {11},
keywords = {Machine learning, regional economy, simulation model, economic evaluation}
}

@article{10.1016/j.asoc.2020.106071,
author = {Liu, Minjie and Zhou, Mingming and Zhang, Tao and Xiong, Naixue},
title = {Semi-supervised learning quantization algorithm with deep features for motor imagery EEG Recognition in smart healthcare application},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106071},
doi = {10.1016/j.asoc.2020.106071},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {13},
keywords = {Convolutional neural networks, Semi-supervised classification, EEG Recognition, Smart healthcare, Cartesian K-means}
}

@inproceedings{10.1007/978-3-030-16145-3_27,
author = {He, Congqing and Peng, Li and Le, Yuquan and He, Jiawei},
title = {Dynamically Weighted Multi-View Semi-Supervised Learning for CAPTCHA},
year = {2019},
isbn = {978-3-030-16144-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-16145-3_27},
doi = {10.1007/978-3-030-16145-3_27},
abstract = {With the development of Optical Character Recognition and artificial intelligence technologies, the security of Behavioral Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA) has become an increasingly difficult task. In order to prevent malicious attacks and maintain network security, most existing works on CAPTCHA are to construct a fine binary classifier model but are not yet capable of detecting new attack means during confrontation. This motivates us to propose a Dynamically Weighted Multi-View Semi-Supervised Learning, dubbed as DWMVSSL method, to relieve this problem. More specifically, our proposed method extracts hidden patterns from multiple perspectives and updates the view weighting dynamically which can constantly detect new attack means. In addition, due to existing some redundant feature in views, we design a Filter Artificial Bee Colony method, named as FABC for feature selection which can efficiently reduce the impact of high dimensional features. The experimental results show that, compared the existing representative baseline methods, our DWMVSSL method can effectively detecting new attacks on confrontation.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part II},
pages = {343–354},
numpages = {12},
keywords = {CAPTCHA, Semi-supervised learning, Multi-view, Feature selection},
location = {Macau, China}
}

@article{10.1155/2021/2788161,
author = {Lv, Yan and Lu, Laijun and Nagaraj, Balakrishnan},
title = {Geological Mineral Energy and Classification Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/2788161},
doi = {10.1155/2021/2788161},
abstract = {In order to mine geological mineral energy and study on geological mineral energy classification, a method based on a wireless sensor was proposed. Of logistic regression, artificial neural networks, random forests, and main wireless sensor algorithms of support vector machine (SVM) with the model in the application of the energy mineral resource prediction practice effects are reviewed and discuss the practical application in the process of sample selection, the wrong points existing in the cost, the uncertainty evaluation, and performance evaluation of the model using wireless sensor algorithm, random forest of the probability distribution of mineralization in the study area is calculated, and five prospecting potential areas are delineated. The results show that the ratio of ore-bearing unit and non-ore-bearing unit is 1 : 1, and the best random forest training model is obtained. 70% of the training sample set was randomly selected as the training set, and the remaining 30% was used as the test set to construct the random forest model. The training accuracy of the model is 96.7%, and the testing accuracy is 96.5%. Both model training accuracy and model testing accuracy are very high, which proves the accuracy of RF model construction and achieves satisfactory results. In this study, a wireless sensor is successfully applied to 3D mineral energy prediction, which makes a positive exploration for mineral resource prediction and evaluation in the future. Finally, the prediction of mineral resource energy based on a wireless sensor is an important trend of future development.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3419604.3419772,
author = {Haddouchi, Maissae and Berrado, Abdelaziz},
title = {An implementation of a multivariate discretization for supervised learning using Forestdisc},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419772},
doi = {10.1145/3419604.3419772},
abstract = {Discretization is a key pre-processing step in Machine Learning that transforms continuous attributes into discrete ones, through different methods available in the literature. In this regard, this work provides the ForestDisc framework that discretizes data based on a supervised, multivariate and hybrid approach. It uses, at first, a splitting process relying on a tree learning ensemble to generate a large set of cut points. It then uses a merging process based on moment matching optimization, to transform this set into a reduced and representative one. ForestDisc is a non-parametric discretizer in the sense that it does not require the user to introduce any initial setting parameters. We implemented ForestDisc algorithm in the "ForestDisc" R package.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {8},
numpages = {6},
keywords = {Data Preprocessing, Moment Matching, Multivariate Discretization, Random Forest, Split Points Selection, Tree Ensembles},
location = {Rabat, Morocco},
series = {SITA'20}
}

@phdthesis{10.5555/AAI28259275,
author = {Zhang, Liming and Yang, Ruixin and Z\"{u}fle, Andreas and Purohit, Hemant},
advisor = {Dieter, Pfoser,},
title = {Explainable Machine Learning for Activity Modeling in GeoAi},
year = {2020},
isbn = {9798557032087},
publisher = {George Mason University},
address = {USA},
abstract = {GeoAI is a recent cutting-edge discipline that combines advancements in Big Geospatial Data Management (Geo) and Artificial Intelligence (AI). This thesis focuses on two GeoAI aspects, (1) AI for Geo, which applies advanced machine-learning-based models to emerging geospatial data; and (2) Geo for AI, which proposes novel machine learning algorithms that have better explainability aligned with geospatial knowledge.AI for Geo applies machine learning to a wealth of emerging spatiotemporal datasets generated by users, such as OpenstreetMap, Twitter, Yelp, or farecard data from transportation systems. This data captures user activities and the dynamics of a changing environment. Our approach to derive knowledge is based on so-called Activity Modeling for spatiotemporal data by proposing specific machine learning methods to tackle the following novel challenges.(i) The data quality and validity of user-generated data are still of some concern and we show the feasibility of using OpenstreetMap Edits for assessing urban change using statistical modeling. (ii) Another challenge is using explainable latent temporal patterns in machine learning models, e.g., the spatial proximity and temporal auto-correlation for user clustering and trajectory synthesis; (iii) The last challenge is how to introduce new data representation (such as continuous-time temporal graphs) and latest deep learning methods (including Factorized Variational Autoencoder and Generative Adversarial Neural Networks) to capture complex high-dimensional information beyond conventional data representation and methods.From the perspective of Geo for AI, machine learning (ML) models help solve many challenging problems such as computer vision, speech processing, and also spatiotemporal data. However, people are expecting good explainability of results for decision making (e.g. healthcare, law enforcement, and self-driving systems) or first-principle scientific domain knowledge (e.g. chemical bonds, physics movement, and biological linkage). As such, this thesis is also motivated by promoting machine learning explainability in Activity Modeling to generate and enforce better explainability for spatiotemporal data and specific applications beyond what is possible with generic models. Specifically, this work addresses(i) the predictive modeling of urban change using a novel autoregression approach based on power-law growth principles and spatiotemporal auto-correlation, (ii) explainable user clustering based on matrix factorization as part of a transfer learning framework leveraging tidal traffic and commuting behaviors, (iii) spatiotemporal trajectory generation using Variantional Autoencoders through a factorized generative model and spatiotemporal-validity constraints on driving behavior and the physical limitations of vehicular movement, and finally (iv) Generative Adversarial Network (GAN) based temporal graph generation using novel deep structures on temporal dynamics and location representation.Overall, this thesis comprises six chapters. Chapter 1 discusses GeoAI and Activity Modeling and highlights how the two perspectives of GeoAI motivate explainability in machine learning. Chapters 2 through 5 cover four specific issues related to Activity Modeling. Chapter 6 summarizes this thesis and identifies future works.},
note = {AAI28259275}
}

@article{10.1016/j.jco.2021.101587,
author = {Dick, Josef and Feischl, Michael},
title = {A quasi-Monte Carlo data compression algorithm for machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0885-064X},
url = {https://doi.org/10.1016/j.jco.2021.101587},
doi = {10.1016/j.jco.2021.101587},
journal = {J. Complex.},
month = dec,
numpages = {25},
keywords = {65C05, 65D30, 65D32, Quasi-Monte Carlo, Big data, Statistical learning, Higher-order methods}
}

@article{10.14778/3352063.3352115,
author = {Sabek, Ibrahim and Mokbel, Mohamed F.},
title = {Machine learning meets big spatial data},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352115},
doi = {10.14778/3352063.3352115},
abstract = {The proliferation in amounts of generated data has propelled the rise of scalable machine learning solutions to efficiently analyze and extract useful insights from such data. Meanwhile, spatial data has become ubiquitous, e.g., GPS data, with increasingly sheer sizes in recent years. The applications of big spatial data span a wide spectrum of interests including tracking infectious disease, climate change simulation, drug addiction, among others. Consequently, major research efforts are exerted to support efficient analysis and intelligence inside these applications by either providing spatial extensions to existing machine learning solutions or building new solutions from scratch. In this 90-minutes tutorial, we comprehensively review the state-of-the-art work in the intersection of machine learning and big spatial data. We cover existing research efforts and challenges in three major areas of machine learning, namely, data analysis, deep learning and statistical inference, as well as two advanced spatial machine learning tasks, namely, spatial features extraction and spatial sampling. We also highlight open problems and challenges for future research in this area.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1982–1985},
numpages = {4},
keywords = {big spatial data, machine learning, scalability}
}

@inproceedings{10.1145/3453688.3461483,
author = {Chen, Zhiyang and Ji, Weiqing and Peng, Yihao and Chen, Datao and Liu, Mingyu and Yao, Hailong},
title = {Machine Learning Based Acceleration Method for Ordered Escape Routing},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461483},
doi = {10.1145/3453688.3461483},
abstract = {Escape routing, especially ordered escape routing, is a critical design stage for both printed circuit boards (PCBs) and integrated fan-out (InFO) wafer-level chip-scale packages. Previous works formulate ordered escape routing as boolean satisfiability (SAT) or integer linear programming (ILP) problems. Although optimal routing solutions can be obtained by above-mentioned approaches, the runtime is unacceptable for large-scale designs due to the exponential time complexity of SAT and ILP solvers. In this paper, we first attempt to address ordered escape routing problems with machine learning. We propose a learning-based method to accelerate existing solvers by reducing the solution space of the original problem. The proposed method is flexible, which can be combined with different ordered escape routing algorithms. Specifically, a fully convolutional neural network is trained to predict the probability of each routing grid to be occupied by routing paths. Thus, routing grids with low-probability usage can be removed to reduce the solution space. Experimental results show that the proposed method is effective for both SAT and ILP solvers of ordered escape routing. It achieves an acceleration of 4∼ 370x on average, with a slight increase in the total wirelength. Also, our model has a strong generalization ability. Although it is trained on $10times 10$ pin array problems, it works well on larger problem sizes such as 14 x 14.},
booktitle = {Proceedings of the 2021 Great Lakes Symposium on VLSI},
pages = {365–370},
numpages = {6},
keywords = {convolutional neural networks, machine learning, ordered escape routing, printed circuit boards},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/3468264.3468536,
author = {Biswas, Sumon and Rajan, Hridesh},
title = {Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468536},
doi = {10.1145/3468264.3468536},
abstract = {In recent years, many incidents have been reported where machine learning models exhibited discrimination among people based on race, sex, age, etc. Research has been conducted to measure and mitigate unfairness in machine learning models. For a machine learning task, it is a common practice to build a pipeline that includes an ordered set of data preprocessing stages followed by a classifier. However, most of the research on fairness has considered a single classifier based prediction task. What are the fairness impacts of the preprocessing stages in machine learning pipeline? Furthermore, studies showed that often the root cause of unfairness is ingrained in the data itself, rather than the model. But no research has been conducted to measure the unfairness caused by a specific transformation made in the data preprocessing stage. In this paper, we introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ML pipeline. We leveraged existing metrics to define the fairness measures of the stages. Then we conducted a detailed fairness evaluation of the preprocessing stages in 37 pipelines collected from three different sources. Our results show that certain data transformers are causing the model to exhibit unfairness. We identified a number of fairness patterns in several categories of data transformers. Finally, we showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline. We used the fairness composition to choose appropriate downstream transformer that mitigates unfairness in the machine learning pipeline.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {981–993},
numpages = {13},
keywords = {fairness, machine learning, models, pipeline, preprocessing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1287/trsc.2021.1084,
author = {Tahir, Adil and Quesnel, Fr\'{e}d\'{e}ric and Desaulniers, Guy and El Hallaoui, Issmail and Yaakoubi, Yassine},
title = {An Improved Integral Column Generation Algorithm Using Machine Learning for Aircrew Pairing},
year = {2021},
issue_date = {November–December 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {6},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2021.1084},
doi = {10.1287/trsc.2021.1084},
abstract = {The crew-pairing problem (CPP) is solved in the first step of the crew-scheduling process. It consists of creating a set of pairings (sequence of flights, connections, and rests forming one or multiple days of work for an anonymous crew member) that covers a given set of flights at minimum cost. Those pairings are assigned to crew members in a subsequent crew-rostering step. In this paper, we propose a new integral column-generation algorithm for the CPP, called improved integral column generation with prediction (I2CGp), which leaps from one integer solution to another until a near-optimal solution is found. Our algorithm improves on previous integral column-generation algorithms by introducing a set of reduced subproblems. Those subproblems only contain flight connections that have a high probability of being selected in a near-optimal solution and are, therefore, solved faster. We predict flight-connection probabilities using a deep neural network trained in a supervised framework. We test I2CGp on several real-life instances and show that it outperforms a state-of-the-art integral column-generation algorithm as well as a branch-and-price heuristic commonly used in commercial airline planning software, in terms of both solution costs and computing times. We highlight the contributions of the neural network to I2CGp.},
journal = {Transportation Science},
month = nov,
pages = {1411–1429},
numpages = {19},
keywords = {crew pairing, machine learning, integral column generation, deep neural network}
}

@inproceedings{10.1145/3183713.3197387,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3197387},
doi = {10.1145/3183713.3197387},
abstract = {There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1645–1650},
numpages = {6},
keywords = {data enrichment, data integration, machine learning},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@article{10.1016/j.patcog.2018.01.036,
author = {Sanakoyeu, Artsiom and Bautista, Miguel A. and Ommer, Bjrn},
title = {Deep unsupervised learning of visual similarities},
year = {2018},
issue_date = {June 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.01.036},
doi = {10.1016/j.patcog.2018.01.036},
abstract = {Unsupervised visual similarity learning is framed as a surrogate classification task.Use weak estimates of local similarities to group samples into compact cliques.Train a ConvNet to learn visual similarities by learning to categorize cliques.Optimization problem to sample training minibatches without conflicting relations.Competitive performance on detailed posture analysis and object classification. Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to computer vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.},
journal = {Pattern Recogn.},
month = jun,
pages = {331–343},
numpages = {13},
keywords = {Deep learning, Human pose analysis, Object retrieval, Self-supervised learning, Visual similarity learning}
}

@inproceedings{10.1145/3416013.3426458,
author = {Ul Mustafa, Raza and Ferlin, Simone and Esteve Rothenberg, Christian and Raca, Darijo and J. Quinlan, Jason},
title = {A Supervised Machine Learning Approach for DASH Video QoE Prediction in 5G Networks},
year = {2020},
isbn = {9781450381208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416013.3426458},
doi = {10.1145/3416013.3426458},
abstract = {Future fifth generation (5G) networks are envisioned to provide improved Quality-of-Experience (QoE) for applications by means of higher data rates, low and ultra-reliable latency and very high reliability. Proving increasing beneficial for mobile devices running multimedia applications. However, there exist two main co-related challenges in multimedia delivery in 5G. Namely, balancing operator provisioning and client expectations. To this end, we investigate how to build a QoE-aware network that guarantees at run-time that the end-to-end user experience meets the end users' expectations at the same that the network's Quality of Service (QoS) varies. The contribution of this paper is twofold: first, we consider a Dynamic Adaptive Streaming over HTTP (DASH) video application in a realistic emulation environment derived from real 5G traces in static and mobility scenarios to assess the QoE performance of three state-of-art Adaptive Bitrate Streaming (ABS) algorithm categories: Hybrid - Elastic and Arbiter+; buffer-based - BBA and Logistic; and rate-based - Exponential and Conventional. Second, we propose a Machine Learning (ML) classifier to predict user satisfaction which considers network metrics, such as RTT, throughput, and number of packets. Our proposed model does not rely on knowledge about the application or specific traffic information. We show that our ML classifiers achieve a QoE prediction accuracy of 87.63 % and 79 % for static and mobility scenarios, respectively.},
booktitle = {Proceedings of the 16th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {57–64},
numpages = {8},
keywords = {5G, DASH, QoE prediction, QoS, machine learning, video streaming},
location = {Alicante, Spain},
series = {Q2SWinet '20}
}

@inproceedings{10.1007/978-3-030-64583-0_7,
author = {Silva, Stefan and Crispim, Jos\'{e}},
title = {An Application of Machine Learning to Study Utilities Expenses in the Brazilian Navy},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_7},
doi = {10.1007/978-3-030-64583-0_7},
abstract = {The extensive Brazilian territory endows its Navy with more than 350 facilities with several distinct activities that transcend military operations. Understanding the variation of all the essential and common costs of those facilities proved to be a challenging and relevant task. This paper presents a machine learning approach to support the decision-making process based on data that represents several facilities attributes, where models were trained, and those with the best performance were further analyzed. Besides data limitations, our results show that predictions and explanations derived from the models can be applied to support decision-making within the organization and contribute with insights to improve management over its resources.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {60–71},
numpages = {12},
keywords = {Military expenditures, Machine learning, Decision-making, Data-driven organization},
location = {Siena, Italy}
}

@article{10.1007/s10115-017-1144-z,
author = {Chen, Zhiyuan and Khoa, Le Dinh and Teoh, Ee Na and Nazir, Amril and Karuppiah, Ettikan Kandasamy and Lam, Kim Sim},
title = {Machine learning techniques for anti-money laundering (AML) solutions in suspicious transaction detection: a review},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1144-z},
doi = {10.1007/s10115-017-1144-z},
abstract = {Money laundering has been affecting the global economy for many years. Large sums of money are laundered every year, posing a threat to the global economy and its security. Money laundering encompasses illegal activities that are used to make illegally acquired funds appear legal and legitimate. This paper aims to provide a comprehensive survey of machine learning algorithms and methods applied to detect suspicious transactions. In particular, solutions of anti-money laundering typologies, link analysis, behavioural modelling, risk scoring, anomaly detection, and geographic capability have been identified and analysed. Key steps of data preparation, data transformation, and data analytics techniques have been discussed; existing machine learning algorithms and methods described in the literature have been categorised, summarised, and compared. Finally, what techniques were lacking or under-addressed in the existing research has been elaborated with the purpose of pinpointing future research directions.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {245–285},
numpages = {41},
keywords = {Anomaly detection, Anti-money laundering, Anti-money laundering typologies, Behavioural modelling, Data mining methods and algorithms, Geographic capability, Link analysis, Risk scoring, Supervised learning, Unsupervised learning}
}

@article{10.1109/TNET.2021.3112082,
author = {Zhang, Xiaoxi and Wang, Jianyu and Lee, Li-Feng and Yang, Tom and Kalra, Akansha and Joshi, Gauri and Joe-Wong, Carlee},
title = {Machine Learning on Volatile Instances: Convergence, Runtime, and Cost Tradeoffs},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3112082},
doi = {10.1109/TNET.2021.3112082},
abstract = {Due to the massive size of the neural network models and training datasets used in machine learning today, it is imperative to distribute stochastic gradient descent (SGD) by splitting up tasks such as gradient evaluation across multiple worker nodes. However, running distributed SGD can be prohibitively expensive because it may require specialized computing resources such as GPUs for extended periods of time. We propose cost-effective strategies to exploit volatile cloud instances that are cheaper than standard instances, but may be interrupted by higher priority workloads. To the best of our knowledge, this work is the first to quantify how variations in the number of active worker nodes (as a result of preemption) affect SGD convergence and the time to train the model. By understanding these trade-offs between preemption probability of the instances, accuracy, and training time, we are able to derive practical strategies for configuring distributed SGD jobs on volatile instances such as Amazon EC2 spot instances and other preemptible cloud instances. Experimental results show that our strategies achieve good training performance at substantially lower cost.},
journal = {IEEE/ACM Trans. Netw.},
month = nov,
pages = {215–228},
numpages = {14}
}

@inproceedings{10.5555/3540261.3541531,
author = {Cousins, Cyrus},
title = {An axiomatic theory of provably-fair welfare-centric machine learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address an inherent difficulty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justified alternative setting, and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimization tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We define a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justification via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss and maximizing welfare. Building upon these concepts, we define fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justified) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on firm theoretical ground, as it yields statistical — and in some cases computational — efficiency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1270},
numpages = {12},
series = {NIPS '21}
}

@article{10.1016/j.ins.2015.01.019,
author = {Peng, Hong and Wang, Jun and P\'{e}rez-Jim\'{e}nez, Mario J. and Riscos-N\'{u}\~{n}ez, Agust\'{\i}n},
title = {An unsupervised learning algorithm for membrane computing},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {304},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.01.019},
doi = {10.1016/j.ins.2015.01.019},
abstract = {This paper focuses on the unsupervised learning problem within membrane computing, and proposes an innovative solution inspired by membrane computing techniques, the fuzzy membrane clustering algorithm. An evolution-communication P system with nested membrane structure is the core component of the algorithm. The feasible cluster centers are represented by means of objects, and three types of membranes are considered: evolution, local store, and global store. Based on the designed membrane structure and the inherent communication mechanism, a modified differential evolution mechanism is developed to evolve the objects in the system. Under the control of the evolution-communication mechanism of the P system, the proposed fuzzy clustering algorithm achieves good fuzzy partitioning for a data set. The proposed fuzzy clustering algorithm is compared to three recently-developed and two classical clustering algorithms for five artificial and five real-life data sets.},
journal = {Inf. Sci.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Data clustering, Evolution-communication P system, Fuzzy clustering, Membrane computing, P system, Unsupervised learning}
}

@article{10.1145/3453158,
author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior},
title = {Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453158},
doi = {10.1145/3453158},
abstract = {In recent years, machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This article comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker’ s goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. To the best of our knowledge, this work is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {108},
numpages = {36},
keywords = {Adversarial learning, adversarial examples, adversarial machine learning, cyber security, deep learning, evasion attacks, poisoning attacks}
}

@phdthesis{10.5555/AAI28131907,
author = {Lu, Xiaoyu},
title = {Modelling, Inference and Optimization in Probabilistic Machine Learning},
year = {2019},
publisher = {University of Oxford (United Kingdom)},
abstract = {Bayesian machine learning has gained tremendous attention in the machine learning community over the past few years. Bayesian methods offer a coherent reasoning for quantifying uncertainties in the decision making procedure, based on the Bayes rule. One of the core advantages of Bayesian methods is the separation of modelling and inference. In other words, the likelihood models are completely independent of the computation of the posterior distribution of the parameters. There are many Bayesian models that are widely used in the machine learning community. For example, non-parametric models such as Gaussian Processes and Dirichlet Processes are flexible models which are able to capture and learn the structure of the data. Bayesian deep learning models, which are based on neural networks, are another example of flexible Bayesian models that are rich enough to represent non-linear structures in the data. The process of inferring the posterior lies at the center of Bayesian inference. When computing the posterior distribution exactly is not feasible, due to intractability of the posterior and the computational or memory constraints, approximate Bayesian inference comes to play. In this PhD thesis, I develop and investigate various Bayesian modelling and inference techniques and apply them to multiple interesting domains and tasks. We begin with Tucker Gaussian Processes(TGP), a class of flexible non-parametric models based on Gaussian Processes (GP). We apply the method to 1) regression problems on structured input data, and 2) collaborative filtering problems where TGP offers an elegant way of incorporating side information. We demonstrate superior results compared with benchmarks on a number of examples across different domains. A closely related line of research based on GPs is Bayesian Optimization (BO). It is a black-box optimizer where one optimizes an objective function through subsequent queries about next input locations to be evaluated at. However, this method does not work well when the input space is non-Euclidean or combinatorial. We alleviate the problem by learning a low dimensional Euclidean representation of the combinatorial input space with variational inference, using Variational Auto-encoder (VAE). The optimization can then be conducted on the low dimensional embedding instead. We apply our method to Automatic Statistician and natural scene understanding, which give promising results. For approximate Bayesian inference, we first propose an algorithm called Relativistic Hamiltonian Monte Carlo (RHMC) which is a variant of MCMC. In particular, we replace Newton's kinetic energy in the Hamiltonian with Einstein's relativistic kinetic energy, which makes the algorithm more robust. There are several extensions to RHMC, including a stochastic gradient version for scalability, a thermostat version based on the temperature of the physical system and a resulting optimization algorithm which gives comparable performance compared with the state-of-the-art. Finally, we propose another sampling based inference method called the Adaptive Importance Sampling with Exploration and Exploitation (Daisee), where we look into the problem of exploration-exploitation in adaptive importance sampling through establishing a natural connection between importance sampling and multi-armed bandit problem. In particular, through a finite-time regret analysis we show that the regret of the proposed algorithm grows sublinearly with time. Further, we propose a hierarchical extension of Daisee to encourage exploration in the region with high uncertainty. The new models proposed in this thesis help to allow for more flexible Bayesian modelling and the inference techniques introduced can open new research directions for efficient and accurate posterior inference. These contribute to Bayesian inference and probabilistic machine learning.},
note = {AAI28131907}
}

@article{10.1145/3485133,
author = {Machado, Gabriel Resende and Silva, Eug\^{e}nio and Goldschmidt, Ronaldo Ribeiro},
title = {Adversarial Machine Learning in Image Classification: A Survey Toward the Defender’s Perspective},
year = {2021},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485133},
doi = {10.1145/3485133},
abstract = {Deep Learning algorithms have achieved state-of-the-art performance for Image Classification. For this reason, they have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been proposed recently in the literature. However, devising an efficient defense mechanism has proven to be a difficult task, since many approaches demonstrated to be ineffective against adaptive attackers. Thus, this article aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, nevertheless, with a defender’s perspective. This article introduces novel taxonomies for categorizing adversarial attacks and defenses, as well as discuss possible reasons regarding the existence of adversarial examples. In addition, relevant guidance is also provided to assist researchers when devising and evaluating defenses. Finally, based on the reviewed literature, this article suggests some promising paths for future research.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {8},
numpages = {38},
keywords = {Computer vision, image classification, adversarial images, deep neural networks, adversarial attacks, defense methods}
}

@inproceedings{10.1145/3485279.3485302,
author = {Swieso, Sloan and Yao, Powen and Miller, Mark and Jothi, Adityan and Zhao, Andrew and Zyda, Michael},
title = {Toward Using Machine Learning-Based Motion Gesture for 3D Text Input},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485302},
doi = {10.1145/3485279.3485302},
abstract = {In this paper, we discuss our implementation of a gesture-based 3-dimensional typing system within virtual reality. Rather than the conventional point-and-click keyboard commonly found in immersive technologies, we explore using unique gestures with the controller to enter a specific key. To map these gestures and movements to their respective key, we utilize machine learning techniques to avoid naive hard-coded implementations. The result of the trained model is a text input system that adapts to the user’s gestures, rather than forcing the user to conform to the system’s definition of a gesture. Our goal is to work toward a viable alternative to standard virtual reality keyboards and typing systems.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {28},
numpages = {2},
keywords = {gesture typing, machine learning, text entry, virtual reality},
location = {Virtual Event, USA},
series = {SUI '21}
}

@inproceedings{10.1145/3461702.3462527,
author = {Hopkins, Aspen and Booth, Serena},
title = {Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462527},
doi = {10.1145/3461702.3462527},
abstract = {Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community---for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints---tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {134–145},
numpages = {12},
keywords = {ML developers, big tech, contextual inquiry, machine learning practice},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1007/978-3-030-52200-1_29,
author = {Brown, Christopher W. and Daves, Glenn Christopher},
title = {Applying Machine Learning to Heuristics for Real Polynomial Constraint Solving},
year = {2020},
isbn = {978-3-030-52199-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52200-1_29},
doi = {10.1007/978-3-030-52200-1_29},
abstract = {This paper considers the application of machine learning to automatically generating heuristics for real polynomial constraint solvers. We consider a specific choice-point in the algorithm for constructing an open Non-uniform Cylindrical Algebraic Decomposition (NuCAD) for a conjunction of constraints, and we learn a heuristic for making that choice. Experiments demonstrate the effectiveness of the learned heuristic. We hope that the approach we take to learning this heuristic, which is not a natural fit to machine learning, can be applied effectively to other choices in constraint solving algorithms.},
booktitle = {Mathematical Software – ICMS 2020: 7th International Conference, Braunschweig, Germany, July 13–16, 2020, Proceedings},
pages = {292–301},
numpages = {10},
keywords = {Non-linear polynomial constraints, Machine learning},
location = {Braunschweig, Germany}
}

@phdthesis{10.5555/AAI28225425,
author = {Liang, Ming},
advisor = {Min, Chi, and David, Lubkeman, and Mesut, Baran, and Ning, Lu,},
title = {A Machine Learning-Based Approach for Synthetic Distribution Feeder Generation},
year = {2020},
isbn = {9798684618680},
publisher = {North Carolina State University},
abstract = {Test systems are widely used by researchers and engineers to test conceptual designs, optimize parameter settings, and validate performance. However, developing high-fidelity distribution feeder models requires access to utility network models and customer data, which is a major barrier for the research community to have unrestrictive, unlimited number of customizable, realistic test systems for research and development purpose. So far, there has been very little attempt made towards the manual and static test system design principles, making creating an ensemble of test systems from actual feeder models a daunting task. Motivated by this, the dissertation aims at developing an end-to-end, machine-learning-based approach for automated, customizable test feeder generation using actual feeder models as inputs.This dissertation presents a novel, automated, generative adversarial networks (GAN) based synthetic feeder generation mechanism, abbreviated as FeederGAN. FeederGAN digests real feeder models represented by directed graphs via a deep learning framework powered by GAN and graph convolutional networks (GCN). Information of a distribution feeder circuit is extracted from its model input files so that the device connectivity is mapped onto the adjacency matrix and the device characteristics, such as circuit types (i.e., 3-phase, 2-phase, and 1-phase) and component attributes (e.g., length and current ratings), are mapped onto the attribute matrix. Then, Wasserstein distance is used to optimize the GAN and GCN is used to discriminate the generated graphs from the actual ones. A greedy method based on graph theory is developed to reconstruct the feeder using the generated adjacency and attribute matrices. After the feeder topologies and attributes are generated, we then use a statistical, rule-based method to generate the load transformers. The rules make the transformers follows line capacity constraints, line attributes constraints and topology&nbsp;constraints. Finally, we write the generated feeder file with load information into OpenDSS format and run combined case studies. The results show that the GAN generated feeders resemble the actual feeder in both topology and attributes verified by visual inspection and by empirical statistics obtained from actual distribution feeders.In the second part of the dissertation, a synthetic load generation method is developed via a novel sequential energy disaggregation (SED) algorithm. The SED algorithm is presented for extracting heating and cooling energy consumptions from residential and small commercial building loads using low-resolution (i.e. 15-minute, 30-minute, and 60-minute) smart meter data. The method is validated using data collected from 137 households in the PECAN street project. Results show that the proposed SED method is computationally efficient, simple to implement, and robust in performance. Based on the SED algorithm developed, case study is conducted on buildings with photovoltaic (PV) systems and electric vehicles (EVs). Among the cases, load database of zero-net energy (ZNE) cases, ZNE ready cases, ZNE with EV cases is built up. Therefore, those load data can be then used together with the generated synthetic feeders to test different planning and operational strategies.},
note = {AAI28225425}
}

@article{10.1504/ijcvr.2021.115165,
author = {Sharma, Vipul and Mir, Roohie Naaz},
title = {Maximum entropy-based semi-supervised learning for automatic detection and recognition of objects using deep ConvNets},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {3},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2021.115165},
doi = {10.1504/ijcvr.2021.115165},
abstract = {Object detection and localisation is one of the major research areas in computer vision that is growing very rapidly. Currently, there is a plethora of pre-trained models for object detection including YOLO, mask RCNN, RCNN, fast RCNN, multi-box, etc. In this paper, we proposed a new framework for object detection called 'maximum entropy-based semi-supervised learning for automatic detection and recognition of objects'. The main objective of this paper is to recognise objects from a number of visual object classes in a realistic scene simultaneously. The major operations of our proposed approach are preprocessing, localisation, segmentation and object detection. In the preprocessing, three processes, noise reduction, intensity normalisation, and morphology are considered. Then localisation and object segmentation is performed using maximum entropy in which optimal threshold is detected and in the end, object detection is performed using deep ConvNet. The performance of the proposed framework is evaluated using MATLAB-R2018b and it is compared with some previous state of the art techniques in terms of localisation error, detection and segmentation accuracy along with computation time.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {328–356},
numpages = {28},
keywords = {maximum entropy, object detection, weakly supervised learning, deep convolutional neural networks, segmentation and localisation}
}

@article{10.1016/j.mejo.2021.105198,
author = {Abazyan, Suren and Melikyan, Vazgen},
title = {Enhanced pin-access prediction and design optimization with machine learning integration},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {116},
number = {C},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2021.105198},
doi = {10.1016/j.mejo.2021.105198},
journal = {Microelectron. J.},
month = oct,
numpages = {5},
keywords = {Pin access, Machine learning, Prediction and optimization}
}

@inproceedings{10.1145/3301275.3302324,
author = {Gil, Yolanda and Honaker, James and Gupta, Shikhar and Ma, Yibo and D'Orazio, Vito and Garijo, Daniel and Gadewar, Shruti and Yang, Qifan and Jahanshad, Neda},
title = {Towards human-guided machine learning},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302324},
doi = {10.1145/3301275.3302324},
abstract = {Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user's knowledge about the data available. We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {614–624},
numpages = {11},
keywords = {automated machine learning (AutoML), human-guided machine learning, scientific workflows, task analysis},
location = {Marina del Ray, California},
series = {IUI '19}
}

@article{10.1016/j.knosys.2016.05.044,
author = {Lopes, Lucas A. and Machado, Vinicius P. and Rab\^{e}lo, Ricardo A.L. and Fernandes, Ricardo A.S. and Lima, Bruno V.A.},
title = {Automatic labelling of clusters of discrete and continuous data with supervised machine learning},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.044},
doi = {10.1016/j.knosys.2016.05.044},
abstract = {This study presents a definition of the labelling problem and a solution that is based on techniques for supervised learning, unsupervised learning and a discretisation model.A method with unsupervised learning is applied to the clustering problem, and a supervised learning algorithm will detect the relevant attributes to define each formed cluster.Some strategies are used to form a methodology that presents a label (based on attributes and values) for each provided cluster.Discretisation methods 226 will be used to determine the ranges of values of the attributes presented in the 227 labels.This methodology is applied to three different databases, in which acceptable results were achieved with an average that exceeds 92.89% of correctly labelled elements. The clustering problem has been considered one of the most relevant problems in the research area of unsupervised learning. However, the comprehension and definition of such clusters is not a trivial task, making necessary their identification, i.e., assign a label to each cluster. To address the problem of labelling clusters, this paper presents a methodology based on techniques for supervised learning, unsupervised learning and a discretization model. Thus, a method with unsupervised learning is applied to the clustering problem, and the supervised learning algorithm is responsible for detecting the meaningful attributes to define each formed cluster. Some strategies are used to form a methodology that presents a label (based on attributes and values) for each provided cluster. Such methodology is applied to three different databases, in which acceptable results were achieved with an average that exceeds 92.89% of correctly labelled elements.},
journal = {Know.-Based Syst.},
month = aug,
pages = {231–241},
numpages = {11},
keywords = {Machine learning, artificial neural networks, clustering, labelling}
}

@article{10.1145/3491234,
author = {Rasoulinezhad, Seyedramin and Roorda, Esther and Wilton, Steve and Leong, Philip H. W. and Boland, David},
title = {Rethinking Embedded Blocks for Machine Learning Applications},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3491234},
doi = {10.1145/3491234},
abstract = {The underlying goal of FPGA architecture research is to devise flexible substrates that implement a wide variety of circuits efficiently. Contemporary FPGA architectures have been optimized to support networking, signal processing, and image processing applications through high-precision digital signal processing (DSP) blocks. The recent emergence of machine learning has created a new set of demands characterized by: (1) higher computational density and (2) low precision arithmetic requirements. With the goal of exploring this new design space in a methodical manner, we first propose a problem formulation involving computing nested loops over multiply-accumulate (MAC) operations, which covers many basic linear algebra primitives and standard deep neural network (DNN) kernels. A quantitative methodology for deriving efficient coarse-grained compute block architectures from benchmarks is then proposed together with a family of new embedded blocks, called MLBlocks. An MLBlock instance includes several multiply-accumulate units connected via a flexible routing, where each configuration performs a few parallel dot-products in a systolic array fashion. This architecture is parameterized with support for different data movements, reuse, and precisions, utilizing a columnar arrangement that is compatible with existing FPGA architectures. On synthetic benchmarks, we demonstrate that for 8-bit arithmetic, MLBlocks offer 6\texttimes{} improved performance over the commercial Xilinx DSP48E2 architecture with smaller area and delay; and for time-multiplexed 16-bit arithmetic, achieves 2\texttimes{} higher performance per area with the same area and frequency. All source codes and data, along with documents to reproduce all the results in this article, are available at .},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = nov,
articleno = {9},
numpages = {30},
keywords = {FPGA Architectures, coarse-grained compute blocks, reconfigurable architecture, neural networks, digital signal processing}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@inproceedings{10.1145/3387168.3387219,
author = {Mohammad, Nurul Izzati and Ismail, Saiful Adli and Kama, Mohd Nazri and Yusop, Othman Mohd and Azmi, Azri},
title = {Customer Churn Prediction In Telecommunication Industry Using Machine Learning Classifiers},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387219},
doi = {10.1145/3387168.3387219},
abstract = {Customer churn is one of the main problems in telecommunication industry. This study aims to identify the factors that influence customer churn and develop an effective churn prediction model as well as provide best analysis of data visualization results. The dataset has been collected from Kaggle open data website. The proposed methodology for analysis of churn prediction covers several phases: data pre-processing, analysis, implementing machine learning algorithms, evaluation of the classifiers and choose the best one for prediction. Data preprocessing process involved three major action, which are data cleaning, data transformation and feature selection. Machine learning classifiers was chosen are Logistic Regression, Artificial Neural Network and Random Forest. Then, classifiers were evaluated by using performance measurement which are accuracy, precision, recall and error rate in order to find the best classifier. Based on this study, the output shows that logistic regression outperform compared to artificial neural network and random forest.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {34},
numpages = {7},
keywords = {Customer Churn, Machine Learning, Prediction, Telecommunication Industry},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@phdthesis{10.5555/AAI28716476,
author = {Gangopadhyay, Ahana and ShiNung, Ching, and Carlos, Ponce, and Baranidharan, Raman, and Shen, Zeng,},
advisor = {Shantanu, Chakrabartty,},
title = {A Neuromorphic Machine Learning Framework Based on the Growth Transform Dynamical System},
year = {2021},
isbn = {9798538124060},
publisher = {Washington University in St. Louis},
abstract = {As computation increasingly moves from the cloud to the source of data collection, there is a growing demand for specialized machine learning algorithms that can perform learning and inference at the edge in energy and resource-constrained environments. In this regard, we can take inspiration from small biological systems like insect brains that exhibit high energy-efficiency within a small form-factor, and show superior cognitive performance using fewer, coarser neural operations (action potentials or spikes) than the high-precision floating-point operations used in deep learning platforms. Attempts at bridging this gap using neuromorphic hardware has produced silicon brains that are orders of magnitude inefficient in energy dissipation as well as performance. This is because neuromorphic machine learning (ML) algorithms are traditionally built bottom-up, starting with neuron models that mimic the response of biological neurons and connecting them together to form a network. Neural responses and weight parameters are therefore not optimized w.r.t. any system objective, and it is not evident how individual spikes and the associated population dynamics are related to a network objective. On the other hand, conventional ML algorithms follow a top-down synthesis approach, starting from a system objective (that usually only models task efficiency), and reducing the problem to the model of a non-spiking neuron with non-local updates and little or no control over the population dynamics. I propose that a reconciliation of the two approaches may be key to designing scalable spiking neural networks that optimize for both energy and task efficiency under realistic physical constraints, while enabling spike-based encoding and learning based on local updates in an energy-based framework like traditional ML models.To this end, I first present a neuron model implementing a mapping based on polynomial growth transforms, which allows for independent control over spike forms and transient firing statistics. I show how spike responses are generated as a result of constraint violation while minimizing a physically plausible energy functional involving a continuous-valued neural variable, that represents the local power dissipation in a neuron. I then show how the framework could be extended to coupled neurons in a network by remapping synaptic interactions in a standard spiking network. I show how the network could be designed to perform a limited amount of learning in an energy-efficient manner even without synaptic adaptation by appropriate choices of network structure and parameters - through spiking SVMs that learn to allocate switching energy to neurons that are more important for classification and through spiking associative memory networks that learn to modulate their responses based on global activity. Lastly, I describe a backpropagation-less learning framework for synaptic adaptation where weight parameters are optimized w.r.t. a network-level loss function that represents spiking activity across the network, but which produces updates that are local. I show how the approach can be used for unsupervised and supervised learning such that minimizing a training error is equivalent to minimizing the network-level spiking activity. I build upon this framework to introduce end-to-end spiking neural network (SNN) architectures and demonstrate their applicability for energy and resource-efficient learning using a benchmark dataset.},
note = {AAI28716476}
}

@inproceedings{10.1145/3448016.3452759,
author = {Grafberger, Stefan and Guha, Shubha and Stoyanovich, Julia and Schelter, Sebastian},
title = {MLINSPECT: A Data Distribution Debugger for Machine Learning Pipelines},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452759},
doi = {10.1145/3448016.3452759},
abstract = {Machine Learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this wide-spread use are garnering attention from policymakers, scientists, and the media. ML applications are often very brittle with respect to their input data, which leads to concerns about their reliability, accountability, and fairness. While bias detection cannot be fully automated, computational tools can help pinpoint particular types of data issues.We recently proposed mlinspect, a library that enables lightweight lineage-based inspection of ML preprocessing pipelines. In this demonstration, we show how mlinspect can be used to detect data distribution bugs in a representative pipeline. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines, can handle both relational and matrix data, and does not require manual code instrumentation. The library is publicly available at https://github.com/stefan-grafberger/mlinspect.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2736–2739},
numpages = {4},
keywords = {data distribution debugging, machine learning pipelines, responsible data science, technical bias},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1145/3418526,
author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/3418526},
doi = {10.1145/3418526},
abstract = {Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of GD and SGD and show that they are truly efficient—their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical first-order stationary points.},
journal = {J. ACM},
month = feb,
articleno = {11},
numpages = {29},
keywords = {(stochastic) gradient descent, Saddle points, efficiency, perturbations}
}

@article{10.1016/j.patrec.2015.08.009,
author = {Yang, Yun and Liu, Xingchen},
title = {A robust semi-supervised learning approach via mixture of label information},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {68},
number = {P1},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2015.08.009},
doi = {10.1016/j.patrec.2015.08.009},
abstract = {Due to the fact that limited amounts of labeled data are normally available in real-world, semi-supervised learning has become a popular option, where we expect to use unlabeled data information to improve the learning performance. However, how to use such unlabeled information to make the predicted labels more reliable remains to be a key for any successful learning. In this paper, we propose a semi-supervised learning framework via combination of semi-supervised clustering and semi-supervised classification. In our approach, the predicted labels are selected by both the constrained k-means and safe semi-supervised SVM (S4VMs) to improve the reliability of the predicted labels. Extensive evaluations on collection of benchmarks and real-world action recognition datasets show that the proposed technique outperforms the others.},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {15–21},
numpages = {7},
keywords = {Classification, Clustering, Semi-supervised learning}
}

@inproceedings{10.1145/3359992.3366640,
author = {Sanchez, Odnan Ref and Ferlin, Simone and Pelsser, Cristel and Bush, Randy},
title = {Comparing Machine Learning Algorithms for BGP Anomaly Detection using Graph Features},
year = {2019},
isbn = {9781450369992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359992.3366640},
doi = {10.1145/3359992.3366640},
abstract = {The Border Gateway Protocol (BGP) coordinates the connectivity and reachability among Autonomous Systems, providing efficient operation of the global Internet. Historically, BGP anomalies have disrupted network connections on a global scale, i.e., detecting them is of great importance. Today, Machine Learning (ML) methods have improved BGP anomaly detection using volume and path features of BGP's update messages, which are often noisy and bursty. In this work, we identified different graph features to detect BGP anomalies, which are arguably more robust than traditional features. We evaluate such features through an extensive comparison of different ML algorithms, i.e., Naive Bayes classifier (NB), Decision Trees (DT), Random Forests (RF), Support Vector Machines (SVM), and Multi-Layer Perceptron (MLP), to specifically detect BGP path leaks. We show that SVM offers a good trade-off between precision and recall. Finally, we provide insights into the graph features' characteristics during the anomalous and non-anomalous interval and provide an interpretation of the ML classifier results.},
booktitle = {Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks},
pages = {35–41},
numpages = {7},
keywords = {BGP, anomaly detection, graph features, machine learning},
location = {Orlando, FL, USA},
series = {Big-DAMA '19}
}

@inproceedings{10.5555/3495724.3496555,
author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
title = {Unsupervised learning of visual features by contrasting cluster assignments},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of con-trastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or "views") of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a "swapped" prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {831},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1007/s00500-020-05253-4,
author = {Al-Yarimi, Fuad Ali Mohammed and Munassar, Nabil Mohammed Ali and Bamashmos, Mohammed Hasan Mohammed and Ali, Mohammed Yousef Salem},
title = {RETRACTED ARTICLE: Feature optimization by discrete weights for heart disease prediction using supervised learning},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {3},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05253-4},
doi = {10.1007/s00500-020-05253-4},
abstract = {The topic predictive analytics is the ray that lightning the way to patch the gap between accuracy in decision-making by the expertise and the inexperience. In particular, the health domain is more crucial about disease prediction accuracy. The disease diagnosis by clinical practitioner correlates to his exposer toward the clinical observations of the disease. However, the perceptions of an experienced clinical practitioner on a medical record often fail to identify the premature states of the disease, which costs patient life in the sector of critical diseases such as heart diseases. Hence, contemporary computer science engineering research has more attention to define substantial predictive analytics built by machine learning toward heart disease prediction. The critical objective to define predictive analytics with minimal false alarming is centric to potential training data corpus, and the optimal feature selection. In order to these arguments, the contribution of this manuscript aimed to portray the feature selection approach to perform supervised learning and label the given patient record is prone to heart disease or not with minimal false alarming. The contribution is a dynamic n-gram Features Optimization by Discrete Weights of the feature correlation. The experimental study signified the performance of the proposed model compared to the contemporary methods of feature selection for heart disease prediction.},
journal = {Soft Comput.},
month = feb,
pages = {1821–1831},
numpages = {11},
keywords = {Coronary Heart Disease (CHD), Decision Trees (DT), Nearest Neighbor Algorithm (K-NN), Support vector machines (SVM), Decision support system (DSS)}
}

@inproceedings{10.1145/3314221.3322485,
author = {Iyer, Arun and Jonnalagedda, Manohar and Parthasarathy, Suresh and Radhakrishna, Arjun and Rajamani, Sriram K.},
title = {Synthesis and machine learning for heterogeneous extraction},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3322485},
doi = {10.1145/3314221.3322485},
abstract = {We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data.  We use machine learning models ("ML models") such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called "field constraints") to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity.  More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds --- handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {301–315},
numpages = {15},
keywords = {Data extraction, Heterogeneous data, Machine Learning, Program synthesis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{10.3233/JIFS-200862,
author = {Elavarasan, Dhivya and Vincent, Durai Raj},
title = {Reinforced XGBoost machine learning model for sustainable intelligent agrarian applications},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200862},
doi = {10.3233/JIFS-200862},
abstract = {The development in science and technical intelligence has incited to represent an extensive amount ofdata from various fields of agriculture. Therefore an objective rises up for the examination of the available data and integrating with processes like crop enhancement, yield prediction, examination of plant infections etc. Machine learning has up surged with tremendous processing techniques to perceive new contingencies in the multi-disciplinary agrarian advancements. In this pa- per a novel hybrid regression algorithm, reinforced extreme gradient boosting is proposed which displays essentially improved execution over traditional machine learning algorithms like artificial neural networks, deep Q-Network, gradient boosting, ran- dom forest and decision tree. Extreme gradient boosting constructs new models, which are essentially, decision trees learning from the mistakes of their predecessors by optimizing the gradient descent loss function. The proposed hybrid model performs reinforcement learning at every node during the node splitting process of the decision tree construction. This leads to effective utilizationofthesamplesbyselectingtheappropriatesplitattributeforenhancedperformance. Model’sperformanceisevaluated by means of Mean Square Error, Root Mean Square Error, Mean Absolute Error, and Coefficient of Determination. To assure a fair assessment of the results, the model assessment is performed on both training and test dataset. The regression diagnostic plots from residuals and the results obtained evidently delineates the fact that proposed hybrid approach performs better with reduced error measure and improved accuracy of 94.15% over the other machine learning algorithms. Also the performance of probability density function for the proposed model delineates that, it can preserve the actual distributional characteristics of the original crop yield data more approximately when compared to the other experimented machine learning models.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7605–7620},
numpages = {16},
keywords = {Crop yield prediction, reinforcement learning, extreme gradient boosting, intelligent agrarian application}
}

@article{10.3233/JIFS-189481,
author = {Wang, Linuo and Ramachandran, Varatharajan},
title = {Simulation of sports movement training based on machine learning and brain-computer interface},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189481},
doi = {10.3233/JIFS-189481},
abstract = {Injuries and hidden dangers in training have a greater impact on athletes ’careers. In particular, the brain function that controls the motor function area has a greater impact on the athlete ’s competitive ability. Based on this, it is necessary to adopt scientific methods to recognize brain functions. In this paper, we study the structure of motor brain-computer and improve it based on traditional methods. Moreover, supported by machine learning and SVM technology, this study uses a DSP filter to convert the preprocessed EEG signal X into a time series, and adjusts the distance between the time series to classify the data. In order to solve the inconsistency of DSP algorithms, a multi-layer joint learning framework based on logistic regression model is proposed, and a brain-machine interface system of sports based on machine learning and SVM is constructed. In addition, this study designed a control experiment to improve the performance of the method proposed by this study. The research results show that the method in this paper has a certain practical effect and can be applied to sports.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6409–6420},
numpages = {12},
keywords = {Machine learning, SVM, sports, brain-computer interface}
}

@article{10.1016/j.patrec.2021.10.008,
author = {Shin, Yu-Hyun and Baek, Seung Jun},
title = {Hopfield-type neural ordinary differential equation for robust machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.10.008},
doi = {10.1016/j.patrec.2021.10.008},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {180–187},
numpages = {8},
keywords = {Neural ODE, Adversarial defense, Hopfield-type network, Image classification, 41A05, 41A10, 65D05, 65D17}
}

@article{10.1007/s11192-021-03951-w,
author = {Mihaljevi\'{c}, Helena and Santamar\'{\i}a, Luc\'{\i}a},
title = {Disambiguation of author entities in ADS using supervised learning and graph theory methods},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {126},
number = {5},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-021-03951-w},
doi = {10.1007/s11192-021-03951-w},
abstract = {Disambiguation of authors in digital libraries is essential for many tasks, including efficient bibliographical searches and scientometric analyses to the level of individuals. The question of how to link documents written by the same person has been given much attention by academic publishers and information retrieval researchers alike. Usual approaches rely on publications’ metadata such as affiliations, email addresses, co-authors, or scholarly topics. Lack of homogeneity in the structure of bibliographic collections and discipline-specific dissimilarities between them make the creation of general-purpose disambiguators arduous. We present an algorithm to disambiguate authorships in the Astrophysics Data System (ADS) following an established semi-supervised approach of training a classifier on authorship pairs and clustering the resulting graphs. Due to the lack of high-signal features such as email addresses and citations, we engineer additional content- and location-based features via text embeddings and named-entity recognition. We train various nonlinear tree-based classifiers and detect communities from the resulting weighted graphs through label propagation, a fast yet efficient algorithm that requires no tuning. The resulting procedure reaches reasonable complexity and offers possibilities for interpretation. We apply our method to the creation of author entities in a recent ADS snapshot. The algorithm is evaluated on 39 manually-labeled author blocks comprising 9545 authorships from 562 author profiles. Our best approach utilizes the Random Forest classifier and yields a micro- and macro-averaged BCubed F1 score of 0.95 and 0.87, respectively. We release our code and labeled data publicly to foster the development of further disambiguation procedures for ADS.},
journal = {Scientometrics},
month = may,
pages = {3893–3917},
numpages = {25},
keywords = {Author name disambiguation, Record linkage, Supervised learning, Label Propagation, Information retrieval, Digital libraries}
}

@inproceedings{10.1109/ICSE43902.2021.00024,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Are Machine Learning Cloud APIs Used Correctly?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00024},
doi = {10.1109/ICSE43902.2021.00024},
abstract = {Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {125–137},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s00521-020-04874-y,
author = {Adi, Erwin and Anwar, Adnan and Baig, Zubair and Zeadally, Sherali},
title = {Machine learning and data analytics for the IoT},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04874-y},
doi = {10.1007/s00521-020-04874-y},
abstract = {The Internet of Things (IoT) applications have grown in exorbitant numbers, generating a large amount of data required for intelligent data processing. However, the varying IoT infrastructures (i.e., cloud, edge, fog) and the limitations of the IoT application layer protocols in transmitting/receiving messages become the barriers in creating intelligent IoT applications. These barriers prevent current intelligent IoT applications to adaptively learn from other IoT applications. In this paper, we critically review how IoT-generated data are processed for machine learning analysis and highlight the current challenges in furthering intelligent solutions in the IoT environment. Furthermore, we propose a framework to enable IoT applications to adaptively learn from other IoT applications and present a case study in how the framework can be applied to the real studies in the literature. Finally, we discuss the key factors that have an impact on future intelligent applications for the IoT.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {16205–16233},
numpages = {29},
keywords = {Cybersecurity, Internet of Things, Intelligent systems, Machine learning}
}

@inproceedings{10.1145/3079628.3079630,
author = {Arevalillo-Herr\'{a}ez, Miguel and Ayesh, Aladdin and Santos, Olga C. and Arnau-Gonz\'{a}lez, Pablo},
title = {Combining Supervised and Unsupervised Learning to Discover Emotional Classes},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079630},
doi = {10.1145/3079628.3079630},
abstract = {Most previous work in emotion recognition has fixed the available classes in advance, and attempted to classify samples into one of these classes using a supervised learning approach. In this paper, we present preliminary work on combining supervised and unsupervised learning to discover potential latent classes which were not initially considered. To illustrate the potential of this hybrid approach, we have used a Self-Organizing Map (SOM) to organize a large number of Electroencephalogram (EEG) signals from subjects watching videos, according to their internal structure. Results suggest that a more useful labelling scheme could be produced by analysing the resulting topology in relation to user reported valence levels (i.e., pleasantness) for each signal, refining the original set of target classes.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {355–356},
numpages = {2},
keywords = {affective computing, class discovery, cluster analysis, eeg, personalization, user modelling},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@article{10.1007/s00165-021-00538-3,
author = {Bu, Lei and Liang, Yongjuan and Xie, Zhunyi and Qian, Hong and Hu, Yi-Qi and Yu, Yang and Chen, Xin and Li, Xuandong},
title = {Machine learning steered symbolic execution framework for complex software code},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00538-3},
doi = {10.1007/s00165-021-00538-3},
abstract = {During program traversing, symbolic execution collects path
conditions and feeds them to a constraint solver to obtain feasible
solutions. However, complex path conditions, like nonlinear
constraints, which widely appear in programs, are hard to be handled
efficiently by the existing solvers. In this paper, we adapt the
classical symbolic execution framework with a machine learning
approach for constraint satisfaction. The approach samples and
learns from different solutions to identify potentially feasible
area. This sampling-learning style solving can be applied in
different class of complex problems easily. Therefore, incorporating
this approach, our framework, MLBSE, supports the symbolic
execution of not only simple linear path conditions, but also
nonlinear arithmetic operations, and even black-box function calls
of library methods. Meanwhile, thanks to the theoretical foundation
of the machine learning based approach, when the solver fails to
solve a path condition, we can have an estimation of the confidence
in the satisfiability (ECS) of the problem to give users insights
about how the problem is analyzed and whether they could ultimately
find a solution. We implement MLBSE on the basis of Symbolic
Path Finder (SPF) into a fully automatic Java symbolic execution
engine. Users can feed their code to MLBSE directly, which is
very convenient to use. To evaluate its performance, 22 real case
programs are used as the benchmarks for MLBSE to generate test
cases, which involve a total number of 1042 methods that are full of
nonlinear operations, floating-point arithmetic as well as native
method calls. Experiment results show that the coverage achieved by
MLBSE is much higher than the state-of-the-art tools.},
journal = {Form. Asp. Comput.},
month = jun,
pages = {301–323},
numpages = {23},
keywords = {Symbolic execution, Machine learning, Nonlinear path condition, Constraint solving}
}

@article{10.1007/s42979-020-00233-9,
author = {Das, Dibakar and Bapat, Jyotsna and Das, Debabrata},
title = {Unsupervised Learning Based Capacity Augmentation in SDN Assisted Wireless Networks},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {4},
url = {https://doi.org/10.1007/s42979-020-00233-9},
doi = {10.1007/s42979-020-00233-9},
abstract = {Future tactile internet is likely to have a combination of underlying wired and wireless networks with heterogeneous (legacy and new) access technologies to support diverse applications, e.g., Internet of Things (IoT). In this context, Opportunistic Network (ON) can be an important paradigm in wireless networks to help augment capacity of network for varying internet traffic requirements. Software Defined Networking (SDN), with its logically centralized control plane, is expected to ease implementation of functions, such as radio resource management, across wireless networks with multiple Radio Access Technologies (multi-RAT). Hence, tactile internet is likely to work over an intelligent SDN controlled cloud-based implementation of wired and wireless technologies, and necessitating opportunistic network capacity augmentation with appropriate RAT. This paper presents a novel SDN assisted architecture for futuristic wireless networks which augments network capacity on need basis using unsupervised Machine Learning (ML) to create ON cells with appropriate RAT. Subsequently, we define utilities for the Wireless Network Infrastructure (WNI) and the User Equipment (UE) to evaluate the benefit of creation of ON cells. A game theoretic model is developed to understand the strategies of the two players, i.e., WNI and UE, while using the ON cell resources. The Nash Equilibria (NE) of the game reveal that both UE and WNI gain by co-operating with each other and lose otherwise in utilizing the augmented network capacity. Simulation results also confirm this observation.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {13},
keywords = {SDN, Opportunistic network, 5G, Multi-RAT networks, Wireless access networks}
}

@inproceedings{10.1145/3243250.3243254,
author = {Wu, YanTong and Liu, Yang and Li, XueMing},
title = {Position Estimation of Camera Based on Unsupervised Learning},
year = {2018},
isbn = {9781450364829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243250.3243254},
doi = {10.1145/3243250.3243254},
abstract = {It is an exciting task to recover the scene's 3D structure and camera pose from the video sequence. Most of the current solutions divide it into two parts, monocular depth recovery and camera pose estimation. The monocular depth recovery is often studied as an independent part, and a better depth estimation is used to solve the pose. While camera pose is still estimated by traditional SLAM (Simultaneous Localization And Mapping) methods in most cases. The application of unsupervised method for monocular depth recovery and pose estimation has benefited from the study of [1] and achieved good results. In this paper, we improve the method of [1]. Our emphasis is laid on the improvement of the idea and related theory, introducing a more reasonable inter frame constraints and finally synthesize the camera trajectory with inter frame pose estimation in the unified world coordinate system. And our results show better performance.},
booktitle = {Proceedings of the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {30–35},
numpages = {6},
keywords = {Pose estimation, Track stitching, Unsupervised learning},
location = {Union, NJ, USA},
series = {PRAI 2018}
}

@inproceedings{10.1007/978-3-030-74251-5_9,
author = {Z\"{o}ller, Marc-Andr\'{e} and Nguyen, Tien-Dung and Huber, Marco F.},
title = {Incremental Search Space Construction for Machine Learning Pipeline Synthesis},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_9},
doi = {10.1007/978-3-030-74251-5_9},
abstract = {Automated machine learning (AutoML) aims for constructing machine learning (ML) pipelines automatically. Many studies have investigated efficient methods for algorithm selection and hyperparameter optimization. However, methods for ML pipeline synthesis and optimization considering the impact of complex pipeline structures containing multiple preprocessing and classification algorithms have not been studied thoroughly. In this paper, we propose a data-centric approach based on meta-features for pipeline construction and hyperparameter optimization inspired by human behavior. By expanding the pipeline search space incrementally in combination with meta-features of intermediate data sets, we are able to prune the pipeline structure search space efficiently. Consequently, flexible and data set specific ML pipelines can be constructed. We prove the effectiveness and competitiveness of our approach on 28 data sets used in well-established AutoML benchmarks in comparison with state-of-the-art AutoML frameworks.},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {103–115},
numpages = {13},
keywords = {Pipeline structure search, Meta-learning, AutoML},
location = {Porto, Portugal}
}

@phdthesis{10.5555/AAI28717263,
author = {Ogidi-Ekoko, Onoriode Nathaniel and J., Wierer, Jonathan and Sushil, Kumar, and Michael, Stavola,},
advisor = {Nelson, Tansu,},
title = {Machine Learning–Inspired III-Nitride Photonics Design and Power Devices},
year = {2021},
isbn = {9798460408559},
publisher = {Lehigh University},
address = {USA},
abstract = {III-Nitrides are a very versatile class of semiconductors which have found great success for a number of applications including ultra-bright LEDs and laser diodes. One notable promising application still at an earlier stage of advancement is power electronics. Power electronics technology is a critical component of modern-day energy infrastructure. From electric grids to electric vehicles to consumer electronics, the need for highly efficient electrical power conversion to achieve huge energy savings on a global scale cannot be overstated. For decades, silicon, and more recently, silicon carbide (SiC), have been the semiconductors of choice for the low voltage and high voltage tiers of power electronics, respectively. However, while silicon technology is significantly more mature and cheaper than other material systems, due to its smaller bandgap, it is also fundamentally limited for high voltage, high temperature and high frequency applications. These fundamental limits are gradually being reached at current levels of technological advancement. There is therefore a need to explore other material systems in order to make the much-needed breakthroughs for the next generation of power devices.Gallium Nitride, owing to its wide bandgap property and consequently higher critical field, has a much higher theoretical limit (power figure of merit) than Si and SiC and can support significantly higher breakdown voltages than both for a given specific on resistance. This fact allows it to compete very favorably against both, if certain critical challenges can be addressed, including the higher substrate cost and some practical device limitations. For instance, for MOS-based devices, there has been a significant challenge with finding a high-quality dielectric for GaN that has excellent insulating properties and low interface state density. This work focuses on this issue, specifically investigating the electrical properties of MgO/GaN MOS capacitors. MgO has been considered to be a very good dielectric match for GaN, given its similar lattice structure, high k-value, wide bandgap and favorably high conduction band offset to GaN. In this work, the electrical properties of atomic layer deposited MgO/GaN MOS capacitors are investigated using current-voltage (I-V), capacitance-voltage (C-V), and conductance-voltage (G-V) characterization methods to determine leakage current characteristics, capacitive behavior, and interface trap density of the MOS capacitor structure, all of which can give insights into actual device performance. A study of the effect of annealing and mitigation efforts using Al2O3 in a gate stack design is also presented.The second part of this work addresses the design of GaN-based subwavelength gratings using machine learning–based approaches. Traditionally, III-Nitride photonics design has been driven by human effort and creativity which has been remarkably successful, evidenced by signature breakthroughs in LED technology and laser design. However, with advances in computing technology, the potential immense benefits of leveraging machine-driven optimization techniques for photonics design cannot be ignored. There is a need to capitalize on these gains. To this end, as an example, this work takes up the design of complex-shaped and conventional binary GaN-based subwavelength gratings using machine learning–based techniques, notably the differential evolution algorithm, to design the gratings for ultrabroadband reflectivity at two wavelength regimes of 500 nm and 1.55 μm. GaN subwavelength gratings are designed for high reflectivity as mirrors for vertical cavity surface emitting lasers (VCSELs) as a potential replacement for conventional distributed Bragg reflectors (DBR) used to achieve high reflectivity in VCSEL devices. The III-Nitride DBR challenges of low refractive index contrast, low p-type doping and lateral current control, which have to this point hampered the commercialization of III-Nitride VCSELs, may be surmountable by replacing the DBRs with monolithic subwavelength gratings, as has been demonstrated for other material systems such as InP, hence the motivation for this work. Rather than designing by a trial-and-error process, the algorithm optimizes the design to satisfy tight constraints for stopband width maximization (reflectivity greater than 99%), achieving a relative bandwidth (Δλ/λcenter) of 30% or more, well above what has hitherto been achievable using III-Nitride DBRs.},
note = {AAI28717263}
}

@article{10.5555/2627435.2627447,
author = {Fox-Roberts, Patrick and Rosten, Edward},
title = {Unbiased generative semi-supervised learning},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs--an unusual property for a semi-supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {367–443},
numpages = {77},
keywords = {Kullback-Leibler, asymptotic bounds, bias, generative model, semi-supervised}
}

@article{10.1109/TITS.2020.3003111,
author = {Gatto, Rubens Cruz and Forster, Carlos Henrique Quartucci},
title = {Audio-Based Machine Learning Model for Traffic Congestion Detection},
year = {2021},
issue_date = {Nov. 2021},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {1524-9050},
url = {https://doi.org/10.1109/TITS.2020.3003111},
doi = {10.1109/TITS.2020.3003111},
abstract = {The present work approaches intelligent traffic evaluation and congestion detection using sound sensors and machine learning. For this, two important problems are addressed: traffic condition assessment from audio data, and analysis of audio under uncontrolled environments. By modeling the traffic parameters and the sound generation from passing vehicles and using the produced audio as a source of data for learning the traffic audio patterns, we provide a solution that copes with the time, the cost and the constraints inherent to the activity of traffic monitoring. External noise sources were introduced to produce more realistic acoustic scenes and to verify the robustness of the methods presented. Audio-based monitoring becomes a simple and low-cost option, comparing to other methods based on detector loops, or GPS, and as good as camera-based solutions, without some of the common problems of image-based monitoring, such as occlusions and light conditions. The approach is evaluated with data from audio analysis of traffic registered in locations around the city of São Jose dos Campos, Brazil, and audio files from places around the world, downloaded from YouTube. Its validation shows the feasibility of traffic automatic audio monitoring as well as using machine learning algorithms to recognize audio patterns under noisy environments.},
journal = {Trans. Intell. Transport. Sys.},
month = nov,
pages = {7200–7207},
numpages = {8}
}

@inproceedings{10.1007/978-3-030-77772-2_21,
author = {Hart, Kyle M. and Goodman, Ari B. and O’Shea, Ryan P.},
title = {Automatic Generation of Machine Learning Synthetic Data Using ROS},
year = {2021},
isbn = {978-3-030-77771-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77772-2_21},
doi = {10.1007/978-3-030-77772-2_21},
abstract = {Data labeling is a time intensive process. As such, many data scientists use various tools to aid in the data generation and labeling process. While these tools help automate labeling, many still require user interaction throughout the process. Additionally, most target only a few network frameworks. Any researchers exploring multiple frameworks must find additional tools or write conversion scripts. This paper presents an automated tool for generating synthetic data in arbitrary network formats. It uses Robot Operating System (ROS) and Gazebo, which are common tools in the robotics community. Through ROS paradigms, it allows extensive user customization of the simulation environment and data generation process. Additionally, a plugin-like framework allows the development of arbitrary data format writers without the need to change the main body of code. Using this tool, the authors were able to generate an arbitrarily large image dataset for three unique training formats using approximately 15&nbsp;min of user setup time and a variable amount of hands-off run time, depending on the dataset size. The source code for this data generation tool is available at},
booktitle = {Artificial Intelligence in HCI: Second International Conference, AI-HCI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {310–325},
numpages = {16},
keywords = {Machine learning, Data generation, ROS}
}

@article{10.1016/j.comcom.2020.02.008,
author = {Gupta, Rajesh and Tanwar, Sudeep and Tyagi, Sudhanshu and Kumar, Neeraj},
title = {Machine Learning Models for Secure Data Analytics: A taxonomy and threat model},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {153},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2020.02.008},
doi = {10.1016/j.comcom.2020.02.008},
journal = {Comput. Commun.},
month = mar,
pages = {406–440},
numpages = {35},
keywords = {Big data, Secure Data Analytics, Data reduction, Machine learning models, Threat model, Data security and privacy}
}

@article{10.1145/3214306,
author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
title = {Machine Learning for Survival Analysis: A Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3214306},
doi = {10.1145/3214306},
abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {110},
numpages = {36},
keywords = {Cox model, Machine learning, censoring, concordance index, hazard rate, regression, survival analysis, survival data}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.1007/978-3-030-95481-9_4,
author = {d’Amato, Claudia},
title = {Mining the Semantic Web with Machine Learning: Main Issues that Need to Be Known},
year = {2021},
isbn = {978-3-030-95480-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95481-9_4},
doi = {10.1007/978-3-030-95481-9_4},
abstract = {The Semantic Web (SW) is characterized by the availability of a vast amount of semantically annotated data collections. Annotations are provided by exploiting ontologies acting as shared vocabularies. Additionally ontologies are endowed with deductive reasoning capabilities which allow to make explicit knowledge that is formalized implicitly. Along the years a large number of data collections have been developed and interconnected, as testified by the Linked Open Data Cloud. Currently, seminal examples are represented by the numerous Knowledge Graphs (KGs) that have been built, either as enterprise KGs or open KGs, that are freely available. All of them are characterized by very large data volumes, but also incompleteness and noise. These characteristics have made the exploitation of deductive reasoning services less feasible from a practical viewpoint, opening up&nbsp;to alternative solutions, grounded on Machine Learning (ML), for mining knowledge from the vast amount of information available. Actually, ML methods have been exploited in the SW for solving several problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level), and concept leaning. Whilst initially symbol-based solutions have been mostly targeted, recently numeric-based approaches are receiving major attention because of the need to scale on the very large data volumes. Nevertheless, data collections in the SW have peculiarities that can hardly be found in other fields. As such the application of ML methods for solving the targeted problems is not straightforward. This paper extends&nbsp;[20], by surveying the most representative symbol-based and numeric-based solutions and related problems, with a special focus on the main issues that need to be considered and solved when ML methods are adopted in the SW field as well as by analyzing the main peculiarities and drawbacks for each solution.},
booktitle = {Reasoning Web. Declarative Artificial Intelligence : 17th International Summer School 2021, Leuven, Belgium, September 8–15, 2021, Tutorial Lectures},
pages = {76–93},
numpages = {18},
keywords = {Semantic Web, Machine learning, Symbol-based methods, Numeric-based methods},
location = {Leuven, Belgium}
}

@article{10.1109/TNSM.2020.3031333,
author = {Gij\'{o}n, Carolina and Toril, Mat\'{\i}as and Luna-Ram\'{\i}rez, Salvador and Bejarano-Luque, Juan L. and Mar\'{\i}-Altozano, Mar\'{\i}a Luisa},
title = {Estimating Pole Capacity From Radio Network Performance Statistics by Supervised Learning},
year = {2020},
issue_date = {Dec. 2020},
publisher = {IEEE Press},
volume = {17},
number = {4},
issn = {1932-4537},
url = {https://doi.org/10.1109/TNSM.2020.3031333},
doi = {10.1109/TNSM.2020.3031333},
abstract = {Network dimensioning is a critical task for cellular operators to avoid degraded user experience and unnecessary upgrades of network resources with changing mobile traffic patterns. For this purpose, smart network planning tools require accurate cell and user capacity estimates. In these tools, throughput is often used as a capacity metric due to its close relationship with user satisfaction. In this work, a comprehensive analysis is carried out to compare different well-known Supervised Learning (SL) algorithms for estimating cell and user throughput in the DownLink in busy hours from radio measurements collected on a cell basis in the Operation Support System (OSS). The considered SL approaches include random forest, shallow multi-layer perceptron, support vector regression and k-nearest neighbors. Such algorithms are compared with classical multiple linear regression and deep learning approaches considered in previous works. All these algorithms are tested in two radio access technologies: High Speed DownLink Packet Access (HSDPA) and Long Term Evolution (LTE). To this end, two datasets with the most relevant performance indicators per technology are collected from live cellular networks. Results show that non-deep SL algorithms are the most appropriate option for applications with storage constraints, such as network planning tools, since they provide a higher accuracy with reduced datasets.},
journal = {IEEE Trans. on Netw. and Serv. Manag.},
month = dec,
pages = {2090–2101},
numpages = {12}
}

@inproceedings{10.1145/3437963.3441736,
author = {Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
title = {Theoretical Understandings of Product Embedding for E-commerce Machine Learning},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441736},
doi = {10.1145/3437963.3441736},
abstract = {Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {256–264},
numpages = {9},
keywords = {information theory, machine learning theory, product relation, representation learning, sufficient dimension reduction},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@inproceedings{10.1145/3387906.3388618,
author = {Cruz, Daniel and Santana, Amanda and Figueiredo, Eduardo},
title = {Detecting bad smells with machine learning algorithms: an empirical study},
year = {2020},
isbn = {9781450379601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387906.3388618},
doi = {10.1145/3387906.3388618},
abstract = {Bad smells are symptoms of bad design choices implemented on the source code. They are one of the key indicators of technical debts, specifically, design debt. To manage this kind of debt, it is important to be aware of bad smells and refactor them whenever possible. Therefore, several bad smell detection tools and techniques have been proposed over the years. These tools and techniques present different strategies to perform detections. More recently, machine learning algorithms have also been proposed to support bad smell detection. However, we lack empirical evidence on the accuracy and efficiency of these machine learning based techniques. In this paper, we present an evaluation of seven different machine learning algorithms on the task of detecting four types of bad smells. We also provide an analysis of the impact of software metrics for bad smell detection using a unified approach for interpreting the models' decisions. We found that with the right optimization, machine learning algorithms can achieve good performance (F1 score) for two bad smells: God Class (0.86) and Refused Parent Bequest (0.67). We also uncovered which metrics play fundamental roles for detecting each bad smell.},
booktitle = {Proceedings of the 3rd International Conference on Technical Debt},
pages = {31–40},
numpages = {10},
keywords = {bad smells detection, empirical software engineering, machine learning, software measurement, software quality},
location = {Seoul, Republic of Korea},
series = {TechDebt '20}
}

@article{10.1155/2021/6643763,
author = {Chen, Binjie and Wei, Fushan and Gu, Chunxiang and Choo, Kim-Kwang Raymond},
title = {Bitcoin Theft Detection Based on Supervised Machine Learning Algorithms},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/6643763},
doi = {10.1155/2021/6643763},
abstract = {Since its inception, Bitcoin has been subject to numerous thefts due to its enormous economic value. Hackers steal Bitcoin wallet keys to transfer Bitcoin from compromised users, causing huge economic losses to victims. To address the security threat of Bitcoin theft, supervised learning methods were used in this study to detect and provide warnings about Bitcoin theft events. To overcome the shortcomings of the existing work, more comprehensive features of Bitcoin transaction data were extracted, the unbalanced dataset was equalized, and five supervised methods—the k-nearest neighbor (KNN), support vector machine (SVM), random forest (RF), adaptive boosting (AdaBoost), and multi-layer perceptron (MLP) techniques—as well as three unsupervised methods—the local outlier factor (LOF), one-class support vector machine (OCSVM), and Mahalanobis distance-based approach (MDB)—were used for detection. The best performer among these algorithms was the RF algorithm, which achieved recall, precision, and F1 values of 95.9%. The experimental results showed that the designed features are more effective than the currently used ones. The results of the supervised methods were significantly better than those of the unsupervised methods, and the results of the supervised methods could be further improved after equalizing the training set.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {10}
}

@article{10.1007/s10796-020-10056-x,
author = {Alahakoon, Damminda and Nawaratne, Rashmika and Xu, Yan and De Silva, Daswin and Sivarajah, Uthayasankar and Gupta, Bhumika},
title = {Self-Building Artificial Intelligence and Machine Learning to Empower Big Data Analytics in Smart Cities},
year = {2020},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-020-10056-x},
doi = {10.1007/s10796-020-10056-x},
abstract = {The emerging information revolution makes it necessary to manage vast amounts of unstructured data rapidly. As the world is increasingly populated by IoT devices and sensors that can sense their surroundings and communicate with each other, a digital environment has been created with vast volumes of volatile and diverse data. Traditional AI and machine learning techniques designed for deterministic situations are not suitable for such environments. With a large number of parameters required by each device in this digital environment, it is desirable that the AI is able to be adaptive and self-build (i.e. self-structure, self-configure, self-learn), rather than be structurally and parameter-wise pre-defined. This study explores the benefits of self-building AI and machine learning with unsupervised learning for empowering big data analytics for smart city environments. By using the growing self-organizing map, a new suite of self-building AI is proposed. The self-building AI overcomes the limitations of traditional AI and enables data processing in dynamic smart city environments. With cloud computing platforms, the self-building AI can integrate the data analytics applications that currently work in silos. The new paradigm of the self-building AI and its value are demonstrated using the IoT, video surveillance, and action recognition applications.},
journal = {Information Systems Frontiers},
month = aug,
pages = {221–240},
numpages = {20},
keywords = {Big data analytics, Self-building AI, Machine learning, Smart cities, Self-organizing maps}
}

@inproceedings{10.1145/3459104.3459144,
author = {Jan Hagendorfer, Elias},
title = {Knowledge Incorporation for Machine Learning in Condition Monitoring: A Survey},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459144},
doi = {10.1145/3459104.3459144},
abstract = {Model-based condition monitoring (MBCM) solves the inverse problem of inferring a systems state, including possible faults, from sensor observations. Constructing these models in a knowledge-based manner following the laws of physics is hard due to the inverse nature of the problem and unknown fault types. As a result, it has become more attractive to build a model solely from past observations via machine learning (ML). Although highly promising, shortcomings of ML in the scientific domain, including physically inconsistent results and lack of interpretability, became apparent. This led to recent efforts to enhance machine learning with scientific knowledge including a combination of knowledge-based and data-driven modelling, often referred to as hybrid models. The main contributions of this work are: (1) a link of shortcomings of machine learning in CM to a lack of knowledge; (2) a categorization of unique approaches with respect to required knowledge and mechanism of incorporation that have either been applied in condition monitoring or show potential from their application to scientific problems; (3) derivation of promising research directions uncovered as vacant spaces in the categorization.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {230–240},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3375627.3375808,
author = {Leben, Derek},
title = {Normative Principles for Evaluating Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375808},
doi = {10.1145/3375627.3375808},
abstract = {There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {86–92},
numpages = {7},
keywords = {algorithmic decision-making, discrimination, fairness, machine learning, political philosophy},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{10.1007/s11277-021-08879-1,
author = {Hosseini, Soodeh and Fard, Reyhane Hafezi},
title = {Machine Learning Algorithms for Predicting Electricity Consumption of Buildings},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {121},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08879-1},
doi = {10.1007/s11277-021-08879-1},
abstract = {Given that the population is increasing and also energy resources are decreasing, in this study we examine the amount of domestic energy consumption. The purpose of this study is to predict the factors affecting energy consumption in buildings. For this prediction, algorithms of decision tree, random forests and K-nearest neighbors have been used. These algorithms are available in Orange software. In this study, univariate regression algorithm is used to select the best factors. This algorithm identifies the most important factors affecting energy consumption and their impact. The results of this study show that the overall height, roof area, surface and relative compaction have the greatest impact on energy consumption of buildings. The percentage of forecast error for cooling load and heating load are 1.128 and 0.404, respectively. Also, among the tested algorithms, random forest gets the best result.},
journal = {Wirel. Pers. Commun.},
month = dec,
pages = {3329–3341},
numpages = {13},
keywords = {Energy, Building energy consumption, Regression, Decision tree algorithm, K-nearest neighbor algorithm (KNN), Random forest algorithm}
}

@inproceedings{10.1007/978-3-030-84532-2_23,
author = {Alatrany, A. and Hussain, A. and Mustafina, J. and Al-Jumeily, D.},
title = {A Novel Hybrid Machine Learning Approach Using Deep Learning for the Prediction of Alzheimer Disease Using Genome Data},
year = {2021},
isbn = {978-3-030-84531-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-84532-2_23},
doi = {10.1007/978-3-030-84532-2_23},
abstract = {Genome-wide association studies are aimed at identifying associations between commonly occurring variations in a group of individuals and a phonotype, in which the Deoxyribonucleic acid is genotyped in the form of single nucleotide polymorphisms. Despite the exsistence of various research studies for the prediction of chronic diseases using human genome data, more investigations are still required. Machine learning algorithms are widely used for prediction and genome-wide association studies. In this research, Random Forest was utilised for selecting most significant single nucleotide polymorphisms associated to Alzheimer’s Disease. Deep learning model for the prediction of the disease was then developed. Our extesnive similation results indicated that this hybrid model is promising in predicting individuals that suffer from Alzheimer’s disease, achieving area under the curve of 0.9 and 0.93 using Convolutional Neural Network and Multilayer perceptron respectively.},
booktitle = {Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Shenzhen, China, August 12–15, 2021, Proceedings, Part III},
pages = {253–266},
numpages = {14},
keywords = {GWAS, Machine learning, Random forest, CNN, ANN},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3461615.3486575,
author = {Vo\ss{}, Hendric and Wersing, Heiko and Kopp, Stefan},
title = {Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning},
year = {2021},
isbn = {9781450384711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461615.3486575},
doi = {10.1145/3461615.3486575},
abstract = {Detecting mental states of human users is crucial for the development of cooperative and intelligent robots, as it enables the robot to understand the user’s intentions and desires. Despite their importance, it is difficult to obtain a large amount of high quality data for training automatic recognition algorithms as the time and effort required to collect and label such data is prohibitively high. In this paper we present a multimodal machine learning approach for detecting dis-/agreement and confusion states in a human-robot interaction environment, using just a small amount of manually annotated data. We collect a data set by conducting a human-robot interaction study and develop a novel preprocessing pipeline for our machine learning approach. By combining semi-supervised and supervised architectures, we are able to achieve an average F1-score of 81.1% for dis-/agreement detection with a small amount of labeled data and a large unlabeled data set, while simultaneously increasing the robustness of the model compared to the supervised approach.},
booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction},
pages = {317–323},
numpages = {7},
keywords = {agreement - disagreement detection, complex user states, confusion detection, deep learning, neural networks, semi-supervised, supervised, unsupervised},
location = {Montreal, QC, Canada},
series = {ICMI '21 Companion}
}

@inproceedings{10.1145/3313831.3376177,
author = {Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
title = {Understanding and Visualizing Data Iteration in Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376177},
doi = {10.1145/3313831.3376177},
abstract = {Successful machine learning (ML) applications require iterations on both modeling and the underlying data. While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models. We also identify common types of data iterations and associated analysis tasks and challenges. To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions. We present two case studies where developers apply system to their own evolving datasets on production ML projects. Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {data iteration, evolving datasets, interactive interfaces, machine learning iteration, visual analytics},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3485730.3493448,
author = {Toussaint, Wiebke and Mathur, Akhil and Ding, Aaron Yi and Kawsar, Fahim},
title = {Characterising the Role of Pre-Processing Parameters in Audio-based Embedded Machine Learning},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493448},
doi = {10.1145/3485730.3493448},
abstract = {When deploying machine learning (ML) models on embedded and IoT devices, performance encompasses more than an accuracy metric: inference latency, energy consumption, and model fairness are necessary to ensure reliable performance under heterogeneous and resource-constrained operating conditions. To this end, prior research has studied model-centric approaches, such as tuning the hyperparameters of the model during training and later applying model compression techniques to tailor the model to the resource needs of an embedded device. In this paper, we take a data-centric view of embedded ML and study the role that pre-processing parameters in the data pipeline can play in balancing the various performance metrics of an embedded ML system. Through an in-depth case study with audio-based keyword spotting (KWS) models, we show that pre-processing parameter tuning is a remarkable tool that model developers can adopt to trade-off between a model's accuracy, fairness, and system efficiency, as well as to make an embedded ML model resilient to unseen deployment conditions.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {439–445},
numpages = {7},
keywords = {audio keyword spotting, embedded machine learning, fairness, pre-processing parameters},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1007/978-3-030-86044-8_4,
author = {C\'{a}mara, Javier and Silva, Mariana and Garlan, David and Schmerl, Bradley},
title = {Explaining Architectural Design Tradeoff Spaces: A Machine Learning Approach},
year = {2021},
isbn = {978-3-030-86043-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86044-8_4},
doi = {10.1007/978-3-030-86044-8_4},
abstract = {In software design, guaranteeing the correctness of run-time system behavior while achieving an acceptable balance among multiple quality attributes remains a challenging problem. Moreover, providing guarantees about the satisfaction of those requirements when systems are subject to uncertain environments is even more challenging. While recent developments in architectural analysis techniques can assist architects in exploring the satisfaction of quantitative guarantees across the design space, existing approaches are still limited because they do not explicitly link design decisions to satisfaction of quality requirements. Furthermore, the amount of information they yield can be overwhelming to a human designer, making it difficult to distinguish the forest through the trees. In this paper, we present an approach to analyzing architectural design spaces that addresses these limitations and provides a basis to enable the explainability of design tradeoffs. Our approach combines dimensionality reduction techniques employed in machine learning pipelines with quantitative verification to enable architects to understand how design decisions contribute to the satisfaction of strict quantitative guarantees under uncertainty across the design space. Our results show feasibility of the approach in two case studies and evidence that dimensionality reduction is a viable approach to facilitate comprehension of tradeoffs in poorly-understood design spaces.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021, Virtual Event, Sweden, September 13-17, 2021, Proceedings},
pages = {49–65},
numpages = {17},
keywords = {Tradeoff analysis, Uncertainty, Dimensionality reduction}
}

@article{10.1016/j.finel.2021.103572,
author = {Mai, Hau T. and Kang, Joowon and Lee, Jaehong},
title = {A machine learning-based surrogate model for optimization of truss structures with geometrically nonlinear behavior},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {196},
number = {C},
issn = {0168-874X},
url = {https://doi.org/10.1016/j.finel.2021.103572},
doi = {10.1016/j.finel.2021.103572},
journal = {Finite Elem. Anal. Des.},
month = nov,
numpages = {14},
keywords = {Surrogate model, Deep neural network, Neural network, Machine learning, Geometric nonlinear, Truss optimization}
}

@inproceedings{10.5555/3504035.3504515,
author = {Park, Sungrae and Park, JunKeon and Shin, Su-Jin and Moon, Il-Chul},
title = {Adversarial dropout for supervised and semi-supervised learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Recently, training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has improved the generalization performance of neural networks. In contrast to the biased individual inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between 1) the training supervision and 2) the outputs from the network with the dropouts. The identified adversarial dropouts are used to automatically reconfigure the neural network in the training process, and we demonstrated that the simultaneous training on the original and the reconfigured network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST, SVHN, and CIFAR-10. We analyzed the trained model to find the performance improvement reasons. We found that adversarial dropout increases the sparsity of neural networks more than the standard dropout. Finally, we also proved that adversarial dropout is a regularization term with a rank-valued hyper parameter that is different from a continuous-valued parameter to specify the strength of the regularization.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {480},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1504/ijhpcn.2020.113779,
author = {Chouhan, Lokesh and Chauhan, Nancy and Mahapatra, Amitosh Swain and Agarwal, Vidushi},
title = {A survey on the applications of machine learning in wireless sensor networks},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {4},
issn = {1740-0562},
url = {https://doi.org/10.1504/ijhpcn.2020.113779},
doi = {10.1504/ijhpcn.2020.113779},
abstract = {With the dawn of the 21st century and the growth of fast, always available and low power networks, wireless sensor networks are being implemented in diverse use-cases. Wireless sensor networks are being deployed to observe, explore and control the physical world. Wireless sensor networks are generally deployed in dynamic environments. Sensor networks utilise machine learning techniques to avoid the unnecessary redesign of a wireless sensor network deployment for adapting to changing requirements. Machine learning is also used to maximise the security, efficiency, lifetime, and resource utilisation in such networks. In this paper, we present an extensive literature survey of various machine learning applications that are used or are in research to address the operational and non-operational challenges in wireless sensor networks.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jan,
pages = {197–220},
numpages = {23},
keywords = {clustering, data aggregation, machine learning, security, wireless sensor networks}
}

@article{10.1016/j.neucom.2018.11.053,
author = {V. Utkin, Lev},
title = {An imprecise extension of SVM-based machine learning models},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.053},
doi = {10.1016/j.neucom.2018.11.053},
journal = {Neurocomput.},
month = feb,
pages = {18–32},
numpages = {15},
keywords = {Machine learning, Support vector machine, Duality, Classification, Regression, Interval-valued data, Imprecise model}
}

@article{10.1007/s00371-019-01746-y,
author = {Liu, Li and Chen, Siqi and Chen, Xiuxiu and Wang, Tianshi and Zhang, Long},
title = {Fuzzy weighted sparse reconstruction error-steered semi-supervised learning for face recognition},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {8},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-019-01746-y},
doi = {10.1007/s00371-019-01746-y},
abstract = {Since the number of labeled data is limited in the semi-supervised learning settings, we propose a fuzzy weighted sparse reconstruction error-steered semi-supervised learning method for face recognition. The fuzzy membership functions are introduced to the reconstruction error calculation for the unlabeled data. A weight function is utilized to capture the locality property of data when learning the sparse coefficients. The fuzzy weighted sparse reconstruction error-steered semi-supervised learning not only inherits the advantages of sparse representation classification techniques and neighborhood methods, but also steers the reconstruction errors of unlabeled data. Experimental studies on well-known face image datasets demonstrate that the proposed method outperforms the comparative approaches.},
journal = {Vis. Comput.},
month = aug,
pages = {1521–1534},
numpages = {14},
keywords = {Membership function, Sparse representation, Fuzzy}
}

@inproceedings{10.1145/3463858.3463869,
author = {Junior Mele, Umberto and Maria Gambardella, Luca and Montemanni, Roberto},
title = {Machine Learning Approaches for the Traveling Salesman Problem: A Survey},
year = {2021},
isbn = {9781450389921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463858.3463869},
doi = {10.1145/3463858.3463869},
abstract = {Machine Learning techniques have been applied in many contexts with great success. In this survey, we focus on their applications in the Combinatorial Optimization (CO) domain, and in particular to the Traveling Salesman Problem (TSP). We propose an intuitive and simple mind map helpful to navigate through the wide existing literature, indicating the approaches we consider most promising. Different ML techniques introduced to solve the TSP are discussed and reviewed; and their differences and limitations are delved. Open problems for future research in this area are finally highlighted.},
booktitle = {Proceedings of the 2021 8th International Conference on Industrial Engineering and Applications (Europe)},
pages = {182–186},
numpages = {5},
keywords = {Artificial Intelligence, Combinatorial Optimization, Machine Learning, Traveling Salesman Problem},
location = {Barcelona, Spain},
series = {ICIEA 2021-Europe}
}

@phdthesis{10.5555/AAI28652512,
author = {Chen, You-Lin and Biao, Wu, Wei and Mihai, Anitescu, and S, Tsay, Ruey},
advisor = {Mladen, Kolar,},
title = {Stochastic and Online Learning with Applications to Machine Learning and Statistics},
year = {2021},
isbn = {9798460477845},
publisher = {The University of Chicago},
abstract = {This dissertation develops rigorous analyses of stochastic and online learning with applications to modern machine learning models and statistical methodologies. There is no doubt that big data are rapidly expanding nowadays in all fields like science and engineering domains. While the potential of massive data is considerable, in many scenarios of employing machine learning and statistic algorithms to imaging recognizing, natural language processing, and genetic studies, practitioners face many challenges, including high-dimensional data (the feature size is much large than the sample size), over-fitting, discrimination, and hidden confounding. Those problems impose additional difficulties due to over-parameterization, ill-conditioning, non-convex objectives and constraints, and limited power of computation and data storage. Therefore, fully exploiting data's value requires novel learning techniques. In this thesis, we discuss the following topics. First, the theorems of accelerated first-order methods on strongly convex objective functions with the growth condition are derived. The growth condition, which is more realistic but rarely addressed in the literature, states the variance of the stochastic gradients can be dominated by a multiplicative part and an additive part. We show that there exists a trade-off between the convergence rate and robustness for multiplicative noise. Next, the no-regret analysis of online learning for non-linear models is considered. We establish an error control for biased stochastic gradient descent, which leads to a no-regret analysis for the circumstance where we only receive a non-convex approximation of a convex loss function. These results can be applied to a game-theoretical framework for building neural network classifiers with fairness constraints. Finally, canonical correlation analysis with low-rank constraints is studied. We prove algorithmic and statistical properties of two-dimensional canonical correlation analysis under mild conditions of the data generating process and further develop the error bound of using first-order stochastic methods, an effective initialization scheme, and a deflation procedure for extracting multiple canonical components. We believe that our works contribute not only to cutting-edge research in the field of optimization but also to complex and large-scale data analysis.},
note = {AAI28652512}
}

@article{10.1016/j.compbiomed.2020.104027,
author = {Petrovi\'{c}, Nata\v{s}a and Moy\`{a}-Alcover, Gabriel and Jaume-i-Cap\'{o}, Antoni and Gonz\'{a}lez-Hidalgo, Manuel},
title = {Sickle-cell disease diagnosis support selecting the most appropriate machine learning method: Towards a general and interpretable approach for cell morphology analysis from microscopy images},
year = {2020},
issue_date = {Nov 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104027},
doi = {10.1016/j.compbiomed.2020.104027},
journal = {Comput. Biol. Med.},
month = nov,
numpages = {14},
keywords = {Red blood cell, Sickle-cell disease, Microscopy image, Machine learning, Interpretability, Morphology analysis}
}

@inproceedings{10.1145/3338503.3357719,
author = {Tofighi-Shirazi, Ramtine and Asavoae, Irina-Mariuca and Elbaz-Vincent, Philippe and Le, Thanh-Ha},
title = {Defeating Opaque Predicates Statically through Machine Learning and Binary Analysis},
year = {2019},
isbn = {9781450368353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338503.3357719},
doi = {10.1145/3338503.3357719},
abstract = {We present a new approach that bridges binary analysis techniques with machine learning classification for the purpose of providing a static and generic evaluation technique for opaque predicates, regardless of their constructions. We use this technique as a static automated deobfuscation tool to remove the opaque predicates introduced by obfuscation mechanisms. According to our experimental results, our models have up to 98% accuracy at detecting and deobfuscating state-of-the-art opaque predicates patterns. By contrast, the leading edge deobfuscation methods based on symbolic execution show less accuracy mostly due to the SMT solvers constraints and the lack of scalability of dynamic symbolic analyses. Our approach underlines the efficiency of hybrid symbolic analysis and machine learning techniques for a static and generic deobfuscation methodology.},
booktitle = {Proceedings of the 3rd ACM Workshop on Software Protection},
pages = {3–14},
numpages = {12},
keywords = {deobfuscation, machine learning, obfuscation, opaque predicate, software protection, symbolic execution},
location = {London, United Kingdom},
series = {SPRO'19}
}

@inproceedings{10.1145/3460418.3479316,
author = {Hamanaka, Satoki and Sasaki, Wataru and Okoshi, Tadashi and Nakazawa, Jin and Yagasaki, Kaori and Komatsu, Hiroko},
title = {A Comparative Study of CIPN Symptom Estimation Methods Based on Machine Learning},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479316},
doi = {10.1145/3460418.3479316},
abstract = {Chemotherapy-induced peripheral neuropathy (CIPN) is a common side effect of anticancer medicines that causes muscular weakness and falls in cancer patients. Therefore, we compared the methods for identifying and estimating the factors that cause symptoms in CIPN based on factor analysis and four different machine learning algorithms. The results of factor analysis showed that the latent factors causing CIPN can be classified into three categories: “Morning body condition”, “Pain in the legs” and “Dizziness”. We chose models for machine learning based on the premise of multi-task learning and built numerous machine learning models to estimate various CIPN symptoms. According to the results of the comparison studies, TabNet has the best generalization performance.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {117–120},
numpages = {4},
keywords = {chemotherapy-induced peripheral neuropathy, fall detection, machine learning, mobile sensing},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@article{10.1007/s10614-020-10013-5,
author = {Zhang, Jun and Li, Lan and Chen, Wei},
title = {Predicting Stock Price Using Two-Stage Machine Learning Techniques},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {4},
issn = {0927-7099},
url = {https://doi.org/10.1007/s10614-020-10013-5},
doi = {10.1007/s10614-020-10013-5},
abstract = {Stock market forecasting is considered to be a challenging topic among time series forecasting. This study proposes a novel two-stage ensemble machine learning model named SVR-ENANFIS for stock price prediction by combining features of support vector regression (SVR) and ensemble adaptive neuro fuzzy inference system (ENANFIS). In the first stage, the future values of technical indicators are forecasted by SVR. In the second stage, ENANFIS is utilized to forecast the closing price based on prediction results of first stage. Finally, the proposed model SVR-ENANFIS is tested on 4 securities randomly selected from the Shanghai and Shenzhen Stock Exchanges with data collected from 2012 to 2017, and the predictions are completed 1–10, 15 and 30 days in advance. The experimental results show that the proposed model SVR-ENANFIS has superior prediction performance than single-stage model ENANFIS and several two-stage models such as SVR-Linear, SVR-SVR, and SVR-ANN.},
journal = {Comput. Econ.},
month = apr,
pages = {1237–1261},
numpages = {25},
keywords = {Fusion models, Adaptive neuro fuzzy inference system (ANFIS), Stock market, Support vector regression (SVR)}
}

@inproceedings{10.1145/3501409.3501576,
author = {Han, Ao and Zhao, Zhenyu and Feng, Chaochao and Zhang, Shuzheng},
title = {Stage-based Path Delay Prediction with Customized Machine Learning Technique},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501576},
doi = {10.1145/3501409.3501576},
abstract = {Static timing analysis is an important timing analysis technique in the physical design process of integrated circuits, facing the challenge of speed and accuracy trade-off in advanced nodes. Expensive and burdensome path-based analysis (PBA) forces designers to adopt faster graph-based analysis (GBA) in more early flows at the cost of pessimism. Existing work focuses on reducing pessimism but ignores the degree of optimism. In this paper, we propose a stage-based delay model based on machine learning technique with customized loss function to rapidly generate predicted PBA timing results from the pessimistic GBA timing report with considering the asymmetric loss. The model could also enable the designers to identify the false violation path in GBA report with less time cost to reduce the over-design and margin in post-route optimization phase. Experimental results demonstrate that the mean absolute error of predicted PBA slack divergence reduces 66.7%~79.8% compared to GBA-PBA slack divergence (from 17.79ps to 5.92ps and 3.6ps) with about 3X runtime overhead reduction on a 28nm industrial ASIC for each corner. It can also correct about 75.6% false violation paths in GBA timing report.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {926–933},
numpages = {8},
keywords = {Customized machine learning, False violation path calibration, PBA prediction, Static timing analysis},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1016/j.ijinfomgt.2021.102353,
author = {Young, Amber Grace and Majchrzak, Ann and Kane, Gerald C.},
title = {Organizing workers and machine learning tools for a less oppressive workplace},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2021.102353},
doi = {10.1016/j.ijinfomgt.2021.102353},
journal = {Int. J. Inf. Manag.},
month = aug,
numpages = {9},
keywords = {Artificial intelligence, Emancipatory organizing theory, Oppression, Organizing for the future of work, Normative theory, Machine learning}
}

@article{10.1145/3398069,
author = {Thieme, Anja and Belgrave, Danielle and Doherty, Gavin},
title = {Machine Learning in Mental Health: A Systematic Review of the HCI Literature to Support the Development of Effective and Implementable ML Systems},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3398069},
doi = {10.1145/3398069},
abstract = {High prevalence of mental illness and the need for effective mental health care, combined with recent advances in AI, has led to an increase in explorations of how the field of machine learning (ML) can assist in the detection, diagnosis and treatment of mental health problems. ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. Despite the potential opportunities for using ML within mental health, this is an emerging research area, and the development of effective ML-enabled applications that are implementable in practice is bound up with an array of complex, interwoven challenges. Aiming to guide future research and identify new directions for advancing development in this important domain, this article presents an introduction to, and a systematic review of, current ML work regarding psycho-socially based mental health conditions from the computing and HCI literature. A quantitative synthesis and qualitative narrative review of 54 papers that were included in the analysis surfaced common trends, gaps, and challenges in this space. Discussing our findings, we (i) reflect on the current state-of-the-art of ML work for mental health, (ii) provide concrete suggestions for a stronger integration of human-centered and multi-disciplinary approaches in research and development, and (iii) invite more consideration of the potentially far-reaching personal, social, and ethical implications that ML models and interventions can have, if they are to find widespread, successful adoption in real-world mental health contexts.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {34},
numpages = {53},
keywords = {AI applications, Mental health, ethics, health care, interaction design, interpretability, machine learning, mental illness, real-world interventions, society + AI, systematic review}
}

@inproceedings{10.1145/3442188.3445918,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Remi and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {datasets, machine learning, requirements engineering},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{10.1145/3205942,
author = {Fiebrink, Rebecca and Gillies, Marco},
title = {Introduction to the Special Issue on Human-Centered Machine Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3205942},
doi = {10.1145/3205942},
abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {7},
numpages = {7},
keywords = {Human-centered machine learning, interactive machine learning}
}

@inproceedings{10.1145/3394885.3431547,
author = {Dhar, Tonmoy and Poojary, Jitesh and Li, Yaguang and Kunal, Kishor and Madhusudan, Meghna and Sharma, Arvind K. and Manasi, Susmita Dey and Hu, Jiang and Harjani, Ramesh and Sapatnekar, Sachin S.},
title = {Fast and Efficient Constraint Evaluation of Analog Layout Using Machine Learning Models},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431547},
doi = {10.1145/3394885.3431547},
abstract = {Placement algorithms for analog circuits explore numerous layout configurations in their iterative search. To steer these engines towards layouts that meet the electrical constraints on the design, this work develops a fast feasibility predictor to guide the layout engine. The flow first discerns rough bounds on layout parasitics and prunes the feature space. Next, a Latin hypercube sampling technique is used to sample the reduced search space, and the labeled samples are classified by a linear support vector machine (SVM). If necessary, a denser sample set is used for the SVM, or if the constraints are found to be nonlinear, a multilayer perceptron (MLP) is employed. The resulting machine learning model demonstrated to rapidly evaluate candidate placements in a placer, and is used to build layouts for several analog blocks.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {158–163},
numpages = {6},
keywords = {Analog layout, machine learning, performance analysis},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.1016/j.jcp.2019.05.039,
author = {Vesselinov, V.V. and Mudunuru, M.K. and Karra, S. and O'Malley, D. and Alexandrov, B.S.},
title = {Unsupervised machine learning based on non-negative tensor factorization for analyzing reactive-mixing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Academic Press Professional, Inc.},
address = {USA},
volume = {395},
number = {C},
issn = {0021-9991},
url = {https://doi.org/10.1016/j.jcp.2019.05.039},
doi = {10.1016/j.jcp.2019.05.039},
journal = {J. Comput. Phys.},
month = oct,
pages = {85–104},
numpages = {20},
keywords = {Non-negative tensor factorization, Unsupervised machine learning, Structure-preserving feature extraction, Reactive-mixing, Anisotropic dispersion, Non-negative solutions}
}

@article{10.1007/s00158-020-02819-6,
author = {Wu, Rih-Teng and Liu, Ting-Wei and Jahanshahi, Mohammad R. and Semperlotti, Fabio},
title = {Design of one-dimensional acoustic metamaterials using machine learning and cell concatenation},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {5},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-020-02819-6},
doi = {10.1007/s00158-020-02819-6},
abstract = {Metamaterial systems have opened new, unexpected, and exciting paths for the design of acoustic devices that only few years ago were considered completely out of reach. However, the development of an efficient design methodology still remains challenging due to highly intensive search in the design space required by the conventional optimization-based approaches. To address this issue, this study develops two machine learning (ML)-based approaches for the design of one-dimensional periodic and non-periodic metamaterial systems. For periodic metamaterials, a reinforcement learning (RL)-based approach is proposed to design a metamaterial that can achieve user-defined frequency band gaps. This RL-based approach surpasses conventional optimization-based methods in the reduction of computation cost when a near-optimal solution is acceptable. Leveraging the capability of exploration in RL, the proposed approach does not require any training datasets generation and therefore can be deployed for online metamaterial design. For non-periodic metamaterials, a neural network (NN)-based approach capable of learning the behavior of individual material units is presented. By assembling the NN representation of individual material units, a surrogate model of the whole metamaterial is employed to determine the properties of the resulting assembly. Interestingly, the proposed approach is capable of modeling different metamaterial assemblies satisfying user-defined properties while requiring only a one-time network training procedure. Also, the NN-based approach does not need a pre-defined number of material unit cells, and it works when the physical model of the unit cell is not well understood, or the situation where only the sensor measurements of the unit cell are available. The robustness of the proposed two approaches is validated through numerical simulations and design examples.},
journal = {Struct. Multidiscip. Optim.},
month = may,
pages = {2399–2423},
numpages = {25},
keywords = {Acoustic metamaterial, Phononic crystal, Machine learning, Reinforcement learning, Neural network}
}

@article{10.1007/s00146-019-00910-1,
author = {Yalur, Tolga},
title = {Interperforming in AI: question of ‘natural’ in machine learning and recurrent neural networks},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-019-00910-1},
doi = {10.1007/s00146-019-00910-1},
abstract = {This article offers a critical inquiry of contemporary neural network models as an instance of machine learning, from an interdisciplinary perspective of AI studies and performativity. It shows the limits on the architecture of these network systems due to the misemployment of ‘natural’ performance, and it offers ‘context’ as a variable from a performative approach, instead of a constant. The article begins with a brief review of machine learning-based natural language processing systems and continues with a concentration on the relevant model of recurrent neural networks, which is applied in most commercial research such as Facebook AI Research. It demonstrates that the logic of performativity is not brought into account in all recurrent nets, which is an integral part of human performance and languaging, and it argues that recurrent network models, in particular, fail to grasp human performativity. This logic works similarly to the theory of performativity articulated by Jacques Derrida in his critique of John L. Austin’s concept of the performative. Applying Jacques Derrida’s work on performativity, and linguistic traces as spatially organized entities that allow for this notion of performance, the article argues that recurrent nets fall into the trap of taking ‘context’ as a constant, of treating human performance as a ‘natural’ fix to be encoded, instead of performative. Lastly, the article applies its proposal more concretely to the case of Facebook AI Research’s Alice and Bob.},
journal = {AI Soc.},
month = sep,
pages = {737–745},
numpages = {9},
keywords = {Performativity, Machine learning, Natural language processing, Recurrent neural networks, Derrida, Facebook}
}

@article{10.1162/neco_a_01094,
author = {Abarbanel, Henry D. I. and Rozdeba, Paul J. and Shirman, Sasha},
title = {Machine learning: Deepest learning as statistical data assimilation problems},
year = {2018},
issue_date = {August 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {30},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco_a_01094},
doi = {10.1162/neco_a_01094},
abstract = {We formulate an equivalence between machine learning and the formulation of statistical data assimilation as used widely in physical and biological sciences. The correspondence is that layer number in a feedforward artificial network setting is the analog of time in the data assimilation setting. This connection has been noted in the machine learning literature. We add a perspective that expands on how methods from statistical physics and aspects of Lagrangian and Hamiltonian dynamics play a role in how networks can be trained and designed. Within the discussion of this equivalence, we show that adding more layers making the network deeper is analogous to adding temporal resolution in a data assimilation framework. Extending this equivalence to recurrent networks is also discussed.We explore how one can find a candidate for the global minimum of the cost functions in the machine learning context using a method from data assimilation. Calculations on simple models from both sides of the equivalence are reported.Also discussed is a framework in which the time or layer label is taken to be continuous, providing a differential equation, the Euler-Lagrange equation and its boundary conditions, as a necessary condition for a minimum of the cost function. This shows that the problem being solved is a two-point boundary value problem familiar in the discussion of variational methods. The use of continuous layers is denoted "deepest learning."These problems respect a symplectic symmetry in continuous layer phase space. Both Lagrangian versions and Hamiltonian versions of these problems are presented. Their well-studied implementation in a discrete time/layer, while respecting the symplectic structure, is addressed. The Hamiltonian version provides a direct rationale for backpropagation as a solution method for a certain two-point boundary value problem.},
journal = {Neural Comput.},
month = aug,
pages = {2025–2055},
numpages = {31}
}

@article{10.1007/s10844-019-00582-9,
author = {Ram\'{\i}rez, Jaime and Flores, M. Julia},
title = {Machine learning for music genre: multifaceted review and experimentation with audioset},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {55},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-019-00582-9},
doi = {10.1007/s10844-019-00582-9},
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {469–499},
numpages = {31},
keywords = {Machine learning, Datasets, Music information retrieval, Classification algorithms, Music, Feed-forward neural networks}
}

@inproceedings{10.1007/978-3-031-04083-2_4,
author = {Molnar, Christoph and K\"{o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
title = {General Pitfalls of&nbsp;Model-Agnostic Interpretation Methods for&nbsp;Machine Learning Models},
year = {2020},
isbn = {978-3-031-04082-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-04083-2_4},
doi = {10.1007/978-3-031-04083-2_4},
abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
booktitle = {XxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers},
pages = {39–68},
numpages = {30},
keywords = {Interpretable machine learning, Explainable AI},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3461702.3462554,
author = {Segal, Shahar and Adi, Yossi and Pinkas, Benny and Baum, Carsten and Ganesh, Chaya and Keshet, Joseph},
title = {Fairness in the Eyes of the Data: Certifying Machine-Learning Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462554},
doi = {10.1145/3461702.3462554},
abstract = {We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {926–935},
numpages = {10},
keywords = {cryptography, fairness, machine-learning, privacy},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1145/3264745,
author = {Chen, Chaochao and Chang, Kevin Chen-Chuan and Li, Qibing and Zheng, Xiaolin},
title = {Semi-supervised Learning Meets Factorization: Learning to Recommend with Chain Graph Model},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3264745},
doi = {10.1145/3264745},
abstract = {Recently, latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predict missing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which is mainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this article, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field. The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {73},
numpages = {24},
keywords = {Semi-supervised learning, chain graph model, data sparsity, latent factor model}
}

@article{10.3103/S0147688220060076,
author = {Shelmanov, A. O. and Devyatkin, D. A. and Isakov, V. A. and Smirnov, I. V.},
title = {Open Information Extraction from Texts: Part II. Extraction of Semantic Relationships Using Unsupervised Machine Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {47},
number = {6},
issn = {0147-6882},
url = {https://doi.org/10.3103/S0147688220060076},
doi = {10.3103/S0147688220060076},
journal = {Sci. Tech. Inf. Process.},
month = dec,
pages = {340–347},
numpages = {8},
keywords = {open information extraction, semantic relations, unsupervised machine learning, neural networks, autoencoder}
}

@article{10.1016/j.adhoc.2021.102685,
author = {Sharma, Parjanay and Jain, Siddhant and Gupta, Shashank and Chamola, Vinay},
title = {Role of machine learning and deep learning in securing 5G-driven industrial IoT applications},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2021.102685},
doi = {10.1016/j.adhoc.2021.102685},
journal = {Ad Hoc Netw.},
month = dec,
numpages = {34},
keywords = {Industrial internet of things, Security, Machine learning, Deep learning, Artificial intelligence, Block chain, Smart city}
}

@phdthesis{10.5555/AAI28763987,
author = {Wang, Yuyang and de, Veciana, Gustavo and Nuria, Gonz\'{a}lez Prelcic, and Aldebaro, Klautau, and Lili, Qiu,},
advisor = {Jr, Heath, Robert W.,},
title = {Millimeter Wave Vehicular Link Configuration Using Machine Learning},
year = {2020},
isbn = {9798538144792},
publisher = {The University of Texas at Austin},
abstract = {Millimeter-wave (MmWave) vehicular communication enables massive sensor data sharing and various emerging applications related to safety, traffic efficiency and infotainment. Estimating and tracking beams in mmWave vehicular communication, however, is challenging due to the use of large antenna arrays and high mobility in the vehicular context. Fortunately, wireless cellular communication systems have access to vast data resources, which can make beam training more efficient. Data-driven approaches are able to leverage side information and underlying channel statistics to optimize link configuration in mmWave vehicular communication with negligible overhead. indentIn the first part of this dissertation, we develop a situational awareness-aided beam alignment solution using machine learning. Situational awareness, defined as the locations and shapes of the receiver and its surrounding vehicles, can be obtained from sensors to extract environment information and retrieve good beam directions. We formulate mmWave beam selection as a multi-class classification problem, based on hand-crafted features that capture the situational awareness in different coordinates. We provide a comprehensive comparison among the different classification models and various levels of situational awareness. To demonstrate the scalability of the proposed beam selection solution in the large antenna array regime, we propose two solutions to recommend multiple beams and exploit an extra phase of beam sweeping among the recommended beams.In the second part of this dissertation, we develop mmWave vehicular beam alignment solutions with relaxed requirements of connected vehicles and sensor information sharing. The proposed model focuses on designing compressive sensing techniques that leverage the underlying channel angular statistics in site-specific areas using fewer channel measurements. We investigate the problem from an online learning-based approach that optimizes the sensing matrix on the fly and an offline approach that designs the compressive sensing framework using a convolutional neural network. We incorporate hardware constraints of the phased array in the sensing matrix optimization. We investigate structures in frequency-domain channels and propose solutions to optimize power allocated for different subcarriers.  Numerical results show that data-driven approaches can achieve accurate link configuration for mmWave vehicular communication with negligible training overhead.},
note = {AAI28763987}
}

@inproceedings{10.1145/3366424.3383418,
author = {G. Harris, Christopher},
title = {Methods to Evaluate Temporal Cognitive Biases in Machine Learning Prediction Models},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383418},
doi = {10.1145/3366424.3383418},
abstract = {When asked to rank or rate a list of items, humans are often affected by cognitive biases, which may lead to inconsistent decisions over time. These inconsistencies become part of machine learning prediction algorithms trained on human judgments, leading to misalignment and consequently affecting the metrics used to evaluate their correctness. In this paper, we propose new accuracy metrics, built upon commonly used statistics- and decision support-based metrics. Each of these metrics is designed to address the varying nature of human judgment and to evaluate the importance of decisions that change over time due to cognitive biases.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {572–575},
numpages = {4},
keywords = {Data Science, Fairness, Machine Learning, Decision Making, Temporal Evaluation, Cognitive Bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3426020.3426138,
author = {Bhandari, Khadak Singh and Seo, Changho and Cho, Gi Hwan},
title = {Towards Sensor-Cloud Based Efficient Smart Healthcare Monitoring Framework using Machine Learning},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426138},
doi = {10.1145/3426020.3426138},
abstract = {The adaptation of sensor-cloud in healthcare infrastructure has enabled the use of machine learning techniques for efficient healthcare provisioning. In the context of the smart healthcare system, biomedical wireless sensor networks (BWSNs) are one of the key infrastructure enabling the development of healthcare applications and services. With the increasing number of healthcare information collected through BWSN, different types of medical data can be exploited to design a predictive analytics system, thereby transforming the traditional healthcare system. In this paper, we propose and highlight smart healthcare monitoring framework using state-of-the-art technologies. In particular, we focus on sensor-cloud computing and machine learning as emerging technologies, which are suitable for a proactive healthcare system by the advancement in various aspects, including computational capability, data storage, and learning techniques. Besides, we describe the components of our proposed framework with data analysis techniques and sensor-cloud layered architecture.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {380–383},
numpages = {4},
keywords = {Body area network, Healthcare system, Machine learning, Sensor-cloud},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@article{10.1016/j.neucom.2019.08.036,
author = {Zhao, Mingbo and Zhang, Yue and Zhang, Zhao and Liu, Jiao and Kong, Weijian},
title = {ALG: Adaptive low-rank graph regularization for scalable semi-supervised and unsupervised learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.036},
doi = {10.1016/j.neucom.2019.08.036},
journal = {Neurocomput.},
month = dec,
pages = {16–27},
numpages = {12},
keywords = {Semi-supervised learning, Unsupervised learning, Spectral clustering, Adaptive low-rank model}
}

@article{10.1016/j.compind.2021.103510,
author = {Crawford, Bryn and Sourki, Reza and Khayyam, Hamid and S. Milani, Abbas},
title = {A machine learning framework with dataset-knowledgeability pre-assessment and a local decision-boundary crispness score: An industry 4.0-based case study on composite autoclave manufacturing},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2021.103510},
doi = {10.1016/j.compind.2021.103510},
journal = {Comput. Ind.},
month = nov,
numpages = {13},
keywords = {Composite, Industry 4.0, Quality control, Knowledge engineering, Model explainability, Anomaly detection, Machine learning}
}

@inproceedings{10.1007/978-3-030-58520-4_42,
author = {Devaranjan, Jeevan and Kar, Amlan and Fidler, Sanja},
title = {Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation},
year = {2020},
isbn = {978-3-030-58519-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58520-4_42},
doi = {10.1007/978-3-030-58520-4_42},
abstract = {Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII},
pages = {715–733},
numpages = {19},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.image.2019.04.003,
author = {Cui, Yan and Jiang, Jielin and Hu, Zuojin and Jiang, Xiaoyan and Yan, Wuxia and Zhang, Min-ling},
title = {Neighborhood kinship preserving hashing for supervised learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.003},
doi = {10.1016/j.image.2019.04.003},
journal = {Image Commun.},
month = aug,
pages = {31–40},
numpages = {10},
keywords = {Hashing learning, Kinship preserving hashing, Discriminant information, Discriminant hashing, Robust distance metric}
}

@inproceedings{10.1145/3430984.3430999,
author = {Moghe, Ritwik Prashant and Rathee, Sunil and Nayak, Bharath and Adusumilli, Kranthi Mitra},
title = {Machine Learning based Batching Prediction System for Food&nbsp;Delivery},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3430999},
doi = {10.1145/3430984.3430999},
abstract = {Delivery time estimates are an important factor for online food delivery platforms. These platforms also depend on batching - delivering two orders together - to increase efficiency and reduce cost. In this paper we propose a novel system for enhanced delivery time estimates for batched orders. The system is based on multiple machine learning algorithms that work together to make the predictions. We observe that the system leads to an increase in the number of times the food is delivered within the estimated delivery times by about 6%.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {316–322},
numpages = {7},
keywords = {Customer Experience, Deep Learning, Food Delivery, Machine Learning, Zero Inflated Regression},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3447545.3451172,
author = {Nguyen, Minh-Tri and Truong, Hong-Linh},
title = {Demonstration Paper: Monitoring Machine Learning Contracts with QoA4ML},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451172},
doi = {10.1145/3447545.3451172},
abstract = {Using machine learning (ML) services, both service customers and providers need to monitor complex contractual constraints of ML service that are strongly related to ML models and data. Therefore, establishing and monitoring comprehensive ML contracts are crucial in ML serving. This paper demonstrates a set of features and utilities of the QoA4ML framework for ML contracts.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {169–170},
numpages = {2},
keywords = {ML serving, SLO/SLA, service contract, system monitoring},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1016/j.comnet.2021.108217,
author = {Khan, Ammar Ahmed and Khan, Muhammad Mubashir and Khan, Kashif Mehboob and Arshad, Junaid and Ahmad, Farhan},
title = {A blockchain-based decentralized machine learning framework for collaborative intrusion detection within UAVs},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108217},
doi = {10.1016/j.comnet.2021.108217},
journal = {Comput. Netw.},
month = sep,
numpages = {13},
keywords = {Unmanned aerial vehicles, UAV, Blockchain, Machine learning, Decentralized machine learning, Collaborative intrusion detection}
}

@inproceedings{10.1609/aaai.v33i01.33018827,
author = {Peng, Guozhu and Wang, Shangfei},
title = {Dual semi-supervised learning for facial action unit recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33018827},
doi = {10.1609/aaai.v33i01.33018827},
abstract = {Current works on facial action unit (AU) recognition typically require fully AU-labeled training samples. To reduce the reliance on time-consuming manual AU annotations, we propose a novel semi-supervised AU recognition method leveraging two kinds of readily available auxiliary information. The method leverages the dependencies between AUs and expressions as well as the dependencies among AUs, which are caused by facial anatomy and therefore embedded in all facial images, independent on their AU annotation status. The other auxiliary information is facial image synthesis given AUs, the dual task of AU recognition from facial images, and therefore has intrinsic probabilistic connections with AU recognition, regardless of AU annotations. Specifically, we propose a dual semi-supervised generative adversarial network for AU recognition from partially AU-labeled and fully expression-labeled facial images. The proposed network consists of an AU classifier C, an image generator G, and a discriminator D. In addition to minimize the supervised losses of the AU classifier and the face generator for labeled training data, we explore the probabilistic duality between the tasks using adversary learning to force the convergence of the face-AU-expression tuples generated from the AU classifier and the face generator, and the ground-truth distribution in labeled data for all training data. This joint distribution also includes the inherent AU dependencies. Furthermore, we reconstruct the facial image using the output of the AU classifier as the input of the face generator, and create AU labels by feeding the output of the face generator to the AU classifier. We minimize reconstruction losses for all training data, thus exploiting the informative feedback provided by the dual tasks. Within-database and cross-database experiments on three benchmark databases demonstrate the superiority of our method in both AU recognition and face synthesis compared to state-of-the-art works.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1083},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@phdthesis{10.5555/AAI28028890,
author = {Banjade, Huta and Perdew, John and Ruzsinzsky, Adrienn and Carnevale, Vincenzo},
advisor = {Qimin, Yan,},
title = {Machine Learning and Computation: Exploring Structure-property Correlations in Inorganic Crystalline Materials},
year = {2020},
isbn = {9798664751116},
publisher = {Temple University},
address = {USA},
abstract = {Kohn-Sham Density Functional Theory (DFT) has been the most successful tool to probe the electronic structure, mainly the ground-state total energies and densities of many condensed matter systems has led to the development of various databases such as Materials Project (MP), Inorganic Crystal Structure Database (ICSD), and many others. These databases ignited the interest of the material science community towards Machine Learning (ML), leading to the development of a new sub-field in material science called material-informatics, which aims to uncover the interrelation between known features and material properties. ML techniques can handle and identify relationships in complex and arbitrarily high-dimensional spaces data, which are almost impossible for human reasoning. Unlike DFT, the ML approach uses data from past computations or experiments. In many cases, ML models have shown their superiority over DFT in terms of accuracy and efficiency in predicting various physical and chemical properties of materials. The incorporation of material property data obtained from atomistic simulations is crucial important to make continuous progress in data-driven methods. In this direction, we use DFT with Perdew-Burke-Ernzerhof (PBE), and Heyd–Scuseria–Ernzerhof (HSE) functionals, to introduce a family of mono-layer isostructural semiconducting tellurides M2N2Te8, with M = {Ti, Zr, Hf} and N = {Si, Ge}. These compounds have been identified to possess direct band gaps that are tunable from 1.0 eV to 1.3 eV, which are well suited for photonics and optoelectronics applications. Additionally, in-plane transport behavior is observed, and small electron and hole (0.11-015 Me) masses are identified along the dominant transport direction. High carrier mobility is found in these compounds, which shows great promise for applications in high-speed electronic devices. Detailed analysis of electronic structures reveals the presence of metal center bicapped trigonal prism as the structural building blocks in these compounds; a common feature in most of the group V chalcogenides helps to understand the atomic origins of promising properties of this unique class of 2D telluride materials.Atomistic simulations based on DFT theory played a vital role in the development of data-driven materials discovery process. However, the resource-based constraints have limited the high-throughput discovery process by using DFT. The main motivation of our work towards the application of machine learning in material science is to assist the discovery process using available material property data in various databases. Incorporation of physical principles in a network-based machine learning (ML) architecture is a fundamental step toward the continued development of artificial intelligence for materials science and condensed matter physics. In this work, as inspired by the Pauling's rule, we propose that structure motifs (polyhedral formed by cations and surrounding anions) in inorganic crystals can serve as a central input to a machine learning framework for crystalline inorganic materials. We demonstrated that an unsupervised learning algorithm Motif2Vec is able to convert the presence of structural motifs and their connections in a large set of crystalline compounds into unique vectors. The connections among complex materials can be largely determined by the presence of different structural motifs, and their clustering information is identified by our Motif2Vec algorithm. To demonstrate the novel use of structure motif information, we show that a motif-centric learning framework can be effectively created by combining motif information with the recently developed atom-based graph neural networks to form an atom-motif hybrid graph network (AMDNet). Taking advantage of node and edge information on both atomic and motif level, the AMDNet is more accurate than a single graph network in predicting electronic structure related material properties such as band gaps. The work illustrates the route toward the fundamental design of graph neural network learning architecture for complex materials properties by incorporating beyond-atom physical principles. Due to the limitations in resources, it is not feasible to synthesize hundreds of thousands of materials listed in various databases by experiment or compute their detailed properties by using various electronic structure codes and state-of-the-art computational tools. Hence, the identification of an alternative route to screen such databases is very desirable. If identified, this route would be very helpful in reducing the material search space for any application. Categorizing materials based on their structural building blocks is very important to study the underlying physics and to understand the possible mechanisms for any application. Based on structure motifs, we purpose a novel way to categorize, analyze, and visualize the material space called a material network. The connection between any two nodes in this network is determined by using the calculated similarity value (Tanimoto-coeffecient) between each motif and its surrounding information, encoded in terms of a feature vector of length 64. By mapping a known compound, the network thus constructed can be used to screen compounds for the desired application. All the connections of the mapped compound are identified and extracted as a subgraph for further analysis. In our test screening for the transparent conducting oxides (TCO), the proposed network is successful in identifying compounds that are already listed as TCO in the literature. Thus, this indicates its usefulness in reducing the search space for the new TCO materials and various applications. This motif-based material network can serve as an alternate route for functional material discovery and design.},
note = {AAI28028890}
}

@inproceedings{10.5555/3540261.3540696,
author = {Lee, Kookjin and Trask, Nathaniel and Stinis, Panos},
title = {Machine learning structure preserving brackets for forecasting irreversible processes},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either "black-box" or penalty-based approaches.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {435},
numpages = {12},
series = {NIPS '21}
}

@article{10.1016/j.knosys.2020.105479,
author = {Chen, Yiyang and Zhou, Yingwei},
title = {Machine learning based decision making for time varying systems: Parameter estimation and performance optimization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.105479},
doi = {10.1016/j.knosys.2020.105479},
journal = {Know.-Based Syst.},
month = feb,
numpages = {9},
keywords = {Machine learning, Model predictive control, Time varying system}
}

@article{10.1016/j.neunet.2019.09.007,
author = {Hao, Yunzhe and Huang, Xuhui and Dong, Meng and Xu, Bo},
title = {A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {121},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.09.007},
doi = {10.1016/j.neunet.2019.09.007},
journal = {Neural Netw.},
month = jan,
pages = {387–395},
numpages = {9},
keywords = {Spiking neural networks, Dopamine-modulated spike-timing dependent plasticity, Pattern recognition, Supervised learning, Biologically plausibility}
}

@inproceedings{10.1145/3211346.3211348,
author = {Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
title = {Relay: a new IR for machine learning frameworks},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211348},
doi = {10.1145/3211346.3211348},
abstract = {Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {58–68},
numpages = {11},
keywords = {compilers, differentiable programming, intermediate representation, machine learning},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@inproceedings{10.5555/3540261.3540878,
author = {Alabdulmohsin, Ibrahim and Lucic, Mario},
title = {A near-optimal algorithm for debiasing trained machine learning models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {617},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-94437-7_7,
author = {Ihde, Nina and Marten, Paula and Eleliemy, Ahmed and Poerwawinata, Gabrielle and Silva, Pedro and Tolovski, Ilin and Ciorba, Florina M. and Rabl, Tilmann},
title = {A Survey of&nbsp;Big Data, High Performance Computing, and&nbsp;Machine Learning Benchmarks},
year = {2021},
isbn = {978-3-030-94436-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-94437-7_7},
doi = {10.1007/978-3-030-94437-7_7},
abstract = {In recent years, there has been a convergence of Big Data (BD), High Performance Computing (HPC), and Machine Learning (ML) systems. This convergence is due to the increasing complexity of long data analysis pipelines on separated software stacks. With the increasing complexity of data analytics pipelines comes a need to evaluate their systems, in order to make informed decisions about technology selection, sizing and scoping of hardware. While there are many benchmarks for each of these domains, there is no convergence of these efforts. As a first step, it is also necessary to understand how the individual benchmark domains relate.In this work, we analyze some of the most expressive and recent benchmarks of BD, HPC, and ML systems. We propose a taxonomy of those systems based on individual dimensions such as accuracy metrics and common dimensions such as workload type. Moreover, we aim at enabling the usage of our taxonomy in identifying adapted benchmarks for their BD, HPC, and ML systems. Finally, we identify challenges and research directions related to the future of converged BD, HPC, and ML system benchmarking.},
booktitle = {Performance Evaluation and Benchmarking: 13th TPC Technology Conference, TPCTC 2021, Copenhagen, Denmark, August 20, 2021, Revised Selected Papers},
pages = {98–118},
numpages = {21},
keywords = {Benchmarking, Big Data, HPC, Machine Learning},
location = {Copenhagen, Denmark}
}

@article{10.3233/JIFS-189556,
author = {Liu, Tao and Xin, Baogui and Wu, Fan and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Urban green economic planning based on improved genetic algorithm and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189556},
doi = {10.3233/JIFS-189556},
abstract = {There are many factors that need to be considered when planning a city’s green economy, so it is difficult to simulate the planning effect through manual models. In order to improve the effect of urban green economic planning, this paper improves the traditional algorithm and combines the principle of machine learning algorithm to build a model that can be used in urban green economic planning. Moreover, this paper considers the measurement of green economic efficiency from the perspective of input, expected output and undesired output. In addition, this paper compares and analyzes the green efficiency calculated by the SE-SBM model, including horizontal comparison analysis and vertical comparison analysis, and conducts model simulation analysis in combination with data simulation research. Finally, this paper sets the simulation area, combines the data to perform model performance analysis, summarizes the data with statistical analysis methods, and draws charts. The research results show that the model constructed in this paper has a certain effect and can be applied to the design stage of urban green planning.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7309–7322},
numpages = {14},
keywords = {Genetic algorithm, improved algorithm, machine learning, green economy}
}

@inproceedings{10.1145/3359986.3361204,
author = {Allen, Nathan and Raje, Yash and Ro, Jin Woo and Roop, Partha},
title = {A compositional approach for real-time machine learning},
year = {2019},
isbn = {9781450369978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359986.3361204},
doi = {10.1145/3359986.3361204},
abstract = {Cyber-Physical Systems are highly safety critical, especially since they have to provide both functional and timing guarantees. Increasingly, Cyber-Physical Systems such as autonomous vehicles are relying on Artificial Neural Networks in their decision making and this has obvious safety implications. While many formal approaches have been recently developed for ensuring functional correctness of machine learning modules involving Artificial Neural Networks, the issue of timing correctness has received scant attention.This paper proposes a new compiler from the well known Keras Neural Network library to hardware to mitigate the above problem. In the developed approach, we compile networks of Artificial Neural Networks, called Meta Neural Networks, to hardware implementations using a new synchronous semantics for their execution. The developed semantics enables compilation of Meta Neural Networks to a parallel hardware implementation involving limited hardware resources. The developed compiler is semantics driven and guarantees that the generated implementation is deterministic and time predictable. The compiler also provides a better alternative for the realisation of non-linear functions in hardware. Overall, we show that the developed approach is significantly more efficient than a software approach, without the burden of complex algorithms needed for software Worst Case Execution Time analysis.},
booktitle = {Proceedings of the 17th ACM-IEEE International Conference on Formal Methods and Models for System Design},
articleno = {7},
numpages = {5},
keywords = {hardware, machine learning, neural networks, semantics},
location = {La Jolla, California},
series = {MEMOCODE '19}
}

@inproceedings{10.1145/3316781.3323470,
author = {Zizzo, Giulio and Hankin, Chris and Maffeis, Sergio and Jones, Kevin},
title = {Adversarial Machine Learning Beyond the Image Domain},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3323470},
doi = {10.1145/3316781.3323470},
abstract = {Machine learning systems have had enormous success in a wide range of fields from computer vision, natural language processing, and anomaly detection. However, such systems are vulnerable to attackers who can cause deliberate misclassification by introducing small perturbations. With machine learning systems being proposed for cyber attack detection such attackers are cause for serious concern. Despite this the vast majority of adversarial machine learning security research is focused on the image domain. This work gives a brief overview of adversarial machine learning and machine learning used in cyber attack detection and suggests key differences between the traditional image domain of adversarial machine learning and the cyber domain. Finally we show an adversarial machine learning attack on an industrial control system.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {176},
numpages = {4},
keywords = {adversarial machine learning, intrusion detection, neural networks},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@article{10.1016/j.compbiomed.2021.104794,
author = {Liu, Minliang and Liang, Liang and Ismail, Yasmeen and Dong, Hai and Lou, Xiaoying and Iannucci, Glen and Chen, Edward P. and Leshnower, Bradley G. and Elefteriades, John A. and Sun, Wei},
title = {Computation of a probabilistic and anisotropic failure metric on the aortic wall using a machine learning-based surrogate model},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104794},
doi = {10.1016/j.compbiomed.2021.104794},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {11},
keywords = {Failure metric, Surrogate model, Aortic aneurysm, Machine learning, Neural network}
}

@article{10.1007/s00542-020-04888-5,
author = {Kumari, Rashmi and Nigam, Akriti and Pushkar, Shashank},
title = {Machine learning technique for early detection of Alzheimer’s disease},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {12},
issn = {0946-7076},
url = {https://doi.org/10.1007/s00542-020-04888-5},
doi = {10.1007/s00542-020-04888-5},
abstract = {Alzheimer’s disease (AD) is non-repairable brain disorder which impacts a person’s thinking along with shrinking the size of the brain, ultimately resulting in the death of the patient. It is necessary for the treatment of initial stages in AD so that the further degeneration could be delayed. This diagnosis can be achieved with the application of machine learning techniques which employ various optimization and probabilistic techniques. Hence with an objective of distinguishing people with normal brain ageing from those who would develop Alzheimer’s disease, this paper presents an effective machine learning model that successfully diagnosed AD, cMCI, ncMCI and CN which are being detected during pre-stages by itself.},
journal = {Microsyst. Technol.},
month = dec,
pages = {3935–3944},
numpages = {10}
}

@inproceedings{10.1145/3356401.3356409,
author = {Mai, Tieu Long and Navet, Nicolas and Migge, J\"{o}rn},
title = {On the use of supervised machine learning for assessing schedulability: application to ethernet TSN},
year = {2019},
isbn = {9781450372237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356401.3356409},
doi = {10.1145/3356401.3356409},
abstract = {In this work, we ask if Machine Learning (ML) can provide a viable alternative to conventional schedulability analysis to determine whether a real-time Ethernet network meets a set of timing constraints. Otherwise said, can an algorithm learn what makes it difficult for a system to be feasible and predict whether a configuration will be feasible without executing a schedulability analysis? To get insights into this question, we apply a standard supervised ML technique, k-nearest neighbors (k-NN), and compare its accuracy and running times against precise and approximate schedulability analyses developed in Network-Calculus. The experiments consider different TSN scheduling solutions based on priority levels combined for one of them with traffic shaping. The results obtained on an automotive network topology suggest that k-NN is efficient at predicting the feasibility of realistic TSN networks, with an accuracy ranging from 91.8% to 95.9% depending on the exact TSN scheduling mechanism and a speedup of 190 over schedulability analysis for 106 configurations. Unlike schedulability analysis, ML leads however to a certain rate "false positives" (i.e., configurations deemed feasible while they are not). Nonetheless ML-based feasibility assessment techniques offer new trade-offs between accuracy and computation time that are especially interesting in contexts such as design-space exploration where false positives can be tolerated during the exploration process.},
booktitle = {Proceedings of the 27th International Conference on Real-Time Networks and Systems},
pages = {143–153},
numpages = {11},
keywords = {machine learning, schedulability analysis, time sensitive networking (TSN), timing verification},
location = {Toulouse, France},
series = {RTNS '19}
}

@inproceedings{10.1007/978-3-030-27192-3_17,
author = {Audah, M. Z. Fatimah and Chin, Tan Saw and Zulfadzli, Y. and Lee, C. K. and Rizaluddin, K.},
title = {Towards Efficient and Scalable Machine Learning-Based QoS Traffic Classification in Software-Defined Network},
year = {2019},
isbn = {978-3-030-27191-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27192-3_17},
doi = {10.1007/978-3-030-27192-3_17},
abstract = {Internet Service Provider (ISP) has the responsibility to fulfill the Quality of Service (QoS) of various types of applications. The centralized network controller in Software Defined Networking (SDN) provides the chance to instil intelligence in managing network resources based on QoS requirements. A fined-grained QoS Traffic Engineering can be realized by identifying different traffic flow types and categorizing them according to various application/classes. Previous methods include port-based classification and Deep Packet Inspection (DPI), which have been found non-accurate and highly computational. Thus, machine learning (ML) based traffic classifier has gained much attention from the research community, which can be seen from an increase number of works being published. This paper identifies the issues in ML-based traffic classification (TC) in order to devised the best solution; i.e. the TC framework should be scalable to accommodate network expansion, can accurately identify flows according to their source applications/classes, while maintaining an efficient run-time and memory requirement. Therefore, based on these findings, this work proposed a TC engine comprises of Training and Feature Selection Module and Classifier Model, which is placed at the data plane. The training and feature selection will be done offline and regularly to keep the Classifier Model updated. In the proposed solution, the SDN switch forwards the packets the Classifier Model, which classify the packets with accurate applications and send them to the control plane. Finally, the controller will perform resource and queue management according to the labeled packets and updates the flow tables via the switch. The proposed solution will be the starting point in solving efficiency and scalability issues in SDN-ISP TC.},
booktitle = {Mobile Web and Intelligent Information Systems: 16th International Conference, MobiWIS 2019, Istanbul, Turkey, August 26–28, 2019, Proceedings},
pages = {217–229},
numpages = {13},
keywords = {Traffic Classification, Software Defined Networking, Machine Learning},
location = {Istanbul, Turkey}
}

@article{10.1155/2021/6511552,
author = {Chen, Qiaoshan and Cai, Shousong and Gu, Xiaomin and Gupta, Punit},
title = {Construction of the Luxury Marketing Model Based on Machine Learning Classification Algorithm},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6511552},
doi = {10.1155/2021/6511552},
abstract = {China has become the world’s largest luxury goods consumer market due to its population base. In view of the bright prospects of the luxury consumer market, major companies have entered and want to get a share. For the luxury goods industry, traditional mass marketing methods are not able to serve corporate sales and marketing strategies more effectively, and targeted marketing is clearly much more efficient than randomized marketing. Therefore, in this paper, based on consumer buying habits and characteristics data of luxury goods, the paper uses a machine learning algorithm to build a personalized marketing strategy model. And the paper uses historical data to model and form deductions to predict the purchase demand of each consumer and evaluate the possibility of customers buying different goods, including cosmetics, jewelry, and clothing.},
journal = {Sci. Program.},
month = jan,
numpages = {11}
}

@inproceedings{10.1145/3387940.3391489,
author = {Schumacher, Max Eric Henry and Le, Kim Tuyen and Andrzejak, Artur},
title = {Improving Code Recommendations by Combining Neural and Classical Machine Learning Approaches},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391489},
doi = {10.1145/3387940.3391489},
abstract = {Code recommendation systems for software engineering are designed to accelerate the development of large software projects. A classical example is code completion or next token prediction offered by modern integrated development environments. A particular challenging case for such systems are dynamic languages like Python due to limited type information at editing time. Recently, researchers proposed machine learning approaches to address this challenge. In particular, the Probabilistic Higher Order Grammar technique (Bielik et al., ICML 2016) uses a grammar-based approach with a classical machine learning schema to exploit local context. A method by Li et al., (IJCAI 2018) uses deep learning methods, in detail a Recurrent Neural Network coupled with a Pointer Network. We compare these two approaches quantitatively on a large corpus of Python files from GitHub. We also propose a combination of both approaches, where a neural network decides which schema to use for each prediction. The proposed method achieves a slightly better accuracy than either of the systems alone. This demonstrates the potential of ensemble-like methods for code completion and recommendation tasks in dynamically typed languages.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {476–482},
numpages = {7},
keywords = {code recommendations, machine learning, neural networks},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1007/s00521-021-05820-2,
author = {Mastoi, Qurat-ul-ain and Memon, Muhammad Suleman and Lakhan, Abdullah and Mohammed, Mazin Abed and Qabulio, Mumtaz and Al-Turjman, Fadi and Abdulkareem, Karrar Hameed},
title = {Machine learning-data mining integrated approach for premature ventricular contraction prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {18},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05820-2},
doi = {10.1007/s00521-021-05820-2},
abstract = {Cardiac arrhythmias impose a significant burden on the healthcare environment due to the increasing ratio of mortality worldwide. Arrhythmia and abnormal ECG heartbeat are the possible symptoms of severe heart diseases that can lead to death. Premature ventricular contraction (PVC) is a common form of cardiac arrhythmia which begins from the lower chamber of the heart, and frequent occurrence of PVC beat might lead to mortality. ECG signals are the noninvasive and primary tool used to identify the actual life threat related to the heart. Nowadays, in society, the computer-assisted technique reduces doctors' burden to evaluate heart disease and heart arrhythmia automatically. Regardless of well-equipped and well-developed health facilities that are available for monitoring the cardiac condition, the success stories are yet unsatisfactorily due to the complexity of the cardiac disorder. The most challenging part in ECG signal analysis is to extract the accurate features relevant to the arrhythmia for classification due to the inter-patient variation. There are many morphological changes present in the ECG signals. Hence, there is a gap in the usage of appropriate methods for the extraction of features and classification models, which reduce the biased diagnosis of PVC arrhythmia. To predict PVC arrhythmia accurately is a quite challenging task owing to (a) QRS negative (b) long compensatory pause (c) p-wave (d) biased diagnosis of PVC detection due to the small feature set. This study presents a new approach for PVC prediction using derived predictor variables from the electrocardiograph (ECG-MLII) signals: R–R wave interval, previous R–R wave interval, QRS duration, and verification of P-wave whether it is present or absent using threshold technique. We propose the machine learning-data mining MACDM integrated approach using five different models of multiple logistic regression and four classifiers, namely, Random Forest (RF), K-Nearest Neighbor (KNN), Support vector machine (SVM), and Na\"{\i}ve Bayes (NB). The experiment was conducted on the public benchmark MIT-BIH-AR to evaluate the performance of our proposed MACDM technique. The multiple logistic regression models constructed as a function of all independent variables achieved an accuracy of 99.96%, sensitivity 98.9%, specificity 99.20%, PPV 99.25%, and Youden's index parameter 98.24%. Thus, it is proved that this computer-aided method helps our medical practitioners improve the efficiency of their services.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {11703–11719},
numpages = {17},
keywords = {Premature ventricular contraction, PVC prediction, ECG, Heart, Multiple logistic regression, Machine learning, Data mining}
}

@phdthesis{10.5555/AAI28714415,
author = {Li, Yanying and TH, Han, Tian and RL, Liu, Rong and YN, Ning, Yue},
advisor = {HW, Wang, Hui},
title = {Fair Machine Learning over Crowdsourced and Graph-Structured Data},
year = {2021},
isbn = {9798544292791},
publisher = {Stevens Institute of Technology},
address = {USA},
abstract = {Fair machine learning has gained considerable interests in recent years, and has been widely adapted in every aspect of life. A multitude of formal, mathematical definitions of  fairness in machine learning has been proposed in the last few years. Recently many fairness-enhancing machine learning methods have been designed to deal with various applications and scenarios. However, the research of fairness in learning over crowdsourced data and graph-structured data is largely limited. The goal of my thesis is to study algorithmic fairness in the contexts of crowd-powered data analytics and graph analytics. We have made the following research achievements. First, we focus on fair truth discovery for crowdsourcing systems. One important research problem for crowdsourcing systems is truth discovery, which aims to aggregate noisy answers contributed by the workers to obtain the correct answer (truth) of each task. However, since the collected answers are highly prone to the workers' bias, aggregating these biased answers without proper treatment will unavoidably lead to discriminatory truth discovery results. In this thesis, we address the fairness issue for truth discovery from biased crowdsourced answers. We design new algorithms that incorporate truth discovery with fairness. The algorithms estimate both worker bias and truth iteratively, and dynamically select bias to be removed from the answers during truth inference. We perform an extensive set of experiments on both synthetic and real-world crowdsourcing datasets. Experimental results demonstrate the effectiveness of our mitigation algorithms.Second, we focus on the fair classification problem under the circumstance of using applicants' social information in peer-to-peer lending. Peer-to-peer (P2P) lending marketplaces on the Web have been growing over the last decade. Since the applicants on P2P lending platforms may lack sufficient financial history for assessment, quite a few P2P lending service providers have been utilizing the applicants' social relationships to improve the risk prediction accuracy of loan applications. However, utilizing the information of applicants' social relationships may introduce discrimination in lending decisions. In this thesis, we analyze and evaluate the impact of users'  social relationships on the fairness of risk prediction for P2P lending. We consider two types of fairness notions in the literature, namely individual fairness and counterfactual fairness, and design two new algorithms that mitigate bias by generalizing social features for both fairness metrics. Experimental results show that our mitigation algorithms can reduce bias while utilizing social relationships effectively. Third, we focus on fair link prediction on social networks. Link prediction has been widely applied in social network analysis. Despite its importance, link prediction algorithms can be biased by disfavoring the links between individuals in particular demographic groups. We study one particular type of bias, namely, the bias in predicting inter-group links (i.e., the links across different demographic groups), on social network graphs. First, we formalize the definition of bias in link prediction by providing quantitative measurements of accuracy disparity, which measures the difference in prediction accuracy of inter-group and intra-group links. Second, by extensive empirical studies over real-world datasets, we unveil the existence of bias in  five existing state-of-the-art link prediction algorithms. Third, we identify one of the underlying causes of bias in link prediction as the imbalanced density across intra-group and inter-group edges in the training graph. Based on the identified cause, we design a pre-processing bias mitigation method named FairLP that modifies the training graph to balance the distribution of intra-group and inter-group links. FairLP is model-agnostic and thus is compatible with any existing link prediction algorithm. Our experimental results on real-world social network graph data demonstrate that FairLP achieves better trade-off between fairness and prediction accuracy compared with the existing fairness-enhancing link prediction methods.  Fourth, we focus on fair and privacy-preserving heterogeneous graph embedding. Due to the popularity of heterogeneous graphs (HGs) in real-world scenarios, HG embeddings have drawn considerable attentions in recent years. Despite the significant efforts on developing embedding models, two important issues are largely ignored. First, as machine learning algorithms may be systematically discriminated against particular group of individuals, it is important to ensure that HG embeddings are bias-free for downstream tasks. Second, as HG embeddings are typically shared among multiple parties instead of the original graphs, it is vital to ensure that these embeddings are privacy-preserving. Both fairness and privacy are equally important. In this thesis, we address the trade-off between fairness, privacy, and utility of HG embeddings. Regarding fairness, we consider compositional fairness that specifies multiple fairness constraints on different node types in HGs. Regarding privacy, we consider differential privacy, the privacy standard for data analysis. Meeting both requirements of compositional fairness and differential privacy may lead to significant utility loss of embeddings. To address this challenge, we design the Compositionally Fair, Privacy-preserving Graph Embedding (CoPE) algorithm which consists of two components: (1) the one-discriminator encoder (ODE) that realizes compositional fairness; and (2) the privacy enhancer that equips ODE with differential privacy. Our experiment results on two real-world HG datasets show that the utility of the embeddings learned by systemc are much better than the existing work, especially when strong privacy is required and the number of fairness constraints is larger than two. Besides the issue of algorithmic fairness in machine learning, we consider data acquisition on online data marketplaces. We design DANCE, a middleware that provides the desired data acquisition service. DANCE searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.},
note = {AAI28714415}
}

@inproceedings{10.5555/3045390.3045589,
author = {Bai, Qinxun and Rosenberg, Steven and Wu, Zheng and Sclaroff, Stan},
title = {Differential geometric regularization for supervised learning of classifiers},
year = {2016},
publisher = {JMLR.org},
abstract = {We study the problem of supervised learning for both binary and multiclass classification from a unified geometric perspective. In particular, we propose a geometric regularization technique to find the submanifold corresponding to an estimator of the class probability P(y|x). The regularization term measures the volume of this sub-manifold, based on the intuition that overfitting produces rapid local oscillations and hence large volume of the estimator. This technique can be applied to regularize any classification function that satisfies two requirements: firstly, an estimator of the class probability can be obtained; secondly, first and second derivatives of the class probability estimator can be calculated. In experiments, we apply our regularization technique to standard loss functions for classification, our RBF-based implementation compares favorably to widely used regularization methods for both binary and multiclass classification.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1879–1888},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1007/s11277-021-08283-9,
author = {Mishra, Kamta Nath and Pandey, Subhash Chandra},
title = {Fraud Prediction in Smart Societies Using Logistic Regression and k-fold Machine Learning Techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {119},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08283-9},
doi = {10.1007/s11277-021-08283-9},
abstract = {The credit/debit card deceit detection is an enormously difficult task. However, it is a well known problem of our cloud based mobile internet society and it must be solved by technocrats in the welfare of societal mental harassments. The main problem in executing credit/debit card fraud detection technique is the availability of limited amount of fraud related data like transaction amount, transaction date, transaction time, address, and vendor category code related to the frauds. It is the truth of mobile internet world that there are billions of potential places and e-commerce websites where a credit/debit card can be used by fraudulent people for online transactions and payments which make it exceedingly thorny to trace the pattern of frauds. Moreover, the problem of fraud detection in cloud— Internet of Things (IoT) based smart societies has numerous constraints like continuous change in the behavior of normal and fraudulent persons, the fraudulent people try to develop and use new method for executing frauds, and very little availability of frauds related bench mark data sets. In this research article, the authors have presented logistic regression based k-fold machine learning technique (MLT) for fraud detection and prevention in cloud-IoT based smart societal environment. The k-fold method creates multiple folds of bank transactions related data before implementing logistic regression and MLT. The logistic regression performs logic based regression analysis and the intelligent machine learning approach performs registration, classification, clustering, dimensionality reduction, deep learning, training, and reinforcement learning steps on the received bank transactions data. The implementation of proposed methodology and its further analysis using intelligent machine learning tools like ROC (Receiver Operating Characteristic) curve, confusion matrix, mean-recall score value, and precision recall curves for European banks day-to-day transactions related bench mark data set reveal that the proposed methodology is efficient, accurate, and reliable for detecting frauds in cloud-IoT based smart societal environment.},
journal = {Wirel. Pers. Commun.},
month = jul,
pages = {1341–1367},
numpages = {27},
keywords = {Cloud-IoT based distributed environment, Confusion matrix, Fraud detection, Fraudulent, Logistic regression, Machine learning, Mean-recall-score, ROC curve}
}

@inproceedings{10.5555/3157382.3157469,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of opportunity in supervised learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/3306127.3332143,
author = {Strobel, Martin},
title = {Aspects of Transparency in Machine Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The fact that machine learning is growing more and more entrenched in almost every aspect of society, combined with the opacity of various of its algorithms has induced the relatively young research area of transparent machine learning. The aim of this domain is to provide explanations for automated decisions to increase public trust. In my thesis, I am going to consider certain problems that arise from this research agenda. Particularly, I so far considered, the dilemma of conflicting explanations and the issue of privacy concerns arising from transparency.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2449–2451},
numpages = {3},
keywords = {cooperative games: theory &amp; analysis, game theory for practical applications, social choice theory, transparent machine learning, values in mas (privacy, safety, security, transparency, $dots$},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.3103/S0146411621060043,
author = {Gochhait, S. and Butt, Sh. Aziz and De-La-Hoz-Franco, E. and Shaheen, Q. and Luis, D. M. Jorge and Pi\~{n}eres-Espitia, G. and Mercado-Polo, D.},
title = {A Machine Learning Solution for Bed Occupancy Issue for Smart Healthcare Sector},
year = {2021},
issue_date = {Nov 2021},
publisher = {Allerton Press, Inc.},
address = {USA},
volume = {55},
number = {6},
issn = {0146-4116},
url = {https://doi.org/10.3103/S0146411621060043},
doi = {10.3103/S0146411621060043},
journal = {Autom. Control Comput. Sci.},
month = nov,
pages = {546–556},
numpages = {11},
keywords = {healthcare, machine learning, bed occupancy rate, algorithms}
}

@phdthesis{10.5555/AAI28860524,
author = {Lin, Fanghua and Gonzalo, Arce, and David, Edwards, and David, Stockman, and Xiao, Fang, and Mei, Wang,},
advisor = {Paul, Laux,},
title = {Corporate Leverage in the US: Economic Explanation and Machine Learning Exploration},
year = {2021},
isbn = {9798780624196},
publisher = {University of Delaware},
address = {USA},
abstract = {This dissertation studies corporate leverage in the United States from the dual perspectives of economic explanation and machine learning prediction. Corporate leverage in our study is defined as the ratio of debt to assets. Our economic study proposes and investigates an explanation for the coexistence of two leverage patterns across US firms: low (or even zero) leverage for some firms and also some firms with very high leverage, even as the mean leverage ratio remains fairly stable over time. These patterns of the cross-sectional distribution of leverage have become even more pronounced in recent years. The explanation we propose centers on production flexibility, defined as the ability of firms' to quickly adjust their output in response to demand conditions. Production flexibility is found to have increased in recent years, and is a companion pattern to the low-leverage pattern and the increasing high-leverage pattern. We posit that the combined influence of production flexibility along with the state of a firm's financial constraints and its lenders' concerns about being taken advantage of by managers or shareholders leads to a plausible explanation for the  patterns in leverage. The core idea is that when financial constraints do not exist, high production flexibility indicates a low operation risk, which enables firms to take a high financial risk from debt as a substitute. Furthermore, firms where agency problem concerns are not so great can easily benefit from production flexibility to achieve growth, and the growth leads to lower leverage. However, for firms where agency problems are of greater concern to lenders, the production flexibility might be used to the detriment of lenders, leading to a positive relationship between leverage and production flexibility in the cross-section. Our model predicts that, as production flexibility has increased over the decades, we should observe a stronger low-leverage pattern for firms with less serious agency problems, and we should observe growing leverage for firms with more serious agency problems in extreme cases. We conduct an empirical investigation of these issues, and find results consistent with this reasoning, especially for firms listed in Nasdaq. We do not propose that growing production flexibility is the only reason for the leverage patterns we investigate, but we believe the reasoning and evidence suggests it is part of the explanation. Secondly, we conduct a study from a machine learning perspective. Our empirical finance study is conducted using standard panel-data regressions methods. Such methods typically involve controlling for a large set of unexplained ``fixed effects" in the cross-section, leaving only part of the variation to be explained by economic factors. The fixed effects are, mechanically, dummy variables whose coefficients need to be estimated. In the context of relatively short time series relative to the breadth of the cross-section, panel-data regressions are not usually considered to be useful for prediction. For prediction purposes, we reason that it may be useful to avoid using the information in the data to estimate pre-specified fixed effects, but instead to introduce clustering dummies only as needed in the model training process sequentially. The training algorithm we suggest is a variation on the traditional machine learning method "Gradient Boosting Machine" (GBM for short) and is called "PanelGBM" in our study. This approach relaxes assumptions of linearity which are inherent in a standard panel-data regression approach as well as other assumptions regarding clusters and data in other machine learning models that are designed for panel data analysis. Via a simulation study, we establish that our algorithm provides good accuracy and stability relative to other prediction methods for a variety of underlying true models and these clustering dummy variables that are learned in the model training process help improving model performance significantly. The algorithm could have applications in a variety of settings; in the context of this dissertation, we apply the algorithm to predict leverage for US firms and find that it performs much better than other models. Even a linear model with the clustering dummies learned from the PanelGBM performs much better than traditional panel regression model.},
note = {AAI28860524}
}

@phdthesis{10.5555/AAI28027596,
author = {Shah Mohammadi, Fatemeh and Ptucha, Raymond and Maywar, Drew and Markopoulos, Panos and Cahill, Nathan and Hensel, Edward},
advisor = {Andres, Kwasinski,},
title = {Machine Learning-enabled Resource Allocation for Underlay Cognitive Radio Networks},
year = {2020},
isbn = {9798664705553},
publisher = {Rochester Institute of Technology},
abstract = {Due to the rapid growth of new wireless communication services and applications, much attention has been directed to frequency spectrum resources and the way they are regulated. Considering that the radio spectrum is a natural limited resource, supporting the ever increasing demands for higher capacity and higher data rates for diverse sets of users, services and applications is a challenging task which requires innovative technologies capable of providing new ways of efficiently exploiting the available radio spectrum. Consequently, dynamic spectrum access (DSA) has been proposed as a replacement for static spectrum allocation policies. The DSA is implemented in three modes including interweave, overlay and underlay mode [1]. The key enabling technology for DSA is cognitive radio (CR), which is among the core prominent technologies for the next generation of wireless communication systems. Unlike conventional radio which is restricted to only operate in designated spectrum bands, a CR has the capability to operate in different spectrum bands owing to its ability in sensing, understanding its wireless environment, learning from past experiences and proactively changing the transmission parameters as needed. These features for CR are provided by an intelligent software package called the cognitive engine (CE). In general, the CE manages radio resources to accomplish cognitive functionalities and allocates and adapts the radio resources to optimize the performance of the network. Cognitive functionality of the CE can be achieved by leveraging machine learning techniques. Therefore, this thesis explores the application of two machine learning techniques in enabling the cognition capability of CE. The two considered machine learning techniques are neural network-based supervised learning and reinforcement learning. Specifically, this thesis develops resource allocation algorithms that leverage the use of machine learning techniques to find the solution to the resource allocation problem for heterogeneous underlay cognitive radio networks (CRNs). The proposed algorithms are evaluated under extensive simulation runs.The first resource allocation algorithm uses a neural network-based learning paradigm to present a fully autonomous and distributed underlay DSA scheme where each CR operates based on predicting its transmission effect on a primary network (PN). The scheme is based on a CE with an artificial neural network that predicts the adaptive modulation and coding configuration for the primary link nearest to a transmitting CR, without exchanging information between primary and secondary networks. By managing the effect of the secondary network (SN) on the primary network, the presented technique maintains the relative average throughput change in the primary network within a prescribed maximum value, while also finding transmit settings for the CRs that result in throughput as large as allowed by the primary network interference limit. The second resource allocation algorithm uses reinforcement learning and aims at distributively maximizing the average quality of experience (QoE) across transmission of CRs with different types of traffic while satisfying a primary network interference constraint. To best satisfy the QoE requirements of the delay-sensitive type of traffics, a cross-layer resource allocation algorithm is derived and its performance is compared against a physical-layer algorithm in terms of meeting end-to-end traffic delay constraints. Moreover, to accelerate the learning performance of the presented algorithms, the idea of transfer learning is integrated. The philosophy behind transfer learning is to allow well-established and expert cognitive agents (i.e. base stations or mobile stations in the context of wireless communications) to teach newly activated and naive agents. Exchange of learned information is used to improve the learning performance of a distributed CR network. This thesis further identifies the best practices to transfer knowledge between CRs so as to reduce the communication overhead.    The investigations in this thesis propose a novel technique which is able to accurately predict the modulation scheme and channel coding rate used in a primary link without the need to exchange information between the two networks (e.g. access to feedback channels), while succeeding in the main goal of determining the transmit power of the CRs such that the interference they create remains below the maximum threshold that the primary network can sustain with minimal effect on the average throughput. The investigations in this thesis also provide a physical-layer as well as a cross-layer machine learning-based algorithms to address the challenge of resource allocation in underlay cognitive radio networks, resulting in better learning performance and reduced communication overhead.},
note = {AAI28027596}
}

@inproceedings{10.1007/978-3-030-53552-0_15,
author = {Lucas, Flavien and Billot, Romain and Sevaux, Marc and S\"{o}rensen, Kenneth},
title = {Reducing Space Search in Combinatorial Optimization Using Machine Learning Tools},
year = {2020},
isbn = {978-3-030-53551-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-53552-0_15},
doi = {10.1007/978-3-030-53552-0_15},
abstract = {A new metaheuristic, called Feature-Guided MNS (FG-MNS) is proposed, combining well-known local search with simple machine learning techniques. In this metaheuristic, a solution is represented by features (mean depth of each route, standard deviation of the length of each route, etc.). The solver uses decision trees to define promising areas in the features space. The search is mainly focused on the promising areas, in order to minimize the exploration time, and to improve the quality of the found solutions. Additional neighborhoods, guided by the features are proposed.},
booktitle = {Learning and Intelligent Optimization: 14th International Conference, LION 14, Athens, Greece, May 24–28, 2020, Revised Selected Papers},
pages = {143–150},
numpages = {8},
keywords = {Metaheuristic, Machine learning, Data mining, Vehicle routing},
location = {Athens, Greece}
}

@article{10.1145/3466826.3466835,
author = {Eduardo A. Sousa, Jose and Oliveira, Vinicius C. and Almeida Valadares, Julia and Borges Vieira, Alex and Bernardino, Heder S. and Moraes Villela, Saulo and Dias Goncalves, Glauber},
title = {Fighting Under-price DoS Attack in Ethereum with Machine Learning Techniques},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3466826.3466835},
doi = {10.1145/3466826.3466835},
abstract = {Ethereum is one of the most popular cryptocurrency currently and it has been facing security threats and attacks. As a consequence, Ethereum users may experience long periods to validate transactions. Despite the maintenance on the Ethereum mechanisms, there are still indications that it remains susceptible to a sort of attacks. In this work, we analyze the Ethereum network behavior during an under-priced DoS attack, where malicious users try to perform denial-of-service attacks that exploit flaws in the fee mechanism of this cryptocurrency. We propose the application of machine learning techniques and ensemble methods to detect this attack, using the available transaction attributes. The proposals present notable performance as the Decision Tree models, with AUC-ROC, F-score and recall larger than 0.94, 0.82, and 0.98, respectively.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = may,
pages = {24–27},
numpages = {4},
keywords = {attacks, blockchain, dos, ethereum, machine learning}
}

@article{10.1007/s00371-021-02287-z,
author = {Tian, Ye and Zhang, Liguo and Sun, Jianguo and Yin, Guisheng and Dong, Yuxin},
title = {Consistency regularization teacher–student semi-supervised learning method for target recognition in SAR images},
year = {2021},
issue_date = {Dec 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {12},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-021-02287-z},
doi = {10.1007/s00371-021-02287-z},
abstract = {Synthetic aperture radar automatic target recognition (SAR-ATR) is a hotspot in the field of remote sensing, which has been widely used in disaster monitoring, environmental monitoring, resource exploration, and crop yield estimates. In recent years, deep convolutional neural networks (DCNNs) have achieved promising performance among a variety of supervised classification methods under the condition of sufficiently labeled samples. However, it is expensive and time-consuming to collect large amounts of labeled samples suitable for DCNNs in SAR domains. To reduce the dependence of SAR-ATR on labeled samples, in this work, a consistency regularization teacher–student semi-supervised (CRTS) method for SAR-ATR is proposed, in which consistency regularization is applied to analyze and divide unlabeled samples and the teacher–student structure is introduced to generate pseudo-labels for the divided unlabeled samples. Firstly, by using consistent pseudo-label prediction, unlabeled samples are divided into consistent unlabeled samples and confident unlabeled samples. Then, in order to improve the quality of pseudo-labeled labels, the student model is used to generate a pseudo-label for consistent unlabeled samples, and the teacher model which is ensembled by the other two student models labels the confident unlabeled samples. Finally, these pseudo-label unlabeled samples are mixed with the labeled samples and trained together to improve recognition performance. Experiments are conducted on the MSTAR dataset, and the results demonstrate the effectiveness of the proposed method. As compared with several state-of-the-art methods, the recognition accuracy shows the superiority of the proposed method, especially when the training dataset is limited.},
journal = {Vis. Comput.},
month = aug,
pages = {4179–4192},
numpages = {14},
keywords = {SAR automatic target recognition, Semi-supervised learning, Consistency regularization, Teacher–student structure}
}

@inproceedings{10.1145/3313831.3376447,
author = {Yan, Jing Nathan and Gu, Ziwei and Lin, Hubert and Rzeszotarski, Jeffrey M.},
title = {Silva: Interactively Assessing Machine Learning Fairness Using Causality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376447},
doi = {10.1145/3313831.3376447},
abstract = {Machine learning models risk encoding unfairness on the part of their developers or data sources. However, assessing fairness is challenging as analysts might misidentify sources of bias, fail to notice them, or misapply metrics. In this paper we introduce Silva, a system for exploring potential sources of unfairness in datasets or machine learning models interactively. Silva directs user attention to relationships between attributes through a global causal view, provides interactive recommendations, presents intermediate results, and visualizes metrics. We describe the implementation of Silva, identify salient design and technical challenges, and provide an evaluation of the tool in comparison to an existing fairness optimization tool.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {bias, interactive system, machine learning fairness},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1007/s11277-020-07657-9,
author = {Nawrocki, Piotr and Sniezynski, Bartlomiej},
title = {Adaptive Context-Aware Energy Optimization for Services on Mobile Devices with Use of Machine Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {115},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07657-9},
doi = {10.1007/s11277-020-07657-9},
abstract = {In this paper we present an original adaptive task scheduling system, which optimizes the energy consumption of mobile devices using machine learning mechanisms and context information. The system learns how to allocate resources appropriately: how to schedule services/tasks optimally between the device and the cloud, which is especially important in mobile systems. Decisions are made taking the context into account (e.g. network connection type, location, potential time and cost of executing the application or service). In this study, a supervised learning agent architecture and service selection algorithm are proposed to solve this problem. Adaptation is performed online, on a mobile device. Information about the context, task description, the decision made and its results such as power consumption are stored and constitute training data for a supervised learning algorithm, which updates the knowledge used to determine the optimal location for the execution of a given type of task. To verify the solution proposed, appropriate software has been developed and a series of experiments have been conducted. Results show that as a result of the experience gathered and the learning process performed, the decision module has become more efficient in assigning the task to either the mobile device or cloud resources.},
journal = {Wirel. Pers. Commun.},
month = dec,
pages = {1839–1867},
numpages = {29},
keywords = {Adaptation, Context-aware system, Energy optimization, Machine learning, Mobile cloud computing}
}

@article{10.1007/s42979-021-00872-6,
author = {Sakhrawi, Zaineb and Sellami, Asma and Bouassida, Nadia},
title = {Software Enhancement Effort Prediction Using Machine-Learning Techniques: A Systematic Mapping Study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00872-6},
doi = {10.1007/s42979-021-00872-6},
abstract = {Accurate prediction of software enhancement effort is a key success in software project management. To increase the accuracy of estimates, several proposals used machine-learning (ML) techniques for predicting the software project effort. However, there is no clear evidence for determining which techniques to select for predicting more accurate effort within the context of enhancement projects. This paper aims to present a systematic mapping study (SMS) related to the use of ML techniques for predicting software enhancement effort (SEME). A SMS was performed by reviewing relevant papers from 1995 through 2020. We followed well-known guidelines. We selected 30 relevant studies; 19 from journals and 11 conferences proceedings through 4 search engines. Some of the key findings indicate that (1) there is relatively little activity in the area of SEME, (2) most of the successful studies cited focused on regression problems for enhancement maintenance effort prediction, (3) SEME is the dependent variable the most commonly used in software enhancement project planning, and the enhancement size (or the functional change size) is the most used independent variables, (4) several private datasets were used in the selected studies, and there is a growing demand for the use of commonly published datasets, and (5) only single models were employed for SEME prediction. Results indicate that much more work is needed to develop repositories in all prediction models. Based on the findings obtained in this SMS, estimators should be aware that SEME using ML techniques as part of non-algorithmic models demonstrated increased accuracy prediction over the algorithmic models. The use of ML techniques generally provides a reasonable accuracy when using the enhancement functional size as independent variables.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {15},
keywords = {Systematic mapping study (SMS), Functional change (FC), Software enhancement effort (SEME) prediction, Machine learning (ML)}
}

@inproceedings{10.1145/3459990.3465193,
author = {Aki Tamashiro, Mariana and Van Mechelen, Maarten and Schaper, Marie-Monique and Sejer Iversen, Ole},
title = {Introducing Teenagers to Machine Learning through Design Fiction: An Exploratory Case Study},
year = {2021},
isbn = {9781450384520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459990.3465193},
doi = {10.1145/3459990.3465193},
abstract = {In this paper, we describe an exploratory study of how design fiction can be used to introduce machine learning (ML) to 14-15 year old students in an engaging way. The study describes three sessions conducted online that combined hands-on and design fiction activities related to supervised ML, focusing on facial analysis. Based on semi-structured interviews and field notes, we discuss how the design fiction approach seemed to be engaging and supportive for students' reflection on their relationship with technology. In future work, we will expand the study to include a larger number of students, assess their learning, and further explore connections between ML and its societal implications.},
booktitle = {Proceedings of the 20th Annual ACM Interaction Design and Children Conference},
pages = {471–475},
numpages = {5},
keywords = {design fiction, emerging technologies, machine learning},
location = {Athens, Greece},
series = {IDC '21}
}

@inproceedings{10.1145/3384419.3430566,
author = {Papst, Franz},
title = {Privacy-preserving machine learning for time series data: PhD forum abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430566},
doi = {10.1145/3384419.3430566},
abstract = {Machine learning has a lot of potential when applied to time series sensor data, yet a lot of this potential is currently not utilized, due to privacy concerns of parties in charge of this data. In this work I want to apply privacy-preserving techniques to machine learning for time series data, in order to unleash the dormant potential of this type of data.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {813–814},
numpages = {2},
keywords = {privacy preserving machine learning, sensor data, time series data},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/3299869.3314050,
author = {Agrawal, Pulkit and Arya, Rajat and Bindal, Aanchal and Bhatia, Sandeep and Gagneja, Anupriya and Godlewski, Joseph and Low, Yucheng and Muss, Timothy and Paliwal, Mudit Manu and Raman, Sethu and Shah, Vishrut and Shen, Bochao and Sugden, Laura and Zhao, Kaiyu and Wu, Ming-Chuan},
title = {Data Platform for Machine Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314050},
doi = {10.1145/3299869.3314050},
abstract = {In this paper, we present a purpose-built data management system, MLdp, for all machine learning (ML) datasets. ML applications pose some unique requirements different from common conventional data processing applications, including but not limited to: data lineage and provenance tracking, rich data semantics and formats, integration with diverse ML frameworks and access patterns, trial-and-error driven data exploration and evolution, rapid experimentation, reproducibility of the model training, strict compliance and privacy regulations, etc. Current ML systems/services, often named MLaaS, to-date focus on the ML algorithms, and offer no integrated data management system. Instead, they require users to bring their own data and to manage their own data on either blob storage or on file systems. The burdens of data management tasks, such as versioning and access control, fall onto the users, and not all compliance features, such as terms of use, privacy measures, and auditing, are available. MLdp offers a minimalist and flexible data model for all varieties of data, strong version management to guarantee re-producibility of ML experiments, and integration with major ML frameworks. MLdp also maintains the data provenance to help users track lineage and dependencies among data versions and models in their ML pipelines. In addition to table-stake features, such as security, availability and scalability, MLdp's internal design choices are strongly influenced by the goal to support rapid ML experiment iterations, which cycle through data discovery, data exploration, feature engineering, model training, model evaluation, and back to data discovery. The contributions of this paper are: 1) to recognize the needs and to call out the requirements of an ML data platform, 2) to share our experiences in building MLdp by adopting existing database technologies to the new problem as well as by devising new solutions, and 3) to call for actions from our communities on future challenges.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1803–1816},
numpages = {14},
keywords = {data platform, data streaming access, data version control, dataset management for machine learning, physical data layout},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.1007/s10489-018-1361-5,
author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Cri\c{s}an, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1361-5},
doi = {10.1007/s10489-018-1361-5},
abstract = {Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX---MIN Ant System (MMAS) in the background. We extended the MMAS---Algorithm in a way, that the human can directly interact and influence the ants. This is done by "traveling" with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant's behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.},
journal = {Applied Intelligence},
month = jul,
pages = {2401–2414},
numpages = {14},
keywords = {Ant Colony Optimization, Combinatorial optimization, Human-in-the-loop, Interactive machine learning}
}

@article{10.1007/s11277-018-6114-6,
author = {Jeridi, Mohamed Hechmi and Khlaifi, Hacen and Bouatay, Amine and Ezzedine, Tahar},
title = {Targets Classification Based on Multi-sensor Data Fusion and Supervised Learning for Surveillance Application},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {105},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-6114-6},
doi = {10.1007/s11277-018-6114-6},
abstract = {In surveillance application scenarios, like border security and area monitoring, potential targets to be detected may be either an unarmed person, a soldier carrying ferrous weapon or a vehicle. Detection is the first phase of a monitoring process, followed by the target classification phase and finally their tracking if required. This work focuses on classification step, where we introduce our classification approach not too resource-intensive, easy to implement and suitable for large scale environment. For that, we used probabilistic reasoning techniques to address multi sensing data correlation and take advantage of multi-sensor data fusion, then, based on adopted fusion architecture, we implemented our trained classification model in a fusion node, to make the classification more accurate.},
journal = {Wirel. Pers. Commun.},
month = mar,
pages = {313–333},
numpages = {21},
keywords = {Surveillance, Target classification, Probabilistic approach, Data fusion, Machine learning, Wireless sensor network}
}

@inbook{10.5555/3454287.3454547,
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
title = {Safe exploration for interactive machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {260},
numpages = {11}
}

@article{10.1007/s00521-020-05051-x,
author = {Yin, Xiangbao},
title = {Driven by machine learning to intelligent damage recognition of terminal optical components},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05051-x},
doi = {10.1007/s00521-020-05051-x},
abstract = {In order to realize the terminal optical element online detection system in the Shenguang III system, each optical element in each terminal optical component in the target room is detected. The research on the optical damage of terminal optical components focuses on the search for damage points, the extraction of damage information, and the classification of damage types. In addition, damage classification and identification of terminal optical components are performed through machine learning, and infrared nondestructive testing is used as technical support to improve the identification model and reduce the complexity of the spectral model. After studying the preprocessing and dimensionality reduction methods of near-infrared spectroscopy, this paper compares the effects of different preprocessing methods and screening feature methods and combines different modeling methods to conduct experiments. The research results show that the method proposed in this paper has certain effects.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {789–804},
numpages = {16},
keywords = {Machine learning, Terminal optics, Damage identification, Infrared nondestructive testing}
}

@inproceedings{10.1007/978-3-030-61078-4_2,
author = {Huo, Dongdong and Liu, Chao and Wang, Xiao and Li, Mingxuan and Wang, Yu and Wang, Yazhe and Liu, Peng and Xu, Zhen},
title = {A Machine Learning-Assisted Compartmentalization Scheme for&nbsp;Bare-Metal Systems},
year = {2020},
isbn = {978-3-030-61077-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61078-4_2},
doi = {10.1007/978-3-030-61078-4_2},
abstract = {A primary concern in creating compartments (i.e., protection domains) for bare-metal systems is to adopt the applicable compartmentalization policy. Existing studies have proposed several typical policies in literature. However, neither of the policies consider the influence of unsafe functions on the compartment security that a vulnerable function would expose unpredictable attack surfaces, which could be exploited to manipulate any contents that are stored in the same compartment. In this paper, we design a machine learning-assisted compartmentalization scheme, which adopts a new policy that takes every function’s security into full account, to create compartments for bare-metal systems. First, the scheme takes advantage of the machine learning method to predict how likely a function holds an exploitable security bug. Second, the prediction results are used to create a new instrumented firmware that isolates vulnerable and normal functions into different compartments. Further, the scheme provides some optional optimization plans to the developer to improve the performance. The PoC of the scheme is incorporated into an LLVM-based compiler and evaluated on a Cortex-M based IoT device. Compared with the firmware adopting other typical policies, the firmware with the new policy not only shows better security but also assures the overhead basically unchanged.},
booktitle = {Information and Communications Security: 22nd International Conference, ICICS 2020, Copenhagen, Denmark, August 24–26, 2020, Proceedings},
pages = {20–35},
numpages = {16},
keywords = {Bare-metal systems, Compartmentalization policy, Machine learning},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3397537.3397552,
author = {Reimann, Lars and Kniesel-W\"{u}nsche, G\"{u}nter},
title = {Achieving guidance in applied machine learning through software engineering techniques},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397552},
doi = {10.1145/3397537.3397552},
abstract = {Development of machine learning (ML) applications is hard. Producing successful applications requires, among others, being deeply familiar with a variety of complex and quickly evolving application programming interfaces (APIs). It is therefore critical to understand what prevents developers from learning these APIs, using them properly at development time, and understanding what went wrong when it comes to debugging. We look at the (lack of) guidance that currently used development environments and ML APIs provide to developers of ML applications, contrast these with software engineering best practices, and identify gaps in the current state of the art. We show that current ML tools fall short of fulfilling some basic software engineering gold standards and point out ways in which software engineering concepts, tools and techniques need to be extended and adapted to match the special needs of ML application development. Our findings point out ample opportunities for research on ML-specific software engineering.},
booktitle = {Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {7–12},
numpages = {6},
keywords = {guidance, learnability, machine learning, software engineering, usability},
location = {Porto, Portugal},
series = {Programming '20}
}

@article{10.1007/s00291-020-00615-8,
author = {Furian, Nikolaus and O’Sullivan, Michael and Walker, Cameron and \c{C}ela, Eranda},
title = {A machine learning-based branch and price algorithm for a sampled vehicle routing problem},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {43},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/s00291-020-00615-8},
doi = {10.1007/s00291-020-00615-8},
abstract = {Planning of operations, such as routing of vehicles, is often performed repetitively in rea-world settings, either by humans or algorithms solving mathematical problems. While humans build experience over multiple executions of such planning tasks and are able to recognize common patterns in different problem instances, classical optimization algorithms solve every instance independently. Machine learning (ML) can be seen as a computational counterpart to the human ability to recognize patterns based on experience. We consider variants of the classical Vehicle Routing Problem with Time Windows and Capacitated Vehicle Routing Problem, which are based on the assumption that problem instances follow specific common patterns. For this problem, we propose a ML-based branch and price framework which explicitly utilizes those patterns. In this context, the ML models are used in two ways: (a) to predict the value of binary decision variables in the optimal solution and (b) to predict branching scores for fractional variables based on full strong branching. The prediction of decision variables is then integrated in a node selection policy, while a predicted branching score is used within a variable selection policy. These ML-based approaches for node and variable selection are integrated in a reliability-based branching algorithm that assesses their quality and allows for replacing ML approaches by other (classical) better performing approaches at the level of specific variables in each specific instance. Computational results show that our algorithms outperform benchmark branching strategies. Further, we demonstrate that our approach is robust with respect to small changes in instance sizes.},
journal = {OR Spectr.},
month = sep,
pages = {693–732},
numpages = {40},
keywords = {Vehicle routing, Machine learning, Branch and price, Branching strategies}
}

@inproceedings{10.5555/3540261.3541931,
author = {Jung, Yonghan and Tian, Jin and Bareinboim, Elias},
title = {Double machine learning density estimation for local treatment effects with instruments},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Local treatment effects are a common quantity found throughout the empirical sciences that measure the treatment effect among those who comply with what they are assigned. Most of the literature is focused on estimating the average of such quantity, which is called the "local average treatment effect (LATE)" [31]). In this work, we study how to estimate the density of the local treatment effect, which is naturally more informative than its average. Specifically, we develop two families of methods for this task, namely, kernel-smoothing and model-based approaches. The kernel-smoothing-based approach estimates the density through some smooth kernel functions. The model-based approach estimates the density by projecting it onto a finite-dimensional density class. For both approaches, we derive the corresponding double/debiased machine learning-based estimators [13]. We further study the asymptotic convergence rates of the estimators and show that they are robust to the biases in nuisance function estimation. The use of the proposed methods is illustrated through both synthetic and a real dataset called 401(k).},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1670},
numpages = {13},
series = {NIPS '21}
}

@article{10.1177/1094342019852127,
author = {Dongarra, Jack and Tourancheau, Bernard and Deelman, Ewa and Mandal, Anirban and Jiang, Ming and Sakellariou, Rizos},
title = {The role of machine learning in scientific workflows},
year = {2019},
issue_date = {Nov 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {33},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342019852127},
doi = {10.1177/1094342019852127},
abstract = {Machine learning (ML) is being applied in a number of everyday contexts from image recognition, to natural language processing, to autonomous vehicles, to product recommendation. In the science realm, ML is being used for medical diagnosis, new materials development, smart agriculture, DNA classification, and many others. In this article, we describe the opportunities of using ML in the area of scientific workflow management. Scientific workflows are key to today’s computational science, enabling the definition and execution of complex applications in heterogeneous and often distributed environments. We describe the challenges of composing and executing scientific workflows and identify opportunities for applying ML techniques to meet these challenges by enhancing the current workflow management system capabilities. We foresee that as the ML field progresses, the automation provided by workflow management systems will greatly increase and result in significant improvements in scientific productivity.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {1128–1139},
numpages = {12},
keywords = {Scientific workflows, machine learning, workflow systems, anomaly detection, workflow composition}
}

@inproceedings{10.1145/3465480.3466928,
author = {Ren, Haoyu and Anicic, Darko and Runkler, Thomas A.},
title = {The synergy of complex event processing and tiny machine learning in industrial IoT},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466928},
doi = {10.1145/3465480.3466928},
abstract = {Focusing on comprehensive networking, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various intelligent sensors play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. Complex event processing (CEP) and machine learning (ML) have been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive, concerns are raised since transmitting such an amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. Decentralized on-device ML and CEP provide a solution where data is processed primarily on edge devices. Thus communications can be minimized. However, this is no mean feat because most IIoT edge devices are resource-constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and μCEP, we now shift the computation from the cloud to the resource-constrained IIoT devices and allow users to adapt on-device ML models and CEP reasoning rules flexibly on the fly. Lastly, we demonstrate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {126–135},
numpages = {10},
keywords = {complex event processing, industrial IoT, tiny machine learning},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@article{10.1016/j.jss.2019.110486,
author = {Barbez, Antoine and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {A machine-learning based ensemble method for anti-patterns detection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110486},
doi = {10.1016/j.jss.2019.110486},
journal = {J. Syst. Softw.},
month = mar,
numpages = {11},
keywords = {Software quality, Anti-patterns, Machine learning, Ensemble methods}
}

@inproceedings{10.1145/3301275.3302280,
author = {Arendt, Dustin and Saldanha, Emily and Wesslen, Ryan and Volkova, Svitlana and Dou, Wenwen},
title = {Towards rapid interactive machine learning: evaluating tradeoffs of classification without representation},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302280},
doi = {10.1145/3301275.3302280},
abstract = {Our contribution is the design and evaluation of an interactive machine learning interface that rapidly provides the user with model feedback after every interaction. To address visual scalability, this interface communicates with the user via a "tip of the iceberg" approach, where the user interacts with a small set of recommended instances for each class. To address computational scalability, we developed an O(n) classification algorithm that incorporates user feedback incrementally, and without consulting the data's underlying representation matrix. Our computational evaluation showed that this algorithm has similar accuracy to several off-the-shelf classification algorithms with small amounts of labeled data. Empirical evaluation revealed that users performed better using our design compared to an equivalent active learning setup.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {591–602},
numpages = {12},
keywords = {active learning, hierarchical clustering, interactive machine learning, representation-free classifier, transduction learning, visual interactive labeling},
location = {Marina del Ray, California},
series = {IUI '19}
}

@phdthesis{10.5555/AAI28320203,
author = {Wang, Wentao and Wu, Qiuwen and Ge, Yaorong and Palta, Manisha and Sheng, Yang},
advisor = {Jackie, Wu, Q.},
title = {Automated Generation of Radiotherapy Treatment Plans Using Machine Learning Methods},
year = {2021},
isbn = {9798738631467},
publisher = {Duke University},
address = {USA},
abstract = {With the development of medical linear accelerator technologies, the precision and complexity of external beam radiation therapy have increased tremendously over the years. The goal of radiation therapy has always been to push the limit to irradiate the target volume while preserving normal tissues. To achieve this goal, treatment planning for radiation therapy has become a labor-intensive and time-consuming task, which requires a high level of experience and knowledge from the planner. Therefore, automated treatment planning, or auto-planning, is of particular interest in radiation therapy research. The advantages of auto-planning are reduced planning time and increased plan quality consistency.Since the treatment planning workflow has multiple steps, auto-planning includes the automation of different planning procedures, such as contouring, beam placement, and inverse optimization, which can be achieved in different approaches. The main approaches are knowledge-based planning, automated rule implementation and reasoning, and multicriteria optimization. We can generally consider such novel auto-planning applications as artificial intelligence (AI). This study primarily focuses on treatment plan generation using knowledge-based planning and machine learning techniques. The study includes two main projects: automated beam setting for whole breast radiation therapy (WBRT) and fluence map prediction for intensity modulated radiation therapy (IMRT).In WBRT planning, tangential beams are used to irradiate the entire breast volume and avoid the organs-at-risk (OARs) (i.e., the lungs and the heart) as much as possible. The placement of the beams is vital in determining the planning target volume (PTV) coverage and normal tissue sparing. Furthermore, planners need to take multiple clinical considerations into account, e.g., avoiding the contralateral breast and the heart, and use a variety of techniques to meet the demands. Therefore, we developed an automated beam setting program which takes simple user settings and optimizes target coverage and OAR sparing. The program can be launched from the Eclipse Treatment Planning System (TPS) as a binary plug-in script, which generates a graphical user interface to accept user inputs. Several beam geometries are supported: tangential beams only (supine), tangential plus supraclavicular (SCV) beams (supine), and prone beams. For all geometries, the program calculates the optimal gantry angles, collimator angles, isocenter location, jaw sizes, and MLC shapes. The borders of the SCV beams are also matched to the tangential beams by using couch kicks on the main tangential fields. For the supine geometries, a coefficient was learned from existing clinical plans to balance between the PTV and lung coverages. The program searches from an initial setting based on breast wires and finds the optimal setting. For the prone geometry, the planner can set a margin to customize the coverage near the PTV-lung interface. The program has been implemented together with a WBRT fluence prediction program, which creates electronic compensation (ECOMP) plans from the given beam settings. This automated workflow can significantly reduce the workload of the forward planned ECOMP plans. The results showed that the AI plans achieved similar or better plan quality compared to the manual plans.In IMRT planning, inverse optimization is the standard practice to create treatment plans. Dose-volume histogram (DVH) constraints and priorities are set by the planner to start the optimization and often continuously tuned throughout the planning process until the optimal dose distribution is achieved. The actual parameters to be optimized are fluence map intensities of the IMRT beams. Numerous efforts have been devoted in KBP to predict either the DVH or the dose of the optimal plan. The rationale is that, given the patient anatomy and the physician's prescription, the DVH or dose in the final plan can be predicted based on similar previous plans. The predicted DVH or dose can then be used as a reference to either evaluate the plan quality or generate new plans by converting them into inverse optimization objectives, which is a process also known as dose mimicking. However, most dose mimicking techniques are still in the development stage and not yet commercially available. We explored the feasibility to directly predict optimal fluence maps and generate IMRT plans without inverse optimization.In order to achieve fluence map prediction, we first investigated the correlation between patient anatomy and fluence maps. A database of patient anatomy and fluence maps was built with pancreas SBRT cases. Treatment planning was done on 2D axial slices with in-house dose calculation and fluence optimization algorithms. For a new slice, an atlas matching method was developed to search for the most anatomically similar slice in the database and initialize the optimization with the existing fluence. The atlas-guided fluence optimization reduced the optimization cost and offered a small dosimetric improvement compared to uniform initialization.With more training data, deep learning methods were experimented to predict fluence maps from patient anatomy. A deep learning framework consisting of two convolutional neural networks (CNN) was developed. As each plan has several beams, all beam doses must add up to the optimal plan's total dose, while each beam dose is deposited only by said beam's fluence map. Therefore, the BD-CNN predicts the individual beam doses (BD) for an IMRT plan, which tries to minimize the prediction error for both the beam doses and the total dose. Once the beam doses are available, each fluence map (FM) is generated separately by the FM-CNN. As the fluence maps exist in the beam's eye view (BEV), a projection of the 3D beam dose onto the 2D BEV is necessary. The resulting dose map is used as the input to the FM-CNN, which predicts the fluence map as the output. The predicted fluence maps are imported into the TPS for leaf sequencing and dose calculation, generating a deliverable plan.These projects are retrospective studies using anonymized patient data for training and testing. The development of the deep learning framework was split into several stages: the initial test of the feasibility was conducted for pancreas stereotactic body radiation therapy (SBRT) with a single PTV, unified dose constraints, and a fixed 9-beam geometry; the networks were then modified to allow variable dose inputs and multiple PTVs for pancreas SBRT with simultaneous integrated boost (SIB); a transfer learning technique was applied to the training of the framework for adrenal SBRT plans with different beam settings and dose constraints, using the pancreas model as the base model. The framework has evolved to be more robust and support different sites and planning styles over time.The AI plans with predicted fluence maps achieved similar plan quality as manual plans for most cases. For some cases with particularly challenging patient anatomies, the AI plans can struggle to reach the high standard of the expert plans. Fluence map prediction is a viable way to directly generate IMRT plans without inverse optimization. This application may be especially useful for adaptive treatment planning.},
note = {AAI28320203}
}

@article{10.1145/3197978,
author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
title = {A Survey on Compiler Autotuning using Machine Learning},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3197978},
doi = {10.1145/3197978},
abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {96},
numpages = {42},
keywords = {Autotuning, compilers, machine learning, optimizations, phase ordering}
}

@article{10.1155/2021/1896953,
author = {Ali Mohamed, Mohamed and El-henawy, Ibrahim Mahmoud and Salah, Ahmad and Khalil, Ahmed Mostafa},
title = {Usages of Spark Framework with Different Machine Learning Algorithms},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/1896953},
doi = {10.1155/2021/1896953},
abstract = {Sensors, satellites, mobile devices, social media, e-commerce, and the Internet, among others, saturate us with data. The Internet of Things, in particular, enables massive amounts of data to be generated more quickly. The Internet of Things is a term that describes the process of connecting computers, smart devices, and other data-generating equipment to a network and transmitting data. As a result, data is produced and updated on a regular basis to reflect changes in all areas and activities. As a consequence of this exponential growth of data, a new term and idea known as big data have been coined. Big data is required to illuminate the relationships between things, forecast future trends, and provide more information to decision-makers. The major problem at present, however, is how to effectively collect and evaluate massive amounts of diverse and complicated data. In some sectors or applications, machine learning models are the most frequently utilized methods for interpreting and analyzing data and obtaining important information. On their own, traditional machine learning methods are unable to successfully handle large data problems. This article gives an introduction to Spark architecture as a platform that machine learning methods may utilize to address issues regarding the design and execution of large data systems. This article focuses on three machine learning types, including regression, classification, and clustering, and how they can be applied on top of the Spark platform.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {7}
}

@inproceedings{10.5555/3465085.3465095,
author = {Najari, Arman and Pajarito, Diego and Markopoulou, Areti},
title = {Data modeling of cities, a machine learning application},
year = {2020},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Today cities are the main humankind settlement relying on intricate systems. The technological growth in the context of urban produces a vast amount of data. Analyzing and visualizing this data have provided insights into this complex environment. However, these mere approaches are inert and, due to technical constraints, hard to be integrated into urban planning.Consequently, we adopt a research hypothesis in which the adaptability and complexity of urban systems can be replicated, or partially replicated, by the machine-learning algorithms.The current study aims to define a process for evaluating the capabilities for analyzing and predicting urban data with machine-learning (ML) algorithms. This process starts with constructing a data structure for inputting into the ML algorithm. Following this, different tests are applied to identify valid combinations of data and models that allow understanding of urban patterns. The bicycle-sharing system is used as a case study. The process ends discussing the options to replicate the experiment in different urban areas as well as to adapt it to different problems.Different datasets from different cities have been explored and considered for this experiment. Across many cities' open dataset platforms, the NYD platform offered the most reliable data. From the sub-systems of the city, the mobility network was selected as a case study for exploration. More specifically, data on shared-bicycle mobility and use were selected as a result of its exciting raise as travel choices reported by New York City's department of transportation. The urban data analysis and prediction process of this research paper identifies the neighborhood as the unit of the model. Additionally, to illustrate and analyze the relationship between the selected mobility sub-system and other urban systems, contextual indicators such as land use indexes were added in the modeling process.Despite the prediction modeling machine for the bike-sharing system coming out of this study, the main achievement is the introduction of a collection of analysis and prediction processes for urban data beyond mobility. For further advancement, implementing this approach in a different urban system and context is crucial. By this means, the replicability of the process could be evaluated and tested.},
booktitle = {Proceedings of the 11th Annual Symposium on Simulation for Architecture and Urban Design},
articleno = {10},
numpages = {8},
keywords = {bicycle mobility data, big-data, machine learning, predictive urban model, urban analytics},
location = {Virtual Event, Austria},
series = {SimAUD '20}
}

@phdthesis{10.5555/AAI29755694,
author = {Friedberg, Rina Siller and F., Bent, Stacey},
advisor = {Susan, Athey, and Trevor, Hastie, and Art, Owen,},
title = {Topics In Machine Learning for Causal Inference With Applications In Social Science},
year = {2020},
isbn = {9798357505439},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {Random forests are a powerful method for non-parametric regression, but are limited in their ability to fit smooth signals. Taking the perspective of random forests as an adaptive kernel method, we pair the forest kernel with a local linear regression adjustment to better capture smoothness. The resulting procedure, local linear forests, enables us to improve on asymptotic rates of convergence for random forests with smooth signals, and provides substantial gains in accuracy on both real and simulated data. We prove a central limit theorem valid under regularity conditions on the forest and smoothness constraints, propose a computationally efficient construction for confidence intervals, and discuss an extension to local linear causal forests for learning heterogeneous treatment effects.Following this deep dive into local linear forests, we discuss two applications of machine learning for causal inference. The first is a retirement reform in Denmark, in which shifting eligibility ages for an early retirement program provide an opportunity to analyze heterogeneous treatment effects of the age of retirement eligibility. The second is a randomized controlled trial in Nairobi, Kenya, aiming to lower rates of gender-based violence against adolescent students living in informal settlements. In the latter example, we explore how local linear causal forests help to uncover and emphasize trends in marginalized student responses to the intervention. In both cases, we address how machine learning and causal inference are powerful tools to discover patterns in individual treatment effects, and to advocate for marginalized groups when estimates reveal troubling patterns in the data.},
note = {AAI29755694}
}

@phdthesis{10.5555/AAI28869486,
author = {Kaptanoglu, Alan and Uri, Shumlak, and Kai-Mei, Fu, and Gerald, Seidler,},
advisor = {Steven, Brunton,},
title = {An Exploration of Data-Driven System Identification and Machine Learning for Plasma Physics},
year = {2021},
isbn = {9798780639763},
publisher = {University of Washington},
abstract = {Plasma is the most common state of visible matter in the universe and provides a myriad of scientific and engineering applications. However, the complexity of these systems poses a significant challenge for understanding and controlling plasmas. Fortunately, machine learning is increasingly used to handle complex, nonlinear systems, and the field of machine learning is advancing at an unprecedented pace, propelled forward by advances in sensing technology and computing power. This thesis summarizes work towards applying modern machine learning algorithms for fluid and plasma physics applications, with a focus on the understanding and control of magnetohydrodynamic (MHD) phenomena and fusion-relevant plasmas. Although this work is primarily focused on machine learning, first conventional numerical techniques are used to implement a two-temperature Hall-MHD model into the 3D PSI-Tet code, followed by an investigation of the plasma dynamics in the HIT-SI experiment. These simulations agree well with experimental measurements, and indicate that low-densities are required for significant closed flux surfaces—a recommendation that is now helping to guide the next generation of experimental design. Next, plasma modeling with machine learning is discussed in the context of the hierarchy of plasma models and it is illustrated that there is "plenty of room at the bottom" for physics-constrained reduced order models that approximate more complex MHD or kinetic plasma models. Variants of the dynamic mode decomposition are explored on experimental data and simulations of the HIT-SI plasma device and indicate promise for magnetic mode spectroscopy and forecasting diagnostic measurements. Continuing, analytic reduced-order modeling methods are extended using techniques in system identification for extracting reduced-order models directly from data. In the process, new methods are invented to enforce physical constraints and stability in data-driven fluid and plasma models. For instance, the ability to build data-driven models that obey global conservation of energy or global conservation of cross-helicity is demonstrated, with promise for efficient simulations of ideal and resistive MHD turbulence. With the new functionality implemented into the open-source PySINDy code as part of this work, advanced system identification methods that can robustly extract dynamical equations from data are available to the larger scientific community. In total, this work illustrates that new machine learning methods can be directly tied with known physical laws in plasma physics, have promise to significantly impact much of the plasma physics and nonlinear systems fields, and can provide complementary, interpretable methods to the relatively black-box deep learning techniques that are frequently used in the plasma physics field for extracting diagnostic information, building reduced-order models, and performing real-time control.},
note = {AAI28869486}
}

@inproceedings{10.1145/3351095.3372834,
author = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and van Moorsel, Aad},
title = {The relationship between trust in AI and trustworthy machine learning technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372834},
doi = {10.1145/3351095.3372834},
abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {272–283},
numpages = {12},
keywords = {artificial intelligence, machine learning, trust, trustworthiness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@inbook{10.5555/3454287.3454316,
author = {Li, Xueting and Liu, Sifei and Mello, Shalini De and Wang, Xiaolong and Kautz, Jan and Yang, Ming-Hsuan},
title = {Joint-task self-supervised learning for temporal correspondence},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions and establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet. The project website can be found at https://sites.google.com/view/uvc2019/.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {29},
numpages = {11}
}

@inproceedings{10.1109/CDC45484.2021.9683128,
author = {Manchester, Ian R. and Revay, Max and Wang, Ruigang},
title = {Contraction-Based Methods for Stable Identification and Robust Machine Learning: a Tutorial},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC45484.2021.9683128},
doi = {10.1109/CDC45484.2021.9683128},
abstract = {This tutorial paper provides an introduction to recently developed tools for machine learning, especially learning dynamical systems (system identification), with stability and robustness constraints. The main ideas are drawn from contraction analysis and robust control, but adapted to problems in which large-scale models can be learnt with behavioural guarantees. We illustrate the methods with applications in robust image recognition and system identification.},
booktitle = {2021 60th IEEE Conference on Decision and Control (CDC)},
pages = {2955–2962},
numpages = {8},
location = {Austin, TX, USA}
}

@inproceedings{10.5555/2969239.2969348,
author = {Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {Unsupervised learning by program synthesis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our techniques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures, and a technique for applying program synthesis tools to noisy data.},
booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 1},
pages = {973–981},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@article{10.1145/3450626.3459873,
author = {Yang, Kaizhi and Chen, Xuejin},
title = {Unsupervised learning for cuboid shape abstraction via joint segmentation from point clouds},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459873},
doi = {10.1145/3450626.3459873},
abstract = {Representing complex 3D objects as simple geometric primitives, known as shape abstraction, is important for geometric modeling, structural analysis, and shape synthesis. In this paper, we propose an unsupervised shape abstraction method to map a point cloud into a compact cuboid representation. We jointly predict cuboid allocation as part segmentation and cuboid shapes and enforce the consistency between the segmentation and shape abstraction for self-learning. For the cuboid abstraction task, we transform the input point cloud into a set of parametric cuboids using a variational auto-encoder network. The segmentation network allocates each point into a cuboid considering the point-cuboid affinity. Without manual annotations of parts in point clouds, we design four novel losses to jointly supervise the two branches in terms of geometric similarity and cuboid compactness. We evaluate our method on multiple shape collections and demonstrate its superiority over existing shape abstraction methods. Moreover, based on our network architecture and learned representations, our approach supports various applications including structured shape generation, shape interpolation, and structural shape clustering.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {152},
numpages = {11},
keywords = {3D shape abstraction, 3D structural representation, joint segmentation, point clouds}
}

@phdthesis{10.5555/AAI28024829,
author = {Wang, Xueshe and Virgin, Lawrence and Stanton, Samuel and Gavin, Henri},
advisor = {P, Mann, Brian},
title = {Nonlinear Energy Harvesting with Tools from Machine Learning},
year = {2020},
isbn = {9798664793543},
publisher = {Duke University},
address = {USA},
abstract = {Energy harvesting is a process where self-powered electronic devices scavenge ambient energy and convert it to electrical power. Traditional linear energy harvesters which operate based on linear resonance work well only when excitation frequency is close to its natural frequency. While various control methods applied to an energy harvester realize resonant frequency tuning, they are either energy-consuming or exhibit low efficiency when operating under multi-frequency excitations. In order to overcome these limitations in a linear energy harvester, researchers recently suggested using "nonlinearity" for broad-band frequency response.Based on existing investigations of nonlinear energy harvesting, this dissertation introduced a novel type of energy harvester designs for space efficiency and intentional nonlinearity: translational-to-rotational conversion. Two dynamical systems were presented: 1) vertically forced rocking elliptical disks, and 2) non-contact magnetic transmission. Both systems realize the translational-to-rotational conversion and exhibit nonlinear behaviors which are beneficial to broad-band energy harvesting. This dissertation also explores novel methods to overcome the limitation of nonlinear energy harvesting—the presence of coexisting attractors. A control method was proposed to render a nonlinear harvesting system operating on the desired attractor. This method is based on reinforcement learning and proved to work with various control constraints and optimized energy consumption.Apart from investigations of energy harvesting, several techniques were presented to improve the efficiency for analyzing generic linear/nonlinear dynamical systems: 1) an analytical method for stroboscopically sampling general periodic functions with arbitrary frequency sweep rates, and 2) a model-free sampling method for estimating basins of attraction using hybrid active learning.},
note = {AAI28024829}
}

@article{10.1016/j.jbi.2021.103842,
author = {Morid, Mohammad Amin and Lau, Michael and Del Fiol, Guilherme},
title = {Predictive analytics for step-up therapy: Supervised or semi-supervised learning?},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {119},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103842},
doi = {10.1016/j.jbi.2021.103842},
journal = {J. of Biomedical Informatics},
month = jul,
numpages = {9},
keywords = {Step-up therapy, Semi-supervised learning, Rheumatoid arthritis, Resource planning, Chronic care management}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {IoT, deep learning, machine learning, orchestration}
}

@article{10.5555/3455716.3455964,
author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
title = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {248},
numpages = {43},
keywords = {energy efficiency, green computing, reinforcement learning, deep learning, climate change}
}

@article{10.1007/s00158-021-02896-1,
author = {Li, Zhixiang and Ma, Wen and Yao, Shuguang and Xu, Ping and Hou, Lin and Deng, Gongxun},
title = {A machine learning based optimization method towards removing undesired deformation of energy-absorbing structures},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {64},
number = {2},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-021-02896-1},
doi = {10.1007/s00158-021-02896-1},
abstract = {Optimization for the energy-absorbing structures can achieve their better crashworthiness and lightweight performance. However, traditional optimization methods cannot handle categorical responses such as deformation modes. This results some undesirable deformations which often appear in the optimization solution, making it difficult to guarantee the accuracy of optimization. To this end, a machine learning based optimization method for energy-absorbing structures was proposed in this study to remove the undesired deformations. In this method, a DOE method was used to get representative sample points in the design space; the machine learning techniques were adopted to build the prediction models for deformation modes and numerical responses; the Nondominated Sorting Genetic algorithm II (NSGA-II) was utilized for the multi-objective optimization. A case study on optimization of a shrink tube used in train energy absorption was used to verify the effectiveness of the optimization method. The optimization result for the shrink tube illustrated that the machine learning based optimization method can effectively remove the undesirable deformations for energy-absorbing structures. This study may pave a new way to improve the accuracy of energy-absorbing structure optimization.},
journal = {Struct. Multidiscip. Optim.},
month = aug,
pages = {919–934},
numpages = {16},
keywords = {Optimization, Machine learning, Energy-absorbing structure, Deformation mode}
}

@inproceedings{10.1007/978-3-030-89432-0_5,
author = {Mogg, Raymond and Enoch, Simon Yusuf and Kim, Dong Seong},
title = {A Framework for Generating Evasion Attacks for Machine Learning Based Network Intrusion Detection Systems},
year = {2021},
isbn = {978-3-030-89431-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89432-0_5},
doi = {10.1007/978-3-030-89432-0_5},
abstract = {Intrusion Detection System (IDS) plays a vital role in detecting anomalies and cyber-attacks in networked systems. However, sophisticated attackers can manipulate the IDS’ attacks samples to evade possible detection. In this paper, we present a network-based IDS and investigate the viability of generating interpretable evasion attacks against the IDS through the application of a machine learning technique and an evolutionary algorithm. We employ a genetic algorithm to generate optimal attack features for certain attack categories, which are evaluated against a decision tree-based IDS in terms of their fitness measurements. To demonstrate the feasibility of our approach, we perform experiments based on the NSL-KDD dataset and analyze the algorithm performance.},
booktitle = {Information Security Applications: 22nd International Conference, WISA 2021, Jeju Island, South Korea, August 11–13, 2021, Revised Selected Papers},
pages = {51–63},
numpages = {13},
keywords = {Adversarial machine learning, Evasion attacks, Genetic algorithms, Intrusion detection},
location = {Jeju, Korea (Republic of)}
}

@inproceedings{10.1145/3454127.3458773,
author = {Haiba, Somaya and MAZRI, TOMADER},
title = {Build a malware detection software for IOT network Using Machine learning},
year = {2021},
isbn = {9781450388719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3454127.3458773},
doi = {10.1145/3454127.3458773},
abstract = {Any system based on IOT devices must provide a reliable and secure network to transmit and manipulate the data from the smallest technologies to the final server. They are the devices of nowadays, and our future for sure, it will convert all domains of our live starting from the smart home, industry, to e-healthcare systems. To achieve the quality of performances we should have an assurance of security and trustworthiness beginning by the small device used to capture information to the final terminal of the network system, using all the exist possibilities that we can implement to ensure a secure employment. Until now, machine learning become the most powerful application, which provides for any systems the capacity of auto-learning, and improving itself taking a help from the old experiences and without any human intervention or being explicit programmed. Furthermore, the usage of IOT networks, as we know is growing and evaluate enormously so the menaces keep up to date exploiting the weakness of these tools. For that, we propose to make a look about, What machine learning is coming with, to perform more security and analyses the threats, and how can it be used to detect any endanger before it can gain control. Knowing that the reproducible use of resource-constrained IOT devices, the number of IOT Malwares has exploded variously with many ways of breakthrough and then the requirement, to have an efficient malware detection adequate with this grown-up is on immense increasing importance. In this paper, we discuss the usability of machine learning applications in malwares detection software for IOT networks to elicitation the standards, which can us, use to build a powerful model to improve the network security of this small technologies answering to pervious questions.},
booktitle = {Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security},
articleno = {64},
numpages = {8},
keywords = {IOT Malwares, IOT networks, Machine-learning, Malware Detection, Security},
location = {KENITRA, AA, Morocco},
series = {NISS '21}
}

@article{10.1007/s00521-020-05023-1,
author = {Han, Chaoliang and Zhang, Qi},
title = {Optimization of supply chain efficiency management based on machine learning and neural network},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {5},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05023-1},
doi = {10.1007/s00521-020-05023-1},
abstract = {Supply chain management is of great significance to business operations and socioeconomic development. However, the current supply chain efficiency management cannot effectively control the risk caused by the inefficient supply chain management. In order to study the improvement in supply chain efficiency management, supported by machine learning and neural network technology, this study builds a supply chain risk management model based on learning and neural network. Moreover, this study evaluates the risk indicator system based on the current status of supply chain management. In addition, the model simulation research is carried out in the MATLAB platform, and the validity analysis of the model is performed with examples. Finally, after training the data through the training model, the risk assessment value is output, and strategies for coping with the risk are given. The research shows that the model proposed in this paper has a certain practical effect and can be considered for application.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1419–1433},
numpages = {15},
keywords = {Machine learning, Neural network, Supply chain, Efficiency, Performance evaluation}
}

@inproceedings{10.1145/3396851.3402122,
author = {Kolluri, Ramachandra Rao and de Hoog, Julian},
title = {Adaptive Control Using Machine Learning for Distributed Storage in Microgrids},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402122},
doi = {10.1145/3396851.3402122},
abstract = {The falling costs of solar photovoltaic systems and energy storage mean that these are being increasingly deployed in microgrids across the globe. Distributed storage can provide benefits for its owner, but can also play a key role in improving microgrid stability and resilience. However, most approaches to date assume that a central authority can control multiple nodes or households in the network. This introduces significant communication and control requirements, and may introduce points of failure. In this work we provide an initial exploration of how a machine learning model, trained on optimal control solutions, can be used locally at each node in the network to emulate a similar behaviour. The aim is for the trained model to provide benefits both for the individual energy storage owners, while also enabling community-level cooperative behaviour - all in a low communication-overhead, privacy-preserving manner. It is experimentally shown that a neural network trained on limited data from optimal schedules can learn node interactions and network characteristics, and can achieve partial voltage regulation for the entire microgrid. This can be done while still achieving a small (3%) network-wide cost savings compared to a scenario in which no distributed storage is present, can be implemented only locally, and does not introduce any significant requirements for central control and communication.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {509–515},
numpages = {7},
keywords = {Energy storage, machine learning, microgrids, neural networks, optimization, solar photovoltaics, voltage regulation},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1007/978-3-030-64580-9_46,
author = {Mandarano, Nicholas and Regis, Rommel G. and Bloom, Elizabeth},
title = {Machine Learning and Statistical Models for the Prevalence of Multiple Sclerosis},
year = {2020},
isbn = {978-3-030-64579-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64580-9_46},
doi = {10.1007/978-3-030-64580-9_46},
abstract = {Multiple sclerosis is an immune-mediated disease affecting approximately 2.5 million people worldwide. Its cause is unknown and there is currently no cure. MS tends to be more prevalent in countries that are farther from the equator. Moreover, smoking and obesity are believed to increase the risk of developing the disease. This article builds machine learning and statistical models for the MS prevalence in a country in terms of its distance from the equator and the smoking and adult obesity prevalence in that country. To build the models, the center of population of a country is approximated by finding a point on the surface of the Earth that minimizes a weighted sum of squared distances from the major cities of the country. This study compares the predictive performance of several machine learning models, including first and second order multiple regression, random forest, neural network and support vector regression.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II},
pages = {559–571},
numpages = {13},
keywords = {Multiple sclerosis, Multiple regression, Random forest, Neural network, Support vector regression, Geographic population center, Constrained optimization},
location = {Siena, Italy}
}

@inproceedings{10.1145/2491627.2491655,
author = {Dumitrescu, Cosmin and Mazo, Raul and Salinesi, Camille and Dauron, Alain},
title = {Bridging the gap between product lines and systems engineering: an experience in variability management for automotive model based systems engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491655},
doi = {10.1145/2491627.2491655},
abstract = {We present in this paper an experience in modeling a family of parking brake systems, with shared assets and alternative solutions, and relate them to the needs of Renault in terms of variability management. The models are realized using a set of customized tools for model based systems engineering and variability management, based on SysML models. The purpose is to present an industrial context that requires the adoption of a product line approach and of variability modeling techniques, outside of a pure-software domain. At Renault, the interest is in identifying variations and reuse opportunities early in the product development cycle, as well as in preparing vehicle configuration specifications during the systems engineering process. This would lead to lowering the engineering effort and to higher quality and confidence in carry-over and carry across based solutions. We advocate for a tight integration of variability management with the model based systems engineering approach, which needs to address methodological support, modeling techniques and efficient tools for interactive configuration, adapted for engineering activities.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {254–263},
numpages = {10},
keywords = {systems engineering, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3183713.3183751,
author = {Lu, Yao and Chowdhery, Aakanksha and Kandula, Srikanth and Chaudhuri, Surajit},
title = {Accelerating Machine Learning Inference with Probabilistic Predicates},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183751},
doi = {10.1145/3183713.3183751},
abstract = {Classic query optimization techniques, including predicate pushdown, are of limited use for machine learning inference queries, because the user-defined functions (UDFs) which extract relational columns from unstructured inputs are often very expensive; query predicates will remain stuck behind these UDFs if they happen to require relational columns that are generated by the UDFs. In this work, we demonstrate constructing and applying probabilistic predicates to filter data blobs that do not satisfy the query predicate; such filtering is parametrized to different target accuracies. Furthermore, to support complex predicates and to avoid per-query training, we augment a cost-based query optimizer to choose plans with appropriate combinations of simpler probabilistic predicates. Experiments with several machine learning workloads on a big-data cluster show that query processing improves by as much as 10x.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1493–1508},
numpages = {16},
keywords = {image analysis, inference, machine learning, model cascades, probabilistic predicates, query processing, user-defined functions, video analysis},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@phdthesis{10.5555/AAI28496475,
author = {Ginley, Brandon and John, Tomaszewski, and Wen, Dong, and Scott, Doyle,},
advisor = {Pinaki, Sarder,},
title = {Investigation of Machine Learning Algorithms for Pathologic Assessment of Digitized Kidney Biopsies},
year = {2021},
isbn = {9798516939983},
publisher = {State University of New York at Buffalo},
address = {USA},
abstract = {The intent of this thesis is to study the reliability of machine learning and image analysis techniques to replicate diagnostic tests performed on kidney biopsies as part of conventional pathology analysis. The kidney biopsy is a fundamental component to the assessment of kidney disease, as the microscopic evaluation of kidney tissue can not only identify the cause of renal disease but also the severity. Conventional analysis of biopsied tissues, however, is limited for many reasons. The primary limitation is that each biopsied tissue contains overwhelming amounts of microscopic data that are not fully utilized in traditional pathology reporting due to the constraints of time in busy clinical practice. This leads to several secondary issues, the first being that diagnostic schema utilize structural evaluation of only select portions of the biopsy whose identification has high agreement among practicing pathologists and/or high prognostic value. This, combined with human nature, leads to a third limitation; i.e. regardless of the classification system used, there is always some level of inter- and intra-rater variability among pathologists. One practical example of these limitations is in the assessment of interstitial fibrosis and tubular atrophy (IFTA). IFTA is a complex morphologic manifestation of chronic kidney damage, where the tubules reduce both in physical size and function while scar tissue accumulates in the surrounding interstitium. This can involve anywhere from a small portion of a few tubules to all of the thousands of tubules captured in the biopsy. Because there is not enough time for a renal pathologist to measure the size of every tubule individually, instead, the pathologist visually estimates the total percent of the biopsy involved by IFTA in cerebro. This method of IFTA assessment has been shown to have high prognostic power, but has been consistently criticized as subjective and an oversimplification of the true underlying pathology of IFTA, with many calls to improve its pathologic definition. These conventional limitations have led to a significant amount of interest in the applications of computers to analyze biopsied data. Computers can analyze large data at a much faster rate of speed than humans, and moreover, they are indefatigable. This property opens up significant opportunity to assess the thousands of structures contained in a renal biopsy at a precise and detailed level. Despite the exceptional promise, application of computational algorithms to kidney pathology has traditionally been very limited due to the vast structural heterogeneity and complexity found in human kidney tissue. However, recent rapid development of machine learning algorithms, such as convolutional neural networks (CNN's), has created extremely powerful software for image object detection. Despite rapid adoption and significant success of these tools in oncology, validation of these tools for kidney pathology has again been very limited. Given the success in other areas, we hypothesized that optimally designed image analysis pipelines can be used to replicate pathologist assessment of kidney tissues to an equal degree of reliability as would be expected from human renal pathologists. We supported this hypothesis with two major works showing proof of concept: the first on a wholly digital pipeline to classify kidney biopsies according to the severity of diabetic nephropathy (DN), and the second to estimate prognostic markers of chronic kidney damage from whole kidney biopsies. In the first work, we developed an image analysis pipeline that was capable of classifying the stages of diabetic nephropathy (DN) with equal reliability as was observed among renal pathologists. In this study, CNN's were trained to automatically extract glomeruli from whole biopsy of 48 patients with DN and six control tissues, achieving 0.93 sensitivity and 0.99 specificity on test images. We then studied the performance of CNN's to detect glomerular nuclei in 616 cropped glomerulus images, which was found to be 0.8 sensitivity and 0.99 specificity in the test glomeruli. An additional probabilistic weighting on the output of the CNN allowed biasing towards the nucleus class, yielding an optimized performance of 0.94 sensitivity and 0.93 specificity. Leveraging the CNN nuclear detection and simple color transformation and thresholding, a three-component system modeled the complete glomerular structure (nuclei component, periodic acid-Schiff positive component, and luminal component). From this three-component model of the glomerulus, 232 digital glomerulus-specific features were hand-designed using traditional image analysis routines, such as morphological image processing. These features were measured on all glomeruli of the 54 patients, and fed as input to a recurrent neural network (RNN), whose target was prediction of the DN class. A Cohen's kappa statistic was used to measure the reliability of the RNN classifications against the renal pathologist it was trained on, achieving κ=0.55. Two separate renal pathologists respectively achieved reliabilities against the first pathologist of κ=0.48 and κ=0.68. Therefore, we concluded that the proposed digital pipeline is capable of classifying DN biopsies with similar reliability as renal pathologists.To expand the overall impact of our computational tools, we next investigated use of a machine learning algorithm to detect IFTA and glomerulosclerosis from kidney biopsies. As aforementioned, IFTA is prognostic for CKD progression and outcome. Glomerulosclerosis, the scarring of glomerular capillaries, is also a strong morphologic prognostic marker of CKD progression and outcome, and significantly associated with excess urinary protein excretion. IFTA is conventionally measured by a pathologist visually scanning the biopsy to identify regions of IFTA, and mentally estimating what percent area of the total biopsy is involved. The subjectivity of this process leads to significant reliability concerns among pathologists. Glomerulosclerosis is measured by dividing the number of glomeruli with complete (global) glomerulosclerosis by the total number of observed glomeruli, and typically shows higher reliability. We sought to investigate if machine learning methods can provide a reliable computational method to estimate these markers in kidney biopsy. A total of 205 patients pooled from 6 institutions across 3 continents were studied (refer to Section 4.4.1 for IRB approval information). One hundred sixteen biopsies were used to train and evaluate the performance of CNN's to detect IFTA and glomerulosclerosis in renal biopsy. Compared to the renal pathologist who provided the annotations to train the machine learner, the best of six configurations of CNN's achieved a Matthew's Correlation Coefficient (MCC) of 0.74 for IFTA, 0.94 for glomeruli, and 0.8 for glomerulosclerosis. The reliability of this best model was then evaluated against IFTA and glomerulosclerosis assessment made by four renal pathologists, using the intra-class correlation coefficient (ICC). The ICC among the four renal pathologists and the computer for measurement of IFTA percent was 0.94, as compared to 0.95 measured among the renal pathologists alone. For measurement of glomerulosclerosis ratio, the four pathologists alone achieved an ICC of 0.90, and including the computer raised this value to 0.91. Further, we compared the pixels labeled as IFTA by the pathologists and the computer using Cohen's kappa on the entire biopsy image. The lowest kappa scored by the computer was 0.48, compared to the lowest kappa scored by a human at 0.41. These data suggest the computer is similarly reliable to the assessment of multiple renal pathologists. Finally, for 30 transplant and 57 diabetic patients, we used a series of regression analyses and statistical testing to determine if the human and computer assessment methods show any difference in ability to predict patient outcome. It was determined that a statistically significant difference between the human and computer assessment methods could not be found. Therefore, from these experiments, we conclude that computational measurement of IFTA and glomerulosclerosis can demonstrate performance that is at least as reliable as renal pathologist assessment. These methods may be useful as a stand-in for renal pathologist expertise in scenarios where it is unavailable or infeasible. Through these works, we have demonstrated the feasibility of image analysis algorithms to replicate two common diagnostic tasks, classification of DN and estimation of chronic kidney damage. The first study demonstrated that optimal combinations of traditional image analysis and modern machine learning could create digital pipelines with great utility for pathologic analysis of kidney tissues. The second study demonstrated that these digital pipelines could even achieve similar prognostic power as measurements made by renal pathologist. These successes exposed the renal community to the power of computational processing and the ample opportunities to increase precision in kidney medicine. The tools developed in this thesis, released openly to the research community, will act as a stepping-stone for future investigators to study more interesting, complex, and hypothesis-driven questions, helping usher in the next era of precision kidney medicine.},
note = {AAI28496475}
}

@article{10.1016/j.comcom.2021.07.009,
author = {Miglani, Arzoo and Kumar, Neeraj},
title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {178},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2021.07.009},
doi = {10.1016/j.comcom.2021.07.009},
journal = {Comput. Commun.},
month = oct,
pages = {37–63},
numpages = {27},
keywords = {Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G}
}

@inproceedings{10.1007/978-3-030-64793-3_3,
author = {Granados, Alonso and Miah, Mohammad Sujan and Ortiz, Anthony and Kiekintveld, Christopher},
title = {A Realistic Approach for Network Traffic Obfuscation Using Adversarial Machine Learning},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_3},
doi = {10.1007/978-3-030-64793-3_3},
abstract = {Adversaries are becoming more sophisticated and standard countermeasures such as encryption are no longer enough to prevent traffic analysis from revealing important information about a network. Advanced encryption techniques are intended to mitigate network information exposure, but they remain vulnerable to statistical analysis of traffic features. An adversary can classify different applications and protocols from the observable statistical properties, especially from the meta-data (e.g. packet size, timing, flow directions, etc.). Several approaches are already being developed to protect computer network infrastructure from attacks using traffic analysis, but none of them are fully effective. We investigate solutions based on obfuscating the patterns in network traffic to make it more difficult to accurately use classification to extract information such as protocols or applications in use. A key problem of using obfuscation methods is to determine an appropriate algorithm that introduces minimal changes but preserves the functionality of the protocol. We apply Adversarial Machine Learning techniques to find realistic small perturbations that can improve the security and privacy of a network against traffic analysis. We introduce a novel approach for generating adversarial examples that obtains state-of-the-art performance compared to previous approaches, while considering more realistic constraints on perturbations.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {45–57},
numpages = {13},
keywords = {Network data analysis, Data obfuscation, Adversarial machine learning},
location = {College Park, MD, USA}
}

@article{10.1145/3450494,
author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi (Jason) and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
title = {A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3450494},
doi = {10.1145/3450494},
abstract = {The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing numbers of smart devices and improved hardware, there is interest in performing model training on the device. Given this surge in interest, a comprehensive survey of the field from a device-agnostic perspective sets the stage for both understanding the state of the art and for identifying open challenges and future avenues of research. However, on-device learning is an expansive field with connections to a large number of related topics in AI and machine learning (including online learning, model adaptation, one/few-shot learning, etc.). Hence, covering such a large number of topics in a single survey is impractical. This survey finds a middle ground by reformulating the problem of on-device learning as resource constrained learning where the resources are compute and memory. This reformulation allows tools, techniques, and algorithms from a wide variety of research areas to be compared equitably. In addition to summarizing the state of the art, the survey also identifies a number of challenges and next steps for both the algorithmic and theoretical aspects of on-device learning.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {15},
numpages = {49}
}

@inproceedings{10.1145/3447548.3467201,
author = {Lee, Changhun and Kim, Soohyeok and Lim, Chiehyeon and Kim, Jayun and Kim, Yeji and Jung, Minyoung},
title = {Diet Planning with Machine Learning: Teacher-forced REINFORCE for Composition Compliance with Nutrition Enhancement},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467201},
doi = {10.1145/3447548.3467201},
abstract = {Diet planning is a basic and regular human activity. Previous studies have considered diet planning a combinatorial optimization problem to generate solutions that satisfy a diet's nutritional requirements. However, this approach does not consider the composition of diets, which is critical for diet recipients' to accept and enjoy menus with high nutritional quality. Without this consideration, feasible solutions for diet planning could not be provided in practice. This suggests the necessity of diet planning with machine learning, which extracts implicit composition patterns from real diet data and applies these patterns when generating diets. This work is original research that defines diet planning as a machine learning problem; we describe diets as sequence data and solve a controllable sequence generation problem. Specifically, we develop the Teacher-forced REINFORCE algorithm to connect neural machine translation and reinforcement learning for composition compliance with nutrition enhancement in diet generation. Through a real-world application to diet planning for children, we validated the superiority of our work over the traditional combinatorial optimization and modern machine learning approaches, as well as human (i.e., professional dietitians) performance. In addition, we construct and open the databases of menus and diets to motivate and promote further research and development of diet planning with machine learning. We believe this work with data science will contribute to solving economic and social problems associated with diet planning.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3150–3160},
numpages = {11},
keywords = {controllable sequence generation, diet planning, machine learning, reinforce, teacher-forcing},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1007/s10462-020-09876-9,
author = {Goh, G. D. and Sing, S. L. and Yeong, W. Y.},
title = {A review on machine learning in 3D printing: applications, potential, and challenges},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09876-9},
doi = {10.1007/s10462-020-09876-9},
abstract = {Additive manufacturing (AM) or 3D printing is growing rapidly in the manufacturing industry and has gained a lot of attention from various fields owing to its ability to fabricate parts with complex features. The reliability of the 3D printed parts has been the focus of the researchers to realize AM as an end-part production tool. Machine learning (ML) has been applied in various aspects of AM to improve the whole design and manufacturing workflow especially in the era of industry 4.0. In this review article, various types of ML techniques are first introduced. It is then followed by the discussion on their use in various aspects of AM such as design for 3D printing, material tuning, process optimization, in situ monitoring, cloud service, and cybersecurity. Potential applications in the biomedical, tissue engineering and building and construction will be highlighted. The challenges faced by ML in AM such as computational cost, standards for qualification and data acquisition techniques will also be discussed. In the authors’ perspective, in situ monitoring of AM processes will significantly benefit from the object detection ability of ML. As a large data set is crucial for ML, data sharing of AM would enable faster adoption of ML in AM. Standards for the shared data are needed to facilitate easy sharing of data. The use of ML in AM will become more mature and widely adopted as better data acquisition techniques and more powerful computer chips for ML are developed.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {63–94},
numpages = {32},
keywords = {Machine learning, Artificial intelligence, 3D printing, In-situ monitoring, Additive manufacturing, Process optimization}
}

@article{10.1016/j.jksuci.2018.04.002,
author = {Raj S., Sridhar and M., Nandhini},
title = {Ensemble human movement sequence prediction model with Apriori based Probability Tree Classifier (APTC) and Bagged J48 on Machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.04.002},
doi = {10.1016/j.jksuci.2018.04.002},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = may,
pages = {408–416},
numpages = {9},
keywords = {Data mining, Machine learning, Spatial-temporal-social data, Trajectory analysis, Human movement sequence prediction}
}

@inproceedings{10.1145/3366424.3383562,
author = {G. Harris, Christopher},
title = {Mitigating Cognitive Biases in Machine Learning Algorithms for Decision Making},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383562},
doi = {10.1145/3366424.3383562},
abstract = {Cognitive biases are an ingrained part of the human decision-making process. Nearly all machine learning algorithms that mimic human decision-making use human judgments as training data, which propagates these biases. In this paper, we conduct an empirical study in which 150 applicants are rated for suitability for three separate job openings. We develop an algorithm that learns from human judgments and consequently develops biases based on these human-generated inputs. Next, we explore and apply techniques to mitigate these algorithmic biases, using a combination of pre-processing, in-processing, and post-processing algorithms. The results from our study show that biases can be mitigated using these approaches but involve a tradeoff between complexity and effectiveness.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {775–781},
numpages = {7},
keywords = {Algorithmic Bias, Artificial Intelligence, Cognitive Bias, Data Science, Decision Making, Fairness, Human Resources Tasks, Machine Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3403584,
author = {Hu, Yong and Mettler, Marcel and Mueller-Gritschneder, Daniel and Wild, Thomas and Herkersdorf, Andreas and Schlichtmann, Ulf},
title = {Machine Learning Approaches for Efficient Design Space Exploration of Application-Specific NoCs},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3403584},
doi = {10.1145/3403584},
abstract = {In many Multi-Processor Systems-on-Chip (MPSoCs), traffic between cores is unbalanced. This motivates the use of an application-specific Network-on-Chip (NoC) that is customized and can provide a high performance at low cost in terms of power and area. However, finding an optimized application-specific NoC architecture is a challenging task due to the huge design space.This article proposes to apply machine learning approaches for this task. Using graph rewriting, the NoC Design Space Exploration (DSE) is modelled as a Markov Decision Process (MDP). Monte Carlo Tree Search (MCTS), a technique from reinforcement learning, is used as search heuristic. Our experimental results show that—with the same cost function and exploration budget—MCTS finds superior NoC architectures compared to Simulated Annealing (SA) and a Genetic Algorithm&nbsp;(GA). However, the NoC DSE process suffers from the high computation time due to expensive cycle-accurate SystemC simulations for latency estimation. This article therefore additionally proposes to replace latency simulation by fast latency estimation using a Recurrent Neural Network (RNN). The designed RNN is sufficiently general for latency estimation on arbitrary NoC architectures. Our experiments show that compared to SystemC simulation, the RNN-based latency estimation offers a similar speed-up as the widely used Queuing Theory (QT). Yet, in terms of estimation accuracy and fidelity, the RNN is superior to QT, especially for high-traffic scenarios. When replacing SystemC simulations with the RNN estimation, the obtained solution quality decreases only slightly, whereas it suffers significantly when QT is used.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {44},
numpages = {27},
keywords = {Application-specific networks-on-chip, Monte-Carlo-tree search, design space exploration, recurrent neural network}
}

@article{10.1016/j.compbiomed.2020.104127,
author = {Yang, Zhijian and Olszewski, Daniel and He, Chujun and Pintea, Giulia and Lian, Jun and Chou, Tom and Chen, Ronald C. and Shtylla, Blerta},
title = {Machine learning and statistical prediction of patient quality-of-life after prostate radiation therapy},
year = {2021},
issue_date = {Feb 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104127},
doi = {10.1016/j.compbiomed.2020.104127},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {12},
keywords = {Machine learning, Convolutional neural network, Radiation therapy, Organ sensitivity, Prostate cancer}
}

@article{10.1007/s10845-016-1254-6,
author = {Malaca, Pedro and Rocha, Luis F. and Gomes, D. and Silva, Jo\~{a}o and Veiga, Germano},
title = {Online inspection system based on machine learning techniques: real case study of fabric textures classification for the automotive industry},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1254-6},
doi = {10.1007/s10845-016-1254-6},
abstract = {This paper focus on the classification, in real-time and under uncontrolled lighting, of fabric textures for the automotive industry. Many industrial processes have spatial constraints that limit the effective control of illumination of their vision based systems, hindering their effectiveness. The ability to overcome these problems using robust classification methods with suitable pre-processing techniques and choice of characteristics will increase the efficiency of this type of solutions with obvious production gains and thus economical. For this purpose, this paper studied and analyzed various pre-processing techniques, and selected the most appropriate fabric characteristics for the considered industrial case scenario. The methodology followed was based on the comparison of two different machine learning classifiers, ANN and SVM, using a large set of samples with a large variability of lightning conditions to faithfully simulate the industrial environment. The obtained solution shows the sensibility of ANN over SVM considering the number of features and the size of the training set, showing the better effectiveness and robustness of the last. The characteristics vector uses histogram equalization, Laws filter and Sobel filter, and multi-scale analysis. By using a correlation based method was possible to reduce the number of features used, achieving a better balanced between processing time and classification ratio.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {351–361},
numpages = {11},
keywords = {Automotive industry, Computer vision, Fabric analyses, Machine learning, Perception and recognition, Uncontrolled illumination}
}

@inproceedings{10.1007/978-3-662-60292-8_1,
author = {Baltag, Alexandru and Li, Dazhu and Pedersen, Mina Young},
title = {On the Right Path: A Modal Logic for Supervised Learning},
year = {2019},
isbn = {978-3-662-60291-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-60292-8_1},
doi = {10.1007/978-3-662-60292-8_1},
abstract = {Formal learning theory formalizes the process of inferring a general result from examples, as in the case of inferring grammars from sentences when learning a language. Although empirical evidence suggests that children can learn a language without responding to the correction of linguistic mistakes, the importance of Teacher in many other paradigms is significant. Instead of focusing only on learner(s), this work develops a general framework—the supervised learning game (SLG)—to investigate the interaction between Teacher and Learner. In particular, our proposal highlights several interesting features of the agents: on the one hand, Learner may make mistakes in the learning process, and she may also ignore the potential relation between different hypotheses; on the other hand, Teacher is able to correct Learner’s mistakes, eliminate potential mistakes and point out the facts ignored by Learner. To reason about strategies in this game, we develop a modal logic of supervised learning (SLL). Broadly, this work takes a small step towards studying the interaction between graph games, logics and formal learning theory.},
booktitle = {Logic, Rationality, and Interaction: 7th International Workshop, LORI 2019, Chongqing, China, October 18–21, 2019, Proceedings},
pages = {1–14},
numpages = {14},
keywords = {Formal learning theory, Modal logic, Dynamic logic, Undecidability, Graph games},
location = {Chongqing, China}
}

@article{10.1016/j.asoc.2021.107622,
author = {Fu, Chao and Xu, Che and Xue, Min and Liu, Weiyong and Yang, Shanlin},
title = {Data-driven decision making based on evidential reasoning approach and machine learning algorithms},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {110},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107622},
doi = {10.1016/j.asoc.2021.107622},
journal = {Appl. Soft Comput.},
month = oct,
numpages = {10},
keywords = {Data-driven decision making, Evidential reasoning approach, Machine learning algorithms, Learning of criterion weights, Diagnosis of thyroid nodule}
}

@article{10.1145/3447814,
author = {Jia, Xiaowei and Willard, Jared and Karpatne, Anuj and Read, Jordan S. and Zwart, Jacob A. and Steinbach, Michael and Kumar, Vipin},
title = {Physics-Guided Machine Learning for Scientific Discovery: An Application in Simulating Lake Temperature Profiles},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447814},
doi = {10.1145/3447814},
abstract = {Physics-based models are often used to study engineering and environmental systems. The ability to model these systems is the key to achieving our future environmental sustainability and improving the quality of human life. This article focuses on simulating lake water temperature, which is critical for understanding the impact of changing climate on aquatic ecosystems and assisting in aquatic resource management decisions. General Lake Model (GLM) is a state-of-the-art physics-based model used for addressing such problems. However, like other physics-based models used for studying scientific and engineering systems, it has several well-known limitations due to simplified representations of the physical processes being modeled or challenges in selecting appropriate parameters. While state-of-the-art machine learning models can sometimes outperform physics-based models given ample amount of training data, they can produce results that are physically inconsistent. This article proposes a physics-guided recurrent neural network model (PGRNN) that combines RNNs and physics-based models to leverage their complementary strengths and improves the modeling of physical processes. Specifically, we show that a PGRNN can improve prediction accuracy over that of physics-based models (by over 20% even with very little training data), while generating outputs consistent with physical laws. An important aspect of our PGRNN approach lies in its ability to incorporate the knowledge encoded in physics-based models. This allows training the PGRNN model using very few true observed data while also ensuring high prediction accuracy. Although we present and evaluate this methodology in the context of modeling the dynamics of temperature in lakes, it is applicable more widely to a range of scientific and engineering disciplines where physics-based (also known as mechanistic) models are used.},
journal = {ACM/IMS Trans. Data Sci.},
month = may,
articleno = {20},
numpages = {26},
keywords = {Physics-guided machine learning, deep learning, scientific discovery}
}

@inproceedings{10.1007/978-3-030-61255-9_15,
author = {Cummings, Paul and Crooks, Andrew},
title = {Development of a Hybrid Machine Learning Agent Based Model for Optimization and Interpretability},
year = {2020},
isbn = {978-3-030-61254-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61255-9_15},
doi = {10.1007/978-3-030-61255-9_15},
abstract = {The use of agent-based models (ABMs) has become more widespread over the last two decades allowing resear chers to explore complex systems composed of heterogeneous and locally interacting entities. However, there are several challenges that the agent-based modeling community face. These relate to developing accurate measurements, minimizing a large complex parameter space and developing parsimonious yet accurate models. Machine Learning (ML), specifically deep reinforcement learning has the potential to generate new ways to explore complex models, which can enhance traditional computational paradigms such as agent-based modeling. Recently, ML algorithms have proved an important contribution to the determination of semi-optimal agent behavior strategies in complex environments. What is less clear is how these advances can be used to enhance existing ABMs. This paper presents Learning-based Actor-Interpreter State Representation (LAISR), a research effort that is designed to bridge ML agents with more traditional ABMs in order to generate semi-optimal multi-agent learning strategies. The resultant model, explored within a tactical game scenario, lies at the intersection of human and automated model design. The model can be decomposed into a format that automates aspects of the agent creation process, producing a resultant agent that creates its own optimal strategy and is interpretable to the designer. Our paper, therefore, acts as a bridge between traditional agent-based modeling and machine learning practices, designed purposefully to enhance the inclusion of ML-based agents in the agent-based modeling community.},
booktitle = {Social, Cultural, and Behavioral Modeling: 13th International Conference, SBP-BRiMS 2020, Washington, DC, USA, October 18–21, 2020, Proceedings},
pages = {151–160},
numpages = {10},
keywords = {Agent-based modeling, Machine Learning, Explainable artificial intelligence},
location = {Washington, DC, USA}
}

@inproceedings{10.1145/3443467.3443855,
author = {Zhang, Xiaoyan and Wang, Xiaodong},
title = {An Effective Bridge Cracks Classification Method Based on Machine Learning},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443855},
doi = {10.1145/3443467.3443855},
abstract = {Crack is the most common threat to the safety of bridges. Historical data show that the safety accidents caused by cracks account for more than 90% of the total bridge disasters. After a long period of engineering practice and rigorous theoretical analysis, it was found that 0.3 mm is the maximum allowable for bridge cracks. If the width exceeds the limit, the integrity of the bridge will be destroyed, and even a collapse accident will occur. Therefore, it is important to identify cracks in bridge structure effectively and provide information for structural disaster reduction projects in time. With the development of machine learning, bridge crack detection and classification based on deep learning has been paid more attention. This paper designs a bridge crack classification algorithm based on convolution neural network and support vector machine. Firstly, the captured image data are divided into training set and test set. Secondly, they are preprocessing and extracted features by convolution neural network. Lastly, they are classified by SVM. The proposed algorithm can solve the problems of insufficient samples and low classification accuracy, and realizes the effective and accurate classification.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {790–794},
numpages = {5},
keywords = {Bridge crack, Convolution neural network, Machine learning, Support vector machine},
location = {Xiamen, China},
series = {EITCE '20}
}

@article{10.3233/IP-200256,
author = {Ingrams, Alex and Giest, Sarah and Grimmelikhuijsen, Stephan},
title = {A machine learning approach to open public comments for policymaking},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {4},
issn = {1570-1255},
url = {https://doi.org/10.3233/IP-200256},
doi = {10.3233/IP-200256},
abstract = {In this paper, the author argues that the conflict between the copious amount of digital data processed by public organisations and the need for policy-relevant insights to aid public participation constitutes a ‘public information paradox’. Machine learning (ML) approaches may offer one solution to this paradox through algorithms that transparently collect and use statistical modelling to provide insights for policymakers. Such an approach is tested in this paper. The test involves applying an unsupervised machine learning approach with latent Dirichlet allocation (LDA) analysis of thousands of public comments submitted to the United States Transport Security Administration (TSA) on a 2013 proposed regulation for the use of new full body imaging scanners in airport security terminals. The analysis results in salient topic clusters that could be used by policymakers to understand large amounts of text such as in an open public comments process. The results are compared with the actual final proposed TSA rule, and the author reflects on new questions raised for transparency by the implementation of ML in open rule-making processes.},
journal = {Info. Pol.},
month = jan,
pages = {433–448},
numpages = {16},
keywords = {Public participation, transparency, open regulation, machine learning, algorithms}
}

@inproceedings{10.1145/3459637.3482397,
author = {Wang, Yongjie and Ding, Qinxu and Wang, Ke and Liu, Yue and Wu, Xingyu and Wang, Jinglong and Liu, Yong and Miao, Chunyan},
title = {The Skyline of Counterfactual Explanations for Machine Learning Decision Models},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482397},
doi = {10.1145/3459637.3482397},
abstract = {Counterfactual explanations are minimum changes of a given input to alter the original prediction by a machine learning model, usually from an undesirable prediction to a desirable one. Previous works frame this problem as a constrained cost minimization, where the cost is defined as L1/L2 distance (or variants) over multiple features to measure the change. In real-life applications, features of different types are hardly comparable and it is difficult to measure the changes of heterogeneous features by a single cost function. Moreover, existing approaches do not support interactive exploration of counterfactual explanations. To address above issues, we propose the skyline counterfactual explanations that define the skyline of counterfactual explanations as all non-dominated changes. We solve this problem as multi-objective optimization over actionable features. This approach does not require any cost function over heterogeneous features. With the skyline, the user can interactively and incrementally refine their goals on the features and magnitudes to be changed, especially when lacking prior knowledge to express their needs precisely. Intensive experiment results on three real-life datasets demonstrate that the skyline method provides a friendly way for finding interesting counterfactual explanations, and achieves superior results compared to the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2030–2039},
numpages = {10},
keywords = {counterfactual explanations, interactive query, multi-objective optimization, skyline},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3414274.3414275,
author = {Wang, Hongming},
title = {Stock Price Prediction Based on Machine Learning Approaches},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414275},
doi = {10.1145/3414274.3414275},
abstract = {Research of quantitate investment on stock price prediction is effective to help investors increase profits. Recently, technologies of machine learning have been well applied to explore the issue of stock trading. In this paper, Logistic Regression and Support Vector Machines (SVM) were adopted to solve the problem of predicting the trend of stock movements. The experiment showed that these two models could be effectively used in the stock market of China. Returns based on strategies we constructed were significantly better than the HS300 index. In different models, we analyzed the relationship between stock returns and different models. It found that the SVM model results are optimal. The annual return of the strategy based on SVM reached 17.13% and the maximum Drawdown was 0.32. In the future, we will not only focus on the stock market, but also plan to apply this strategy to other investment fields, such as trading of digital currency. We will also use other algorithms for research and comparison, such as andom forests, XGBoost.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {1–5},
numpages = {5},
keywords = {Annual return, Logistic Regression, Maximum Drawdown, Stock price prediction, Support Vector Machine},
location = {Xiamen, China},
series = {DSIT 2020}
}

@article{10.1145/3482854,
author = {Tarafdar, Naif and Di Guglielmo, Giuseppe and Harris, Philip C. and Krupa, Jeffrey D. and Loncar, Vladimir and Rankin, Dylan S. and Tran, Nhan and Wu, Zhenbin and Shen, Qianfeng and Chow, Paul},
title = {AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3482854},
doi = {10.1145/3482854},
abstract = {&nbsp;AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment &amp; Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {23},
numpages = {32},
keywords = {FPGAs, data center, hardware/software co-design}
}

@inproceedings{10.1145/3105831.3105845,
author = {El-Kilany, Ayman and El Tazi, Neamat and Ezzat, Ehab},
title = {Building Relation Extraction Templates via Unsupervised Learning},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105845},
doi = {10.1145/3105831.3105845},
abstract = {The vast amount of text published daily over the internet pose an opportunity to build unsupervised text mining models with a better or a comparable performance than existing models. In this paper, we investigate the problem of relation extraction and generation from text using an unsupervised model learned from news published online. We propose a clustering-based method to build a dataset of relations examples. News articles are clustered and once a cluster of sentences for each event in each piece of news is formed, relations between important entities in each event cluster are extracted and considered as examples of relations. Relations examples are used to build extraction templates in order to extract and generate readable relations summaries from new instances of news. The proposed unsupervised relation extraction and generation method is evaluated against multiple methods for relation extraction over different datasets where the proposed method has shown a comparable performance.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {228–234},
numpages = {7},
keywords = {Unsupervised Learning, Relation Extraction, Ranking, Clustering},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@inproceedings{10.1007/978-3-030-96282-1_2,
author = {Ahn, Robert and Supakkul, Sam and Zhao, Liping and Kolluri, Kirthy and Hill, Tom and Chung, Lawrence},
title = {Validating Business Problem Hypotheses: A Goal-Oriented and Machine Learning-Based Approach},
year = {2021},
isbn = {978-3-030-96281-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-96282-1_2},
doi = {10.1007/978-3-030-96282-1_2},
abstract = {Validating an elicited business problem hindering a business goal is often more important than finding solutions. For example, validating the impact of a client’s account balance toward an unpaid loan would be critical as a bank can take some actions to mitigate the problem. However, business organizations face difficulties confirming whether some business events are against a business goal. Some challenges to validate a problem are discovering testable factors, preparing relevant data to validate, and analyzing relationships between the business events. This paper proposes a goal-oriented and Machine Learning(ML)-based framework, Gomphy, using a problem hypothesis for validating business problems. We present an ontology and a process, an entity modeling method for a problem hypothesis to find testable factors, a data preparation method to build an ML dataset, and an evaluation method to detect relationships among the business events and goals. To see the strength and weaknesses of our framework, we have validated banking events behind an unpaid loan in one bank as an empirical study. We feel that at least the proposed approach helps validate business events against a goal, providing some insights about the validated problem.},
booktitle = {Big Data – BigData 2021: 10th International Conference, Held as Part of the Services Conference Federation, SCF 2021, Virtual Event, December 10–14, 2021, Proceedings},
pages = {17–33},
numpages = {17}
}

@article{10.1007/s00146-020-01045-4,
author = {Hagendorff, Thilo},
title = {Forbidden knowledge in machine learning reflections on the limits of research and publication},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-020-01045-4},
doi = {10.1007/s00146-020-01045-4},
abstract = {Certain research strands can yield “forbidden knowledge”. This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance, with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up till now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline deliberations on how to deal with questions concerning the dissemination of such information. It proposes a tentative ethical framework for the machine learning community on how to deal with forbidden knowledge and dual-use applications.},
journal = {AI Soc.},
month = sep,
pages = {767–781},
numpages = {15},
keywords = {Publication norms, Dual-use, Governance, Artificial intelligence, Machine learning, Forbidden knowledge}
}

@article{10.1007/s42979-021-00751-0,
author = {Nath, Rajdeep Kumar and Thapliyal, Himanshu and Humble, Travis S.},
title = {A Review of Machine Learning Classification Using Quantum Annealing for Real-World Applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {5},
url = {https://doi.org/10.1007/s42979-021-00751-0},
doi = {10.1007/s42979-021-00751-0},
abstract = {Optimizing the training of a machine learning pipeline helps in reducing training costs and improving model performance. One such optimizing strategy is quantum annealing, which is an emerging computing paradigm that has shown potential in optimizing the training of a machine learning model. The implementation of a physical quantum annealer has been realized by D-wave systems and is available to the research community for experiments. Recent experimental results on a variety of machine learning applications using quantum annealing have shown interesting results where the performance of classical machine learning techniques is limited by limited training data and high dimensional features. This article explores the application of D-wave’s quantum annealer for optimizing machine learning pipelines for real-world classification problems. We review the application domains on which a physical quantum annealer has been used to train machine learning classifiers. We discuss and analyze the experiments performed on the D-Wave quantum annealer for applications such as image recognition, remote sensing imagery, computational biology, and particle physics. We discuss the possible advantages and the problems for which quantum annealing is likely to be advantageous over classical computation.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {11},
keywords = {Quantum computing, Quantum annealing, Optimization, Machine learning, Classification}
}

@phdthesis{10.5555/AAI28264908,
author = {Ferleger, Benjamin I. and Ko, Andrew},
advisor = {J., Chizeck, Howard and A, Herron, Jeffrey},
title = {Machine Learning to Optimize Embedded Adaptive Deep Brain Stimulation},
year = {2020},
isbn = {9798569995226},
publisher = {University of Washington},
abstract = {This thesis focuses on the development and application of novel machine learning approaches to the problem of optimization in adaptive deep brain stimulation. As brain-computer and brain-machine interfacing has rapidly developed in the past few years, attention in the relevant research has shifted from proof-of-concept to proof-of-feasibility. One of the first and, therefore, best-developed neurotechnologies is deep brain stimulation (DBS). DBS is a surgical intervention prescribed for several treatment-refractory neurological conditions. First, a stimulating electrode is chronically implanted into a condition-specific deep brain structure. The parameters of the stimulation provided by this electrode are then set by a clinician. Stimulation remains at these parameters continuously unless a patient actively chooses to disable their treatment. As has been repeatedly demonstrated, DBS is a safe and effective treatment for a number of movement disorders and is under active investigation for potential use in several psychiatric conditions. DBS, however, is not a panacea. Battery replacement requires a revision surgery, and even rechargeable systems' batteries must generally be replaced at least once within a device's lifetime. Additionally, DBS therapy is associated with a number of unpleasant and sometimes dangerous side effects. These side effects can range from transient paresthesias to episodes of depression and mania, and are broadly correlated with high levels of stimulation over long periods of time. In addition to concerns over battery life and side effects, the programming procedure for DBS is based primarily on a back-and-forth between clinicians and patients in a clinical setting. If we define "optimal" treatment as the most complete suppression of symptoms with the least manifestation of side effects, then achieving the corresponding settings is the goal of this procedure. The time consuming nature of this procedure, when considered in the context of clinical time constraints and patient fatigue, means that the parameters selected are far more likely to be the first passable setting than the truly optimal one. Patients are generally given the ability to disable their stimulation or select from a small range of amplitudes, but cannot actively reprogram their devices outside of a clinical setting. One technique with the potential to alleviate concerns about side effects and battery life is adaptive deep brain stimulation (aDBS). aDBS refers to any method that uses feedback on a patient's state to modulate stimulation parameters in real time. This feedback could come in the form of gyroscope and accelerometer data in the case of movement disorders, or could be derived from neural signals that are correlated with the onset of symptoms. These signals are then processed and meaningful features extracted from them, which may in turn be used to determine the appropriate stimulation parameters. This ensures that stimulation is only applied as needed. It is important to note that aDBS systems also intrinsically expand the state space forDBS programming, potentially adding yet more complexity to an already laborious procedure. As directional leads become more common in DBS devices, this expanded programming state space further reduces the likelihood of optimal settings being reached. A twin requirement to developing effective aDBS systems is thus the design of a streamlined procedure for parameter optimization in DBS programming. This may be accomplished through the introduction of an automated programming pipeline. Through the collection of quantified data on symptom severity through the use of gyroscope or accelerometer data and the digitization of patient feedback on side effects, this pipeline could considerably speed testing. In addition, the digitization of the data required to analyze aDBS parameter performance could be integrated with modern optimization techniques, such that a personalized optimal treatment may be determined to within an increased degree of certainty. This work details approaches for resolving these deeply interwoven problems in aDBS treatment through insights from the fields of machine learning and optimization. We begin by considering the current state of the art in adaptive deep brain stimulation, its accomplishments, but especially its limitations. The principal limitations are: a general reliance on distributed systems that hinder free movement in patients; a focus solely on computationally inexpensive, but potentially suboptimal, binary aDBS control strategies; and the lack of an effective pipeline to deploy optimization methods during or after programming. Furthermore, recognition that these limitations are fundamentally interrelated implies that an integrated approach is required. The three key developments detailed in this work are thus themselves closely interrelated. Despite minor reductions in power savings, fully embedded binary aDBS is specifically de-signed to maximize therapeutic efficacy and ease of programming. Our results demonstrate that such a system is prepared for widespread studies in movement disorders. Our graded aDBS system yielded inconclusive results with regards to power savings and therapeutic efficacy. However, basing our approach to feature selection for symptom estimation from neural data on a model-free foundation has yielded promising evidence for relying on data-driven feature extraction. This approach to feature extraction intrinsically requires less direct programming, and instead maximizes the insights that may be gained from the data itself. Our development of a pipeline for automated programming of DBS parameters based on inertial measurements and patient feedback on side effects was designed to generalize easily into future integration with aDBS programming procedures. Finally, our computational approach to extracting information from a tablet- and mobile-based application demonstrates that semi- or fully-automated remote symptom assessment has the potential to significantly improve the future delivery of optimized, individualized treatment. Throughout this work, integration is a key component of discussion and consideration. Each result extracted from the developments discussed herein represents a small step away from the current standard of care. Considered individually, these steps would be taken incompletely different directions. It is the hope of the author that this work instead constitutes a realignment of these disparate goals and an explicit recognition of their inter-relatedness. Only by treating these problems as different faces of the same die can we arrive at truly optimized personalized treatment in aDBS.},
note = {AAI28264908}
}

@inproceedings{10.1007/978-3-031-23793-5_28,
author = {Godard, Pierre and L\"{o}ser, Kevin and Allauzen, Alexandre and Besacier, Laurent and Yvon, Fran\c{c}ois},
title = {Unsupervised Learning of&nbsp;Word Segmentation: Does Tone Matter?},
year = {2018},
isbn = {978-3-031-23792-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-23793-5_28},
doi = {10.1007/978-3-031-23793-5_28},
abstract = {In this paper, we investigate the usefulness of tonal features for unsupervised word discovery, taking Mboshi, a low-resource tonal language from the Bantu family, as our main target language. In a preliminary step, we show that tone annotation improves the performance of supervised learning when using a simplified representation of the data. To leverage this information in an unsupervised setting, we then present a probabilistic model based on a hierarchical Pitman-Yor process that incorporates tonal representations in its backoff structure. We compare our model with a tone-agnostic baseline and analyze if and how tone helps unsupervised segmentation on our small dataset.},
booktitle = {Computational Linguistics and Intelligent Text Processing: 19th International Conference, CICLing 2018, Hanoi, Vietnam, March 18–24, 2018, Revised Selected Papers, Part I},
pages = {349–359},
numpages = {11},
keywords = {Hierarchical Pitman-Yor process, Mboshi, Tonal features},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/3419111.3421296,
author = {Viswanathan, Raajay and Balasubramanian, Arjun and Akella, Aditya},
title = {Network-accelerated distributed machine learning for multi-tenant settings},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421296},
doi = {10.1145/3419111.3421296},
abstract = {Many distributed machine learning (DML) workloads are increasingly being run in shared clusters. Training in such clusters can be impeded by unexpected compute and network contention, resulting in stragglers. We present MLfabric, a contention-aware DML system that manages the performance of a DML job running in a shared cluster. The DML application hands all network communication (gradient and model transfers) to the MLfabric communication library. MLfabric then carefully orders transfers to improve convergence, opportunistically aggregates them at idle DML workers to improve resource efficiency, and replicates them to support new notions of fault tolerance, while systematically accounting for compute stragglers and network contention. We find that MLfabric achieves up to 3x speed-up in training large deep learning models in realistic dynamic cluster settings.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {447–461},
numpages = {15},
keywords = {multi-tenancy, in-network computation, distributed machine learning},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@article{10.1007/s00500-021-05926-8,
author = {AlZubi, Ahmad Ali and Al-Maitah, Mohammed and Alarifi, Abdulaziz},
title = {Cyber-attack detection in healthcare using cyber-physical system and machine learning techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05926-8},
doi = {10.1007/s00500-021-05926-8},
abstract = {Cyber-physical systems have been extensively utilized in healthcare domains to deliver high-quality patient treatment in multifaceted clinical scenarios. The medical device’ heterogeneity involved in these systems (mobile devices and body sensor nodes) introduces enormous attack surfaces and therefore necessitates effective security solutions for these complex environments. Hence, in this study, the cognitive machine learning assisted Attack Detection Framework has been proposed to share healthcare data securely. The Healthcare Cyber-Physical Systems will be proficient in spreading the collected data to cloud storage. Machine learning models predict cyber-attack behavior, and processing this data can offer healthcare specialists decision support. This proposed approach is based on a patient-centric design that safeguards the information on a trusted device like the end-users mobile phones and end-user control data sharing access. Experimental results demonstrate that our suggested model achieves an attack prediction ratio of 96.5%, an accuracy ratio of 98.2%, an efficiency ratio of 97.8%, less delay of 21.3%, and a communication cost of 18.9% to other existing models.},
journal = {Soft Comput.},
month = sep,
pages = {12319–12332},
numpages = {14},
keywords = {Healthcare system, Attack or malicious detection, Machine learning, Cyber-physical system}
}

@inproceedings{10.1145/3445970.3451155,
author = {Kundu, Partha Pratim and Anatharaman, Lux and Truong-Huu, Tram},
title = {An Empirical Evaluation of Automated Machine Learning Techniques for Malware Detection},
year = {2021},
isbn = {9781450383202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445970.3451155},
doi = {10.1145/3445970.3451155},
abstract = {Nowadays, it is increasingly difficult even for a machine learning expert to incorporate all of the recent best practices into their modeling due to the fast development of state-of-the-art machine learning techniques. For the applications that handle big data sets, the complexity of the problem of choosing the best performing model with the best hyper-parameter setting becomes harder. In this work, we present an empirical evaluation of automated machine learning (AutoML) frameworks or techniques that aim to optimize hyper-parameters for machine learning models to achieve the best achievable performance. We apply AutoML techniques to the malware detection problem, which requires achieving the true positive rate as high as possible while reducing the false positive rate as low as possible. We adopt two AutoML frameworks, namely AutoGluon-Tabular and Microsoft Neural Network Intelligence (NNI) to optimize hyper-parameters of a Light Gradient Boosted Machine (LightGBM) model for classifying malware samples. We carry out extensive experiments on two data sets. The first data set is a publicly available data set (EMBER data set), that has been used as a benchmarking data set for many malware detection works. The second data set is a private data set we have acquired from a security company that provides recently-collected malware samples. We provide empirical analysis and performance comparison of the two AutoML frameworks. The experimental results show that AutoML frameworks could identify the set of hyper-parameters that significantly outperform the performance of the model with the known best performing hyper-parameter setting and improve the performance of a LightGBM classifier with respect to the true positive rate from $86.8%$ to $90%$ at $0.1%$ of false positive rate on EMBER data set and from $80.8%$ to $87.4%$ on the private data set.},
booktitle = {Proceedings of the 2021 ACM Workshop on Security and Privacy Analytics},
pages = {75–81},
numpages = {7},
keywords = {malware detection, hyper-parameter optimization, automated machine learning},
location = {Virtual Event, USA},
series = {IWSPA '21}
}

@inproceedings{10.1145/3425174.3425226,
author = {Santos, Sebasti\~{a}o H. N. and da Silveira, Beatriz Nogueira Carvalho and Andrade, Stev\~{a}o A. and Delamaro, M\'{a}rcio and Souza, Simone R. S.},
title = {An Experimental Study on Applying Metamorphic Testing in Machine Learning Applications},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425226},
doi = {10.1145/3425174.3425226},
abstract = {Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {98–106},
numpages = {9},
keywords = {Metamorphic Test, Machine Learning, Experimental Study},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1007/s11042-021-10906-z,
author = {Shahid, Eman and Arain, Qasim Ali},
title = {Indoor positioning: “an image-based crowdsource machine learning approach”},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {17},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10906-z},
doi = {10.1007/s11042-021-10906-z},
abstract = {Various technologies have been utilized today for recognizing client or user in the indoor areas. These technologies incorporate RSSI, Bluetooth Low Energy Beacons, Ultrasound waves, Vision-based advances, for example, fixed camera recordings QR codes, remote gadgets, etc. RSSI fingerprinting technique requires more effort and it is also expensive to be used for indoor localization frameworks working in real-time. In this research, indoor localization based on images is investigated as an option in contrast to other indoor positioning techniques using these days. Image-based indoor positioning is more affordable than RSSI based technologies being utilized. A mobile phone camera is utilized to take the pictures of area inside the building to find the user inside the building. Sensor data from various sensors isn’t required or no extra framework is required to find the client in the building utilizing indoor positioning based on an image. Microsoft Azure Custom Vision Services are utilized to locate the client; MS Azure classifies the pictures in one of the labels made. Strategy’s attainability is demonstrated by various investigations and accomplished accuracy and review is recorded above 90%. The average precision of the trained model is recorded above 95%.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {26213–26235},
numpages = {23},
keywords = {Non-line-of-sight, Locality sensitive hashing learning vector quantization, Location-based services, Wireless sensor network, Wireless local area networking, Received signal strength indication, Radio frequency identification, Robust crowdsourcing-based indoor localization system radio frequency, Principal component analysis scale-invariant feature transform, Indoor positioning system, Global positioning system}
}

@inproceedings{10.1145/2808492.2808507,
author = {Wei-Ya, Ren and Pan, Liu and Guo-Hui, Li},
title = {Semi-supervised learning via nonnegative least squares regression},
year = {2015},
isbn = {9781450335287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808492.2808507},
doi = {10.1145/2808492.2808507},
abstract = {Graph construction is the key step in graph based semi-supervised learning methods. In order to improve the quality of the graph, we consider the nonnegative constraint and noise estimation based on the least squares regression (LSR). A novel graph construction method named nonnegative least squares regression (NLSR) is proposed in this paper. The nonnegative constraint is considered to eliminate subtractive combinations of coefficients and to improve the sparsity of the graph. We consider both small Guassian noise and sparse corrupted noise to improve the robustness of the proposed method. Experiments show that the nonnegative constraint plays the dominate role. Local and global consistency (LGC) is adopted as the semi-supervised learning method. The label propagation error rate is regarded as the evaluation criterion. Extensive experiments show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms in semi-supervised learning, especially in improving LSR method significantly.},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service},
articleno = {15},
numpages = {6},
keywords = {semi-supervised learning, non-negative constraint, least squares regression, graph construction},
location = {Zhangjiajie, Hunan, China},
series = {ICIMCS '15}
}

@phdthesis{10.5555/AAI28157412,
author = {Som, Anirudh and Krishnamurthi, Narayanan and Spanias, Andreas and Li, Baoxin},
advisor = {Pavan, Turaga,},
title = {Building Invariant, Robust and Stable Machine Learning Systems Using Geometry and Topology},
year = {2020},
isbn = {9798557029346},
publisher = {Arizona State University},
address = {USA},
abstract = {Over the past decade, machine learning research has made great strides and significant impact in several fields. Its success is greatly attributed to the development of effective machine learning algorithms like deep neural networks (a.k.a. deep learning), availability of large-scale databases and access to specialized hardware like Graphic Processing Units. When designing and training machine learning systems, researchers often assume access to large quantities of data that capture different possible variations. Variations in the data is needed to incorporate desired invariance and robustness properties in the machine learning system, especially in the case of deep learning algorithms. However, it is very difficult to gather such data in a real-world setting. For example, in certain medical/healthcare applications, it is very challenging to have access to data from all possible scenarios or with the necessary amount of variations as required to train the system. Additionally, the over-parameterized and unconstrained nature of deep neural networks can cause them to be poorly trained and in many cases over-confident which, in turn, can hamper their reliability and generalizability. This dissertation is a compendium of my research efforts to address the above challenges. I propose building invariant feature representations by wedding concepts from topological data analysis and Riemannian geometry, that automatically incorporate the desired invariance properties for different computer vision applications. I discuss how deep learning can be used to address some of the common challenges faced when working with topological data analysis methods. I describe alternative learning strategies based on unsupervised learning and transfer learning to address issues like dataset shifts and limited training data. Finally, I discuss my preliminary work on applying simple orthogonal constraints on deep learning feature representations to help develop more reliable and better calibrated models.},
note = {AAI28157412}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Software product lines, Reuse, Prediction, Post-release defects, Failure-prone files, Change metrics}
}

@inproceedings{10.1007/978-3-030-58604-1_37,
author = {Baldassarre, Federico and Smith, Kevin and Sullivan, Josephine and Azizpour, Hossein},
title = {Explanation-Based Weakly-Supervised Learning of Visual Relations with Graph Networks},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_37},
doi = {10.1007/978-3-030-58604-1_37},
abstract = {Visual relationship detection is fundamental for holistic image understanding. However, the localization and classification of (subject, predicate, object) triplets remain challenging tasks, due to the combinatorial explosion of possible relationships, their long-tailed distribution in natural images, and an expensive annotation process.This paper introduces a novel weakly-supervised method for visual relationship detection that relies on minimal image-level predicate labels. A&nbsp;graph neural network is trained to classify predicates in images from a graph representation of detected objects, implicitly encoding an inductive bias for pairwise relations. We then frame relationship detection as the explanation of such a predicate classifier, i.e. we obtain a complete relation by recovering the subject and object of a predicted predicate.We present results comparable to recent fully- and weakly-supervised methods on three diverse and challenging datasets: HICO-DET for human-object interaction, Visual Relationship Detection for generic object-to-object relations, and UnRel for unusual triplets; demonstrating robustness to non-comprehensive annotations and good few-shot generalization.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {612–630},
numpages = {19},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-030-58449-8_2,
author = {Bertossi, Leopoldo},
title = {Score-Based Explanations in Data Management and Machine Learning},
year = {2020},
isbn = {978-3-030-58448-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58449-8_2},
doi = {10.1007/978-3-030-58449-8_2},
abstract = {We describe some approaches to explanations for observed outcomes in data management and machine learning. They are based on the assignment of numerical scores to predefined and potentially relevant inputs. More specifically, we consider explanations for query answers in databases, and for results from classification models. The described approaches are mostly of a causal and counterfactual nature. We argue for the need to bring domain and semantic knowledge into score computations; and suggest some ways to do this.},
booktitle = {Scalable Uncertainty Management: 14th International Conference, SUM 2020, Bozen-Bolzano, Italy, September 23–25, 2020, Proceedings},
pages = {17–31},
numpages = {15},
location = {Bozen-Bolzano, Italy}
}

@inproceedings{10.1109/ICASSP.2018.8461684,
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
title = {Unsupervised Learning of Semantic Audio Representations},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2018.8461684},
doi = {10.1109/ICASSP.2018.8461684},
abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {126–130},
numpages = {5},
location = {Calgary, AB, Canada}
}

@inproceedings{10.1145/3449726.3459459,
author = {Fellers, Justin and Quevedo, Jose and Abdelatti, Marwan and Steinhaus, Meghan and Sodhi, Manbir},
title = {Selecting between evolutionary and classical algorithms for the CVRP using machine learning: optimization of vehicle routing problems},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459459},
doi = {10.1145/3449726.3459459},
abstract = {Solutions for NP-hard problems are often obtained using heuristics that yield results relatively quickly, at some cost to the objective. Many different heuristics are usually available for the same problem type, and the solution quality of a heuristic may depend on characteristics of the instance being solved. This paper explores the use of machine learning to predict the best heuristic for solving Capacitated Vehicle Routing Problems (CVRPs). A set of 23 features related to the CVRP were identified from the literature. A large set of CVRP instances were generated across the feature space, and solved using four heuristics including a genetic algorithm and a novel self-organizing map. A neural network was trained to predict the best performing heuristic for a given problem instance. The model correctly selected the best heuristic for 79% of the CVRP test instances, while the single best heuristic was dominant for only 50% of the test instances.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {127–128},
numpages = {2},
keywords = {vehicle routing, self-organizing map, neural network, machine learning, genetic algorithm, evolutionary algorithm, algorithm selection},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Interacting products, Machine learning, Multi-objective search, Rule mining, Configuration, Product line}
}

@inproceedings{10.1007/978-3-319-24888-2_32,
author = {Peikari, Mohammad and Zubovits, Judit and Clarke, Gina and Martel, Anne L.},
title = {Clustering Analysis for Semi-supervised Learning Improves Classification Performance of Digital Pathology},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_32},
doi = {10.1007/978-3-319-24888-2_32},
abstract = {Purpose: Completely labeled datasets of pathology slides are often difficult and time consuming to obtain. Semi-supervised learning methods are able to learn reliable models from small number of labeled instances and large quantities of unlabeled data. In this paper, we explored the potential of clustering analysis for semi-supervised support vector machine (SVM) classifier. Method: A clustering analysis method was proposed to find regions of high density prior to finding the decision boundary using a supervised SVM and was compared with another state-of-the-art semi-supervised technique. Different percentages of labeled instances were used to train supervised and semi-supervised SVM learners from an image dataset generated from 50 whole-mount images (8 patients) of breast specimen. Their cross-validated classification performances were compared with each other using the area under the ROC curve measure. Result: Our proposed clustering analysis for semi-supervised learning was able to produce a reliable classification model from small amounts of labeled data. Comparing the proposed method in this study with a well-known implementation of semi-supervised SVM, our method performed much faster and produced better results.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {263–270},
numpages = {8},
location = {Munich, Germany}
}

@inproceedings{10.1007/978-3-030-59710-8_63,
author = {Yang, Hongxu and Shan, Caifeng and Kolen, Alexander F. and de With, Peter H. N.},
title = {Deep Q-Network-Driven Catheter Segmentation in 3D US by Hybrid Constrained Semi-supervised Learning and Dual-UNet},
year = {2020},
isbn = {978-3-030-59709-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59710-8_63},
doi = {10.1007/978-3-030-59710-8_63},
abstract = {Catheter segmentation in 3D ultrasound is important for computer-assisted cardiac intervention. However, a large amount of labeled images are required to train a successful deep convolutional neural network (CNN) to segment the catheter, which is expensive and time-consuming. In this paper, we propose a novel catheter segmentation approach, which requests fewer annotations than the supervised learning method, but nevertheless achieves better performance. Our scheme considers a deep Q learning as the pre-localization step, which avoids voxel-level annotation and which can efficiently localize the target catheter. With the detected catheter, patch-based Dual-UNet is applied to segment the catheter in 3D volumetric data. To train the Dual-UNet with limited labeled images and leverage information of unlabeled images, we propose a novel semi-supervised scheme, which exploits unlabeled images based on hybrid constraints from predictions. Experiments show the proposed scheme achieves a higher performance than state-of-the-art semi-supervised methods, while it demonstrates that our method is able to learn from large-scale unlabeled images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I},
pages = {646–655},
numpages = {10},
keywords = {Hybrid constraint, Semi-supervised learning, Dual-UNet, Deep reinforcement learning, Catheter segmentation},
location = {Lima, Peru}
}

@article{10.1155/2021/9923326,
author = {Gong, Fanghai and Wu, Wenqing},
title = {Workflow Scheduling Based on Mobile Cloud Computing Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9923326},
doi = {10.1155/2021/9923326},
abstract = {In recent years, cloud workflow task scheduling has always been an important research topic in the business world. Cloud workflow task scheduling means that the workflow tasks submitted by users are allocated to appropriate computing resources for execution, and the corresponding fees are paid in real time according to the usage of resources. For most ordinary users, they are mainly concerned with the two service quality indicators of workflow task completion time and execution cost. Therefore, how cloud service providers design a scheduling algorithm to optimize task completion time and cost is a very important issue. This paper proposes research on workflow scheduling based on mobile cloud computing machine learning, and this paper conducts research by using literature research methods, experimental analysis methods, and other methods. This article has deeply studied mobile cloud computing, machine learning, task scheduling, and other related theories, and a workflow task scheduling system model was established based on mobile cloud computing machine learning from different algorithms used in processing task completion time, task service costs, task scheduling, and resource usage The situation and the influence of different tasks on the experimental results are analyzed in many aspects. The algorithm in this paper speeds up the scheduling time by about 7% under a different number of tasks and reduces the scheduling cost by about 2% compared with other algorithms. The algorithm in this paper has been obviously optimized in time scheduling and task scheduling.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.3233/JIFS-189548,
author = {Xu, Min and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {PLC course performance evaluation based on machine learning and image feature retrieval},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189548},
doi = {10.3233/JIFS-189548},
abstract = {PLC is an indispensable technology for modern automation. The future social development will require a large number of PLC technical talents, so higher requirements are put forward for the teaching of PLC courses in colleges and universities. The intelligence and practical effect of the PLC course evaluation system are particularly important. Based on this, this article combines machine learning and image feature retrieval to construct a PLC course performance evaluation system. Moreover, this paper introduces the smoothness of the multispectral image, the smoothness of the blur function and the smoothness between the blur functions of adjacent spectral images as constraints and uses the gradient of the image blur kernel to express the smoothness of the image blur kernel itself. In addition, this article constructs the model system architecture according to the teaching requirements of the course and analyzes its realization process. Finally, in order to verify the performance of the model, this paper conducts system performance verification experiments through practical teaching methods and analyzes the results with statistical methods. The research results show that the PLC performance evaluation system constructed in this paper has a certain effect.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7209–7219},
numpages = {11},
keywords = {Machine learning, image features, feature retrieval, PLC courses, performance evaluation}
}

@article{10.3233/JIFS-189080,
author = {Yontar, Meltem and Namli, \"{O}zge H\"{u}sniye and Yanik, Seda and Kahraman, Cengiz},
title = {Using machine learning techniques to develop prediction models for detecting unpaid credit card customers},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189080},
doi = {10.3233/JIFS-189080},
abstract = {Customer behavior prediction is gaining more importance in the banking sector like in any other sector recently. This study aims to propose a model to predict whether credit card users will pay their debts or not. Using the proposed model, potential unpaid risks can be predicted and necessary actions can be taken in time. For the prediction of customers’ payment status of next months, we use Artificial Neural Network (ANN), Support Vector Machine (SVM), Classification and Regression Tree (CART) and C4.5, which are widely used artificial intelligence and decision tree algorithms. Our dataset includes 10713 customer’s records obtained from a well-known bank in Taiwan. These records consist of customer information such as the amount of credit, gender, education level, marital status, age, past payment records, invoice amount and amount of credit card payments. We apply cross validation and hold-out methods to divide our dataset into two parts as training and test sets. Then we evaluate the algorithms with the proposed performance metrics. We also optimize the parameters of the algorithms to improve the performance of prediction. The results show that the model built with the CART algorithm, one of the decision tree algorithm, provides high accuracy (about 86%) to predict the customers’ payment status for next month. When the algorithm parameters are optimized, classification accuracy and performance are increased.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6073–6087},
numpages = {15},
keywords = {CART, SVM, ANN, parameter optimization, classification, machine learning, Credit card}
}

@article{10.1007/s11042-019-08533-w,
author = {Sidhu, Ravneet Kaur and Kumar, Ravinder and Rana, Prashant Singh},
title = {Machine learning based crop water demand forecasting using minimum climatological data},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {19–20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08533-w},
doi = {10.1007/s11042-019-08533-w},
abstract = {Rice is one of the world’s most popular food crops. Since its production is dependent on intensive water use, water management is critical to ensure sustainability of water resource. However, very limited data is available on water use in rice irrigation. In the present study, traditional machine learning methods have been used to predict the irrigation schedule of rice daily. The data of year 2013-2015 is used to train the models and to further optimise it. The data of 2016-2017 is used for testing the models. Correlation thresholds are used for feature selection which helps in reducing the number of input parameters from the initial 26 to final 11. The models estimated the crop water demand as a function of weather parameters. Results show that Adaboost performed consistently well with an average accuracy of 71% as compared to other models for predicting the irrigation schedule.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13109–13124},
numpages = {16},
keywords = {Weather, Irrigation scheduling, Crop water demand, Drip irrigation, Random forest, Decision tree, Support vector regressor}
}

@inproceedings{10.5555/3504035.3504528,
author = {Shahrampour, Shahin and Beirami, Ahmad and Tarokh, Vahid},
title = {On data-dependent random features for improved generalization in supervised learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The randomized-feature approach has been successfully employed in large-scale kernel approximation and supervised learning. The distribution from which the random features are drawn impacts the number of features required to efficiently perform a learning task. Recently, it has been shown that employing data-dependent randomization improves the performance in terms of the required number of random features. In this paper, we are concerned with the randomized-feature approach in supervised learning for good generaliz-ability. We propose the Energy-based Exploration of Random Features (EERF) algorithm based on a data-dependent score function that explores the set of possible features and exploits the promising regions. We prove that the proposed score function with high probability recovers the spectrum of the best fit within the model class. Our empirical results on several benchmark datasets further verify that our method requires smaller number of random features to achieve a certain generalization error compared to the state-of-the-art while introducing negligible pre-processing overhead. EERF can be implemented in a few lines of code and requires no additional tuning parameters.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {493},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11761-019-00270-0,
author = {Subramanian, E. K. and Tamilselvan, Latha},
title = {A focus on future cloud: machine learning-based cloud security},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-019-00270-0},
doi = {10.1007/s11761-019-00270-0},
abstract = {Recent days have seen an apparent shift in most of the organizations moving towards using cloud environment and various cloud-based services. In order to protect and safeguard the transactions made by organizations over cloud environment, it is highly essential to provide a secure and robust environmental solution across cloud space. Existing approaches such as linear regression and support vector machine have been tried to promote cyber-security in the market by performing static verification of cloud user behaviour in order to identify pre-defined threats. Due to their static nature, these security solutions are restricted in their functionality. When it comes to access control, the decision making involves performing a permit or block operation. Also, the earlier methods face difficulties in terms of data protection over the endpoints which are not managed by the cloud. In order to solve the above-said problems, this paper is focused on designing a novel security solution for cloud applications using machine learning (ML) approaches. The main objective of this paper is to shape the future generation of cloud security using one of the ML algorithms such as convolution neural network because CNN can provide automatic and responsive approaches to enhance security in cloud environment. Instead of focusing only on detecting and identifying sensitive data patterns, ML can provide solutions which incorporate holistic algorithms for secure enterprise data throughout all the cloud applications. The proposed ML algorithm is experimented, results are verified and performance is evaluated by comparing with the existing approaches.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {237–249},
numpages = {13},
keywords = {Malicious threats, Cloud security, Machine learning}
}

@article{10.1007/s00521-021-06288-w,
author = {Chatterjee, Tanmoy and Essien, Aniekan and Ganguli, Ranjan and Friswell, Michael I.},
title = {The stochastic aeroelastic response analysis of helicopter rotors using deep and shallow machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {23},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06288-w},
doi = {10.1007/s00521-021-06288-w},
abstract = {This paper addresses the influence of manufacturing variability of a helicopter rotor blade on its aeroelastic responses. An aeroelastic analysis using finite elements in spatial and temporal domains is used to compute the helicopter rotor frequencies, vibratory hub loads, power required and stability in forward flight. The novelty of the work lies in the application of advanced data-driven machine learning (ML) techniques, such as convolution neural networks (CNN), multi-layer perceptron (MLP), random forests, support vector machines and adaptive Gaussian process (GP) for capturing the nonlinear responses of these complex spatio-temporal models to develop an efficient physics-informed ML framework for stochastic rotor analysis. Thus, the work is of practical significance as (i) it accounts for manufacturing uncertainties, (ii) accurately quantifies their effects on nonlinear response of rotor blade and (iii) makes the computationally expensive simulations viable by the use of ML. A rigorous performance assessment of the aforementioned approaches is presented by demonstrating validation on the training dataset and prediction on the test dataset. The contribution of the study lies in the following findings: (i) The uncertainty in composite material and geometric properties can lead to significant variations in the rotor aeroelastic responses and thereby highlighting that the consideration of manufacturing variability in analyzing helicopter rotors is crucial for assessing their behaviour in real-life scenarios. (ii) Precisely, the substantial effect of uncertainty has been observed on the six vibratory hub loads and the damping with the highest impact on the yawing hub moment. Therefore, sufficient factor of safety should be considered in the design to alleviate the effects of perturbation in the simulation results. (iii) Although advanced ML techniques are harder to train, the optimal model configuration is capable of approximating the nonlinear response trends accurately. GP and CNN followed by MLP achieved satisfactory performance. Excellent accuracy achieved by the above ML techniques demonstrates their potential for application in the optimization of rotors under uncertainty.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {16809–16828},
numpages = {20},
keywords = {Machine learning, Stochastic, Aeroelastic, Helicopter rotor}
}

@inproceedings{10.1145/3400302.3415763,
author = {Xie, Zhiyao and Li, Hai and Xu, Xiaoqing and Hu, Jiang and Chen, Yiran},
title = {Fast IR drop estimation with machine learning},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415763},
doi = {10.1145/3400302.3415763},
abstract = {IR drop constraint is a fundamental requirement enforced in almost all chip designs. However, its evaluation takes a long time, and mitigation techniques for fixing violations may require numerous iterations. As such, fast and accurate IR drop prediction becomes critical for reducing design turnaround time. Recently, machine learning (ML) techniques have been actively studied for fast IR drop estimation due to their promise and success in many fields. These studies target at various design stages with different emphasis, and accordingly, different ML algorithms are adopted and customized. This paper provides a review to the latest progress in ML-based IR drop estimation techniques. It also serves as a vehicle for discussing some general challenges faced by ML applications in electronics design automation (EDA), and demonstrating how to integrate ML models with conventional techniques for the better efficiency of EDA tools.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {13},
numpages = {8},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1155/2021/9943067,
author = {Dai, Xianyan and Li, Shangbin and Wu, Wenqing},
title = {Volleyball Data Analysis System and Method Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9943067},
doi = {10.1155/2021/9943067},
abstract = {After the reform and the opening up, the economy of my country has grown rapidly and people’s lives have become better and better. As a result, there is a lot of time to pay attention to their health, which has promoted the rapid development of my country’s sports industry. Since the 2008 Beijing Olympics, the successful hosting of the Beijing Olympics has been further strengthened. With the rise of the development of sports in our country, the use of machine learning in a large amount of information can process this data and analyze it well. Based on this, this article is aimed at making volleyball players and coaches better understand the technical structure of hiking and the technique of hiking. The paper understands the characteristics of muscle activity over time and uses machine learning methods to analyze a large number of volleyball sports data. In this experiment, 12 volleyball players from a college of physical education were selected. According to the actual situation of the students’ physical fitness and skills, it is more reasonable to divide them into two arms with preswing technology (A type) group and two-arms without preswing technology (B type) group. Mainly study the volleyball spiking action, select the representative front-row 4th position strong attack and the back-row 6th position for comparison and analysis, and analyze the process from the take-off stage to the aerial shot stage in the four stages of the smash through the kinematics, dynamics, and surface electromyography parameters. Experiments have shown that for type A, the left gluteus maximus integral EMG sum value is significantly different between the front and rear rows (P&lt;0.05). The discharge volume of the left gluteus maximus during the front-row spiking process is greater than that of the back-row spiking. This difference is mainly reflected in the kicking stage and the air attack stage. It shows that volleyball data analysis has a very broad prospect of exploration and application, which can create huge social and economic benefits. How to analyze kinematics is also a very demanding research project and is also part of the future analysis of sports data. Academic value and broad practical significance are important.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {11}
}

@article{10.1007/s13748-020-00214-2,
author = {Obukhov, Artem and Krasnyanskiy, Mikhail and Nikolyukin, Maxim},
title = {Algorithm of adaptation of electronic document management system based on machine learning technology},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {4},
url = {https://doi.org/10.1007/s13748-020-00214-2},
doi = {10.1007/s13748-020-00214-2},
abstract = {The topical problem in the development of electronic document management systems (EDMS) is their adaptation and personalization to the individual characteristics of the user. This article discusses the issue of development of an adaptation algorithm using machine learning methods for solving the problem of structural-parametric synthesis of EDMS. In the framework of the presented algorithm, the approaches to the formalization of workflow processes, ways to adapt the interface to the user parameters using artificial neural networks and a comprehensive assessment of the system’s adaptability are considered. The scientific novelty of the approach consists in the algorithmic and software development for automation of the data collection, analysis and interface adaptation through the use and integration of neural networks in the information system. The application of machine learning methods for the formation and adaptation of EDMS interface allows you to automate the process of personalizing it to the user’s individual characteristics, increase the system’s flexibility and provide the best user experience at the first interaction with EDMS based on the intelligent analysis of data about other users. The main scientific results obtained in the article include: formalized criteria for adapting EDMS; algorithm for designing and adapting EDMS; and development of software for adapting EDMS, including a trained neural network and API.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {287–303},
numpages = {17},
keywords = {Artificial neural networks, Machine learning, Criteria of adaptation, Electronic document management system}
}

@inproceedings{10.1145/3297067.3297078,
author = {Su, Shu and Yan, Hui and Ding, Ning},
title = {Machine Learning-Based Charging Network Operation Service Platform Reservation Charging Service System},
year = {2018},
isbn = {9781450366052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297067.3297078},
doi = {10.1145/3297067.3297078},
abstract = {This paper proposes a machine learning-based electric vehicle (EV) reserved charging service system, which takes into consideration the impacts from both the power system and transportation system. The proposed framework of charging network operation service platform links the power system with transportation system through the charging navigation of massive EVs. The "reserved charging + consumption" integrated service model would be great significant for dealing with large-scale integration of electric vehicles. It applies the concept of charging time window to optimization of EV charging prediction for the reserved charging service system, and designs a dynamic dispatching model based on sliding time axis to make charging process of users get rid of constraints of queuing time and charging service fee period.},
booktitle = {Proceedings of the 2018 International Conference on Signal Processing and Machine Learning},
pages = {1–5},
numpages = {5},
keywords = {reservation charging, machine learning, electric vehicle (EV), charging service fee},
location = {Shanghai, China},
series = {SPML '18}
}

@article{10.1016/j.future.2020.09.004,
author = {Mart\'{\i}nez Garre, Jos\'{e} Tom\'{a}s and Gil P\'{e}rez, Manuel and Ruiz-Mart\'{\i}nez, Antonio},
title = {A novel Machine Learning-based approach for the detection of SSH botnet infection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2020.09.004},
doi = {10.1016/j.future.2020.09.004},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {387–396},
numpages = {10},
keywords = {Botnet, Machine learning, Zero-day malware, Honeypot, High interaction}
}

@article{10.1007/s00500-021-05684-7,
author = {Candelieri, Antonio and Perego, Riccardo and Archetti, Francesco},
title = {Green machine learning via augmented Gaussian processes and multi-information source optimization},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {19},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05684-7},
doi = {10.1007/s00500-021-05684-7},
abstract = {Searching for accurate machine and deep learning models is a computationally expensive and awfully energivorous process. A strategy which has been recently gaining importance to drastically reduce computational time and energy consumed is to exploit the availability of different information sources, with different computational costs and different “fidelity,” typically smaller portions of a large dataset. The multi-source optimization strategy fits into the scheme of Gaussian Process-based Bayesian Optimization. An Augmented Gaussian Process method exploiting multiple information sources (namely, AGP-MISO) is proposed. The Augmented Gaussian Process is trained using only “reliable” information among available sources. A novel acquisition function is defined according to the Augmented Gaussian Process. Computational results are reported related to the optimization of the hyperparameters of a Support Vector Machine (SVM) classifier using two sources: a large dataset—the most expensive one—and a smaller portion of it. A comparison with a traditional Bayesian Optimization approach to optimize the hyperparameters of the SVM classifier on the large dataset only is reported.},
journal = {Soft Comput.},
month = oct,
pages = {12591–12603},
numpages = {13},
keywords = {Gaussian processes, Bayesian optimization, Multi information source optimization, Green machine learning, Green AI}
}

@article{10.1016/j.jpdc.2019.03.003,
author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
title = {The security of machine learning in an adversarial setting: A survey},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.03.003},
doi = {10.1016/j.jpdc.2019.03.003},
journal = {J. Parallel Distrib. Comput.},
month = aug,
pages = {12–23},
numpages = {12},
keywords = {Security model, Adversarial example, Adversarial attack, Adversarial setting, Machine learning}
}

@article{10.1007/s12650-018-0531-1,
author = {Jiang, Liu and Liu, Shixia and Chen, Changjian},
title = {Recent research advances on interactive machine learning},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {1343-8875},
url = {https://doi.org/10.1007/s12650-018-0531-1},
doi = {10.1007/s12650-018-0531-1},
abstract = {Interactive machine learning (IML) is an iterative learning process that tightly couples a human with a machine learner, which is widely used by researchers and practitioners to effectively solve a wide variety of real-world application problems. Although recent years have witnessed the proliferation of IML in the field of visual analytics, most recent surveys either focus on a specific area of IML or aim to summarize a visualization field that is too generic for IML. In this paper, we systematically review the recent literature on IML and classify them into a task-oriented taxonomy built by us. We conclude the survey with a discussion of open challenges and research opportunities that we believe are inspiring for future work in IML.},
journal = {J. Vis.},
month = apr,
pages = {401–417},
numpages = {17},
keywords = {Machine learning, Interactive visualization, Interactive machine learning}
}

@inproceedings{10.1145/3229607.3229612,
author = {Mulinka, Pavol and Casas, Pedro},
title = {Stream-based Machine Learning for Network Security and Anomaly Detection},
year = {2018},
isbn = {9781450359047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229607.3229612},
doi = {10.1145/3229607.3229612},
abstract = {Data Stream Machine Learning is rapidly gaining popularity within the network monitoring community as the big data produced by network devices and end-user terminals goes beyond the memory constraints of standard monitoring equipment. Critical network monitoring applications such as the detection of anomalies, network attacks and intrusions, require fast and continuous mechanisms for on-line analysis of data streams. In this paper we consider a stream-based machine learning approach for network security and anomaly detection, applying and evaluating multiple machine learning algorithms in the analysis of continuously evolving network data streams. The continuous evolution of the data stream analysis algorithms coming from the data stream mining domain, as well as the multiple evaluation approaches conceived for benchmarking such kind of algorithms makes it difficult to choose the appropriate machine learning model. Results of the different approaches may significantly differ and it is crucial to determine which approach reflects the algorithm performance the best. We therefore compare and analyze the results from the most recent evaluation approaches for sequential data on commonly used batch-based machine learning algorithms and their corresponding stream-based extensions, for the specific problem of on-line network security and anomaly detection. Similar to our previous findings when dealing with off-line machine learning approaches for network security and anomaly detection, our results suggest that adaptive random forests and stochastic gradient descent models are able to keep up with important concept drifts in the underlying network data streams, by keeping high accuracy with continuous re-training at concept drift detection times.},
booktitle = {Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {1–7},
numpages = {7},
keywords = {Network Attacks, Machine Learning, High-Dimensional Data, Data Stream mining},
location = {Budapest, Hungary},
series = {Big-DAMA '18}
}

@inproceedings{10.1145/3464298.3494884,
author = {S\'{a}nchez-Artigas, Marc and Sarroca, Pablo Gimeno},
title = {Experience Paper: Towards enhancing cost efficiency in serverless machine learning training},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3494884},
doi = {10.1145/3464298.3494884},
abstract = {Function-as-a-Service (FaaS) has raised a growing interest in how to "tame" serverless to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional "serverful" computing. To help in this task, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two key optimizations: a significance filter and a scale-in auto-tuner, and leverages them to specialize model training to the FaaS model. Our results certify that MLLess can be 15X faster than serverful ML systems [24] at a lower cost for ML models (such as sparse logistic regression and matrix factorization) that exhibit fast convergence.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {210–222},
numpages = {13},
keywords = {serverless computing, machine learnig},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.1145/3414080.3414081,
author = {Komendantskaya, Ekaterina and Kokke, Wen and Kienitz, Daniel},
title = {Continuous Verification of Machine Learning: a Declarative Programming Approach},
year = {2020},
isbn = {9781450388214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414080.3414081},
doi = {10.1145/3414080.3414081},
abstract = {In this invited talk, we discuss state of the art in neural network verification. We propose the term continuous verification to characterise the family of methods that explore continuous nature of machine learning algorithms. We argue that methods of continuous verification must rely on robust programming language infrastructure (refinement types, automated proving, type-driven program synthesis), which provides a major opportunity for the declarative programming language community. Keywords: Neural Networks, Verification, AI.},
booktitle = {Proceedings of the 22nd International Symposium on Principles and Practice of Declarative Programming},
articleno = {1},
numpages = {3},
location = {Bologna, Italy},
series = {PPDP '20}
}

@article{10.1007/s11277-021-08284-8,
author = {Hongsong, Chen and Yongpeng, Zhang and Yongrui, Cao and Bhargava, Bharat},
title = {Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {117},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08284-8},
doi = {10.1007/s11277-021-08284-8},
abstract = {Under big data environment, machine learning has been rapidly developed and widely used. It has been successfully applied in computer vision, natural language processing, computer security and other application fields. However, there are many security problems in machine learning under big data environment. For example, attackers can add “poisoned” sample to the data source, and big data process system will process these “poisoned” sample and use machine learning methods to train model, which will directly lead to wrong prediction results. In this paper, machine learning system and machine learning pipeline are proposed. The security problems that maybe occur in each stage of machine learning system under big data processing pipeline are analyzed comprehensively. We use four different attack methods to compare the attack experimental results.The security problems are classified comprehensively, and the defense approaches to each security problem are analyzed. Drone-deploy MapEngine is selected as a case study, we analyze the security threats and defense approaches in the Drone-Cloud machine learning application envirolment. At last,the future development drections of security issues and challenages in the machine learning system are proposed.},
journal = {Wirel. Pers. Commun.},
month = apr,
pages = {3505–3525},
numpages = {21},
keywords = {Case study, Defensive approaches, Security threats, Big data pipeline, Machine learning system}
}

@inproceedings{10.1145/3450439.3451864,
author = {Feng, Jean},
title = {Learning to safely approve updates to machine learning algorithms},
year = {2021},
isbn = {9781450383592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450439.3451864},
doi = {10.1145/3450439.3451864},
abstract = {Machine learning algorithms in healthcare have the potential to continually learn from real-world data generated during healthcare delivery and adapt to dataset shifts. As such, regulatory bodies like the US FDA have begun discussions on how to autonomously approve modifications to algorithms. Current proposals evaluate algorithmic modifications via hypothesis testing and control a definition of online approval error that only applies if the data is stationary over time, which is unlikely in practice. To this end, we investigate designing approval policies for modifications to ML algorithms in the presence of distributional shifts. Our key observation is that the approval policy most efficient at identifying and approving beneficial modifications varies across problem settings. So, rather than selecting a fixed approval policy a priori, we propose learning the best approval policy by searching over a family of approval strategies. We define a family of strategies that range in their level of optimism when approving modifications. To protect against settings where no version of the ML algorithm performs well, this family includes a pessimistic strategy that rescinds approval. We use the exponentially weighted averaging forecaster (EWAF) to learn the most appropriate strategy and derive tighter regret bounds assuming the distributional shifts are bounded. In simulation studies and empirical analyses, we find that wrapping approval strategies within EWAF is a simple yet effective approach to protect against distributional shifts without significantly slowing down approval of beneficial modifications.},
booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
pages = {164–173},
numpages = {10},
keywords = {prediction with expert advice, online learning, non-stationarity, AI/ML-based SaMD},
location = {Virtual Event, USA},
series = {CHIL '21}
}

@article{10.1016/j.ins.2019.02.030,
author = {Gonz\'{a}lez-Carrasco, Israel and Jim\'{e}nez-M\'{a}rquez, Jose Luis and L\'{o}pez-Cuadrado, Jose Luis and Ruiz-Mezcua, Bel\'{e}n},
title = {Automatic detection of relationships between banking operations using machine learning},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {485},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.030},
doi = {10.1016/j.ins.2019.02.030},
journal = {Inf. Sci.},
month = jun,
pages = {319–346},
numpages = {28},
keywords = {Finance, Business analytics, Pattern detection, Big data, Machine learning}
}

@phdthesis{10.5555/AAI28496031,
author = {Bandyopadhyay, Arka Prava and Wu, Liuren and Stroebel, Johannes},
advisor = {Yildiray, Yildirim, and Linda, Allen,},
title = {Strategic Default and Moral Hazard in Real Estate: Insights from Machine Learning Applications},
year = {2021},
isbn = {9798515255688},
publisher = {City University of New York},
address = {USA},
abstract = {Strategic default has been the achilles heel in academic finance for decades. By definition, whether a default has occurred due to strategic motive is unobservable. Moreover, a household has only so many avenues of conducting a strategic default. I use the context of commercial mortgages as property value as well property cashflow co-determine the default decision of these borrowers. I tease out the different strategic aspects of default from the ones emanating from liquidity constraints. The recent advances in Deep Neural Network (DNN), the advent of big data and the computational power associated with it has enabled me to disentangle the motive of default.Also, agency conflicts of brokers during origination of a mortgage loan and the moral hazards thereof has been documented based on the soft information about the borrowers. However, there have been few, if any paper, which retains the soft information about the borrowers, post origination, during the life of the loans. There has been a plethora of research about the biases generated towards foreclosures and other adverse outcomes post securitization for the last decade. But the soft information about the borrowers obtained by the brokers have been lost during the pooling process in securitization and there have been famous papers on the loss of information during the securitization process which happens at arms' length from the original lender. I bridge this gap by using novel data on proprietary call transcripts (textual data) between borrowers and servicers. I am also in the process of procuring audio files which can capture mood, content, tone of these communications.My dissertation documents the use of machine learning (ML) techniques in commercial and residential real estate to answer long-standing questions, which could not previously be answered due to paucity of data and computational resources. In the first chapter, Irun a horserace of Deep Neural Network with other ML models and parametric models to provide a new identification strategy to disentangle liquidity-constrained default and incentives for strategic default. The second chapter attempts to answer the most pressing current socio-economic issue in the United States. Specifically, I compute the social, racial and dollar cost of the CARES Act and find these adhoc policies are as expensive as direct payment of $2,000 to households, if not worse. Finally, in the third chapter I create a novel framework to ingest quantified time-varying soft information from call transcript text data about borrowers in ML models on hard information. I alleviate the information asymmetry between the borrowers and issuers, increase mortgage market efficiency and mitigate the conflict of interest between master servicers and special servicers.There has been recent literature on the applications of supervised, unsupervised and reinforcement learning in mainstream academic finance. But, very little work is done in the highly illiquid opaque real estate literature using the cutting edge methods in Machine Learning. I take a fresh look at some of the long-debated questions in the literature using some of the machine learning techniques. I am also able to able to use the current COVID-19 pandemic as an exogenous shock for robustness check in most of my current research.},
note = {AAI28496031}
}

@phdthesis{10.5555/AAI28962853,
author = {Amoako, Richard and Albert, Romkes, and Purushotham, Tukkaraja, and Kelli, McCormick, and Kurt, Katzenstein,},
advisor = {Andrea, Brickey,},
title = {Applied Machine Learning in Mine Safety and Short-Term Underground Mine Production Scheduling},
year = {2021},
isbn = {9798780619109},
publisher = {South Dakota School of Mines and Technology},
abstract = {Mining is a highly mechanized industry with inherent health and safety hazards. As operations have modernized, a safety-focused culture has been adopted and research-driven innovations have been implemented to reduce injuries and fatalities. A goal of the mining industry is to achieve zero injuries on all mine sites. To this end, researchers are developing ways of incorporating minimization of occupational hazards in the mine planning phase that maximizes net present value while managing safety and health risks.This research explores the potential of machine learning in contributing to robust mine safety analysis of an accident and injury data set over a ten-year period from the Mine Safety and Health Administration through the use of multiclass logistic regression. The analysis provides a means to determine a miner's susceptibility to the following injury classes: non-fatal with no days lost or restricted activity, non-fatal with days lost and/or days of restricted work activity, and fatal and total permanent or partial permanent disability. The results reveal that a miner's experience on their current job is a significant factor in injury occurrence, even for those with decades of total mining experience. From this analysis, machine learning proves to be a tool that can be used beyond basic statistics in providing robust mine accident and injury analysis.Furthermore, the research applies an artificial neural network (ANN), a machine learning tool, to other areas of mining, namely, mine ventilation, respirable dust, and underground mine production scheduling. The purpose of the neural network is to estimate, i.e., predict, the respirable dust emissions for a given mining activity. The ability to accurately estimate respirable dust concentrations can provide valuable information to the planning and ventilation engineers. Having information indicating the potential for higher concentrations of dust allows for appropriate actions to be taken prior to the initiation of the activity, thereby managing the situation proactively, instead of reactively.A means of proactive management would be to incorporate the predicted concentrations and requisite ventilation into short-term production schedules. To this end, the research further explores short-term scheduling formulations in underground mining by applying the principles of rescheduling and deviation minimization. The author improves upon an existing formulation by incorporating more realistic penalty functions for activity and production goal deviations. Activity earliness and tardiness are penalized with exponentially increasing values as deviations increase, while goal penalties are only imposed if certain predetermined production target levels are not met. This culminates in a tool that is able to determine alternative schedules in response to unforeseen operational disruptions. The new penalty systems are evaluated using two different scenarios and the resulting schedules are compared. The comparative analysis shows how operational disruptions impact the schedule and how the new formulation makes up for the ensuing deviations. Future work will focus on developing mathematical constraints for incorporating the ANN predictions and ventilation requirements into the short-term formulation to provide an enhanced proactive approach towards the management of respirable dust exposure.},
note = {AAI28962853}
}

@inproceedings{10.1145/3465416.3483302,
author = {Boykin, C. Malik and Dasch, Sophia T. and Rice Jr., Vincent and Lakshminarayanan, Venkat R. and Togun, Taiwo A. and Brown, Sarah M.},
title = {Opportunities for a More Interdisciplinary Approach to Measuring Perceptions of Fairness in Machine Learning},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483302},
doi = {10.1145/3465416.3483302},
abstract = {As machine learning (ML) is deployed in high-stakes domains, such as disease diagnosis or prison sentencing, questions of fairness have become an area of concern in its development. This interest has produced a variety of statistical fairness definitions derived from classical performance metrics which further expand the decisions that ML practitioners must make in building a system. The need to choose between these definitions raises questions about what conditions influence people to perceive an algorithm as fair or not. Recent results highlight the heavily contextual nature of fairness perceptions, and the specific conditions under which psychological principles such as framing can reliably sway these perceptions. Additional interdisciplinary insights include lessons from the replication crisis within psychology, from which we can glean best-practices for reproducible empirical research. We survey key research at the intersection of ML and psychology, focusing on psychological mechanisms underlying fairness preferences. We conclude by stating the continued need for interdisciplinary research, and underscore best-practices that can inform the state-of-the-art practice. We consider this research to be of a descriptive nature, enabling a deeper understanding and a substantiated discussion.},
booktitle = {Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {1},
numpages = {9},
keywords = {machine learning, fairness, experiment design},
location = {--, NY, USA},
series = {EAAMO '21}
}

@article{10.1007/s11277-019-06715-1,
author = {Dash, Sanjit Kumar and Dash, Siddhant and Mishra, Jibitesh and Mishra, Sasmita},
title = {Opportunistic Mobile Data Offloading Using Machine Learning Approach},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06715-1},
doi = {10.1007/s11277-019-06715-1},
abstract = {Currently, cellular networks both 3G and 4G are heavily overloaded due to increasing usage of mobile applications. Offloading mobile data traffic through opportunistic communications is one of promising solution to solve this problem. This has huge advantage over other kinds of offloading techniques like no extra cost, significant reduction of mobile traffic, high efficiency. As a case of study, we are focusing our investigation in opportunistic communication for optimizing target set selection problem. A new algorithm is proposed for generating target set which uses machine learning paradigm. Since, machine learning is an emerging sector in computer science with lots of potential; we integrated it with offloading procedure to achieve better performance in real world scenario. The efficiency of proposed method is measured by comparing it with other methods like Greedy, Heuristic and Random. A case study is conducted incorporating this approach and performance evaluation is done. It can be ensured from this comparison that the proposed algorithm outperforms its counterparts.},
journal = {Wirel. Pers. Commun.},
month = jan,
pages = {125–139},
numpages = {15},
keywords = {Information Dissemination, Machine Learning, Opportunistic Communication, Target Set Selection, Mobile Data Offloading}
}

@inproceedings{10.1145/3316781.3323472,
author = {Zhang, Jeff Jun and Liu, Kang and Khalid, Faiq and Hanif, Muhammad Abdullah and Rehman, Semeen and Theocharides, Theocharis and Artussi, Alessandro and Shafique, Muhammad and Garg, Siddharth},
title = {Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3323472},
doi = {10.1145/3316781.3323472},
abstract = {Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {175},
numpages = {4},
keywords = {Timing Errors, Security, Robustness, Reliability, Permanent Faults, Machine Learning, Deep Learning, Adversarial Attacks},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.1145/3234200.3234246,
author = {Mulinka, Pavol and Casas, Pedro},
title = {Adaptive Network Security through Stream Machine Learning},
year = {2018},
isbn = {9781450359153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234200.3234246},
doi = {10.1145/3234200.3234246},
abstract = {Stream Machine Learning is rapidly gaining popularity within the network monitoring community as the big data produced by network devices and end-user terminals goes beyond the memory constraints of standard monitoring equipment. We consider a stream-based machine learning approach to network security, conceiving adaptive machine learning algorithms for the analysis of continuously evolving network data streams. Using a sliding-window adaptive-size approach, we show that adaptive random forests models are able to keep up with important concept drifts in the underlying network data streams, by keeping high accuracy with continuous re-training at concept drift detection times.},
booktitle = {Proceedings of the ACM SIGCOMM 2018 Conference on Posters and Demos},
pages = {4–5},
numpages = {2},
keywords = {Data Stream mining, Machine Learning, Network Attacks},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{10.1007/978-3-030-65847-2_3,
author = {Lukyanenko, Roman and Castellanos, Arturo and Storey, Veda C. and Castillo, Alfred and Tremblay, Monica Chiarini and Parsons, Jeffrey},
title = {Superimposition: Augmenting Machine Learning Outputs with Conceptual Models for Explainable AI},
year = {2020},
isbn = {978-3-030-65846-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65847-2_3},
doi = {10.1007/978-3-030-65847-2_3},
abstract = {Machine learning has become almost synonymous with Artificial Intelligence (AI). However, it has many challenges with one of the most important being explainable AI; that is, providing human-understandable accounts of why a machine learning model produces specific outputs. To address this challenge, we propose superimposition as a concept which uses conceptual models to improve explainability by mapping the features that are important to a machine learning model’s decision outcomes to a conceptual model of an application domain. Superimposition is a design method for supplementing machine learning models with structural elements that are used by humans to reason about reality and generate explanations. To illustrate the potential of superimposition, we present the method and apply it to a churn prediction problem.},
booktitle = {Advances in Conceptual Modeling: ER 2020 Workshops CMAI, CMLS, CMOMM4FAIR, CoMoNoS, EmpER, Vienna, Austria, November 3–6, 2020, Proceedings},
pages = {26–34},
numpages = {9},
keywords = {Human categorization, Explainable AI, Conceptual modeling, Superimposition, Machine learning, Artificial intelligence},
location = {Vienna, Austria}
}

@article{10.1007/s00521-019-04626-7,
author = {Ray, Upasana and Chouhan, Usha and Verma, Neha},
title = {Comparative study of machine learning approaches for classification and prediction of selective caspase-3 antagonist for Zika virus drugs},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {15},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04626-7},
doi = {10.1007/s00521-019-04626-7},
abstract = {Zika virus (ZIKV) infection is an enervating and fast-growing disease. The increasing incidences of birth defects (microcephaly) in newborns due to ZIKV represent a public health problem. The viral infection is characterized by an increase in cell death of human neural progenitors and astrocytes, which can be inhibited by suppressing infection-induced caspase-3 activity. The aim of the present work is to develop classification models for the prediction of highly active and low active caspase-3 antagonists and to seek the important structural features related to the high anti-ZIKV property. Here, machine learning (ML) is applied in quantitative structure–activity relationship (QSAR) study. QSAR study is performed on the dataset by means of ML approaches, i.e., multiple linear regression (MLR), linear discriminant analysis (LDA), least square support vector machine (LS-SVM), deep neural net (DNN), k-nearest neighbor (KNN), na\"{\i}ve Bayes (NB) and random forest (RF). MLR, LDA are used for feature selection process and DNN, LS-SVM, KNN, NB, RF classifier for classification. The obtained results confirmed the discriminative capacity of the calculated descriptors. A good correlation is found by regression analysis between the observed and predicted activities as evident by their R2 (0.895) and Rpred2 (0.716) for the molecular descriptor dataset, R2 (0.892) and Rpred2 (0.736) for fingerprint dataset. The classification model obtained using RF (85.71%, 97.57%) and DNN (85.71%, 91.07%) classifier gave better accuracy than other approaches in fingerprint dataset and molecular descriptor dataset, respectively. This work provides an effective method to screen caspase-3 antagonists that will help out further in drug design for Zika virus.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {11311–11328},
numpages = {18},
keywords = {Antagonist, Machine learning, QSAR, Caspase-3}
}

@inproceedings{10.1007/978-3-030-45371-8_6,
author = {Briguglio, William and Saad, Sherif},
title = {Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis},
year = {2019},
isbn = {978-3-030-45370-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45371-8_6},
doi = {10.1007/978-3-030-45371-8_6},
abstract = {In cyberattack detection and prevention systems, cybersecurity analysts always prefer solutions that are as interpretable and understandable as rule-based or signature-based detection. This is because of the need to tune and optimize these solutions to mitigate and control the effect of false positives and false negatives. Interpreting machine learning models is a new and open challenge. However, it is expected that an interpretable machine learning solution will be domain specific. For instance, interpretable solutions for machine learning models in healthcare are different than solutions in malware detection. This is because the models are complex, and most of them work as a black-box. Recently, the increased ability for malware authors to bypass antimalware systems has forced security specialists to look to machine learning for creating robust detection systems. If these systems are to be relied on in the industry, then, among other challenges, they must also explain their predictions. The objective of this paper is to evaluate the current state-of-the-art ML models interpretability techniques when applied to ML-based malware detectors. We demonstrate interpretability techniques in practice and evaluate the effectiveness of existing interpretability techniques in the malware analysis domain.},
booktitle = {Foundations and Practice of Security: 12th International Symposium, FPS 2019, Toulouse, France, November 5–7, 2019, Revised Selected Papers},
pages = {82–97},
numpages = {16},
keywords = {Model reliability, Model robustness, Malware detector interpretability, N-gram, Malware detection, Machine learning, Cybersecurity},
location = {Toulouse, France}
}

@article{10.1016/j.future.2018.06.028,
author = {Shaqoor Nengroo, Ab and Kuppusamy, K.S.},
title = {Machine learning based heterogeneous web advertisements detection using a diverse feature set},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.06.028},
doi = {10.1016/j.future.2018.06.028},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {68–77},
numpages = {10},
keywords = {Machine learning, Content extraction random forest, Web accessibility, Advertisements}
}

@inproceedings{10.1145/3387939.3388613,
author = {Scheerer, Max and Klamroth, Jonas and Reussner, Ralf and Beckert, Bernhard},
title = {Towards classes of architectural dependability assurance for machine-learning-based systems},
year = {2020},
isbn = {9781450379625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387939.3388613},
doi = {10.1145/3387939.3388613},
abstract = {Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.},
booktitle = {Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {31–37},
numpages = {7},
keywords = {software quality, machine learning, dependability, artificial intelligence, architectural-driven self-adaptation},
location = {Seoul, Republic of Korea},
series = {SEAMS '20}
}

@inproceedings{10.1145/3240765.3270589,
author = {Cammarota, Rosario and Banerjee, Indranil and Rosenberg, Ofer},
title = {Machine Learning IP Protection},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1145/3240765.3270589},
doi = {10.1145/3240765.3270589},
abstract = {Machine learning, specifically deep learning is becoming a key technology component in application domains such as identity management, finance, automotive, and healthcare, to name a few. Proprietary machine learning models - Machine Learning IP - are developed and deployed at the network edge, end devices and in the cloud, to maximize user experience. With the proliferation of applications embedding Machine Learning IPs, machine learning models and hyper-parameters become attractive to attackers, and require protection. Major players in the semiconductor industry provide mechanisms on device to protect the IP at rest and during execution from being copied, altered, reverse engineered, and abused by attackers. In this work we explore system security architecture mechanisms and their applications to Machine Learning IP protection.},
booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
pages = {1–3},
numpages = {3},
location = {San Diego, CA, USA}
}

@inproceedings{10.1145/3176258.3176321,
author = {Liao, Cong and Zhong, Haoti and Zhu, Sencun and Squicciarini, Anna},
title = {Server-Based Manipulation Attacks Against Machine Learning Models},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176321},
doi = {10.1145/3176258.3176321},
abstract = {Machine learning approaches have been increasingly applied to various applications for data analytics (e.g. spam filtering, image classification). Further, with the growing adoption of cloud computing, various cloud services have provided an efficient way for users to train, store or deploy machine learning algorithms in an easy-to-use manner. However, the models deployed in the cloud may be exposed to potential malicious attacks launched at the server side. Attackers with access to the server can stealthily manipulate a machine learning model so as to enable misclassification or introduce bias. In this work, we study the problem of manipulation attacks as they occur at the server side. We consider not only traditional supervised learning models but also state-of-the-art deep learning models. In particular, a simple but effective gradient descent based approach is presented to exploit Logistic Regression (LR) and Convolutional Neural Networks (CNN) [16] models. We evaluate manipulation attacks against machine learning or deep learning systems using both Enron email text and MINIST image dataset [17]. Experimental results have demonstrated such attacks can manipulate the model that allows malicious samples to evade detection easily without compromising the overall performance of the systems.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {24–34},
numpages = {11},
keywords = {model manipulation, convolutional neural networks, adversarial machine learning},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@article{10.1007/s11042-020-09178-w,
author = {Carfora, Valentina and Di Massimo, Francesca and Rastelli, Rebecca and Catellani, Patrizia and Piastra, Marco},
title = {Dialogue management in conversational agents through psychology of persuasion and machine learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {47–48},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09178-w},
doi = {10.1007/s11042-020-09178-w},
abstract = {To be really effective, conversational agents must integrate well with the characteristics of the humans with whom they interact. This exploratory study focuses on a method for integrating well-assessed methods from the field of social psychology in the design of task-oriented conversational agents in which the dialogue management module is developed through machine learning. In particular, the aim is to achieve agents whose policies could take into account the psychological features of the human interactants to deliver personalized and more effective messages. The paper presents the psychological study performed and outlines the overall theoretical architecture of the software framework proposed. On the psychosocial side, we first assessed the effectiveness of differently framed messages aimed to reducing red meat consumption taking the Theory of Planned Behavior (TPB) as the psychosocial model of reference. Turning to the machine learning field, the resulting Structural Equation Model (SEM) was first translated into a probabilistic predictor using Dynamic Bayesian Network (DBN). In turn, such DBN became the fundamental element of a Partially Observable Markov Decision Processes (POMDP) in a reinforcement learning setting. The possibility to elicit complete interaction policies was then studied by applying Neural Monte Carlo Tree Search (Neural MCTS) methods. The results thus obtained introduce the possibility to develop new multidisciplinary and integrated techniques for the development of automated dialogue managing systems.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {35949–35971},
numpages = {23},
keywords = {Monte carlo tree search, Reinforcement learning, Machine learning, Psychology of persuasion, Theory of planned behavior, Conversational agent}
}

@inproceedings{10.5555/2486788.2487011,
author = {Gonzalez-Sanchez, Javier},
title = {Toward a software product line for affective-driven self-adaptive systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {One expected characteristic in modern systems is self-adaptation, the capability of monitoring and reacting to changes into the environment. A particular case of self-adaptation is affective-driven self-adaptation. Affective-driven self-adaptation is about having consciousness of user’s affects (emotions) and drive self-adaptation reacting to changes in those affects. Most of the previous work around self-adaptive systems deals with performance, resources, and error recovery as variables that trigger a system reaction. Moreover, most effort around affect recognition has been put towards offline analysis of affect, and to date only few applications exist that are able to infer user’s affect in real-time and trigger self-adaptation mechanisms. In response to this deficit, this work proposes a software product line approach to jump-start the development of affect-driven self-adaptive systems by offering the definition of a domain-specific architecture, a set of components (organized as a framework), and guidelines to tailor those components. Study cases with systems for learning and gaming will confirm the capability of the software product line to provide desired functionalities and qualities.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1381–1384},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inbook{10.5555/3454287.3454483,
author = {Laue, S\"{o}ren and Mitterreiter, Matthias and Giesen, Joachim},
title = {GENO – GENeric optimization for classical machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although optimization is the longstanding algorithmic backbone of machine learning, new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or if there is one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure and thus cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO generates a solver from the declarative specification of an optimization problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well-engineered specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {196},
numpages = {12}
}

@article{10.1145/3460822,
author = {Chakraborty, Saurav and Onuchowska, Agnieszka and Samtani, Sagar and Jank, Wolfgang and Wolfram, Brandon},
title = {Machine Learning for Automated Industrial IoT Attack Detection: An Efficiency-Complexity Trade-off},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3460822},
doi = {10.1145/3460822},
abstract = {Critical city infrastructures that depend on smart Industrial Internet of Things (IoT) devices have been increasingly becoming a target of cyberterrorist or hacker attacks. Although this has led to multiple studies in the recent past, there exists a paucity of literature concerning real-time Industrial IoT attack detection. The goal of this article is to build a machine-learning approach using Industrial IoT sensor readings for accurately tracking down Industrial IoT attacks in real time. We analyze IoT system behavior under a lab-controlled series of attacks on a Secure Water Treatment (SWaT) system. The system is analytically challenging in that it results in sensor readings that resemble waveforms. To that end, we develop a novel early detection method using functional shape analysis (FSA) to extract features from the data that can capture the profile of the waveform. Our results show an efficiency-complexity trade-off between functional and non-functional methods in predicting IoT attacks.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {37},
numpages = {28},
keywords = {functional shape analysis (FSA), machine learning, cybersecurity, Industrial IoT}
}

@article{10.1145/3373269,
author = {Szentimrey, Hannah and Al-Hyari, Abeer and Foxcroft, Jeremy and Martin, Timothy and Noel, David and Grewal, Gary and Areibi, Shawki},
title = {Machine Learning for Congestion Management and Routability Prediction within FPGA Placement},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3373269},
doi = {10.1145/3373269},
abstract = {Placement for Field Programmable Gate Arrays (FPGAs) is one of the most important but time-consuming steps for achieving design closure. This article proposes the integration of three unique machine learning models into the state-of-the-art analytic placement tool GPlace3.0 with the aim of significantly reducing placement runtimes. The first model, MLCong, is based on linear regression and replaces the computationally expensive global router currently used in GPlace3.0 to estimate switch-level congestion. The second model, DLManage, is a convolutional encoder-decoder that uses heat maps based on the switch-level congestion estimates produced by MLCong to dynamically determine the amount of inflation to apply to each switch to resolve congestion. The third model, DLRoute, is a convolutional neural network that uses the previous heat maps to predict whether or not a placement solution is routable. Once a placement solution is determined to be routable, further optimization may be avoided, leading to improved runtimes. Experimental results obtained using 372 benchmarks provided by Xilinx Inc. show that when all three models are integrated into GPlace3.0, placement runtimes decrease by an average of 48%.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {37},
numpages = {25},
keywords = {routing-aware, heterogeneous, field programmable gate array, congestion, UltraScale architecture, Placement}
}

@article{10.1155/2021/9919992,
author = {Jiang, Fengqing and Chen, Xiao and Li, Xingwang},
title = {An Action Recognition Algorithm for Sprinters Using Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/9919992},
doi = {10.1155/2021/9919992},
abstract = {The advancements in modern science and technology have greatly promoted the progress of sports science. Advanced technological methods have been widely used in sports training, which have not only improved the scientific level of training but also promoted the continuous growth of sports technology and competition results. With the development of sports science and the gradual deepening of sport practices, the use of scientific training methods and monitoring approaches has improved the effect of sports training and athletes’ performance. This paper takes sprint as the research problem and constructs the image of sprinter’s action recognition based on machine learning. In view of the shortcomings of traditional dual-stream convolutional neural network for processing long-term video information, the time-segmented dual-stream network, based on sparse sampling, is used to better express the characteristics of long-term motion. First, the continuous video frame data is divided into multiple segments, and a short sequence of data containing user actions is formed by randomly sampling each segment of the video frame sequence. Next, it is applied to the dual-stream network for feature extraction. The optical flow image extraction involved in the dual-stream network is implemented by the system using the Lucas–Kanade algorithm. The system in this paper has been tested in actual scenarios, and the results show that the system design meets the expected requirements of the sprinters.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@inproceedings{10.1007/978-3-030-82824-0_10,
author = {Balta, Dian and Sellami, Mahdi and Kuhn, Peter and Sch\"{o}pp, Ulrich and Buchinger, Matthias and Baracaldo, Nathalie and Anwar, Ali and Ludwig, Heiko and Sinn, Mathieu and Purcell, Mark and Altakrouri, Bashar},
title = {Accountable Federated Machine Learning in Government: Engineering and Management Insights},
year = {2021},
isbn = {978-3-030-82823-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82824-0_10},
doi = {10.1007/978-3-030-82824-0_10},
abstract = {Machine learning offers promising capabilities to improve administrative procedures. At the same time, adequate training of models using traditional learning techniques requires the collection and storage of enough training data in a central place. Unfortunately, due to legislative and jurisdictional constraints, data in a central place is scarce and training a model becomes unfeasible. Against this backdrop, federated machine learning, a technique to collaboratively train models without transferring data to a centralized location, has been recently proposed. With each government entity keeping their data private, new applications that previously were impossible now can be a reality. In this paper, we demonstrate that accountability for the federated machine learning process becomes paramount to fully overcoming legislative and jurisdictional constraints. In particular, it ensures that all government entities' data are adequately included in the model and that evidence on fairness and reproducibility is curated towards trustworthiness. We also present an analysis framework suitable for governmental scenarios and illustrate its exemplary application for online citizen participation scenarios. We discuss our findings in terms of engineering and management implications: feasibility evaluation, general architecture, involved actors as well as verifiable claims for trustworthy machine learning.},
booktitle = {Electronic Participation: 13th IFIP WG 8.5 International Conference, EPart 2021, Granada, Spain, September 7–9, 2021, Proceedings},
pages = {125–138},
numpages = {14},
keywords = {Accountability, Federated learning, Framework, Verifiable claims},
location = {Granada, Spain}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@inproceedings{10.1145/3397166.3409128,
author = {Zhang, Qin and Zhou, Ruiting and Wu, Chuan and Jiao, Lei and Li, Zongpeng},
title = {Online scheduling of heterogeneous distributed machine learning jobs},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3409128},
doi = {10.1145/3397166.3409128},
abstract = {Distributed machine learning (ML) has played a key role in today's proliferation of AI services. A typical model of distributed ML is to partition training datasets over multiple worker nodes to update model parameters in parallel, adopting a parameter server architecture. ML training jobs are typically resource elastic, completed using various time lengths with different resource configurations. A fundamental problem in a distributed ML cluster is how to explore the demand elasticity of ML jobs and schedule them with different resource configurations, such that the utilization of resources is maximized and average job completion time is minimized. To address it, we propose an online scheduling algorithm to decide the execution time window, the number and the type of concurrent workers and parameter servers for each job upon its arrival, with a goal of minimizing the weighted average completion time. Our online algorithm consists of (i) an online scheduling framework that groups unprocessed ML training jobs into a batch iteratively, and (ii) a batch scheduling algorithm that configures each ML job to maximize the total weight of scheduled jobs in the current iteration. Our online algorithm guarantees a good parameterized competitive ratio with polynomial time complexity. Extensive evaluations using real-world data demonstrate that it outperforms state-of-the-art schedulers in today's AI cloud systems.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {111–120},
numpages = {10},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@article{10.1561/2200000058,
author = {Jain, Prateek and Kar, Purushottam},
title = {Non-convex Optimization for Machine Learning},
year = {2017},
issue_date = {Dec 2017},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {10},
number = {3–4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000058},
doi = {10.1561/2200000058},
abstract = {A vast majority of machine learning algorithms train their models and
perform inference by solving optimization problems. In order to capture
the learning and prediction problems accurately, structural constraints
such as sparsity or low rank are frequently imposed or else the objective
itself is designed to be a non-convex function. This is especially true
of algorithms that operate in high-dimensional spaces or that train
non-linear models such as tensor models and deep networks.
The freedom to express the learning problem as a non-convex optimization
problem gives immense modeling power to the algorithm
designer, but often such problems are NP-hard to solve. A popular
workaround to this has been to relax non-convex problems to convex
ones and use traditional methods to solve the (convex) relaxed optimization
problems. However this approach may be lossy and nevertheless
presents significant challenges for large scale optimization.
On the other hand, direct approaches to non-convex optimization
have met with resounding success in several domains and remain the
methods of choice for the practitioner, as they frequently outperform
relaxation-based techniques – popular heuristics include projected gradient
descent and alternating minimization. However, these are often
poorly understood in terms of their convergence and other properties.
This monograph presents a selection of recent advances that bridge
a long-standing gap in our understanding of these heuristics. We hope
that an insight into the inner workings of these methods will allow the
reader to appreciate the unique marriage of task structure and generative
models that allow these heuristic techniques to (provably) succeed.
The monograph will lead the reader through several widely used nonconvex
optimization techniques, as well as applications thereof. The
goal of this monograph is to both, introduce the rich literature in this
area, as well as equip the reader with the tools and techniques needed
to analyze these simple procedures for non-convex problems.},
journal = {Found. Trends Mach. Learn.},
month = dec,
pages = {142–336},
numpages = {200}
}

@inproceedings{10.5555/3327757.3327765,
author = {Wynen, Daan and Schmid, Cordelia and Mairal, Julien},
title = {Unsupervised learning of artistic styles with archetypal style analysis},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to deep image representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6584–6593},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1016/j.knosys.2017.04.014,
author = {Arcelli Fontana, Francesca and Zanoni, Marco},
title = {Code smell severity classification using machine learning techniques},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {128},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.04.014},
doi = {10.1016/j.knosys.2017.04.014},
abstract = {Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.880.96, measured through Spearmans .},
journal = {Know.-Based Syst.},
month = jul,
pages = {43–58},
numpages = {16},
keywords = {Refactoring prioritization, Ordinal classification, Machine learning, Code smells detection, Code smell severity}
}

@book{10.5555/3235555,
author = {Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
title = {Scaling up Machine Learning: Parallel and Distributed Approaches},
year = {2018},
isbn = {1108461743},
publisher = {Cambridge University Press},
address = {USA},
abstract = {This book presents an integrated collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes, in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits, trade-offs, and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters, concurrent programming frameworks including CUDA, MPI, MapReduce, and DryadLINQ, and learning settings (supervised, unsupervised, semi-supervised, and online learning). Extensive coverage of parallelization of boosted trees, SVMs, spectral clustering, belief propagation and other popular learning algorithms and deep dives into several applications make the book equally useful for researchers, students, and practitioners.}
}

@inproceedings{10.1145/3318216.3363338,
author = {Roy, Abhishek and Chhabra, Anshuman and Kamhoua, Charles A. and Mohapatra, Prasant},
title = {A moving target defense against adversarial machine learning},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363338},
doi = {10.1145/3318216.3363338},
abstract = {Adversarial Machine Learning has become the latest threat with the ubiquitous presence of machine learning. In this paper we propose a Moving Target Defense approach to defend against adversarial machine learning, i.e., instead of manipulating the machine learning algorithms, we suggest a switching scheme among machine learning algorithms to defend against adversarial attack. We model the problem as a Stackelberg game between the attacker and the defender. We propose a switching strategy which is the Stackelberg equilibrium of the game. We test our method against rational, and boundedly rational attackers. We show that designing a method against a rational attacker is enough in most scenarios. We show that even under very harsh constraints, e.g., no attack-cost, and availability of attacks which can bring down the accuracy to 0, it is possible to achieve reasonable accuracy in the context of classification. This work shows, that in addition to switching among algorithms, one can think of introducing randomness in tuning parameters, and model choices to achieve better defense against adversarial machine learning.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {383–388},
numpages = {6},
keywords = {moving target defense, cybersecurity, bounded rationality, adversarial machine learning},
location = {Arlington, Virginia},
series = {SEC '19}
}

@article{10.1016/j.asoc.2019.105779,
author = {Chen, Ting-Hsuan},
title = {Do you know your customer? Bank risk assessment based on machine learning},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105779},
doi = {10.1016/j.asoc.2019.105779},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {7},
keywords = {Decision trees, Support vector machines, Machine learning, Risk assessment, Know your customer}
}

@article{10.1016/j.compag.2021.105992,
author = {Kar, Soumyashree and Purbey, Vikram Kumar and Suradhaniwar, Saurabh and Korbu, Lijalem Balcha and Kholov\'{a}, Jana and Durbha, Surya S. and Adinarayana, J. and Vadez, Vincent},
title = {An ensemble machine learning approach for determination of the optimum sampling time for evapotranspiration assessment from high-throughput phenotyping data},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {182},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.105992},
doi = {10.1016/j.compag.2021.105992},
journal = {Comput. Electron. Agric.},
month = mar,
numpages = {14},
keywords = {Sampling time optimization, Ensemble machine learning, Time series classification, Time series forecasting, Evapotranspiration, High throughput phenotyping}
}

@article{10.1145/3369820,
author = {Liu, Tony and Nicholas, Jennifer and Theilig, Max M. and Guntuku, Sharath C. and Kording, Konrad and Mohr, David C. and Ungar, Lyle},
title = {Machine Learning for Phone-Based Relationship Estimation: The Need to Consider Population Heterogeneity},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369820},
doi = {10.1145/3369820},
abstract = {Estimating the category and quality of interpersonal relationships from ubiquitous phone sensor data matters for studying mental well-being and social support. Prior work focused on using communication volume to estimate broad relationship categories, often with small samples. Here we contextualize communications by combining phone logs with demographic and location data to predict interpersonal relationship roles on a varied sample population using automated machine learning methods, producing better performance (F1 = 0.68) than using communication features alone (F1 = 0.62). We also explore the effect of age variation in the underlying training sample on interpersonal relationship prediction and find that models trained on younger subgroups, which is popular in the field via student participation and recruitment, generalize poorly to the wider population. Our results not only illustrate the value of using data across demographics, communication patterns and semantic locations for relationship prediction, but also underscore the importance of considering population heterogeneity in phone-based personal sensing studies.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {145},
numpages = {23},
keywords = {social relationship prediction, semantic location-based features, population heterogeneity, automated machine learning}
}

@inproceedings{10.1109/CDC.2018.8619558,
author = {Shahrampour, Shahin and Beirami, Ahmad and Tarokh, Vahid},
title = {Supervised Learning Using Data-dependent Random Features with Application to Seizure Detection},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC.2018.8619558},
doi = {10.1109/CDC.2018.8619558},
abstract = {The randomized-feature technique has been successfully applied to large-scale supervised learning. Despite being significantly more efficient compared to kernel methods in terms of computational cost, random features can be improved from generalization (prediction accuracy) viewpoint. Recently, it has been shown that such improvement can be achieved using data-dependent randomization. We recently proposed an algorithm based on a data-dependent score function that explores the set of possible random features and exploits the promising regions. The method has shown promising empirical success (on various datasets) in terms of generalization error compared to the state-of-the-art in random features. Restricting our attention to cosine feature maps, in this work, we provide exact theoretical constraints under which the score function converges to the spectrum of the best model in the learning class. We further present another application of the method in Epileptic Seizure Recognition.},
booktitle = {2018 IEEE Conference on Decision and Control (CDC)},
pages = {1168–1173},
numpages = {6},
location = {FL, USA}
}

@inproceedings{10.1007/978-3-030-29765-7_20,
author = {Lim, Suryani and Prade, Henri and Richard, Gilles},
title = {Solving Word Analogies: A Machine Learning Perspective},
year = {2019},
isbn = {978-3-030-29764-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29765-7_20},
doi = {10.1007/978-3-030-29765-7_20},
abstract = {Analogical proportions are statements of the form ‘a is to b as c is to d’, formally denoted 
[inline-graphic not available: see fulltext]
. This means that the way a and b (resp. b and a) differ is the same as c and d (resp. d and c) differ, as revealed by their logical modeling. The postulates supposed to govern such proportions entail that when 
[inline-graphic not available: see fulltext]
 holds, then seven permutations of a,&nbsp;b,&nbsp;c,&nbsp;d still constitute valid analogies. It can also be derived that 
[inline-graphic not available: see fulltext]
 does not hold except if a=b. From a machine learning perspective, this provides guidelines to build training sets of positive and negative examples. We then suggest improved methods to classify word-analogies and also to solve analogical equations. Viewing words as vectors in a multi-dimensional space, we depart from the traditional parallelogram view of analogy to adopt a purely machine-learning approach. In some sense, we learn a functional definition of analogical proportions without assuming any pre-existing formulas. We mainly use the logical properties of proportions to define our training sets and to design proper neural networks, approximating the hidden relations. Using a GloVe embedding, the results we get show high accuracy and improve state of the art on words analogy-solving problems.},
booktitle = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty: 15th European Conference, ECSQARU 2019, Belgrade, Serbia, September 18-20, 2019, Proceedings},
pages = {238–250},
numpages = {13},
location = {Belgrade, Serbia}
}

@article{10.1007/s00530-017-0580-7,
author = {Zhao, Fengjun and Chen, Yanrong and Hou, Yuqing and He, Xiaowei},
title = {Segmentation of blood vessels using rule-based and machine-learning-based methods: a review},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-017-0580-7},
doi = {10.1007/s00530-017-0580-7},
abstract = {Vessel segmentation as a component of medical image processing is the prerequisite for accurate diagnosis of vascular-related diseases. Manual delineation of blood vessels has been turned out to be time consuming and observer dependent. Therefore, much effort has been dedicated to the automatic or semi-automatic vessel segmentation methods. Previous literatures have reviewed the state of vessel segmentation methods from various perspectives. However, their reviews did not take the modern machine-learning methods especially deep neural networks into account. In this paper, we reviewed the state-of-the-art vessel segmentation methods by dividing them into two categories, rule-based, and machine-learning-based methods. The rule-based methods discriminate vessel structure from background relying on intuitively and exquisitely designed rule sets, while the machine-learning-based methods carry out the segmentation by self-learned rules from the previous experience. Instead of exhaustively listing all vessel segmentation methods, this paper focuses on the well-known blood vessel segmentation methods in recent years, to give readers a glimpse of the current state and future direction of segmentation technique for blood vessels.},
journal = {Multimedia Syst.},
month = apr,
pages = {109–118},
numpages = {10},
keywords = {Segmentation, Rule-based, Machine learning, Deep neural network, Blood vessel}
}

@article{10.1007/s10845-021-01817-9,
author = {Kang, SungKu and Jin, Ran and Deng, Xinwei and Kenett, Ron S.},
title = {Challenges of modeling and analysis in cybermanufacturing: a review from a machine learning and computation perspective},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01817-9},
doi = {10.1007/s10845-021-01817-9},
abstract = {In Industry 4.0, smart manufacturing is facing its next stage, cybermanufacturing, founded upon advanced communication, computation, and control infrastructure. Cybermanufacturing will unleash the potential of multi-modal manufacturing data, and provide a new perspective called computation service, as a part of service-oriented architecture (SOA), where on-demand computation requests throughout manufacturing operations are seamlessly satisfied by data analytics and machine learning. However, the complexity of information technology infrastructure leads to fundamental challenges in modeling and analysis under cybermanufacturing, ranging from information-poor datasets to a lack of reproducibility of analytical studies. Nevertheless, existing reviews have focused on the overall architecture of cybermanufacturing/SOA or its technical components (e.g., communication protocol), rather than the potential bottleneck of computation service with respect to modeling and analysis. In this paper, we review the fundamental challenges with respect to modeling and analysis in cybermanufacturing. Then, we introduce the existing efforts in computation pipeline recommendation, which aims at identifying an optimal sequence of method options for data analytics/machine learning without time-consuming trial-and-error. We envision computation pipeline recommendation as a promising research field to address the fundamental challenges in cybermanufacturing. We also expect that computation pipeline recommendation can be a driving force to flexible and resilient manufacturing operations in the post-COVID-19 industry.},
journal = {J. Intell. Manuf.},
month = aug,
pages = {415–428},
numpages = {14},
keywords = {Computation pipelines, Cybermanufacturing, Industry 4.0, Machine learning, Manufacturing modeling and analysis}
}

@article{10.1016/j.asoc.2016.09.049,
author = {Elola, Andoni and Del Ser, Javier and Bilbao, Miren Nekane and Perfecto, Cristina and Alexandre, Enrique and Salcedo-Sanz, Sancho},
title = {Hybridizing Cartesian Genetic Programming and Harmony Search for adaptive feature construction in supervised learning problems},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.09.049},
doi = {10.1016/j.asoc.2016.09.049},
abstract = {HighlightsWe present a new iterative feature construction approach for supervised learning model based on the meta-heuristic Harmony Search (HS) algorithm and Cartesian Genetic Programming.We propose a novel method to incorporate soft information about the relevance of the constructed features in the HS algorithm so as to enhance its convergence.The performance of the proposed scheme is assessed over datasets from the literature, with promising results that support its suitability to deal with legacy datasets. The advent of the so-called Big Data paradigm has motivated a flurry of research aimed at enhancing machine learning models by following very diverse approaches. In this context this work focuses on the automatic construction of features in supervised learning problems, which differs from the conventional selection of features in that new characteristics with enhanced predictive power are inferred from the original dataset. In particular this manuscript proposes a new iterative feature construction approach based on a self-learning meta-heuristic algorithm (Harmony Search) and a solution encoding strategy (correspondingly, Cartesian Genetic Programming) suited to represent combinations of features by means of constant-length solution vectors. The proposed feature construction algorithm, coined as Adaptive Cartesian Harmony Search (ACHS), incorporates modifications that allow exploiting the estimated predictive importance of intermediate solutions and, ultimately, attaining better convergence rate in its iterative learning procedure. The performance of the proposed ACHS scheme is assessed and compared to that rendered by the state of the art in a toy example and three practical use cases from the literature. The excellent performance figures obtained in these problems shed light on the widespread applicability of the proposed scheme to supervised learning with legacy datasets composed by already refined characteristics.},
journal = {Appl. Soft Comput.},
month = mar,
pages = {760–770},
numpages = {11},
keywords = {Supervised learning, Harmony Search, Feature construction, Cartesian Genetic Programming}
}

@article{10.1007/s11219-020-09511-4,
author = {Moreno, Valent\'{\i}n and G\'{e}nova, Gonzalo and Parra, Eugenio and Fraga, Anabel},
title = {Application of machine learning techniques to the flexible assessment and improvement of requirements quality},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09511-4},
doi = {10.1007/s11219-020-09511-4},
abstract = {It is already common to compute quantitative metrics of requirements to assess their quality. However, the risk is to build assessment methods and tools that are both arbitrary and rigid in the parameterization and combination of metrics. Specifically, we show that a linear combination of metrics is insufficient to adequately compute a global measure of quality. In this work, we propose to develop a flexible method to assess and improve the quality of requirements that can be adapted to different contexts, projects, organizations, and quality standards, with a high degree of automation. The domain experts contribute with an initial set of requirements that they have classified according to their quality, and we extract their quality metrics. We then use machine learning techniques to emulate the implicit expert’s quality function. We provide also a procedure to suggest improvements in bad requirements. We compare the obtained rule-based classifiers with different machine learning algorithms, obtaining measurements of effectiveness around 85%. We show as well the appearance of the generated rules and how to interpret them. The method is tailorable to different contexts, different styles to write requirements, and different demands in quality. The whole process of inferring and applying the quality rules adapted to each organization is highly automated.},
journal = {Software Quality Journal},
month = dec,
pages = {1645–1674},
numpages = {30},
keywords = {Flexible assessment, Experts’ judgment, Automatic improvement, Automatic classification, Machine learning, Requirements quality}
}

@inproceedings{10.5555/3045118.3045209,
author = {Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
title = {Unsupervised learning of video representations using LSTMs},
year = {2015},
publisher = {JMLR.org},
abstract = {We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations ("percepts") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {843–852},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1016/j.procs.2017.09.077,
author = {Gopalapillai, Radhakrishnan and Gupta, Deepa and Tsb, Sudarshan},
title = {Pattern Identification of Robotic Environments using Machine Learning Techniques},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.09.077},
doi = {10.1016/j.procs.2017.09.077},
abstract = {Analysis of time series data collected from mobile robots is getting more attention in many application areas. When multiple robots move through an environment to perform certain actions, an understanding of the environment viewed by each robot is essential. This paper presents analysis of robotic data using machine learning techniques when the data consist of multiple views of the environment. Robotic environments have been classified using the data captured by onboard sensors of mobile robots using a set of machine learning algorithms and their performances have been compared The machine learning model is validated using a test environment where some of the objects are displaced or removed from their designated position.},
journal = {Procedia Comput. Sci.},
month = nov,
pages = {63–71},
numpages = {9},
keywords = {feature reduction, Time series data, Object displacement, Machine Learning, Feature selection, Classification}
}

@inproceedings{10.1145/3319535.3354211,
author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
title = {Privacy Risks of Securing Machine Learning Models against Adversarial Examples},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354211},
doi = {10.1145/3319535.3354211},
abstract = {The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {241–257},
numpages = {17},
keywords = {membership inference attacks, machine learning, adversarial examples and defenses},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/3417987,
author = {Waheed, Nazar and He, Xiangjian and Ikram, Muhammad and Usman, Muhammad and Hashmi, Saad Sajid and Usman, Muhammad},
title = {Security and Privacy in IoT Using Machine Learning and Blockchain: Threats and Countermeasures},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3417987},
doi = {10.1145/3417987},
abstract = {Security and privacy of users have become significant concerns due to the involvement of the Internet of Things (IoT) devices in numerous applications. Cyber threats are growing at an explosive pace making the existing security and privacy measures inadequate. Hence, everyone on the Internet is a product for hackers. Consequently, Machine Learning (ML) algorithms are used to produce accurate outputs from large complex databases, where the generated outputs can be used to predict and detect vulnerabilities in IoT-based systems. Furthermore, Blockchain (BC) techniques are becoming popular in modern IoT applications to solve security and privacy issues. Several studies have been conducted on either ML algorithms or BC techniques. However, these studies target either security or privacy issues using ML algorithms or BC techniques, thus posing a need for a combined survey on efforts made in recent years addressing both security and privacy issues using ML algorithms and BC techniques. In this article, we provide a summary of research efforts made in the past few years, from 2008 to 2019, addressing security and privacy issues using ML algorithms and BC techniques in the IoT domain. First, we discuss and categorize various security and privacy threats reported in the past 12 years in the IoT domain. We then classify the literature on security and privacy efforts based on ML algorithms and BC techniques in the IoT domain. Finally, we identify and illuminate several challenges and future research directions using ML algorithms and BC techniques to address security and privacy issues in the IoT domain.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {122},
numpages = {37},
keywords = {machine learning, cybersecurity, Internet of Things, Blockchain}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00034,
author = {Lwakatare, Lucy Ellen and R\r{a}nge, Ellinor and Crnkovic, Ivica and Bosch, Jan},
title = {On the experiences of adopting automated data validation in an industrial machine learning project},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00034},
doi = {10.1109/ICSE-SEIP52600.2021.00034},
abstract = {Background: Data errors are a common challenge in machine learning (ML) projects and generally cause significant performance degradation in ML-enabled software systems. To ensure early detection of erroneous data and avoid training ML models using bad data, research and industrial practice suggest incorporating a data validation process and tool in ML system development process.Aim: The study investigates the adoption of a data validation process and tool in industrial ML projects. The data validation process demands significant engineering resources for tool development and maintenance. Thus, it is important to identify the best practices for their adoption especially by development teams that are in the early phases of deploying ML-enabled software systems.Method: Action research was conducted at a large-software intensive organization in telecommunications, specifically within the analytics R&amp;D organization for an ML use case of classifying faults from returned hardware telecommunication devices.Results: Based on the evaluation results and learning from our action research, we identified three best practices, three benefits, and two barriers to adopting the data validation process and tool in ML projects. We also propose a data validation framework (DVF) for systematizing the adoption of a data validation process.Conclusions: The results show that adopting a data validation process and tool in ML projects is an effective approach of testing ML-enabled software systems. It requires having an overview of the level of data (feature, dataset, cross-dataset, data stream) at which certain data quality tests can be applied.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {248–257},
numpages = {10},
keywords = {software engineering, machine learning, data validation, data quality, data errors},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3386263.3406950,
author = {Servadei, Lorenzo and Mosca, Edoardo and Devarajegowda, Keerthikumara and Werner, Michael and Ecker, Wolfgang and Wille, Robert},
title = {Cost Estimation for Configurable Model-Driven SoC Designs Using Machine Learning},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406950},
doi = {10.1145/3386263.3406950},
abstract = {The complexity of today's System on Chips (SoCs) forces designers to use higher levels of abstractions. Here, early design decisions are conducted on abstract models while different configurations describe how to actually realize the desired SoC. Since those decisions severely affect the final costs of the resulting SoC (in terms of utilized area, power consumption, etc.), a fast and accurate cost estimation is essential at this design stage. Additionally, the resulting costs heavily depend on the adopted logic synthesis algorithms, which optimize the design towards one or more cost objectives. But how to structure a cost estimation method that supports multiple configurations of an SoC, implemented by use of different synthesis strategies, remains an open question. In this work, we address this problem by providing a cost estimation method for a configurable SoC using Machine Learning (ML). A key element of the proposed method is a data representation which describes SoC configurations in a way that is suited for advanced ML algorithms. Experimental evaluations conducted within an industrial environment confirm the accuracy as well as the efficiency of the proposed method.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {405–410},
numpages = {6},
keywords = {machine learning, hardware-software codesign, design automation},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@article{10.1155/2021/9974270,
author = {Rincy N, Thomas and Gupta, Roopam and Fournier-Viger, Philippe},
title = {Design and Development of an Efficient Network Intrusion Detection System Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9974270},
doi = {10.1155/2021/9974270},
abstract = {Today’s internets are made up of nearly half a million different networks. In any network connection, identifying the attacks by their types is a difficult task as different attacks may have various connections, and their number may vary from a few to hundreds of network connections. To solve this problem, a novel hybrid network IDS called NID-Shield is proposed in the manuscript that classifies the dataset according to different attack types. Furthermore, the attack names found in attack types are classified individually helping considerably in predicting the vulnerability of individual attacks in various networks. The hybrid NID-Shield NIDS applies the efficient feature subset selection technique called CAPPER and distinct machine learning methods. The UNSW-NB15 and NSL-KDD datasets are utilized for the evaluation of metrics. Machine learning algorithms are applied for training the reduced accurate and highly merit feature subsets obtained from CAPPER and then assessed by the cross-validation method for the reduced attributes. Various performance metrics show that the hybrid NID-Shield NIDS applied with the CAPPER approach achieves a good accuracy rate and low FPR on the UNSW-NB15 and NSL-KDD datasets and shows good performance results when analyzed with various approaches found in existing literature studies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {35}
}

@inproceedings{10.1007/978-3-030-78361-7_28,
author = {Hara, Kenta and Asahi, Yumi},
title = {Factor Analysis of Continuous Use of Car Services in Japan by Machine Learning},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_28},
doi = {10.1007/978-3-030-78361-7_28},
abstract = {The automotive industry is an important industry for Japan and has a great impact on the Japanese economy. According to the Automotive Industry Strategy 2014 announced by the Ministry of Economy, five problems will occur in the next 10 to 20 years, and countermeasures are required. Five is-sues are increasing environmental and energy constraints, population growth and personal income growth, aging, urban overcrowding and depopulation, and changing values. Moreover, it is affected by external factors which is consumption tax increase, soaring gasoline and so on.As a strategy to increase the number of cars sold, it is possible to develop products that meet the needs of customers. However, in the car industry, the same type of cars of the same manufacturing company are also sold by other sales corporations, and product development that makes a difference between products may involve enormous costs and risks. Therefore, in this re-search, we focus on customers who use car-related services rather than products to differentiate their services. The purpose of this study is to build a Bayesian network that can handle many missing values and difficult elements of observed data and can introduce expert knowledge into the network structure. In addition, we also show that Bayesian networks are useful by comparing them with multiple machine learning models. It used Logistic regression, Decision tree, XGBoost, and Random Forest. The conclusions of this study show that the Bayesian network is more accurate than other analyzes.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {373–384},
numpages = {12},
keywords = {Car, Machine learning, Bayesian network}
}

@article{10.1007/s10994-021-06113-4,
author = {Senanayake, Ransalu and Fremont, Daniel J. and Kochenderfer, Mykel J. and Lomuscio, Alessio R. and Margineantu, Dragos and Ong, Cheng Soon},
title = {Guest Editorial: Special issue on robust machine learning},
year = {2021},
issue_date = {Aug 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {112},
number = {8},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06113-4},
doi = {10.1007/s10994-021-06113-4},
journal = {Mach. Learn.},
month = nov,
pages = {2787–2789},
numpages = {3}
}

@article{10.3233/JIFS-210595,
author = {Neffati, Syrine and Ben Abdellafou, Khaoula and Aljuhani, Ahamed and Taouali, Okba},
title = {An enhanced CAD system based on machine Learning Algorithm for brain MRI classification},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210595},
doi = {10.3233/JIFS-210595},
abstract = {The development of Computer-Aided Design (CAD) and Computer-Aided Manufacturing (CAM) systems in the past decade has led to a remarkable advance in biomedical applications and devices. Particularly, CAM and CAD systems are employed in medical engineering, robotic surgery, clinical medicine, dentistry and other biomedical areas. Hence, the accuracy and precision of the CAD and CAM systems are extremely important for proper treatment. This work suggests a new CAD system for brain image classification by analyzing Magnetic Resonance Images (MRIs) of the brain. Firstly, we use the proposed Downsized Rank Kernel Partial Least Squares (DR-KPLS) as a feature extraction technique. Then, we perform the classification using Support Vector Machines (SVM) and we validate with a k-fold cross validation approach. Further, we utilize the Tabu search metaheuristic approach in order to determine the optimal parameter of the kernel function. The proposed algorithm is entitled DR-KPLS+SVM. The algorithm is tested on the OASIS MRI database. The proposed kernel-based classifier is found to be better performant than the existing methods.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1845–1854},
numpages = {10},
keywords = {classification, KLPS, Optimization, CAD system, Dimensionality reduction}
}

@phdthesis{10.5555/AAI27964671,
author = {Zhang, Linfeng and Selloni, Annabella},
advisor = {Roberto, Car, and Weinan, E.,},
title = {Machine Learning for Multi-Scale Molecular Modeling: Theories, Algorithms, and Applications},
year = {2020},
isbn = {9798662418486},
publisher = {Princeton University},
address = {USA},
abstract = {In recent years, machine learning has emerged as a promising tool for dealing with the difficulty of representing high dimensional functions. This gives us an unprecedented opportunity to revisit theoretical foundations of various scientific fields, develop new schemes, improve existing methodologies, and solve problems that were too complicated for conventional approaches to address.In this dissertation, we identify a list of such problems in the context of multi-scale molecular modeling and propose machine learning based strategies to boost simulations with ab initio accuracy to much larger scales than conventional approaches. We consider two representative challenges: 1) how to go from many-electron-ion to atomistic systems, for which the key has been a general and efficient representation of the potential energy surface generated by electronic structure models; 2) how to go from atomistic to coarse-grained systems, for which one is interested in the free energy of the coarse-grained variables as well as the associated dynamical behavior. Our strategies follow two seemingly obvious but non-trivial principles: 1) machine learning based models should respect important physical constraints like symmetry; 2) to build truly reliable models, efficient algorithms are needed to construct a minimal but truly representative training data set. We use these principles to construct the Deep Potential model for the potential energy surface, the Deep Potential Molecular dynamics (DeePMD) which is a new paradigm for performing ab initio molecular dynamics, a concurrent learning scheme (DP-GEN) for generating the data set on the fly, algorithms for constructing the Wannier centers (Deep Wanner) and for efficiently exploring the free energy landscape (Reinforced Dynamics), as well as a machine learning-based coarse grained molecular dynamics model (DeePCG), etc. Applications of these models and algorithms are presented for problems in chemistry, biology, and materials science. Finally, we present our efforts on developing related open-source software packages, which have now been widely used worldwide by experts and practitioners in the molecular simulation community.},
note = {AAI27964671}
}

@article{10.1016/j.inffus.2018.09.012,
author = {Zitnik, Marinka and Nguyen, Francis and Wang, Bo and Leskovec, Jure and Goldenberg, Anna and Hoffman, Michael M.},
title = {Machine learning for integrating data in biology and medicine: Principles, practice, and opportunities},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.09.012},
doi = {10.1016/j.inffus.2018.09.012},
journal = {Inf. Fusion},
month = oct,
pages = {71–91},
numpages = {21},
keywords = {Machine learning, Heterogeneous data, Systems biology, Personalized medicine, Computational biology}
}

@inproceedings{10.1145/3485730.3493375,
author = {Ranathunga, Tharindu and McGibney, Alan and Rea, Susan},
title = {The convergence of Blockchain and Machine Learning for Decentralized Trust Management in IoT Ecosystems},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493375},
doi = {10.1145/3485730.3493375},
abstract = {The EU data strategy postulates that by 2025 there will be a paradigm shift towards more decentralized intelligence and data processing at the edge. The convergence of a large number of nodes at the IoT edge along with multiple service providers and network operators exposes data owners and resource providers to potential threats. To address cloud-edge risks, trust-based decentralized management is needed. Blockchain technology has created an opportunity to decentralize IoT ecosystems, through its intrinsic properties and together with machine learning (ML) it can be used to provide a trusted backbone for managing IoT ecosystems to support automated and adaptive trust management. This paper presents a novel approach for crosslayer intelligent trust computation modelling leveraging ML and Blockchain for decentralized trust management in IoT ecosystems. The effectiveness of the proposed approach for flow-based trust assessment is demonstrated using the Hyperledger Framework and the Cooja-based simulation environment. Finally, an initial evaluation is presented to understand the performance in terms of scalability and trust convergence of the proposed model.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {499–504},
numpages = {6},
keywords = {Trust, Machine Learning, IoT Ecosystems, Internet of things, Hyperledger, Blockchain},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1145/3494885.3494916,
author = {Aruna, P. and Priya, N.},
title = {Analysis of Machine Learning techniques for Predicting Student Success in an Educational Institution},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494916},
doi = {10.1145/3494885.3494916},
abstract = {The student success is one of the essential components to assess the quality of the educational institutions. Monitoring student performance is of no use unless or otherwise it is done from the early stage. This research work will definitely consider two factors in mind about student success, firstly, Academic success. Secondly, Placement success. The goal of the proposed system is to predict the student's success using machine learning techniques and give feedback to the educational institutions. In this work , discussion of factors which affect the prediction, finding out the data sources and discussion about the various techniques used in prediction have been done.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Software Engineering},
pages = {168–173},
numpages = {6},
keywords = {Success Factors, Student Success, Performance Prediction, Machine Learning Algorithms},
location = {Singapore, Singapore},
series = {CSSE '21}
}

@article{10.1016/j.cose.2021.102379,
author = {Rajasoundaran, S. and Prabu, A.V. and Routray, Sidheswar and Kumar, S.V.N. Santhosh and Malla, Prince Priya and Maloji, Suman and Mukherjee, Amrit and Ghosh, Uttam},
title = {Machine learning based deep job exploration and secure transactions in virtual private cloud systems},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {109},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102379},
doi = {10.1016/j.cose.2021.102379},
journal = {Comput. Secur.},
month = oct,
numpages = {14},
keywords = {Machine learning and job analysis, Job transactions, Zero-trust environment, Security, Cloud system}
}

@inproceedings{10.1145/3477231.3490427,
author = {Reza, Md Farhadur},
title = {Machine learning for design and optimization challenges in multi/many-core network-on-chip},
year = {2021},
isbn = {9781450387118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477231.3490427},
doi = {10.1145/3477231.3490427},
abstract = {Due to the advancement of transistor technology, a single chip processor can now have hundreds of cores. Network-on-Chip (NoC) has been the superior interconnect fabric for multi/many-core on-chip systems because of its scalability and parallelism. Due to the rise of dark silicon with the end of Dennard Scaling, it becomes essential to design energy-efficient, high performance, and reliable heterogeneous many-core NoC based systems. Because of the large and complex design space, multi/many-core NoC design space becomes difficult to explore for optimal trade-offs of energy-performance-reliability at both design-time and run-time. Furthermore, reactive resource management is not effective in preventing problems, such as creating thermal hotspots and exceeding power budget, from happening in adaptive system. Therefore, in this work, we explore machine learning techniques to design and configure the NoC resources in many-core systems based on the learning of the system and applications workloads. Machine learning can automatically learn from past experiences and guide the many-core system intelligently to achieve its objective on performance, power, and reliability. We have presented many-core NoC based systems design and resource management challenges, and explore and propose machine learning models to uncover near-optimal solutions quickly. Simulation results have been demonstrated to show the effectiveness of a machine learning technique.},
booktitle = {Proceedings of the 14th International Workshop on Network on Chip Architectures},
pages = {29–34},
numpages = {6},
keywords = {optimization, network-on-chip (NoC), many-core systems, machine learning (ML), design, communication},
location = {Virtual Event, Greece},
series = {NoCArc '21}
}

@article{10.1007/s11390-020-9688-x,
author = {Zhang, Shu-Zheng and Zhao, Zhen-Yu and Feng, Chao-Chao and Wang, Lei},
title = {A Machine Learning Framework with Feature Selection for Floorplan Acceleration in IC Physical Design},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {2},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9688-x},
doi = {10.1007/s11390-020-9688-x},
abstract = {Floorplan is an important process whose quality determines the timing closure in integrated circuit (IC) physical design. And generating a floorplan with satisfying timing result is time-consuming because much time is spent on the generation-evaluation iteration. Applying machine learning to the floorplan stage is a potential method to accelerate the floorplan iteration. However, there exist two challenges which are selecting proper features and achieving a satisfying model accuracy. In this paper, we propose a machine learning framework for floorplan acceleration with feature selection and model stacking to cope with the challenges, targeting to reduce time and effort in integrated circuit physical design. Specifically, the proposed framework supports predicting post-route slack of static random-access memory (SRAM) in the early floorplan stage. Firstly, we introduce a feature selection method to rank and select important features. Considering both feature importance and model accuracy, we reduce the number of features from 27 to 15 (44% reduction), which can simplify the dataset and help educate novice designers. Then, we build a stacking model by combining different kinds of models to improve accuracy. In 28 nm technology, we achieve the mean absolute error of slacks less than 23.03 ps and effectively accelerate the floorplan process by reducing evaluation time from 8 hours to less than 60 seconds. Based on our proposed framework, we can do design space exploration for thousands of locations of SRAM instances in few seconds, much more quickly than the traditional approach. In practical application, we improve the slacks of SRAMs more than 75.5 ps (177% improvement) on average than the initial design.},
journal = {J. Comput. Sci. Technol.},
month = mar,
pages = {468–474},
numpages = {7},
keywords = {design space exploration, feature selection, machine learning, physical design}
}

@article{10.5555/2594611.2594614,
author = {Sebag, Michele},
title = {A tour of machine learning: An AI perspective},
year = {2014},
issue_date = {January 2014},
publisher = {IOS Press},
address = {NLD},
volume = {27},
number = {1},
issn = {0921-7126},
abstract = {Machine Learning has been at the core of Artificial Intelligence since its inception. Many promises have been held, if one is to consider that Google is a living demonstration of AI. This paper presents a historical perspective on Machine Learning, describing how the emphasis was gradually shifted from logical to statistical induction, from induction to optimization, from the search of hypotheses to the search of representations. The paper concludes with a discussion about the new frontier of Machine Learning.},
journal = {AI Commun.},
month = jan,
pages = {11–23},
numpages = {13},
keywords = {Unsupervised Learning, Statistical Learning, Reinforcement Learning, Machine Reasoning, Machine Learning, Change Of Representation}
}

@article{10.1007/s11227-019-03049-4,
author = {Ahmad, Usman and Song, Hong and Bilal, Awais and Alazab, Mamoun and Jolfaei, Alireza},
title = {Securing smart vehicles from relay attacks using machine learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {4},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03049-4},
doi = {10.1007/s11227-019-03049-4},
abstract = {Due to the rapid developments in intelligent transportation systems, modern vehicles have turned into intelligent transportation means which are able to exchange data through various communication protocols. Today’s vehicles portray best example of a cyber-physical system because of their integration of computational components and physical systems. As the IoT and data remain intrinsically linked together, the evolving nature of the transportation network comes with a risk of virtual vehicle hijacking. In this paper, we propose a combination of machine learning techniques to mitigate the relay attacks on Passive Keyless Entry and Start (PKES) systems. The proposed algorithm uses a set of key fob features that accurately profiles the PKES system and a set of driving features to identify the driver. First relay attack detection is performed, and if a relay attack is not detected, the vehicle is unlocked and algorithm proceeds to gain the driving features and use neural networks to identify whether the current driver is whom he/she claims to be. To assess the machine learning model, we compared the decision tree, SVM, and KNN method using a three-month log of a PKES system. Our test results confirm the effectiveness of the proposed method in recognizing relayed messages. The proposed methods achieve 99.8% accuracy rate. We used a Long Short-Term Memory recurrent neural network for driver identification based on the real-world driving data, which are collected from a driver who drives the vehicles on several routes in real-world traffic conditions.},
journal = {J. Supercomput.},
month = apr,
pages = {2665–2682},
numpages = {18},
keywords = {Security, Driver identification, Relay attacks, PKES, Neural networks, Machine learning}
}

@article{10.5555/3291125.3309641,
author = {Mai, Xiaoyi and Couillet, Romain},
title = {A random matrix analysis and improvement of semi-supervised learning for large dimensional data},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real data sets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3074–3100},
numpages = {27},
keywords = {semi-supervised learning, random matrix theory, kernel methods, high dimensional statistics}
}

@article{10.1016/j.compag.2019.105124,
author = {Gorczyca, Michael T. and Gebremedhin, Kifle G.},
title = {Ranking of environmental heat stressors for dairy cows using machine learning algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {168},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105124},
doi = {10.1016/j.compag.2019.105124},
journal = {Comput. Electron. Agric.},
month = jan,
numpages = {8},
keywords = {Neural network, Random forest, Machine learning, Dairy cows, Heat stress}
}

@article{10.1007/s10586-019-02912-6,
author = {Ta, Nguyen Binh Duong},
title = {
$$FC^{2}$$: cloud-based cluster provisioning for distributed machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-02912-6},
doi = {10.1007/s10586-019-02912-6},
abstract = {Training large, complex machine learning models such as deep neural networks with big data requires powerful computing clusters, which are costly to acquire, use and maintain. As a result, many machine learning researchers turn to cloud computing services for on-demand and elastic resource provisioning capabilities. Two issues have arisen from this trend: (1) if not configured properly, training models on cloud-based clusters could incur significant cost and time, and (2) many researchers in machine learning tend to focus more on model and algorithm development, so they may not have the time or skills to deal with system setup, resource selection and configuration. In this work, we propose and implement $$FC^{2}$$: a system for fast, convenient and cost-effective distributed machine learning over public cloud resources. Central to the effectiveness of $$FC^{2}$$ is the ability to recommend an appropriate resource configuration in terms of cost and execution time for a given model training task. Our approach differs from previous work in that it does not need to manually analyze the code and dataset of the training task in advance. The recommended resource configuration can then be deployed and managed automatically by $$FC^2$$ until the training task is completed. We have conducted extensive experiments with an implementation of $$FC^2$$, using real-world deep neural network models and datasets. The results demonstrate the effectiveness of our approach, which could produce cost saving of up to 80% while maintaining similar training performance compared to much more expensive resource configurations.},
journal = {Cluster Computing},
month = dec,
pages = {1299–1315},
numpages = {17},
keywords = {Cluster deployment, Resource recommendation, Cloud-based clusters, Distributed machine learning}
}

@article{10.1016/j.comcom.2020.01.005,
author = {Poornima, I. Gethzi Ahila and Paramasivan, B.},
title = {Anomaly detection in wireless sensor network using machine learning algorithm},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {151},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2020.01.005},
doi = {10.1016/j.comcom.2020.01.005},
journal = {Comput. Commun.},
month = feb,
pages = {331–337},
numpages = {7},
keywords = {LWPR, Linear weighted projection regression, Machine learning, Anomaly detection, Wireless sensor networks}
}

@article{10.3233/JIFS-189532,
author = {Lu, Shaoqin and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Enterprise supply chain risk assessment based on improved neural network algorithm and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189532},
doi = {10.3233/JIFS-189532},
abstract = {It is of practical significance to study the decision-making subject in the supply chain under the influence of risk aversion to make a decision and make the supply chain compete in an orderly market environment. In order to improve the effect of enterprise supply chain risk assessment, this paper improves the traditional neural network algorithm, combines machine learning methods and supply chain risk assessment time requirements to set system function modules, and builds the overall system structure. Considering the multiple relationship attributes of supply chain risk knowledge, this paper uses a multi-element semantic network to represent the network structure of supply chain risk knowledge, and proposes a multi-level inventory control modelThis is based on the inventory of the coordination center and other retailers’ procurement/relocation strategy models. After building the model, this paper designs a simulation test to verify and analyze the model performance. The research results show that the model proposed in this paper has a certain effect.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7013–7024},
numpages = {12},
keywords = {Improved algorithm, neural network, machine learning, enterprise supply chain, risk assessment}
}

@article{10.1145/3359249,
author = {Chancellor, Stevie and Baumer, Eric P. S. and De Choudhury, Munmun},
title = {Who is the "Human" in Human-Centered Machine Learning: The Case of Predicting Mental Health from Social Media},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359249},
doi = {10.1145/3359249},
abstract = {"Human-centered machine learning" (HCML) combines human insights and domain expertise with data-driven predictions to answer societal questions. This area's inherent interdisciplinarity causes tensions in the obligations researchers have to the humans whose data they use. This paper studies how scientific papers represent human research subjects in HCML. Using mental health status prediction on social media as a case study, we conduct thematic discourse analysis on 55 papers to examine these representations. We identify five discourses that weave a complex narrative of who the human subject is in this research: Disorder/Patient, Social Media, Scientific, Data/Machine Learning, and Person. We show how these five discourses create paradoxical subject and object representations of the human, which may inadvertently risk dehumanization. We also discuss the tensions and impacts of interdisciplinary research; the risks of this work to scientific rigor, online communities, and mental health; and guidelines for stronger HCML research in this nascent area.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {147},
numpages = {32},
keywords = {social media, research ethics, mental health, machine learning, human-centered machine learning}
}

@inproceedings{10.1007/978-3-030-60276-5_55,
author = {Tardy, Paul and de Seynes, Louis and Hernandez, Fran\c{c}ois and Nguyen, Vincent and Janiszek, David and Est\`{e}ve, Yannick},
title = {Leverage Unlabeled Data for Abstractive Speech Summarization with Self-supervised Learning and Back-Summarization},
year = {2020},
isbn = {978-3-030-60275-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60276-5_55},
doi = {10.1007/978-3-030-60276-5_55},
abstract = {Supervised approaches for Neural Abstractive Summarization require large annotated corpora that are costly to build. We present a French meeting summarization task where reports are predicted based on the automatic transcription of the meeting audio recordings. In order to build a corpus for this task, it is necessary to obtain the (automatic or manual) transcription of each meeting, and then to segment and align it with the corresponding manual report to produce training examples suitable for training. On the other hand, we have access to a very large amount of unaligned data, in particular reports without corresponding transcription. Reports are professionally written and well formatted making pre-processing straightforward. In this context, we study how to take advantage of this massive amount of unaligned data using two approaches (i) self-supervised pre-training using a target-side denoising encoder-decoder model; (ii) back-summarization i.e. reversing the summarization process by learning to predict the transcription given the report, in order to align single reports with generated transcription, and use this synthetic dataset for further training. We report large improvements compared to the previous baseline (trained on aligned data only) for both approaches on two evaluation sets. Moreover, combining the two gives even better results, outperforming the baseline by a large margin of +6 ROUGE-1 and ROUGE-L and +5 ROUGE-2 on two evaluation sets.},
booktitle = {Speech and Computer: 22nd International Conference, SPECOM 2020, St. Petersburg, Russia, October 7–9, 2020, Proceedings},
pages = {572–580},
numpages = {9},
keywords = {French, Back-summarization, Self-supervised learning, Semi-supervised learning, Abstractive Summarization},
location = {St. Petersburg, Russia}
}

@article{10.1155/2020/8049504,
author = {Castelli, Mauro and Clemente, Fabiana Martins and Popovi\v{c}, Ale\v{s} and Silva, Sara and Vanneschi, Leonardo and Chan, Felix},
title = {A Machine Learning Approach to Predict Air Quality in California},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/8049504},
doi = {10.1155/2020/8049504},
abstract = {Predicting air quality is a complex task due to the dynamic nature, volatility, and high variability in time and space of pollutants and particulates. At the same time, being able to model, predict, and monitor air quality is becoming more and more relevant, especially in urban areas, due to the observed critical impact of air pollution on citizens’ health and the environment. In this paper, we employ a popular machine learning method, support vector regression (SVR), to forecast pollutant and particulate levels and to predict the air quality index (AQI). Among the various tested alternatives, radial basis function (RBF) was the type of kernel that allowed SVR to obtain the most accurate predictions. Using the whole set of available variables revealed a more successful strategy than selecting features using principal component analysis. The presented results demonstrate that SVR with RBF kernel allows us to accurately predict hourly pollutant concentrations, like carbon monoxide, sulfur dioxide, nitrogen dioxide, ground-level ozone, and particulate matter 2.5, as well as the hourly AQI for the state of California. Classification into six AQI categories defined by the US Environmental Protection Agency was performed with an accuracy of 94.1% on unseen validation data.},
journal = {Complex.},
month = jan,
numpages = {23}
}

@article{10.1155/2021/1999284,
author = {Chang, Yue and Chen, Xudong and Hong, Danfeng},
title = {Estimation of Chronic Illness Severity Based on Machine Learning Methods},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/1999284},
doi = {10.1155/2021/1999284},
abstract = {Chronic diseases are diseases that last one year or more and require a continuous medical care and monitoring. Based on this point, a dataset from an APP called Flaredown helps patients of chronic disease improve their symptoms and conditions. In this study, an illness severity-level model was proposed to give the patient an alert to his or her health condition into three different levels according to their severity. Personal information, treatment conditions, and dietary conditions were analyzed by a statistical measure, TD-IDF. Seven different machine learning models were used and compared to generate the illness severity-level model. The results revealed that the XGBoost model with a F1 score of 0.85 and LightGBM model with a F1 score of 0.84 have the best performance. We also applied feature selection and parameter tuning for these two models to attain better performance, and the final best F1 scores achieved by the XGBoost model and LightGBM model were both 0.85. Sensitivity analysis has shown that the treatment feature and symptom feature have important effects on the classification of the illness severity-level. Based on this, a fusion model was designed to study the data and the final accuracy of the fusion model was 93.3%. Thus, this study provides an effective illness severity-level model for a reference and guidance for the management of high-risk groups of chronic diseases. Patients may use this illness severity-level model to self-monitor their illness conditions and take proactive steps to avoid deterioration of their illness and take further medical care.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3177540.3177555,
author = {Wang, Li-C.},
title = {Machine Learning for Feature-Based Analytics},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177540.3177555},
doi = {10.1145/3177540.3177555},
abstract = {Applying machine learning in Electronic Design Automation (EDA) has received growing interests in recent years. One approach to analyze data in EDA applications can be called feature-based analytics. In this context, the paper explains the inadequacy of adopting a traditional machine learning problem formulation view. Then, an alternative machine learning view is suggested where learning from data is treated as an iterative search process. The theoretical and practical considerations for implementing such a search process are discussed in the context of various applications.},
booktitle = {Proceedings of the 2018 International Symposium on Physical Design},
pages = {74–81},
numpages = {8},
keywords = {version space, machine learning, learnable, feature-based analytics, design automation, Occam's razor},
location = {Monterey, California, USA},
series = {ISPD '18}
}

@article{10.3233/JIFS-189488,
author = {Zhang, Chengyuan and Li, Mingliang and Li, Yongqiang and Ramachandran, Varatharajan},
title = {Financial risk analysis of real estate bubble based on machine learning and factor analysis model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189488},
doi = {10.3233/JIFS-189488},
abstract = {The regional real estate price bubble regulation policy is an external factor for the real estate industry. The effect of real estate regulation is difficult to determine, which is a typical problem of uncertain system analysis and forecasting, and the gray Bayesian network forecasting model is to solve the forecasting problem of economic system subject to external regulation. Based on machine learning and factor analysis models, this paper constructs a real estate bubble financial risk analysis model based on machine learning and factor analysis models. Moreover, starting from the real estate price bubble, which is a hot and difficult issue of the social economy, this paper discusses the causes of the formation of real estate price bubbles and the mechanism of the formation of real estate price bubbles, looks for the importance of policy regulation of real estate price bubbles, and clarifies the functional game model of policy regulation of real estate price bubbles. In addition, this paper uses examples to study the model constructed in this paper. The results show that the model constructed in this paper has a certain effect.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6493–6504},
numpages = {12},
keywords = {Machine learning, factor analysis, real estate bubble, financial risk}
}

@inproceedings{10.1145/3445969.3450431,
author = {Yilmaz, Ibrahim and Kapoor, Kavish and Siraj, Ambareen and Abouyoussef, Mahmoud},
title = {Privacy Protection of Grid Users Data with Blockchain and Adversarial Machine Learning},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450431},
doi = {10.1145/3445969.3450431},
abstract = {Utilities around the world are reported to invest a total of around $30 billion over the next few years for installation of more than 300 million smart meters, replacing traditional analog meters citeinfo. By mid-decade, with full country wide deployment, there will be almost 1.3 billion smart meters in place citeinfo. Collection of fine-grained energy usage data by these smart meters provides numerous advantages such as energy savings for customers with use of demand optimization, a billing system of higher accuracy with dynamic pricing programs, bidirectional information exchange ability between end-users for better consumer-operator interaction, and so on. However, all these perks associated with fine-grained energy usage data collection threaten the privacy of users. With this technology, customers' personal data such as sleeping cycle, number of occupants, and even type and number of appliances stream into the hands of the utility companies and can be subject to misuse. This research paper addresses privacy violation of consumers' energy usage data collected from smart meters and provides a novel solution for the privacy protection while allowing benefits of energy data analytics. First, we demonstrate the successful application of occupancy detection attacks using a deep neural network method that yields high accuracy results. We then introduce Adversarial Machine Learning Occupancy Detection Avoidance with Blockchain (AMLODA-B) framework as a counter-attack by deploying an algorithm based on the Long Short Term Memory (LSTM) model into the standardized smart metering infrastructure to prevent leakage of consumer's personal information. Our privacy-aware approach protects consumers' privacy without compromising the correctness of billing and preserves operational efficiency without use of authoritative intermediaries.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {33–38},
numpages = {6},
keywords = {smart meter, privacy preserving, neural networks, long short term memory, blockchain, adversarial machine learning},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@article{10.1561/2000000102,
author = {Simeone, Osvaldo},
title = {A Brief Introduction to Machine Learning for Engineers},
year = {2018},
issue_date = {Aug 2018},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {12},
number = {3–4},
issn = {1932-8346},
url = {https://doi.org/10.1561/2000000102},
doi = {10.1561/2000000102},
abstract = {This monograph aims at providing an introduction to key
concepts, algorithms, and theoretical results in machine
learning. The treatment concentrates on probabilistic models
for supervised and unsupervised learning problems. It
introduces fundamental concepts and algorithms by building
on first principles, while also exposing the reader to more
advanced topics with extensive pointers to the literature,
within a unified notation and mathematical framework. The
material is organized according to clearly defined categories,
such as discriminative and generative models, frequentist
and Bayesian approaches, exact and approximate inference,
as well as directed and undirected models. This monograph
is meant as an entry point for researchers with an engineering
background in probability and linear algebra.},
journal = {Found. Trends Signal Process.},
month = aug,
pages = {200–431},
numpages = {236}
}

@phdthesis{10.5555/AAI28646717,
author = {Xu, Kaidi and Yanzhi, Wang, and Jennifer, Dy,},
advisor = {Xue, Lin,},
title = {Can We Trust AI? Towards Practical Implementation and Theoretical Analysis in Trustworthy Machine Learning},
year = {2021},
isbn = {9798535511139},
publisher = {Northeastern University},
address = {USA},
abstract = {Deep learning or deep neural networks (DNNs) have achieved extraordinary performance in many application domains such as image classification, object detection and recognition, natural language processing and medical image analysis. It has been well accepted that DNNs are vulnerable to adversarial attacks, which raises concerns of DNNs in security-critical applications and may result in disastrous consequences. Adversarial attacks are usually implemented by generating adversarial examples, i.e., adding sophisticated perturbations onto benign examples, such that adversarial examples are classified by the DNN as target (wrong) labels instead of the correct labels of the benign examples. The adversarial machine learning aims to study this phenomenon and leverage it to build robust machine learning systems and explain DNNs.In this dissertation, we present the mechanism of adversarial machine learning in both empirical and theoretical ways. Specifically, we first introduce a uniform adversarial attack generation framework, structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures. Second, we discuss the feasibility of adversarial attack in the physical world and introduce a powerful framework, Expectation over Transformation (EoT). Utilize EoT with Thin Plate Spline (TPS) transformation, we can generate  Adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes.  Third, we stand on the defense side and propose the first adversarial training method based on Graph Neural Network. Fourth, we introduce Linear relaxation based perturbation analysis (LiRPA) for neural networks, which computes the provable linear bounds of output neurons given a certain amount of input perturbation. LiRPA studies the adversarial example in a theoretical way and can guarantee the test accuracy of a model by given perturbation constraints. Finally, leveraging the efficient LiRPA with branch and bound, we speed up the conventional Linear Programming-based complete verification framework by an order of magnitude.In the future, we plan to study on a novel patch transformer network to truthfully model real-world physical transformations empirically. In addition, at the formal robustness direction, we plan to explore the complete verification in real-time, that given sufficient time, the verifier should give a definite "yes/no" answer for a property under verification efficiently. Our LiRPA framework combined with GPUs can accelerate this procedure potentially.},
note = {AAI28646717}
}

@article{10.1007/s00500-021-06144-y,
author = {Sanguineti, Marcello and T\`{a}nfani, Elena},
title = {Special Issue Optimization for Machine Learning Guest Editorial},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {19},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06144-y},
doi = {10.1007/s00500-021-06144-y},
journal = {Soft Comput.},
month = oct,
pages = {12565–12567},
numpages = {3}
}

@article{10.1007/s00291-020-00604-x,
author = {Sun, Yuan and Ernst, Andreas and Li, Xiaodong and Weiner, Jake},
title = {Generalization of machine learning for problem reduction: a case study on travelling salesman problems},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {43},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/s00291-020-00604-x},
doi = {10.1007/s00291-020-00604-x},
abstract = {Combinatorial optimization plays an important role in real-world problem solving. In the big data era, the dimensionality of a combinatorial optimization problem is usually very large, which poses a significant challenge to existing solution methods.
 In this paper, we examine the generalization capability of a machine learning model for problem reduction on the classic travelling salesman problems (TSP). We demonstrate that our method can greedily remove decision variables from an optimization problem that are predicted not to be part of an optimal solution. More specifically, we investigate our model’s capability to generalize on test instances that have not been seen during the training phase. We consider three scenarios where training and test instances are different in terms of: (1) problem characteristics; (2) problem sizes; and (3) problem types. Our experiments show that this machine learning-based technique can generalize reasonably well over a wide range of TSP test instances with different characteristics or sizes. Since the accuracy of predicting unused variables naturally deteriorates as a test instance is further away from the training set, we observe that, even when tested on a different TSP problem variant, the machine learning model still makes useful predictions about which variables can be eliminated without significantly impacting solution quality.},
journal = {OR Spectr.},
month = sep,
pages = {607–633},
numpages = {27},
keywords = {Travelling salesman problem, Problem reduction, Generalization error, Machine learning, Combinatorial optimization}
}

@inproceedings{10.1145/3446132.3446407,
author = {Yan, Jianzhuo and Geng, Yanan and Xu, Hongxia and Yu, Yongchuan and Tan, Shaofeng and He, Dongdong},
title = {Research on Named Entity Recognition in Chinese EMR Based on Semi-Supervised Learning with Dual Selected Strategy},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446407},
doi = {10.1145/3446132.3446407},
abstract = {With the construction of the electronic medical record system, medical record data begins to accumulate, and how to extract essential information from these resources has become a concern. And named entity recognition(NER) is the first step. With the help of doctors, we built a small Chinese electronic medical record annotation corpus. But the NER supervision method requires a large amount of manually labeled corpus. So to reduce the cost of it and make better use of the unlabeled corpus, this paper proposes a semi-supervised Chinese electronic medical record NER model based on ALBERT-BiLSTM-CRF which named CEMRNER. The model uses a Bidirectional Long Short Term Memory network (BiLSTM) and a Conditional Random Field model (CRF) to train the data and introduces the pre-training language model ALBERT to solve the problem of Chinese representation. At the same time, we propose a dual selected strategy to select the high confidence samples and expand the training set. The dual strategy can ensure the accuracy i automatically labeled data, and reduce the error iteration in semi-supervised learning. The experiment and analysis show that compared with other models, this method is more accurate and comprehensive. The precision, recall rate, and F1Score are 85.45%, 87.81%, and 86.61%, respectively. The paper proves that using a semi-supervised method and pre-training ALBERT can improve the accuracy of recognition under the condition of less labeled data.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {80},
numpages = {10},
keywords = {semi-supervised learning, named entity recognition, dual selected strategy, CRF, BiLSTM, ALBERT},
location = {Sanya, China},
series = {ACAI '20}
}

@article{10.14778/3457390.3457399,
author = {Yuan, Binhang and Jankov, Dimitrije and Zou, Jia and Tang, Yuxin and Bourgeois, Daniel and Jermaine, Chris},
title = {Tensor relational algebra for distributed machine learning system design},
year = {2021},
issue_date = {April 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3457390.3457399},
doi = {10.14778/3457390.3457399},
abstract = {We consider the question: what is the abstraction that should be implemented by the computational engine of a machine learning system? Current machine learning systems typically push whole tensors through a series of compute kernels such as matrix multiplications or activation functions, where each kernel runs on an AI accelerator (ASIC) such as a GPU. This implementation abstraction provides little built-in support for ML systems to scale past a single machine, or for handling large models with matrices or tensors that do not easily fit into the RAM of an ASIC. In this paper, we present an alternative implementation abstraction called the tensor relational algebra (TRA). The TRA is a set-based algebra based on the relational algebra. Expressions in the TRA operate over binary tensor relations, where keys are multi-dimensional arrays and values are tensors. The TRA is easily executed with high efficiency in a parallel or distributed environment, and amenable to automatic optimization. Our empirical study shows that the optimized TRA-based back-end can significantly outperform alternatives for running ML workflows in distributed clusters.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {1338–1350},
numpages = {13}
}

@article{10.1016/j.inffus.2018.10.005,
author = {Diez-Olivan, Alberto and Del Ser, Javier and Galar, Diego and Sierra, Basilio},
title = {Data fusion and machine learning for industrial prognosis: Trends and perspectives towards Industry 4.0},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.10.005},
doi = {10.1016/j.inffus.2018.10.005},
journal = {Inf. Fusion},
month = oct,
pages = {92–111},
numpages = {20},
keywords = {VCM, SVMs, SOM-MQE, SBM, SARMA, RNN, RBM, PoF, PCA, LOF, LAD, KDE, kNN, HMM, GRNN, GRBMs, GMM, FPCA, FFT, EWMA, EM, DWT, DBN, BPNN, ANNs, ANFIS, Industry 4.0, Machine learning, Data fusion, Data-driven prognosis}
}

@inproceedings{10.1145/3338501.3357372,
author = {Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek},
title = {Analyzing the Robustness of Open-World Machine Learning},
year = {2019},
isbn = {9781450368339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338501.3357372},
doi = {10.1145/3338501.3357372},
abstract = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing \o{}odAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that \o{}odAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
booktitle = {Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
pages = {105–116},
numpages = {12},
keywords = {open world recognition, deep learning, adversarial example},
location = {London, United Kingdom},
series = {AISec'19}
}

@inproceedings{10.5555/3495724.3496625,
author = {Zhong, Yaofeng Desmond and Leonard, Naomi Ehrich},
title = {Unsupervised learning of lagrangian dynamics from images for prediction and control},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent approaches for modelling dynamics of physical systems with neural networks enforce Lagrangian or Hamiltonian structure to improve prediction and generalization. However, when coordinates are embedded in high-dimensional data such as images, these approaches either lose interpretability or can only be applied to one particular example. We introduce a new unsupervised neural network model that learns Lagrangian dynamics from images, with interpretability that benefits prediction and control. The model infers Lagrangian dynamics on generalized coordinates that are simultaneously learned with a coordinate-aware variational autoencoder (VAE). The VAE is designed to account for the geometry of physical systems composed of multiple rigid bodies in the plane. By inferring interpretable Lagrangian dynamics, the model learns physical system properties, such as kinetic and potential energy, which enables long-term prediction of dynamics in the image space and synthesis of energy-based controllers.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {901},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@phdthesis{10.5555/AAI28320426,
author = {Peken, Ture and Ditzler, Gregory and Djordjevic, Ivan},
advisor = {Tamal, Bose, and Ravi, Tandon,},
title = {Machine Learning for Channel Estimation and Hybrid Beamforming in Millimeter-Wave Wireless Networks},
year = {2021},
isbn = {9798597009735},
publisher = {The University of Arizona},
abstract = {The continuous growth of mobile users and high-speed wireless applications drives the demand for using the abundant bandwidth of mmWave (millimeter-wave) frequencies. On one hand, a massive number of antennas can be supported due to small wavelengths of mmWave signals, which allow using antennas with small form factors. On the other hand, the free space path loss increases with the square of the frequency, which implies that the path loss would be severe in mmWave frequencies. Fortunately, one can compensate for the performance degradation due to the path loss by using directional beamforming (BF) along with the high gain large antenna array systems (massive MIMO). This dissertation tackles three distinct problems, namely channel estimation in massive MIMO, signal detection in massive MIMO, and efficient design of hybrid BF algorithms. In the first part of this dissertation, we focus on the effective channel estimation for massive MIMO systems to overcome the pilot contamination problem. We present an adaptive independent component analysis (ICA)-based channel estimation method, which outperforms conventional ICA as well as other conventional methods for channel estimation. We also make use of compressive sensing (CS) methods for channel estimation and show the advantages in terms of channel estimation accuracy and complexity.In the second part of this dissertation, we consider the problem of signal detection specifically focusing on the scenarios when non-Gaussian signals need to be detected and the receiver may be equipped with a large number of antennas. We show that for the case of non-Gaussian signal detection it turns out the conventional Neyman-Pearson (NP) detector does not perform well for the low signal-to-noise-ratio (SNR) regime. Motivated by this, we propose a bispectrum detector, which is able to better detect the corresponding non-Gaussian information in the signal. We also present the theoretical analysis for the asymptotic behavior of Probability of False Alarm and Probability of Detection. We show the performance of signal detection (for both Gaussian and non-Gaussian signals) as a function of the number of antennas and sampling rate. We also obtain the scaling behavior of the performance in the massive antenna regime.The third part of this dissertation covers the efficient design of hybrid BF algorithms with a specific focus on massive MIMO systems in mmWave networks. The key challenge in the design of hybrid BF algorithms in such networks is that the computational complexity can be prohibitive. We start by focusing on the fundamental approach of finding BF solutions through singular value decomposition (SVD) and explore the role of ML techniques to perform SVD. The first part of this contribution focuses on the data-driven approach to SVD. We propose three deep neural network (DNN) architectures to approximate the SVD, with varying levels of complexity. The methodology for training these DNN architectures is inspired by the fundamental property of SVD, i.e., it can be used to obtain low-rank approximations. We next explicitly take the constraints of hybrid BF into account (such as quantized phase shifters, power constraints), and propose a novel DNN-based approach for the design of hybrid BF systems. Our results show that DNNs can be an attractive and efficient solution for both estimating the SVD as well as hybrid beamformers. Furthermore, we provide time complexity and memory requirement analyses for the proposed DNN-based and state-of-the-art hybrid BF approaches. We then propose a novel reinforcement learning-based hybrid BF algorithm that applies Q-learning in a supervised manner. We analyze the computational complexity of our algorithm as a function of iteration steps and show that a significant reduction in computational complexity is achieved compared to the exhaustive search.In addition to exploring supervised approaches, in the remaining part of this contribution we also explore unsupervised methods for SVD and hybrid BF. These methods are particularly attractive for scenarios when channel conditions change too fast and we may not have a pre-existing dataset of channels and the corresponding optimal BF solutions, which are required for supervised learning. For unsupervised learning, we explore two techniques namely autoencoders and generative adversarial networks (GANs) for both the SVD and hybrid BF. We first propose a linear autoencoder-based approach for the SVD, and then provide a linear autoencoder-based hybrid BF algorithm, which incorporates the constraints of the hybrid BF. In the last part of this contribution, we focus on two different generative models: variational autoencoders (VAEs) and GANs to reduce the number of training iterations compared to the linear autoencoder-based approach. We first propose VAE and Wasserstein GAN (WGAN) based algorithms for the SVD. We then present a VAE and a novel GAN architecture to find the hybrid BF solutions.},
note = {AAI28320426}
}

@inproceedings{10.1145/3324921.3328786,
author = {Zhang, Yueqian and Simsek, Murat and Kantarci, Burak},
title = {Machine Learning-based Prevention of Battery-oriented Illegitimate Task Injection in Mobile Crowdsensing},
year = {2019},
isbn = {9781450367691},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324921.3328786},
doi = {10.1145/3324921.3328786},
abstract = {Mobile crowdsensing (MCS) is a cloud-inspired and non-dedicated sensing paradigm to enable ubiquitous sensing via built-in sensors of personalized devices. Due to disparate participants and sensing tasks, MCS is vulnerable to threats initiated by malicious participants, which can either be a participant providing sensory data or an end user injecting a fake task aiming at resource (e.g. battery, sensor, etc.) clogging at the participating devices. This paper builds on machine learning-based detection of illegitimate tasks, and investigates the impact of machine learning-based prevention of battery-oriented illegitimate task injection in MCS campaigns. To this end, we introduce two different attack strategies, and test the impact of ML-based detection and elimination of fake tasks on task completion rate, as well as the overall battery drain of participating devices. Simulation results confirm that up to 14% battery power can be saved at the expense of a slight decrease in the completion rate of legitimate tasks.},
booktitle = {Proceedings of the ACM Workshop on Wireless Security and Machine Learning},
pages = {31–36},
numpages = {6},
keywords = {security, mobile crowdsensing, machine learning, attack models},
location = {Miami, FL, USA},
series = {WiseML 2019}
}

@article{10.1155/2020/8873366,
author = {Liu, Jing and Qiao, Yulong and Tsai, Pei-Wei},
title = {Quasiconformal Mapping Kernel Machine Learning-Based Intelligent Hyperspectral Data Classification for Internet Information Retrieval},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/8873366},
doi = {10.1155/2020/8873366},
abstract = {Intelligent internet data mining is an important application of AIoT (Artificial Intelligence of Things), and it is necessary to construct large training samples with the data from the internet, including images, videos, and other information. Among them, a hyperspectral database is also necessary for image processing and machine learning. The internet environment provides abundant hyperspectral data resources, but the hyperspectral data have no class labels and no so high value for applications. So, it is important to label the class information for these hyperspectral data through machine learning-based classification. In this paper, we present a quasiconformal mapping kernel machine learning-based intelligent hyperspectral data classification algorithm for internet-based hyperspectral data retrieval. The contributions include three points: the quasiconformal mapping-based multiple kernel learning network framework is proposed for hyperspectral data classification, the Mahalanobis distance kernel function is as the network nodes with the higher discriminative ability than Euclidean distance-based kernel function learning, and the objective function of measuring the class discriminative ability is proposed to seek the optimal parameters of the quasiconformal mapping projection. Experiments show that the proposed scheme is effective for hyperspectral image classification and retrieval.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.5555/3157382.3157571,
author = {Farnia, Farzan and Tse, David},
title = {A minimax approach to supervised learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given a task of predicting Y from X, a loss function L, and a set of probability distributions r on (X, Y), what is the optimal decision rule minimizing the worst-case expected loss over Γ? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on X constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models as well as some popular regularization schemes. For quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models. Moreover, for the 0-1 loss we derive a classifier which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed Γ by solving a tractable optimization problem. We perform several numerical experiments to show the power of the minimax SVM in outperforming the SVM.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4240–4248},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@article{10.1007/s10845-021-01762-7,
author = {El Koujok, Mohamed and Ghezzaz, Hakim and Amazouz, Mouloud},
title = {Energy inefficiency diagnosis in industrial process through one-class machine learning techniques},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {7},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01762-7},
doi = {10.1007/s10845-021-01762-7},
abstract = {In the era of Industry 4.0, the ease of access to precise measurements in real-time and the existence of machine-learning (ML) techniques will play a vital role in building practical tools to isolate inefficiencies in energy-intensive processes. This paper aims at developing an abnormal event diagnosis (AED) tool based on ML techniques for monitoring the operation of industrial processes. This tool makes it easier for operators to accomplish their tasks and to make quick and accurate decisions to ensure highly efficient processes. One of the most popular ML techniques for AED is the multivariate statistical control (MSC) method; it only requires the dataset of the normal operating conditions (NOC) to detect and identify the variables that contribute to abnormal events (AEs). Despite the popularity of MSC, it is challenging to select the appropriate method for detecting and isolating all possible abnormalities a complex industrial process can experience. To address this limitation and improve efficiency, we have developed a generic methodology that integrates different ML techniques into a unified multiagent based approach, the selected ML techniques are supposed to be built using only the normal operating condition. For the sake of demonstration, we chose a combination of two ML methods: principal component analysis and k-nearest neighbors (k-NN). The k-NN was integrated into the proposed multiagent to take into account the nonlinearity and multimodality that frequently occur in industrial processes. In addition, we modified a k-NN method proposed in the literature to reduce computation time during real-time detection and isolation. Finally, the proposed methodology was successfully validated to monitor the energy efficiency of a reboiler located in a thermomechanical pulp mill.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {2043–2060},
numpages = {18},
keywords = {Decision support systems, Multiagent approach, Augmented intelligence, Machine-learning techniques, Abnormal event diagnosis, Industrial and process efficiency}
}

@inproceedings{10.1145/3097983.3098094,
author = {Yang, Carl and Bai, Lanxiao and Zhang, Chao and Yuan, Quan and Han, Jiawei},
title = {Bridging Collaborative Filtering and Semi-Supervised Learning: A Neural Approach for POI Recommendation},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098094},
doi = {10.1145/3097983.3098094},
abstract = {Recommender system is one of the most popular data mining topics that keep drawing extensive attention from both academia and industry. Among them, POI (point of interest) recommendation is extremely practical but challenging: it greatly benefits both users and businesses in real-world life, but it is hard due to data scarcity and various context. While a number of algorithms attempt to tackle the problem w.r.t. specific data and problem settings, they often fail when the scenarios change. In this work, we propose to devise a general and principled SSL (semi-supervised learning) framework, to alleviate data scarcity via smoothing among neighboring users and POIs, and treat various context by regularizing user preference based on context graphs. To enable such a framework, we develop PACE (Preference And Context Embedding), a deep neural architecture that jointly learns the embeddings of users and POIs to predict both user preference over POIs and various context associated with users and POIs. We show that PACE successfully bridges CF (collaborative filtering) and SSL by generalizing the de facto methods matrix factorization of CF and graph Laplacian regularization of SSL. Extensive experiments on two real location-based social network datasets demonstrate the effectiveness of PACE.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1245–1254},
numpages = {10},
keywords = {semi-supervised learning, recommender systems, neural networks, collaborative filtering},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3395363.3397352,
author = {Sharma, Arnab and Wehrheim, Heike},
title = {Higher income, larger loan? monotonicity testing of machine learning models},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397352},
doi = {10.1145/3395363.3397352},
abstract = {Today, machine learning (ML) models are increasingly applied in decision making. This induces an urgent need for quality assurance of ML models with respect to (often domain-dependent) requirements. Monotonicity is one such requirement. It specifies a software as ''learned'' by an ML algorithm to give an increasing prediction with the increase of some attribute values. While there exist multiple ML algorithms for ensuring monotonicity of the generated model, approaches for checking monotonicity, in particular of black-box models are largely lacking.  In this work, we propose verification-based testing of monotonicity, i.e., the formal computation of test inputs on a white-box model via verification technology, and the automatic inference of this approximating white-box model from the black-box model under test. On the white-box model, the space of test inputs can be systematically explored by a directed computation of test cases. The empirical evaluation on 90 black-box models shows that verification-based testing can outperform adaptive random testing as well as property-based techniques with respect to effectiveness and efficiency.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {200–210},
numpages = {11},
keywords = {Monotonicity, Machine Learning Testing, Decision Tree},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1007/978-3-030-50323-9_10,
author = {Bromberg, Y\'{e}rom-David and Gitzinger, Louison},
title = {DroidAutoML: A Microservice Architecture to Automate the Evaluation of Android Machine Learning Detection Systems},
year = {2020},
isbn = {978-3-030-50322-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50323-9_10},
doi = {10.1007/978-3-030-50323-9_10},
abstract = {The mobile ecosystem is witnessing an unprecedented increase in the number of malware in the wild. To fight this threat, actors from both research and industry are constantly innovating to bring concrete solutions to improve security and malware protection. Traditional solutions such as signature-based anti viruses have shown their limits in front of massive proliferation of new malware, which are most often only variants specifically designed to bypass signature-based detection. Accordingly, it paves the way to the emergence of new approaches based on Machine Learning (ML) technics to boost the detection of unknown malware variants. Unfortunately, these solutions are most often underexploited due to the time and resource costs required to adequately fine tune machine learning algorithms. In reality, in the Android community, state-of-the-art studies do not focus on model training, and most often go through an empirical study with a manual process to choose the learning strategy, and/or use default values as parameters to configure ML algorithms. However, in the ML domain, it is well known admitted that to solve efficiently a ML problem, the tunability of hyper-parameters is of the utmost importance. Nevertheless, as soon as the targeted ML problem involves a massive amount of data, there is a strong tension between feasibility of exploring all combinations and accuracy. This tension imposes to automate the search for optimal hyper-parameters applied to ML algorithms, that is not anymore possible to achieve manually. To this end, we propose a generic and scalable solution to automatically both configure and evaluate ML algorithms to efficiently detect Android malware detection systems. Our approach is based on devOps principles and a microservice architecture deployed over a set of nodes to scale and exhaustively test a large number of ML algorithms and hyper-parameters combinations. With our approach, we are able to systematically find the best fit to increase up&nbsp;to 11% the accuracy of two state-of-the-art Android malware detection systems.},
booktitle = {Distributed Applications and Interoperable Systems: 20th IFIP WG 6.1 International Conference, DAIS 2020, Held as Part of the 15th International Federated Conference on Distributed Computing Techniques, DisCoTec 2020, Valletta, Malta, June 15–19, 2020, Proceedings},
pages = {148–165},
numpages = {18},
keywords = {AutoML, Malware, Android, Machine learning},
location = {Valletta, Malta}
}

@inproceedings{10.1007/978-3-030-00931-1_68,
author = {Ganaye, Pierre-Antoine and Sdika, Micha\"{e}l and Benoit-Cattin, Hugues},
title = {Semi-supervised Learning for Segmentation Under Semantic Constraint},
year = {2018},
isbn = {978-3-030-00930-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00931-1_68},
doi = {10.1007/978-3-030-00931-1_68},
abstract = {Image segmentation based on convolutional neural networks is proving to be a powerful and efficient solution for medical applications. However, the lack of annotated data, presence of artifacts and variability in appearance can still result in inconsistencies during the inference. We choose to take advantage of the invariant nature of anatomical structures, by enforcing a semantic constraint to improve the robustness of the segmentation. The proposed solution is applied on a brain structures segmentation task, where the output of the network is constrained to satisfy a known adjacency graph of the brain regions. This criteria is introduced during the training through an original penalization loss named NonAdjLoss. With the help of a new metric, we show that the proposed approach significantly reduces abnormalities produced during the segmentation. Additionally, we demonstrate that our framework can be used in a semi-supervised way, opening a path to better generalization to unseen data.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part III},
pages = {595–602},
numpages = {8},
keywords = {Medical image segmentation, Convolutional neural network, Semi-supervised learning, Adjacency graph, Constraint},
location = {Granada, Spain}
}

@article{10.3233/JIFS-179953,
author = {Zhang, Fan and Patnaik, Srikanta},
title = {Innovation of English teaching model based on machine learning neural network and image super resolution},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179953},
doi = {10.3233/JIFS-179953},
abstract = {At present, applying image recognition technology to promote English teaching is a kind of teaching innovation that meets the needs of the times. Therefore, based on machine learning neural network and image super-resolution, this study conducted an innovative analysis of English teaching mode. This paper combined the current situation of English teaching classroom to study and analyze English classroom, combined classroom characteristics as the basis of English teaching innovation and constructed a feature recognition model suitable for current English teaching status. Moreover, this paper formed an initial high-resolution image for low-resolution image reconstruction by sparse representation method, and then established a mixed sample spine regression model to re-estimate the high-frequency components of the initial high-resolution image to realize various behavioral characteristics of students in English teaching classroom. In addition, this article builds a verification test. The research shows that the proposed algorithm has certain effects and can provide theoretical reference for subsequent related research.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1805–1816},
numpages = {12},
keywords = {innovation, english teaching, image super-resolution, neural network, Machine learning}
}

@article{10.1145/3462329,
author = {Masadeh, Mahmoud and Elderhalli, Yassmeen and Hasan, Osman and Tahar, Sofiene},
title = {A Quality-assured Approximate Hardware Accelerators–based on Machine Learning and Dynamic Partial Reconfiguration},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3462329},
doi = {10.1145/3462329},
abstract = {Machine learning is widely used these days to extract meaningful information out of the Zettabytes of sensors data collected daily. All applications require analyzing and understanding the data to identify trends, e.g., surveillance, exhibit some error tolerance. Approximate computing has emerged as an energy-efficient design paradigm aiming to take advantage of the intrinsic error resilience in a wide set of error-tolerant applications. Thus, inexact results could reduce power consumption, delay, area, and execution time. To increase the energy-efficiency of machine learning on FPGA, we consider approximation at the hardware level, e.g., approximate multipliers. However, errors in approximate computing heavily depend on the application, the applied inputs, and user preferences. However, dynamic partial reconfiguration has been introduced, as a key differentiating capability in recent FPGAs, to significantly reduce design area, power consumption, and reconfiguration time by adaptively changing a selective part of the FPGA design without interrupting the remaining system. Thus, integrating “Dynamic Partial Reconfiguration” (DPR) with “Approximate Computing” (AC) will significantly ameliorate the efficiency of FPGA-based design approximation. In this article, we propose hardware-efficient quality-controlled approximate accelerators, which are suitable to be implemented in FPGA-based machine learning algorithms as well as any error-resilient applications. Experimental results using three case studies of image blending, audio blending, and image filtering applications demonstrate that the proposed adaptive approximate accelerator satisfies the required quality with an accuracy of 81.82%, 80.4%, and 89.4%, respectively. On average, the partial bitstream was found to be 28.6 smaller than the full bitstream.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = aug,
articleno = {57},
numpages = {19},
keywords = {FPGA, adaptive design, dynamic partial reconfiguration, input-aware approximation, decision tree, approximate hardware accelerator, Approximate computing}
}

@inproceedings{10.1007/978-3-030-45778-5_19,
author = {Bonandrini, Vassia and Bercher, Jean-Fran\c{c}ois and Zangar, Nawel},
title = {Machine Learning Methods for Anomaly Detection in IoT Networks, with Illustrations},
year = {2019},
isbn = {978-3-030-45777-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45778-5_19},
doi = {10.1007/978-3-030-45778-5_19},
abstract = {IoT devices have been the target of 100 million attacks in the first half of 2019 [1]. According to [2], there will be more than 64 billion Internet of Things (IoT) devices by 2025. It is thus crucial to secure IoT networks and devices, which include significant devices like medical kit or autonomous car. The problem is complicated by the wide range of possible attacks and their evolution, by the limited computing resources and storage resources available on devices. We begin by introducing the context and a survey of Intrusion Detection System (IDS) for IoT networks with a state of the art. So as to test and compare solutions, we consider available public datasets and select the CIDDS-001 Dataset. We implement and test several machine learning algorithms and show that it is relatively easy to obtain reproducible results [20] at the state-of-the-art. Finally, we discuss embedding such algorithms in the IoT context and point-out the possible interest of very simple rules.},
booktitle = {Machine Learning for Networking: Second IFIP TC 6 International Conference, MLN 2019, Paris, France, December 3–5, 2019, Revised Selected Papers},
pages = {287–295},
numpages = {9},
keywords = {Internet of Things, IoT, IDS, NIDS, Intrusion detection system, Rules, CIDDS-001},
location = {Paris, France}
}

@inproceedings{10.1145/3318464.3380575,
author = {Jasny, Matthias and Ziegler, Tobias and Kraska, Tim and Roehm, Uwe and Binnig, Carsten},
title = {DB4ML - An In-Memory Database Kernel with Machine Learning Support},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380575},
doi = {10.1145/3318464.3380575},
abstract = {In this paper, we revisit the question of how ML algorithms can be best integrated into existing DBMSs to not only avoid expensive data copies to external ML tools but also to comply with regulatory reasons. The key observation is that database transactions already provide an execution model that allows DBMSs to efficiently mimic the execution model of modern parallel ML algorithms. As a main contribution, this paper presents DB4ML, an in-memory database kernel that allows applications to implement user-defined ML algorithms and efficiently run them inside a DBMS. Thereby, the ML algorithms are implemented using a programming model based on the idea of so called iterative transactions. Our experimental evaluation shows that DB4ML can support user-defined ML algorithms inside a DBMS with the efficiency of modern specialized ML engines. In contrast to DB4ML, these engines not only need to transfer data out of the DBMS but also hardcode the ML algorithms and thus are not extensible.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {159–173},
numpages = {15},
keywords = {parallel and distributed dbmss, machine learning, execution engine},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1007/s11063-020-10314-8,
author = {Delecraz, Sebastien and Becerra-Bonache, Leonor and Favre, Benoit and Nasr, Alexis and Bechet, Frederic},
title = {Multimodal Machine Learning for Natural Language Processing: Disambiguating Prepositional Phrase Attachments with Images},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {5},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10314-8},
doi = {10.1007/s11063-020-10314-8},
abstract = {Although documents are increasingly multimodal, their automatic processing is often monomodal. In particular, natural language processing tasks are typically performed based on the textual modality only. This work extends the syntactic parsing task to the image modality in addition to text. In particular, we address the prepositional phrase attachment problem, a hard and semantic problem for syntactic parsers. Given an image and a caption, the proposed approach resolves syntactic attachment of prepositions in the parse tree according to both visual and lexical features. Visual features are derived from the nature and position of detected objects in the image that are aligned to textual phrases in the caption. A reranker uses this information to reorder syntactic trees produced by a shift-reduce syntactic parser. Trained on the Flickr-PP corpus which contains multimodal gold-standard attachments, this approach yields improvements over a text-only syntactic parser, in particular for the subset of prepositions that encode location, leading to an increase of up to 17 points of attachment accuracy.},
journal = {Neural Process. Lett.},
month = oct,
pages = {3095–3121},
numpages = {27},
keywords = {Prepositional phrase attachment resolution, Natural language processing, Deep neural networks, Multimodal machine learning}
}

@article{10.14778/3476249.3476284,
author = {Li, Side and Kumar, Arun},
title = {Towards an optimized GROUP by abstraction for large-scale machine learning},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476284},
doi = {10.14778/3476249.3476284},
abstract = {Many applications that use large-scale machine learning (ML) increasingly prefer different models for subgroups (e.g., countries) to improve accuracy, fairness, or other desiderata. We call this emerging popular practice learning over groups, analogizing to GROUP BY in SQL, albeit for ML training instead of SQL aggregates. From the systems standpoint, this practice compounds the already data-intensive workload of ML model selection (e.g., hyperparameter tuning). Often, thousands of models may need to be trained, necessitating high-throughput parallel execution. Alas, most ML systems today focus on training one model at a time or at best, parallelizing hyperparameter tuning. This status quo leads to resource wastage, low throughput, and high runtimes. In this work, we take the first step towards enabling and optimizing learning over groups from the data systems standpoint for three popular classes of ML: linear models, neural networks, and gradient-boosted decision trees. Analytically and empirically, we compare standard approaches to execute this workload today: task-parallelism and data-parallelism. We find neither is universally dominant. We put forth a novel hybrid approach we call grouped learning that avoids redundancy in communications and I/O using a novel form of parallel gradient descent we call Gradient Accumulation Parallelism (GAP). We prototype our ideas into a system we call Kingpin built on top of existing ML tools and the flexible massively-parallel runtime Ray. An extensive empirical evaluation on large ML benchmark datasets shows that Kingpin matches or is 4x to 14x faster than state-of-the-art ML systems, including Ray's native execution and PyTorch DDP.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2327–2340},
numpages = {14}
}

@article{10.1016/j.adhoc.2021.102657,
author = {Yuan, Guanghui and Wang, Hao and Khazaei, Ehsan and Khan, Baseem},
title = {Collaborative advanced machine learning techniques in optimal energy management of hybrid AC/DC IoT-based microgrids},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {122},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2021.102657},
doi = {10.1016/j.adhoc.2021.102657},
journal = {Ad Hoc Netw.},
month = nov,
numpages = {9},
keywords = {Microgrid, Optimization, One-class support vector, Forecasting, Advanced machine learning}
}

@article{10.4018/JGIM.292064,
author = {Dubey, Shantanu and Salwan, Prashant and Agarwal, Nitin Kumar},
title = {Application of Machine Learning Algorithm in Managing Deviant Consumer Behaviors and Enhancing Public Service.},
year = {2021},
issue_date = {Nov 2021},
publisher = {IGI Global},
address = {USA},
volume = {30},
number = {5},
issn = {1062-7375},
url = {https://doi.org/10.4018/JGIM.292064},
doi = {10.4018/JGIM.292064},
abstract = {Consumer-deviant behavior costs global utility firms USD 96 billion yearly, attributable to Non-Technical Losses (NTLs). NTLs affect the operations of power systems by overloading lines and transformers, resulting in voltage imbalances and, thereby, impacting services. They also impact the electricity price paid by the honest customers. Traditional meters constitute 98 % of the total electricity meters in India.  This paper argues that while traditional meters have their limitation in checking consumer-deviant behavior, this issue can be resolved with ML-based algorithms. These algorithms can predict suspected cases of theft with reasonable certainty, thereby enabling distribution companies to save money and provide consistent and dependable services to honest customers at reasonable costs. The key learning from this paper  is that even if data is noisy, it is possible to create a Machine Learning Model to detect NTL with 80 percentage plus accuracy.},
journal = {J. Glob. Inf. Manage.},
month = nov,
pages = {1–24},
numpages = {24},
keywords = {Traditional Meters, Power Utilities, Non-Technical Losses, Emerging Economies, Artificial Intelligence}
}

@inproceedings{10.5220/0007768101890197,
author = {Dahab, Sarah and Maag, Stephane},
title = {Suggesting Software Measurement Plans with Unsupervised Learning Data Analysis},
year = {2019},
isbn = {9789897583759},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007768101890197},
doi = {10.5220/0007768101890197},
abstract = {Software measurement processes require to consider more and more data, measures and metrics. Measurement plans become complex, time and resource consuming, considering diverse kinds of software project phases. Experts in charge of defining the measurement plans have to deal with management and performance constraints to select the relevant metrics. They need to take into account a huge number of data though distributed processes. Formal models and standards have been standardized to facilitate some of these aspects. However, the maintainability of the measurements activities is still constituted of complex activities. In this paper, we aim at improving our previous work, which aims at reducing the number of needed software metrics when executing measurement process and reducing the expertise charge. Based on unsupervised learning algorithm, our objective is to suggest software measurement plans at runtime and to apply them iteratively. For that purpose, we propose to generate automatically analysis models using unsupervised learning approach in order to efficiently manage the efforts, time and resources of the experts. An implementation has been done and integrated on an industrial platform. Experiments are processed to show the scalability and effectiveness of our approach. Discussions about the results have been provided. Furthermore, we demonstrate that the measurement process performance could be optimized while being effective, more accurate and faster with reduced expert intervention.},
booktitle = {Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {189–197},
numpages = {9},
keywords = {X-MEANS., Software Metrics, Software Measurement, SVM, Measurement Plan},
location = {Heraklion, Crete, Greece},
series = {ENASE 2019}
}

@article{10.1007/s00521-017-3305-0,
author = {Rao, Routhu Srinivasa and Pais, Alwyn Roshan},
title = {Detection of phishing websites using an efficient feature-based machine learning framework},
year = {2019},
issue_date = {Aug 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {8},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-3305-0},
doi = {10.1007/s00521-017-3305-0},
abstract = {Phishing is a cyber-attack which targets naive online users tricking into revealing sensitive information such as username, password, social security number or credit card number etc. Attackers fool the Internet users by masking webpage as a trustworthy or legitimate page to retrieve personal information. There are many anti-phishing solutions such as blacklist or whitelist, heuristic and visual similarity-based methods proposed to date, but online users are still getting trapped into revealing sensitive information in phishing websites. In this paper, we propose a novel classification model, based on heuristic features that are extracted from URL, source code, and third-party services to overcome the disadvantages of existing anti-phishing techniques. Our model has been evaluated using eight different machine learning algorithms and out of which, the Random Forest (RF) algorithm performed the best with an accuracy of 99.31%. The experiments were repeated with different (orthogonal and oblique) random forest classifiers to find the best classifier for the phishing website detection. Principal component analysis Random Forest (PCA-RF) performed the best out of all oblique Random Forests (oRFs) with an accuracy of 99.55%. We have also tested our model with the third-party-based features and without third-party-based features to determine the effectiveness of third-party services in the classification of suspicious websites. We also compared our results with the baseline models (CANTINA and CANTINA+). Our proposed technique outperformed these methods and also detected zero-day phishing attacks.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {3851–3873},
numpages = {23},
keywords = {Oblique Random Forest, Random Forest, Machine learning algorithms, Heuristic technique, Anti-phishing, Phishing, Cyber-attack}
}

@inproceedings{10.1145/3289402.3289549,
author = {Haddouchi, Maissae and Berrado, Abdelaziz},
title = {Assessing interpretation capacity in Machine Learning: A critical review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289549},
doi = {10.1145/3289402.3289549},
abstract = {Interpretability of Machine Learning (ML) methods and models is a fundamental issue that concerns a wide range of data mining research. This topic is not only an academic concern, but a crucial aspect for public acceptance of ML in practical contexts as well. Indeed, one should know that the lack of interpretability can be a real drawback for various application areas, such as in healthcare, biology, sociology and industrial decision support systems. In fact, an algorithm, which does not give enough information about the learner process and the learned model would be merely discarded in favor of less accurate and more interpretable approaches. Several papers have been proposed to interpret efficient models, such as Neural Networks and Random Forest, but there is still no consensus about what interpretability refers to. Interestingly, the term has been associated with different notions depending on the point of view of each author, as well as the nature of the issue being treated and the users concerned by the explanation. Therefore, this paper primarily aims to provide a painstaking overview of the aspects related to interpretability of ML learning process and resulting models, as reported by the literature, and to organize the aforementioned aspects into metrics that can be used for ML Interpretability scoring.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {49},
numpages = {6},
keywords = {scoring, measures, ML, Interpretability},
location = {Rabat, Morocco},
series = {SITA'18}
}

@article{10.1007/s11042-019-7637-x,
author = {Yuchi, Shu-yi and Xu, Shu},
title = {Research on cooperative classification of multimedia visual images based on deep machine learning model},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7637-x},
doi = {10.1007/s11042-019-7637-x},
abstract = {Aiming at the low accuracy of multimedia visual image cooperative classification, a new method of multimedia visual image cooperative classification based on depth machine learning model is proposed. Firstly, HSV color space model is selected to extract color features of multimedia visual images, Gabor function is used to extract texture features, and shape invariant moments are used to extract shape features. Then the features of multimedia visual images are recognized and classified, and the model parameters are optimized and adjusted according to the deviation of training output. The experimental results show that the average accuracy of this method is 95.892%, and the classification efficiency is high. The classification accuracy of this method is basically above 95%, and the classification accuracy is high. The training time of image type samples is 19&nbsp;s, the testing time of image type is 12&nbsp;s, and the time consumption is low.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {22657–22670},
numpages = {14},
keywords = {Collaborative classification, Visual image, Multimedia, Deep machine learning model}
}

@article{10.1016/j.jss.2007.12.797,
author = {Ajila, Samuel A. and Kaba, Ali B.},
title = {Evolution support mechanisms for software product line process},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {10},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.12.797},
doi = {10.1016/j.jss.2007.12.797},
abstract = {Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies - change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes - architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.},
journal = {J. Syst. Softw.},
month = oct,
pages = {1784–1801},
numpages = {18},
keywords = {Use case modeling, Transient process, Software product line process evolution, Software development process, Product line architecture, Meta-process, Feature-based object oriented model}
}

@inproceedings{10.1109/WAIN52551.2021.00028,
author = {Lewis, Grace A. and Bellomo, Stephany and Ozkaya, Ipek},
title = {Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00028},
doi = {10.1109/WAIN52551.2021.00028},
abstract = {Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {133–140},
numpages = {8},
location = {Madrid, Spain}
}

@inproceedings{10.1007/978-3-030-64580-9_11,
author = {Lorente-Leyva, Leandro L. and Alemany, M. M. E. and Peluffo-Ord\'{o}\~{n}ez, Diego H. and Herrera-Granda, Israel D.},
title = {A Comparison of Machine Learning and Classical Demand Forecasting Methods: A Case Study of Ecuadorian Textile Industry},
year = {2020},
isbn = {978-3-030-64579-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64580-9_11},
doi = {10.1007/978-3-030-64580-9_11},
abstract = {This document presents a comparison of demand forecasting methods, with the aim of improving demand forecasting and with it, the production planning system of Ecuadorian textile industry. These industries present problems in providing a reliable estimate of future demand due to recent changes in the Ecuadorian context. The impact on demand for textile products has been observed in variables such as sales prices and manufacturing costs, manufacturing gross domestic product and the unemployment rate. Being indicators that determine to a great extent, the quality and accuracy of the forecast, generating also, uncertainty scenarios. For this reason, the aim of this work is focused on the demand forecasting for textile products by comparing a set of classic methods such as ARIMA, STL Decomposition, Holt-Winters and machine learning, Artificial Neural Networks, Bayesian Networks, Random Forest, Support Vector Machine, taking into consideration all the above mentioned, as an essential input for the production planning and sales of the textile industries. And as a support, when developing strategies for demand management and medium-term decision making of this sector under study. Finally, the effectiveness of the methods is demonstrated by comparing them with different indicators that evaluate the forecast error, with the Multi-layer Neural Networks having the best results with the least error and the best performance.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II},
pages = {131–142},
numpages = {12},
keywords = {Forecast error, Classical methods, Machine learning, Textile industry, Demand forecasting methods},
location = {Siena, Italy}
}

@inproceedings{10.1145/3290607.3312877,
author = {Browne, Jacob T.},
title = {Wizard of Oz Prototyping for Machine Learning Experiences},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3312877},
doi = {10.1145/3290607.3312877},
abstract = {Machine learning is being adopted in a wide range of products and services. Despite its adoption, design and research processes for machine learning experiences have yet to be cemented in the user experience community. Prototyping machine learning experiences is noted to be particularly challenging. This paper suggests Wizard of Oz prototyping to help designers incorporate human-centered design processes into the development of machine learning experiences. This paper also surfaces a set of topics to consider in evaluating Wizard of Oz machine learning prototypes.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {user interface, user experience, prototyping, machine learning, design, Wizard of Oz},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{10.5555/3172077.3172210,
author = {Liu, Jia and Gong, Maoguo and Miao, Qiguang},
title = {Modeling hebb learning rule for unsupervised learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {This paper presents to model the Hebb learning rule and proposes a neuron learning machine (NLM). Hebb learning rule describes the plasticity of the connection between presynaptic and postsynaptic neurons and it is unsupervised itself. It formulates the updating gradient of the connecting weight in artificial neural networks. In this paper, we construct an objective function via modeling the Hebb rule. We make a hypothesis to simplify the model and introduce a correlation based constraint according to the hypothesis and stability of solutions. By analysis from the perspectives of maintaining abstract information and increasing the energy based probability of observed data, we find that this biologically inspired model has the capability of learning useful features. NLM can also be stacked to learn hierarchical features and reformulated into convolutional version to extract features from 2-dimensional data. Experiments on single-layer and deep networks demonstrate the effectiveness of NLM in unsupervised feature learning.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2315–2321},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.3233/JIFS-179960,
author = {Li, Pengpeng and Jiang, Shuai and Patnaik, Srikanta},
title = {Analysis of the characteristics of English part of speech based on unsupervised machine learning and image recognition model},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179960},
doi = {10.3233/JIFS-179960},
abstract = {If there are more external interference factors in the process of intelligent recognition in English, the recognition accuracy will be greatly reduced. It is of great academic value and application significance to deeply study feature recognition of English part-of-speech and realize automatic image processing of English recognition. Based on unsupervised machine learning and image recognition technology, this study combines the actual factors of English recognition to set the corresponding influencing factors and proposes a reliable method to identify multi-body rotating characters. This method utilizes the principle of the periodic characteristics of the trajectory rotation on the feature space. Moreover, this study conducts a comparative analysis of recognition accuracy by comparative experiments. In addition, this paper analyzes the recognition principles of 4 fonts in detail. The research results show that the proposed method has certain effects and can provide theoretical reference for subsequent related research.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1891–1901},
numpages = {11},
keywords = {characteristic analysis, English recognition, feature recognition, image recognition, Unsupervised learning}
}

@article{10.1155/2021/8868355,
author = {Ali, Elmustafa Sayed and Hasan, Mohammad Kamrul and Hassan, Rosilah and Saeed, Rashid A. and Hassan, Mona Bakri and Islam, Shayla and Nafi, Nazmus Shaker and Bevinakoppa, Savitri and Ahmed, Fawad},
title = {Machine Learning Technologies for Secure Vehicular Communication in Internet of Vehicles: Recent Advances and Applications},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/8868355},
doi = {10.1155/2021/8868355},
abstract = {Recently, interest in Internet of Vehicles’ (IoV) technologies has significantly emerged due to the substantial development in the smart automobile industries. Internet of Vehicles’ technology enables vehicles to communicate with public networks and interact with the surrounding environment. It also allows vehicles to exchange and collect information about other vehicles and roads. IoV is introduced to enhance road users’ experience by reducing road congestion, improving traffic management, and ensuring the road safety. The promised applications of smart vehicles and IoV systems face many challenges, such as big data collection in IoV and distribution to attractive vehicles and humans. Another challenge is achieving fast and efficient communication between many different vehicles and smart devices called Vehicle-to-Everything (V2X). One of the vital questions that the researchers need to address is how to effectively handle the privacy of large groups of data and vehicles in IoV systems. Artificial Intelligence technology offers many smart solutions that may help IoV networks address all these questions and issues. Machine learning (ML) is one of the highest efficient AI tools that have been extensively used to resolve all mentioned problematic issues. For example, ML can be used to avoid road accidents by analyzing the driving behavior and environment by sensing data of the surrounding environment. Machine learning mechanisms are characterized by the time change and are critical to channel modeling in-vehicle network scenarios. This paper aims to provide theoretical foundations for machine learning and the leading models and algorithms to resolve IoV applications’ challenges. This paper has conducted a critical review with analytical modeling for offloading mobile edge-computing decisions based on machine learning and Deep Reinforcement Learning (DRL) approaches for the Internet of Vehicles (IoV). The paper has assumed a Secure IoV edge-computing offloading model with various data processing and traffic flow. The proposed analytical model considers the Markov decision process (MDP) and ML in offloading the decision process of different task flows of the IoV network control cycle. In the paper, we focused on buffer and energy aware in ML-enabled Quality of Experience (QoE) optimization, where many recent related research and methods were analyzed, compared, and discussed. The IoV edge computing and fog-based identity authentication and security mechanism were presented as well. Finally, future directions and potential solutions for secure ML IoV and V2X were highlighted.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {23}
}

@article{10.1016/j.comnet.2019.06.015,
author = {Bahtiyar, \c{S}erif and Yaman, Mehmet Bar\i{}\c{s} and Alt\i{}ni\u{g}ne, Can Y\i{}lmaz},
title = {A multi-dimensional machine learning approach to predict advanced malware},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {160},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.06.015},
doi = {10.1016/j.comnet.2019.06.015},
journal = {Comput. Netw.},
month = sep,
pages = {118–129},
numpages = {12},
keywords = {Classification, Prediction, API Call, Machine learning, Advanced malware}
}

@inproceedings{10.1145/3442381.3450092,
author = {Gu, Ziwei and Yan, Jing Nathan and Rzeszotarski, Jeffrey M.},
title = {Understanding User Sensemaking in Machine Learning Fairness Assessment Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450092},
doi = {10.1145/3442381.3450092},
abstract = {A variety of systems have been proposed to assist users in detecting machine learning (ML) fairness issues. These systems approach bias reduction from a number of perspectives, including recommender systems, exploratory tools, and dashboards. In this paper, we seek to inform the design of these systems by examining how individuals make sense of fairness issues as they use different de-biasing affordances. In particular, we consider the tension between de-biasing recommendations which are quick but may lack nuance and ”what-if” style exploration which is time consuming but may lead to deeper understanding and transferable insights. Using logs, think-aloud data, and semi-structured interviews we find that exploratory systems promote a rich pattern of hypothesis generation and testing, while recommendations deliver quick answers which satisfy participants at the cost of reduced information exposure. We highlight design requirements and trade-offs in the design of ML fairness systems to promote accurate and explainable assessments.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {658–668},
numpages = {11},
keywords = {User Sensemaking, Interactive Interfaces, Algorithmic Bias},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1007/s10559-020-00240-x,
author = {Norkin, V. I.},
title = {Generalized Gradients in Dynamic Optimization, Optimal Control, and Machine Learning Problems*},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {56},
number = {2},
issn = {1060-0396},
url = {https://doi.org/10.1007/s10559-020-00240-x},
doi = {10.1007/s10559-020-00240-x},
abstract = {Problems of nonsmooth nonconvex dynamic optimization, optimal control (in discrete time), including feedback control, and machine learning are considered from a common point of view. An analogy between controlling discrete dynamical systems and multilayer neural network learning problems with nonsmooth objective functionals and connections is traced. Methods for computing generalized gradients for such systems based on the Hamilton–Pontryagin functions are developed. Gradient (stochastic) algorithms for optimal control and learning are extended to nonconvex nonsmooth dynamic systems.},
journal = {Cybernetics and Sys. Anal.},
month = mar,
pages = {243–258},
numpages = {16},
keywords = {stochastic smoothing, stochastic generalized gradient, stochastic optimization, nonsmooth noncovex optimization, deep learning, multilayer neural networks, machine learning, optimal control, dynamic optimization}
}

@inproceedings{10.1007/978-3-030-74432-8_2,
author = {Bonizzoni, Paola and De Felice, Clelia and Petescia, Alessia and Pirola, Yuri and Rizzi, Raffaella and Stoye, Jens and Zaccagnino, Rocco and Zizza, Rosalba},
title = {Can We Replace Reads by Numeric Signatures? Lyndon Fingerprints as Representations of Sequencing Reads for Machine Learning},
year = {2021},
isbn = {978-3-030-74431-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74432-8_2},
doi = {10.1007/978-3-030-74432-8_2},
abstract = {Representations of biological sequences facilitating sequence comparison are crucial in several bioinformatics tasks. Recently, the Lyndon factorization has been proved to preserve common factors in overlapping reads&nbsp;[6], thus leading to the idea of using factorizations of sequences to define measures of similarity between reads. In this paper we propose as a signature of sequencing reads the notion of fingerprint, i.e., the sequence of lengths of consecutive factors in Lyndon-based factorizations of the reads. Surprisingly, fingerprints of reads are effective in preserving sequence similarities while providing a compact representation of the read, and so, k-mers extracted from a fingerprint, called k-fingers, can be used to capture sequence similarity between reads.We first provide a probabilistic framework to estimate the behaviour of fingerprints. Then we experimentally evaluate the effectiveness of this representation for machine learning algorithms for classifying biological sequences. In particular, we considered the problem of assigning RNA-Seq reads to the most likely gene from which they were generated. Our results show that fingerprints can provide an effective machine learning interpretable representation, successfully preserving sequence similarity.},
booktitle = {Algorithms for Computational Biology: 8th International Conference, AlCoB 2021, Missoula, MT, USA, June 7–11, 2021, Proceedings},
pages = {16–28},
numpages = {13},
keywords = {Sequence mining, Machine learning, Read representation, Lyndon factorization, Sequence analysis},
location = {Missoula, MT, USA}
}

@inproceedings{10.1145/3478905.3478982,
author = {Cheng, Fangyuan and Jia, Junmin},
title = {Quantitative interactive investment algorithm based on machine learning and data mining},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478982},
doi = {10.1145/3478905.3478982},
abstract = {Quantitative investment is a mean to predict the development of securities and conduct transactions by using computer algorithms, but it usually ignores the guiding role of investors' personal preferences in deciding investment plans. Thus, we suggest a quantitative interactive investment algorithm to integrate the rational goals of investment with the individual preference indicators of decision-making investors. The interactive multi-objective solution algorithm creatively combines the decision tree algorithm which mine the investor's individual preference index from the investor's decision data with the second-generation non-dominated sorting genetic algorithm (NSGA-II). This method solves the problem of the lack of personalized choices in current quantitative investment methods by adding decision-makers’ preference indicators.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {396–401},
numpages = {6},
keywords = {multi-objective optimization, machine learning, decision maker preference, Quantitative investment},
location = {Shanghai, China},
series = {DSIT 2021}
}

@article{10.1155/2020/9302318,
author = {Bijalwan, Anchit and Li, Huaizhi},
title = {Botnet Forensic Analysis Using Machine Learning},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/9302318},
doi = {10.1155/2020/9302318},
abstract = {Botnet forensic analysis helps in understanding the nature of attacks and the modus operandi used by the attackers. Botnet attacks are difficult to trace because of their rapid pace, epidemic nature, and smaller size. Machine learning works as a panacea for botnet attack related issues. It not only facilitates detection but also helps in prevention from bot attack. The proposed inquisition model endeavors improved quality of results by comprehensive botnet detection and forensic analysis. This scenario has been applied in eight different combinations of ensemble classifier technique to detect botnet evidence. The study is also compared to the ensemble-based classifiers with the single classifier using different parameters. The results exhibit that the proposed model can improve accuracy over a single classifier.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {9}
}

@article{10.1007/s11276-019-02042-2,
author = {Li, Mingwei and Zhang, Jilin and Wan, Jian and Ren, Yongjian and Zhou, Li and Wu, Baofu and Yang, Rui and Wang, Jue},
title = {Distributed machine learning load balancing strategy in cloud computing services},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-019-02042-2},
doi = {10.1007/s11276-019-02042-2},
abstract = {Mobile service computing is a new cloud computing model that provides various cloud services for mobile intelligent terminal users through mobile internet access. The quality of service is an essential problem faced by mobile service computing. In this paper, we demonstrate a series of research studies on how to accelerate the training of a distributed machine learning (ML) model based on cloud service. Distributed ML has become the mainstream way of today’s ML models training. In traditional distributed ML based on bulk synchronous parallel, the temporary slowdown of any node in the cluster will delay the calculation of other nodes because of the frequent occurrence of synchronous barriers, resulting in overall performance degradation. Our paper proposes a load balancing strategy named adaptive fast reassignment (AdaptFR). Based on this, we built a distributed parallel computing model called adaptive-dynamic synchronous parallel (A-DSP). A-DSP uses a more relaxed synchronization model to reduce the performance consumption caused by synchronous operations while ensuring the consistency of the model. At the same time, A-DSP also implements the AdaptFR load balancing strategy, which addresses the straggler problem caused by the performance difference between nodes under the premise of ensuring the accuracy of the model. The experiments show that A-DSP can effectively improve the training speed while ensuring the accuracy of the model in the distributed ML model training.},
journal = {Wirel. Netw.},
month = nov,
pages = {5517–5533},
numpages = {17},
keywords = {Adaptive fast reassignment, Load balancing, Distributed machine learning, Cloud service, Mobile service computing}
}

@article{10.1007/s42979-021-00904-1,
author = {Mian Qaisar, Saeed and Alyamani, Nehal and Waqar, Asad and Krichen, Moez},
title = {Machine Learning with Adaptive Rate Processing for Power Quality Disturbances Identification},
year = {2021},
issue_date = {Jan 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {1},
url = {https://doi.org/10.1007/s42979-021-00904-1},
doi = {10.1007/s42979-021-00904-1},
abstract = {Power quality disturbances (PQDs) commonly occur in large-scale power systems and networks that rise critical issues. Therefore, an automated recognition and mitigation of PQDs is necessary. In this scenario, an efficient adaptive rate technique is proposed for viable features mining and recognition of PQDs. Event-driven A/D converters (EDADCs) are used to acquire PQD signals. To accurately segment the sampled signal, an appealing approach is used. A time-domain evaluation is carried out in the next phase to investigate the characteristics of these fragments. The mature machine learning algorithms are employed to carry out the classification. Compared to conventional counterparts, the findings indicate a decrease of 13.26 times in collected information. An average maximum identification precision of 99.33% is achieved by the proposed method. Compared to predecessors, this confirms the considerable performance of the processing and power usage of the engineered solution while achieving high recognition accuracy.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {6},
keywords = {Classification, Compression ratio, Power quality disturbances, Time-domain features extraction, Adaptive rate sampling}
}

@inproceedings{10.1109/SEAMS.2019.00015,
author = {Jamshidi, Pooyan and C\'{a}mara, Javier and Schmerl, Bradley and K\"{a}stner, Christian and Garlan, David},
title = {Machine learning meets quantitative planning: enabling self-adaptation in autonomous robots},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00015},
doi = {10.1109/SEAMS.2019.00015},
abstract = {Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {39–50},
numpages = {12},
keywords = {self-adaptive systems, robotics systems, quantitative planning, machine learning, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@phdthesis{10.5555/AAI28093137,
author = {Nava, Michael J. and Boettiger, Carl and Walker, Joan},
advisor = {W, Rector, James},
title = {Evaluation and Application of Machine Learning Techniques to Data Conditioning Problems in Microseismic Data},
year = {2020},
isbn = {9798515247379},
publisher = {University of California, Berkeley},
abstract = {Hydraulic fracturing has evolved dramatically over the past decades. A number of new techniques have emerged in order to maximize  production from organic-rich shale. For example, multistage fracturing, dynamically varying pumping parameters, horizontal drilling and finely-tuned perforation shots have all led to incremental improvements in the industry. With these engineering advancements, so too has the ability to monitor microseismic fractures expanded. An added benefit, or potentially an unintended consequence, of this new era of high frequency, high precision acoustic monitoring equipment is the generation of large scale digital data. With any real data set, there will inevitably be data conditioning problems that exist. Whether missing values, corrupt data, or poor experimental design and execution, there will be some constraint or obstacle that inhibits the cultivation of knowledge and insights.The objective of this dissertation is to identify and understand where those limitations exist, to understand the genesis of those constraints - whether they arise from some physical limitation or from common data recording issues - and then apply an interdisciplinary approach to overcome those limitations. To this end, we identify limitations caused by a typical, cost-effective microseismic monitoring geometry and pivot to understand and characterize microseismic events through spectral analysis. We build features that provide insight into the nature of microseismicity present in the data, which would otherwise elude us. Next, we incorporate information that is typically lost in the presence of high amplitude resonance and leverage this newly found data to identify specific microseismic attributes to make marked improvements on event location estimates. Through the inclusion of head waves and the use of inversion techniques, we reduce the uncertainty of microseismic event locations significantly. This is a fundamental step toward understanding the behavior of hydraulic fractures far beneath the surface of the earth.Next, we turn to data science to continue to overcome data quality issues present in the data from a hydraulic fracturing project in the Marcellus shale. Specifically, machine learning and deep learning methodologies are applied to the data in order to recover meaningful information. The benefits of this are twofold. First, this work provides a data-driven approach to imputation through various learning methods. Second, it provides an understanding of the limitations and computational time required for various learning methods. This information will aid in the decision making of engineers who desire a more accurate solution or an accurate solution that can be used in real-time analysis.Finally, we culminate the dissertation with an exploration into the ability to leverage ensemble learning methods to overcome poorly conditioned data sets with the objective of improving automated analysis steps. Specifically, we create an extensible computational paradigm that enables the automatic picking of waveform first arrivals. This is typically an arduous, time-consuming analysis step that suffers from inconsistent picks based on subjective assessment. Moving away from a human-in-the-loop system enables more transparency and reproducibility. Additionally, the total time for end-to-end analysis of first arrivals is dramatically decreased. Given the extensibility of this framework, expanding the use of the system to include full waveform classification is an appropriate next step.},
note = {AAI28093137}
}

@phdthesis{10.5555/AAI28031740,
author = {Tarzanagh, Davoud Ataee and Yunmei, Chen, and M, Pardalos, Panos},
advisor = {Ward, Hager, William and George, Michailidis,},
title = {Numerical Optimization Methods and Theory for Large-Scale Machine Learning Problems},
year = {2020},
isbn = {9798357533906},
publisher = {University of Florida},
address = {USA},
abstract = {The focus of this thesis is to develop numerical optimization methods for solving large-scale machine learning problems. Through case studies on graphical models estimation, tensor factorization, and training of deep neural networks, we discuss how optimization problems arise in modern statistical and machine learning applications and what makes them challenging. In particular, we consider the fundamental questions that arise when trading between multiple criteria such as computation, communication, and storage while maintaining statistical performance. Can we provide theoretical results that show there must be tradeoffs? Can we develop new optimization procedures that are both theoretically optimal and practically relevant?To answer these questions, we provide convergence rate, regret bounds, and fundamental error bounds on the performance of optimization algorithms subject to the constraints specified. These theoretical results allow us to guarantee the optimality of the proposed algorithms and show some of the practical benefits that a focus on multiple optimality criteria brings.More specifically, the central contributions of the thesis are the following:We develop several deterministic and stochastic optimization methods, applicable to general classes of constrained optimization problems, including methods that are automatically adaptive to the structure of the underlying problem, and parallelize naturally to attain linear speedup in the number of processors available.We provide convergence rates, regret bounds and relative error bounds demonstrating the optimality of these methods.We develop new optimization algorithms for tensor factorization, graphical models estimation and training deep neural networks, and provide convergence guarantees for each of these algorithms.},
note = {AAI28031740}
}

@inproceedings{10.1145/3392063.3394438,
author = {Zimmermann-Niefield, Abigail and Polson, Shawn and Moreno, Celeste and Shapiro, R. Benjamin},
title = {Youth making machine learning models for gesture-controlled interactive media},
year = {2020},
isbn = {9781450379816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392063.3394438},
doi = {10.1145/3392063.3394438},
abstract = {Machine learning (ML) technologies are ubiquitous and increasingly influential in daily life. They are powerful tools people can use to build creative, personalized systems in a wide variety of contexts. We believe ML has vast potential for young people to use to make creative projects, especially when used in conjunction with programming. This potential is understudied. We know little about what projects youth might create, or what computational practices they could engage in while building them. We combined a beginner-level ML modeling toolkit with a beginning programming tool and then investigated how young people created and remixed projects to incorporate custom ML-based gestural inputs. We found that (1) participants were able to build and integrate ML models of their own gestures into programming projects; (2) the design of their gestures ranged from coherent to disjoint with respect to the narratives, characters, and actions of their interactive worlds; and (3) they tested their projects by assessing the programmed vs. modeled aspects of them as distinct units. We conclude with a discussion of how we might support youth in combining code and ML modeling going forward.},
booktitle = {Proceedings of the Interaction Design and Children Conference},
pages = {63–74},
numpages = {12},
keywords = {scratch, modeling, mobile, machine learning, education, computer science education, K12},
location = {London, United Kingdom},
series = {IDC '20}
}

@inproceedings{10.1145/3269206.3269241,
author = {Zhu, Jiajing and Liu, Yongguo and Yang, Shangming and Zhai, Shuangqing and Zhang, Yi and Wen, Chuanbiao},
title = {A Supervised Learning Framework for Prediction of Incompatible Herb Pair in Traditional Chinese Medicine},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269241},
doi = {10.1145/3269206.3269241},
abstract = {Adverse drug-drug interaction has been a critical issue for the development of drugs. In Traditional Chinese Medicine, adverse herb-herb interaction is a negative reaction in patients after the absorption of decoction of Incompatible Herb Pair (IHP). Recently, many methods have been proposed for IHP research, but most of them focused on revealing and analyzing the adverse reaction of some known IHPs, despite that a number of new IHPs have been discovered by accidents. Up to now, IHPs have been a serious threat to public health in the TCM medication. In this paper, we propose a novel supervised learning framework for potential IHP prediction. In this framework, we model the prediction task as a non-negative matrix tri-factorization problem, in which two important herb attributes (efficacy and flavor) and their correlation are incorporated to characterize the incompatible relationship among herbs. A hypothetical test method is adopted to evaluate the statistical significance of dissimilar characteristics of two attributes and the results are used as a regularization term to improve the accuracy of IHP prediction. Experiments on the real-world IHP dataset demonstrate that the proposed framework is very effective for prediction of potential IHPs.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1799–1802},
numpages = {4},
keywords = {traditional chinese medicine, supervised learning framework, non-negative matrix tri-factorization, incompatible herb pair prediction},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1007/978-3-030-61527-7_2,
author = {Briggs, Emma and Hollm\'{e}n, Jaakko},
title = {Mitigating Discrimination in Clinical Machine Learning Decision Support Using Algorithmic Processing Techniques},
year = {2020},
isbn = {978-3-030-61526-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61527-7_2},
doi = {10.1007/978-3-030-61527-7_2},
abstract = {Discrimination on the basis of protected characteristics - such as race or gender - within Machine Learning (ML) is an insufficiently addressed yet pertinent issue. This line of investigation is particularly lacking within clinical decision-making, for which the consequences can be life-altering. Certain real-world clinical ML decision tools are known to demonstrate significant levels of discrimination. There is currently indication that fairness can be improved during algorithmic processing, but this has not been widely examined for the clinical setting. This paper therefore explores the extent to which novel algorithmic processing techniques may be able to mitigate discrimination against protected groups in clinical resource-allocation ML decision-support algorithms. Specifically, three state-of-the-art discrimination mitigation techniques are compared, one for each stage of algorithmic processing, when applied to a real-world clinical ML decision algorithm which is known to discriminate with regards to racial characteristics. The results are promising, revealing that such techniques could significantly improve the fairness of clinical resource-allocation ML decision tools, particularly during pre- and post- processing. Discrimination is shown to be reduced to arbitrary levels at little to no cost to accuracy. Similar studies are needed to consolidate these results. Other future recommendations include working towards a generalisable framework for ML fairness in healthcare.},
booktitle = {Discovery Science: 23rd International Conference, DS 2020, Thessaloniki, Greece, October 19–21, 2020, Proceedings},
pages = {19–33},
numpages = {15},
keywords = {Fairness, Machine Learning, Clinical decision support, Resource-allocation},
location = {Thessaloniki, Greece}
}

@inproceedings{10.1145/3307650.3322267,
author = {Tarsa, Stephen J. and Chowdhury, Rangeen Basu Roy and Sebot, Julien and Chinya, Gautham and Gaur, Jayesh and Sankaranarayanan, Karthik and Lin, Chit-Kwan and Chappell, Robert and Singhal, Ronak and Wang, Hong},
title = {Post-silicon CPU adaptation made practical using machine learning},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322267},
doi = {10.1145/3307650.3322267},
abstract = {Processors that adapt architecture to workloads at runtime promise compelling performance per watt (PPW) gains, offering one way to mitigate diminishing returns from pipeline scaling. State-of-the-art adaptive CPUs deploy machine learning (ML) models on-chip to optimize hardware by recognizing workload patterns in event counter data. However, despite breakthrough PPW gains, such designs are not yet widely adopted due to the potential for systematic adaptation errors in the field.This paper presents an adaptive CPU based on Intel SkyLake that (1) closes the loop to deployment, and (2) provides a novel mechanism for post-silicon customization. Our CPU performs predictive cluster gating, dynamically setting the issue width of a clustered architecture while clock-gating unused resources. Gating decisions are driven by ML adaptation models that execute on an existing microcontroller, minimizing design complexity and allowing performance characteristics to be adjusted with the ease of a firmware update. Crucially, we show that although adaptation models can suffer from statistical blindspots that risk degrading performance on new workloads, these can be reduced to minimal impact with careful design and training.Our adaptive CPU improves PPW by 31.4% over a comparable non-adaptive CPU on SPEC2017, and exhibits two orders of magnitude fewer Service Level Agreement (SLA) violations than the state-of-the-art. We show how to optimize PPW using models trained to different SLAs or to specific applications, e.g. to improve datacenter hardware in situ. The resulting CPU meets real world deployment criteria for the first time and provides a new means to tailor hardware to individual customers, even as their needs change.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {adaptive hardware, clustered architectures, machine learning, runtime optimization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3448016.3452788,
author = {Phani, Arnab and Rath, Benjamin and Boehm, Matthias},
title = {LIMA: Fine-grained Lineage Tracing and Reuse in Machine Learning Systems},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452788},
doi = {10.1145/3448016.3452788},
abstract = {Machine learning (ML) and data science workflows are inherently exploratory. Data scientists pose hypotheses, integrate the necessary data, and run ML pipelines of data cleaning, feature engineering, model selection and hyper-parameter tuning. The repetitive nature of these workflows, and their hierarchical composition from building blocks exhibits high computational redundancy. Existing work addresses this redundancy with coarse-grained lineage tracing and reuse for ML pipelines. This approach allows using existing ML systems, but views entire algorithms as black boxes, and thus, fails to eliminate fine-grained redundancy and to handle internal non-determinism. In this paper, we introduce LIMA, a practical framework for efficient, fine-grained lineage tracing and reuse inside ML systems. Lineage tracing of individual operations creates new challenges and opportunities. We address the large size of lineage traces with multi-level lineage tracing and reuse, as well as lineage deduplication for loops and functions; exploit full and partial reuse opportunities across the program hierarchy; and integrate this framework with task parallelism and operator fusion. The resulting framework performs fine-grained lineage tracing with low overhead, provides versioning and reproducibility, and is able to eliminate fine-grained redundancy. Our experiments on a variety of ML pipelines show performance improvements up to 12.4x.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1426–1439},
numpages = {14},
keywords = {reuse of intermediates, ml systems, lineage-based reuse, lineage tracing},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1145/3450288,
author = {Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young and Zhu, Liming},
title = {A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450288},
doi = {10.1145/3450288},
abstract = {Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {95},
numpages = {39},
keywords = {systematic literature review, software engineering, privacy, edge learning, distributed learning, Federated learning}
}

@article{10.1162/evco_a_00215,
author = {Kerschke, Pascal and Kotthoff, Lars and Bossek, Jakob and Hoos, Holger H. and Trautmann, Heike},
title = {Leveraging TSP Solver Complementarity through Machine Learning},
year = {2018},
issue_date = {Winter 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {26},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00215},
doi = {10.1162/evco_a_00215},
abstract = {The Travelling Salesperson Problem (TSP) is one of the best-studied NP-hard problems. Over the years, many different solution approaches and solvers have been developed. For the first time, we directly compare five state-of-the-art inexact solvers—namely, LKH, EAX, restart variants of those, and MAOS—on a large set of well-known benchmark instances and demonstrate complementary performance, in that different instances may be solved most effectively by different algorithms. We leverage this complementarity to build an algorithm selector, which selects the best TSP solver on a per-instance basis and thus achieves significantly improved performance compared to the single best solver, representing an advance in the state of the art in solving the Euclidean TSP. Our in-depth analysis of the selectors provides insight into what drives this performance improvement.},
journal = {Evol. Comput.},
month = dec,
pages = {597–620},
numpages = {24},
keywords = {machine learning., performance modeling, automated algorithm selection, Travelling Salesperson Problem}
}

@phdthesis{10.5555/AAI28723232,
author = {Wan, Zhong Yi and Hadjiconstantinou, Nicolas and Marzouk, Youssef M},
advisor = {Sapsis, Themistoklis P},
title = {Physics-constrained Machine Learning Strategies for Turbulent Flows and Bubble Dynamics},
year = {2020},
publisher = {Massachusetts Institute of Technology},
address = {USA},
abstract = {Machine learning (ML) has in recent years become a sizzling trend in almost every science and engineering discipline. It enables scientists and engineers to make decisions or draw conclusions directly using information extracted from data, bypassing the necessity to unravel the delicate inner workings of the underlying phenomena. This, however, comes at the expense of having to search through an immense space of potential architectures and parameters for an optimized model that, not only provides the best description to the available data, but also applies to unseen cases. To cope with such difficulties, it is imperative that ample constraints are imposed on the architecture and parameter space, in order to facilitate efficient and generalizable learning. For physical systems, first-principle knowledge makes up a natural set of constraints that should be integrated into the ML system. Past research efforts have mainly emphasized utilizing physical knowledge for cases where the system states are perfectly defined. For reduced-order settings, this condition is not satisfied and consequently, the existing physical knowledge is often incomplete and/or being compromised in accuracy. As a result, it remains a challenge to effectively leverage such knowledge in the design and implementation of ML frameworks. The objective of this thesis is to address the existing gaps and present physics-constrained ML strategies which work specifically in reduced-order spaces. The first part of this thesis focuses on using ML to improve imperfect reduced-order models, obtained from two common methods: (i) orthogonal subspace projection and (ii) slow-manifold reduction. The first case is particularly important for the modeling of rare extreme events such as those found in geophysical sciences. In these problems, there is typically little data available to train a data-driven model, while reduced-order models of the full equations also fail to capture the relevant dynamics. We present a modeling strategy that allows the ML and physical model to complement each other. Specifically, physics-based equations are projected to a subspace which contains critical dynamical components associated with rare events and then combined with a data-driven model. In this way, the projected equations assist in modeling these events, which appear less frequently in data streams. On the other hand, observations are often plentiful in other regions of the state space, allowing ML to capture dynamics unaccounted by the projected equations. The effectiveness of this strategy is demonstrated through the prediction and modeling of extreme dissipation events in turbulent fluid flows. Next we present a strategy for improving slow-manifold reduced-order models, suitable for systems with separated time scales. Our strategy employs ML to model the 'fast variables' of the system in terms of the 'slow variables', which can then be integrated with the equation-based dynamics of the latter to provide a complete description of the system evolution. In this way, we constrain ML to a specific part of the dynamics which cannot be easily derived or expressed analytically. We demonstrate the strategy through the modeling of finite-size (inertial) particle dynamics in generic fluid flows, a problem of critical importance for modeling bubbles in multi-phase flows, aerosols in the atmosphere, and ocean drifters. We first utilize training data obtained from the classical Maxey-Riley equation of motion and secondly via high-fidelity multi-phase direct numerical simulations. In both cases, we show that the kinematics, i.e. the relationship between position (slow variable) and velocity (fast variable), can be effectively learned from limited trajectories and directly utilized to model interactions with complex, turbulent flows. We carefully study transferability of the ML models into different fluid flows and different parameters. To deal the problem of limited data the ML approach is complemented with a data-augmentation technique that enforces physical symmetries of the problem such as iso-tropicity of the particle dynamics. In the second part of the thesis, we consider a complementary problem related to the parameterization of the unmodeled variables of a reduced-order model with respect to the modeled/dynamically-resolved reduced-order states. This problem is particularly meaningful when explicit dynamical modeling of the target variables is prohibitive. We present a formulation of this problem in the context of atmospheric modeling, where the spatially small-scaled features are much harder to measure/model than the corresponding large scales due to the high intrinsic dimensionality and the lack of predictability resulting from instabilities. To address these issues, we introduce the Stochastic Machine-Learning (SMaL) parameterization framework that decomposes the times-series for the small scales into a deterministic (predictable) and a stochastic (unpredictable) component. The deterministic component is directly captured with ML in terms of the large scale time-series. On the other hand, the local-in-time statistics of the stochastic components are estimated and learned with separate models. We then construct a non-stationary Gaussian process which enables efficiently drawing an ensemble of small-scaled trajectories that are consistent with the large scales. The SMaL framework is illustrated on a realistic application, where the small-scaled vorticity fields are parameterized in terms of the large-scaled vorticity and temperature data from reanalysis over Europe. We show that the small-scaled random samples exhibit realistic characteristics in terms of the spatial spectrum, single-point probability density functions, and temporal spectral content. Copies available exclusively from MIT Libraries, libraries.mit.edu/docs |docs mit.edu.},
note = {AAI28723232}
}

@inproceedings{10.1145/3471985.3472377,
author = {Park, Hayeon and Diane Cuebas, Rut and We, Kyoung-Soo and Hoon Kim, Sung and Lee, Chang-Gun},
title = {Oil Leakage Detection in Automobile Shock Absorber using Machine Learning Classifiers},
year = {2021},
isbn = {9781450387484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471985.3472377},
doi = {10.1145/3471985.3472377},
abstract = {In this paper, we describe a lightweight and accurate fault diagnosis method that detects oil leakage in automobile shock absorbers. Our approach includes a machine learning classifier as its base. These classifiers are quick and maintain a low computational load that are suitable for the automotive environment where only low-performance Electronic Control Units (ECUs) are available. However, these classifiers do not produce sufficiently accurate results when raw sensor data is used as the input. To solve this issue, we have developed an approach that firstly includes sensor data selection, where only sensors that have a strong impact on accuracy are used as input. And secondly, this reduced input dataset undergoes preprocessing using Fast Fourier Transform (FFT) to further improve accuracy. Thus, our methodology produces an oil leakage detection methodology for automobile shock absorbers that addresses the limitations of fault detection in an automotive system by being both lightweight and accurate.},
booktitle = {2021 the 5th International Conference on Robotics, Control and Automation},
pages = {71–77},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {ICRCA 2021}
}

@article{10.1016/j.compbiomed.2015.10.013,
author = {Nath, Abhigyan and Subbiah, Karthikeyan},
title = {Unsupervised learning assisted robust prediction of bioluminescent proteins},
year = {2016},
issue_date = {January 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {68},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2015.10.013},
doi = {10.1016/j.compbiomed.2015.10.013},
abstract = {Bioluminescence plays an important role in nature, for example, it is used for intracellular chemical signalling in bacteria. It is also used as a useful reagent for various analytical research methods ranging from cellular imaging to gene expression analysis. However, identification and annotation of bioluminescent proteins is a difficult task as they share poor sequence similarities among them. In this paper, we present a novel approach for within-class and between-class balancing as well as diversifying of a training dataset by effectively combining unsupervised K-Means algorithm with Synthetic Minority Oversampling Technique (SMOTE) in order to achieve the true performance of the prediction model. Further, we experimented by varying different levels of balancing ratio of positive data to negative data in the training dataset in order to probe for an optimal class distribution which produces the best prediction accuracy. The appropriately balanced and diversified training set resulted in near complete learning with greater generalization on the blind test datasets. The obtained results strongly justify the fact that optimal class distribution with a high degree of diversity is an essential factor to achieve near perfect learning. Using random forest as the weak learners in boosting and training it on the optimally balanced and diversified training dataset, we achieved an overall accuracy of 95.3% on a tenfold cross validation test, and an accuracy of 91.7%, sensitivity of 89. 3% and specificity of 91.8% on a holdout test set. It is quite possible that the general framework discussed in the current work can be successfully applied to other biological datasets to deal with imbalance and incomplete learning problems effectively. Combination of unsupervised learning with SMOTE for imbalance learning problems.Effective handling of between class and within class imbalance.Diversification of the training set with optimal class distribution.Does not require evolutionary information for prediction.},
journal = {Comput. Biol. Med.},
month = jan,
pages = {27–36},
numpages = {10},
keywords = {Training set diversity, SMOTE, Optimal class distribution, K-Means, Class imbalance}
}

@inproceedings{10.1145/3337821.3337854,
author = {Hiebel, Jason and Brown, Laura E. and Wang, Zhenlin},
title = {Machine Learning for Fine-Grained Hardware Prefetcher Control},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337854},
doi = {10.1145/3337821.3337854},
abstract = {Modern architectures provide hardware memory prefetching capabilities which can be configured at runtime. While hardware prefetching can provide substantial performance improvements for many programs, prefetching can also increase contention for shared resources such as last-level cache and memory bandwidth. In turn, this contention can degrade performance in multi-core workloads. In this paper, we model fine-grained hardware prefetcher control as a contextual bandit, and propose a framework for learning prefetcher control policies which adjust hardware prefetching usage at runtime according to workload performance behavior. We train our policies on profiling data, wherein hardware memory prefetchers are enabled or disabled randomly at regular intervals over the course of a workload's execution. The learned prefetcher control policies provide up to a 4.3% average performance improvement over a set of memory bandwidth intensive workloads.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {3},
numpages = {9},
keywords = {hardware prefetcher control, contextual bandit},
location = {Kyoto, Japan},
series = {ICPP '19}
}

@inbook{10.5555/3454287.3454540,
author = {Dai, Wang-Zhou and Xu, Qiuling and Yu, Yang and Zhou, Zhi-Hua},
title = {Bridging machine learning and logical reasoning by abductive learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Perception and reasoning are two representative abilities of intelligence that are integrated seamlessly during human problem-solving processes. In the area of artificial intelligence (AI), the two abilities are usually realised by machine learning and logic programming, respectively. However, the two categories of techniques were developed separately throughout most of the history of AI. In this paper, we present the abductive learning targeted at unifying the two AI paradigms in a mutually beneficial way, where the machine learning model learns to perceive primitive logic facts from data, while logical reasoning can exploit symbolic domain knowledge and correct the wrongly perceived facts for improving the machine learning models. Furthermore, we propose a novel approach to optimise the machine learning model and the logical reasoning model jointly. We demonstrate that by using abductive learning, machines can learn to recognise numbers and resolve unknown mathematical operations simultaneously from images of simple hand-written equations. Moreover, the learned models can be generalised to longer equations and adapted to different tasks, which is beyond the capability of state-of-the-art deep learning models.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {253},
numpages = {12}
}

@article{10.1007/s11063-019-10113-w,
author = {Zhou, Fuqiang and Wang, Lin and Li, Zuoxin and Zuo, Wangxia and Tan, Haishu},
title = {Unsupervised Learning Approach for Abnormal Event Detection in Surveillance Video by Hybrid Autoencoder},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {2},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-019-10113-w},
doi = {10.1007/s11063-019-10113-w},
abstract = {Abnormal detection plays an important role in video surveillance. LSTM encoder–decoder is used to learn representation of video sequences and applied for detecting abnormal event in complex environment. The learned representation of LSTM encoder–decoder is learned from encoder, and it is crucial for decoder. However, LSTM encoder–decoder generally fails to account for the global context of the learned representation with a fixed dimension representation. In this paper, we explore a hybrid autoencoder architecture, which not only extracts better spatio-temporal context, but also improves the extrapolate capability of the corresponding decoder by the shortcut connection. The experiment shows that the hybrid model performs better than the state-of-the-art anomaly detection methods in both qualitative and quantitative ways on benchmark datasets.},
journal = {Neural Process. Lett.},
month = oct,
pages = {961–975},
numpages = {15},
keywords = {Abnormality detection, LSTM, Autoencoder}
}

@inproceedings{10.1145/3267809.3267812,
author = {Wang, Hao and Niu, Di and Li, Baochun},
title = {Dynamic and Decentralized Global Analytics via Machine Learning},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3267812},
doi = {10.1145/3267809.3267812},
abstract = {Operating at a large scale, data analytics has become an essential tool for gaining insights from operational data, such as user online activities. With the volume of data growing exponentially, data analytic jobs have expanded from a single datacenter to multiple geographically distributed datacenters. Unfortunately, designed originally for a single datacenter, the software stack that supports data analytics is oblivious to on-the-fly resource variations on inter-datacenter networks, which negatively affects the performance of analytic queries. Existing solutions that optimize query execution plans before their execution are not able to quickly adapt to resource variations at query runtime.In this paper, we present Turbo, a lightweight and non-intrusive data-driven system that dynamically adjusts query execution plans for geo-distributed analytics in response to runtime resource variations across datacenters. A highlight of Turbo is its ability to use machine learning at runtime to accurately estimate the time cost of query execution plans, so that adjustments can be made when necessary. Turbo is non-intrusive in the sense that it does not require modifications to the existing software stack for data analytics. We have implemented a real-world prototype of Turbo, and evaluated it on a cluster of 33 instances across 8 regions in the Google Cloud platform. Our experimental results have shown that Turbo can achieve a cost estimation accuracy of over 95% and reduce query completion times by 41%.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {14–25},
numpages = {12},
keywords = {Machine Learning, Distributed Systems, Data Analytics},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@article{10.1109/TPAMI.2019.2911077,
author = {Koller, Oscar and Camgoz, Necati Cihan and Ney, Hermann and Bowden, Richard},
title = {Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Language Videos},
year = {2020},
issue_date = {Sept. 2020},
publisher = {IEEE Computer Society},
address = {USA},
volume = {42},
number = {9},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2019.2911077},
doi = {10.1109/TPAMI.2019.2911077},
abstract = {In this work we present a new approach to the field of weakly supervised learning in the video domain. Our method is relevant to sequence learning problems which can be split up into sub-problems that occur in parallel. Here, we experiment with sign language data. The approach exploits sequence constraints within each independent stream and combines them by explicitly imposing synchronisation points to make use of parallelism that all sub-problems share. We do this with multi-stream HMMs while adding intermediate synchronisation constraints among the streams. We embed powerful CNN-LSTM models in each HMM stream following the hybrid approach. This allows the discovery of attributes which on their own lack sufficient discriminative power to be identified. We apply the approach to the domain of sign language recognition exploiting the sequential parallelism to learn sign language, mouth shape and hand shape classifiers. We evaluate the classifiers on three publicly available benchmark data sets featuring challenging real-life sign language with over 1,000 classes, full sentence based lip-reading and articulated hand shape recognition on a fine-grained hand shape taxonomy featuring over 60 different hand shapes. We clearly outperform the state-of-the-art on all data sets and observe significantly faster convergence using the parallel alignment approach.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = sep,
pages = {2306–2320},
numpages = {15}
}

@article{10.1007/s00366-019-00842-w,
author = {Thai, Duc-Kien and Tu, Tran Minh and Bui, Tinh Quoc and Bui, T.-T.},
title = {Gradient tree boosting machine learning on predicting the failure modes of the RC panels under impact loads},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {1},
issn = {0177-0667},
url = {https://doi.org/10.1007/s00366-019-00842-w},
doi = {10.1007/s00366-019-00842-w},
abstract = {This paper proposed a new approach in predicting the local damage of reinforced concrete (RC) panels under impact loading using gradient boosting machine learning (GBML), one of the most powerful techniques in machine learning. A number of experimental data on the impact test of RC panels were collected for training and testing of the proposed model. With the lack of test data due to the high cost and complexity of the structural behavior of the panel under impact loading, it was a challenge to predict the failure mode accurately. To overcome this challenge, this study proposed a machine-learning model that uses a robust technique to solve the problem with a minimal amount of resources. Although the accuracy of the prediction result was not as high as expected due to the lack of data and the unbalance experimental output features, this paper provided a new approach that may alternatively replace the conventional method in predicting the failure mode of RC panel under impact loading. This approach is also expected to be widely used for predicting the structural behavior of component and structures under complex and extreme loads.},
journal = {Eng. with Comput.},
month = jan,
pages = {597–608},
numpages = {12},
keywords = {Gradient boosting, XGboost, Machine learning, Local damage, Reinforced concrete, Impact loading}
}

@inproceedings{10.1145/3341105.3373967,
author = {Singh, Adarsh Pal and Chaudhari, Sachin},
title = {Embedded machine learning-based data reduction in application-specific constrained IoT networks},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373967},
doi = {10.1145/3341105.3373967},
abstract = {Reducing the amount of wireless data transmissions in constrained battery-powered sensor nodes is an effective way of prolonging their lifetime. In this paper, we present a machine learning-based data transmission reduction scheme for application-specific IoT networks. Though many error thresholding-based data prediction schemes have been explored in the past, this is the first work to incorporate machine learning in constrained sensor nodes to reduce data transmissions. We also provide a generic overview and comparison of five traditional supervised machine learning algorithms in the context of offloading trained models to memory and computationally constrained microcontrollers. The proposed data reduction scheme is validated on an occupancy estimation testbed deployed in our lab. Experimental results demonstrate 99.91% overall reduction in data transmissions while imparting similar performance and 18 to 82 times lesser transmissions than Shewhart change detection algorithm.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {747–753},
numpages = {7},
keywords = {wireless sensor networks, machine learning, internet of things, embedded systems, distributed detection, data transmission reduction},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1145/3466721,
author = {Chowdhury, Morshed and Ray, Biplob and Chowdhury, Sujan and Rajasegarar, Sutharshan},
title = {A Novel Insider Attack and Machine Learning Based Detection for the Internet of Things},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3466721},
doi = {10.1145/3466721},
abstract = {Due to the widespread functional benefits, such as supporting internet connectivity, having high visibility and enabling easy connectivity between sensors, the Internet of Things (IoT) has become popular and used in many applications, such as for smart city, smart health, smart home, and smart vehicle realizations. These IoT-based systems contribute to both daily life and business, including sensitive and emergency situations. In general, the devices or sensors used in the IoT have very limited computational power, storage capacity, and communication capabilities, but they help to collect a large amount of data as well as maintain communication with the other devices in the network. Since most of the IoT devices have no physical security, and often are open to everyone via radio communication and via the internet, they are highly vulnerable to existing and emerging novel security attacks. Further, the IoT devices are usually integrated with the corporate networks; in this case, the impact of attacks will be much more significant than operating in isolation. Due to the constraints of the IoT devices, and the nature of their operation, existing security mechanisms are less effective for countering the attacks that are specific to the IoT-based systems. This article presents a new insider attack, named loophole attack, that exploits the vulnerabilities present in a widely used IPv6 routing protocol in IoT-based systems, called RPL (Routing over Low Power and Lossy Networks). To protect the IoT system from this insider attack, a machine learning based security mechanism is presented. The proposed attack has been implemented using a Contiki IoT operating system that runs on the Cooja simulator, and the impacts of the attack are analyzed. Evaluation on the collected network traffic data demonstrates that the machine learning based approaches, along with the proposed features, help to accurately detect the insider attack from the network traffic data.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {26},
numpages = {23},
keywords = {machine learning, insider attack, RPL security, RPL, IoT, Contiki}
}

@inproceedings{10.1145/3394941.3394943,
author = {du Preez, Anli and Bekker, James},
title = {A Machine Learning Decision Support Framework for Industrial Engineering Purposes},
year = {2020},
isbn = {9781450375283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394941.3394943},
doi = {10.1145/3394941.3394943},
abstract = {Data is currently one of the most critical and influential emerging assets. However, the true potential of data is yet to be exploited since, currently, about 1% of generated data is ever actually analyzed for value creation [1]. There is a data gap where data is not explored due to the lack of data analytics infrastructure and the required data analytics skills. This study developed a decision support framework for data analytics by following Jabareen's framework development methodology. The study focused on machine learning algorithms, which is a subset of data analytics. The developed framework is designed to assist data analysts with little experience, in choosing the appropriate machine learning algorithm given the purpose of their application.},
booktitle = {Proceedings of the 2020 International Conference on Industrial Engineering and Industrial Management},
pages = {9–14},
numpages = {6},
keywords = {value creation, machine learning, data analytics, Industrial engineering},
location = {Paris, France},
series = {IEIM 2020}
}

@article{10.1145/3460961,
author = {Abitbol, Roy and Shimshoni, Ilan and Ben-Dov, Jonathan},
title = {Machine Learning Based Assembly of Fragments of Ancient Papyrus},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3460961},
doi = {10.1145/3460961},
abstract = {The task of assembling fragments in a puzzle-like manner into a composite picture plays a significant role in the field of archaeology as it supports researchers in their attempt to reconstruct historic artifacts. In this article, we propose a method for matching and assembling pairs of ancient papyrus fragments containing mostly unknown scriptures.Papyrus paper is manufactured from papyrus plants and therefore portrays typical thread patterns resulting from the plant’s stems. The proposed algorithm is founded on the hypothesis that these thread patterns contain unique local attributes such that nearby fragments show similar patterns reflecting the continuations of the threads. We posit that these patterns can be exploited using image processing and machine learning techniques to identify matching fragments. The algorithm and system which we present support the quick and automated classification of matching pairs of papyrus fragments as well as the geometric alignment of the pairs against each other.The algorithm consists of a series of steps and is based on deep-learning and machine learning methods. The first step is to deconstruct the problem of matching fragments into a smaller problem of finding thread continuation matches in local edge areas (squares) between pairs of fragments. This phase is solved using a convolutional neural network ingesting raw images of the edge areas and producing local matching scores. The result of this stage yields very high recall but low precision. Thus, we utilize these scores in order to conclude about the matching of entire fragments pairs by establishing an elaborate voting mechanism. We enhance this voting with geometric alignment techniques from which we extract additional spatial information. Eventually, we feed all the data collected from these steps into a Random Forest classifier in order to produce a higher order classifier capable of predicting whether a pair of fragments is a match.Our algorithm was trained on a batch of fragments which was excavated from the Dead Sea caves and is dated circa the 1st century BCE. The algorithm shows excellent results on a validation set which is of a similar origin and conditions. We then tried to run the algorithm against a real-life set of fragments for which we have no prior knowledge or labeling of matches. This test batch is considered extremely challenging due to its poor condition and the small size of its fragments. Evidently, numerous researchers have tried seeking matches within this batch with very little success. Our algorithm performance on this batch was sub-optimal, returning a relatively large ratio of false positives. However, the algorithm was quite useful by eliminating 98% of the possible matches thus reducing the amount of work needed for manual inspection. Indeed, experts that reviewed the results have identified some positive matches as potentially true and referred them for further investigation.},
journal = {J. Comput. Cult. Herit.},
month = jul,
articleno = {33},
numpages = {21},
keywords = {thread patterns, neural networks, matching, fragments, dead sea scrolls, assembly, Papyrus}
}

@article{10.1145/3319616,
author = {Wu, Tongshuang and Weld, Daniel S. and Heer, Jeffrey},
title = {Local Decision Pitfalls in Interactive Machine Learning: An Investigation into Feature Selection in Sentiment Analysis},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3319616},
doi = {10.1145/3319616},
abstract = {Tools for Interactive Machine Learning (IML) enable end users to update models in a “rapid, focused, and incremental”—yet local—manner. In this work, we study the question of local decision making in an IML context around feature selection for a sentiment classification task. Specifically, we characterize the utility of interactive feature selection through a combination of human-subjects experiments and computational simulations. We find that, in expectation, interactive modification fails to improve model performance and may hamper generalization due to overfitting. We examine how these trends are affected by the dataset, learning algorithm, and the training set size. Across these factors we observe consistent generalization issues. Our results suggest that rapid iterations with IML systems can be dangerous if they encourage local actions divorced from global context, degrading overall model performance. We conclude by discussing the implications of our feature selection results to the broader area of IML systems and research.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {24},
numpages = {27},
keywords = {text classification, performance analysis, Machine learning}
}

@article{10.5555/3292879.3292881,
title = {Machine learning: the new 'big thing' for competitive advantage},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {4},
issn = {1755-2087},
abstract = {While machine learning ML has existed for a long time, the business world's focus on it may seem like an overnight development. The technology has been steadily growing since the early days of big data. However, the ability to give machines access to big data and allow them to apply complex mathematical calculations repetitively, while learning for themselves, is a new development. The key objective of this article is to propose a conceptual model for successful implementation of ML in organisations. This article also covers some of the potential benefits of ML, and provides a guide to some of the opportunities that are available for using ML in various businesses. Furthermore, this study reviews key attributes of successful ML platforms and illustrates how to overcome some of the key implementation challenges. Finally, this study highlights some of the successful implementations of ML solutions used in the manufacturing and service industry.},
journal = {Int. J. Knowl. Eng. Data Min.},
month = jan,
pages = {277–305},
numpages = {29}
}

@article{10.1016/j.jbi.2020.103621,
author = {Pfohl, Stephen R. and Foryciarz, Agata and Shah, Nigam H.},
title = {An empirical characterization of fair machine learning for clinical risk prediction},
year = {2021},
issue_date = {Jan 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {113},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2020.103621},
doi = {10.1016/j.jbi.2020.103621},
journal = {J. of Biomedical Informatics},
month = jan,
numpages = {67},
keywords = {Clinical risk prediction, Bias, Algorithmic fairness}
}

@phdthesis{10.5555/AAI27543453,
author = {Tan, Chaowei and Li, Kang and Yu, Jingjin and Dou, Xin},
advisor = {N, Metaxas, Dimitris},
title = {Machine Learning Based Image Segmentation for Large-Scale Osteoarthritis Analysis},
year = {2020},
isbn = {9798662401983},
publisher = {Rutgers The State University of New Jersey, School of Graduate Studies},
abstract = {Osteoarthritis (OA) is the most common degenerative joint disease worldwide, tending to occur in the joints of hip and knee. Large adult population in the United States have been affected by OA, and by 2030, an estimated 20 percent of Americans (about 70 million people) may be at increased risk for this disease. Effective medical image segmentation methods play fundamental roles in the clinical analysis of the disease. In this dissertation, three machine learning based segmentation for knee cartilage, femoral head-neck junction and thigh muscular/adipose tissue are discussed, respectively. Furthermore, large-scale OA analysis on the knee and hip joints could be further implemented based on these segments.Knee cartilages (i.e., femoral, tibial, and patellar cartilage) are essential tissue for knee radiographic OA diagnosis. Effective segmentation of knee cartilages in large-sized and high-resolution 3D magnetic resonance (MR) data is firstly proposed. The key contribution is an adversarial learning based collaborative multi-agent network. The method employs three parallel segmentation agents to label cartilages in their respective region of interest (ROI), and then fuses the three cartilages by a ROI-fusion layer and drive a collaborative learning by an adversarial sub-network. The ROI-fusion layer not only fuses the individual cartilages, but also backpropagates the training loss from the adversarial sub-network to each agent to enable joint learning of shape and spatial constraints. The proposed scheme is shown robust and accurate in knee cartilage segmentation, and it is effective for cartilage biomarkers (e.g., surface area, volume) estimation in large-scale quantitative tests. Second, a deep multi-task learning network is exploited for the shape-preserved segmentation of the proximal part of femur (i.e., femoral head and neck) in 2D MR images. This method combines the tasks of region identification and boundary distance regression, and thus enables the task-specific feature learning for continuous segmented object with smooth boundary. This bone joint depiction could help the measurements of the femoral head-neck morphology and reflect the evolution of hip OA. In the last part of the dissertation, the muscular and adipose tissue extraction in 3D MR thigh data is investigated by an integrated framework. Specifically, deformable models and learning based detection/classification are integrated into the framework to enable robust tissue quantification for a large-scale analysis of OA-related thigh tissue changes.},
note = {AAI27543453}
}

@article{10.1016/j.cosrev.2020.100285,
author = {GM, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
title = {A comprehensive survey and analysis of generative models in machine learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2020.100285},
doi = {10.1016/j.cosrev.2020.100285},
journal = {Comput. Sci. Rev.},
month = nov,
numpages = {29},
keywords = {Bayesian inference, Neural networks, Deep learning, Machine learning, Generative models}
}

@inproceedings{10.1145/3430199.3430216,
author = {Wang, Qian and Xue, Tongxin and Wu, Yi and Hu, Fan and Han, Pengfei},
title = {Detection of Key Structure of Auroral Images Based on Weakly Supervised Learning},
year = {2020},
isbn = {9781450375511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430199.3430216},
doi = {10.1145/3430199.3430216},
abstract = {Weakly supervised learning is of interest and research by many people due to the large savings in labeling costs. To solve the high cost of manual labeling in the research of aurora image detection, an Aurora multi-scale network for aurora image dataset is proposed based on weakly-supervised learning. Firstly, the feature learning mechanism of dynamic hierarchical mimicking is adopted to improve the classification performance of the convolutional neural network based on the aurora image. Then, the multi-scale constraint is imposed on the network through the multi-branch input and output of different sizes. The final output of the auroral image class activation maps with more ideal results, the critical structure detection of auroral images based on imagelevel annotation is realized. Experiments show that the algorithm in this paper can effectively improve the class activation maps results of the auroral image, and has an ideal detection effect on the vital structure of the auroral image.},
booktitle = {Proceedings of the 2020 3rd International Conference on Artificial Intelligence and Pattern Recognition},
pages = {110–114},
numpages = {5},
keywords = {Weakly supervised, Image detection, Dynamic hierarchical mimickins, Aurora multi-scale network},
location = {Xiamen, China},
series = {AIPR '20}
}

@inproceedings{10.5555/3200334.3200373,
author = {Ayala, Brenda Reyes and Chen, Jiangping},
title = {A machine learning approach to evaluating translation quality},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We explored supervised machine learning (ML) techniques to understand and predict the adequacy and fluency of English-Spanish machine translation. Five experiments were conducted using three classifiers in Weka, an open-source ML tool. We found that the highest performance was achieved by applying a dimensionality reduction approach to the classification task, which included collapsing a numeric scale of quality to two categories: high quality and low quality. Our results showed that the Support Vector Machine classifier performed the best at predicting the adequacy (65.65%) and fluency (65.77%) of the translations. More research is needed to explore the methodologies of applying ML to translation evaluation.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {281–282},
numpages = {2},
keywords = {machine translation evaluation, machine learning, Weka},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@article{10.3233/JIFS-179915,
author = {Patnaik, Srikanta and Patnaik, Srikanta},
title = {Applied machine learning and management of volatility, uncertainty, complexity &amp; ambiguity (V.U.C.A)},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179915},
doi = {10.3233/JIFS-179915},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1409–1416},
numpages = {8}
}

@article{10.1016/j.patcog.2014.07.025,
author = {Wang, Shijun and Li, Diana and Petrick, Nicholas and Sahiner, Berkman and Linguraru, Marius George and Summers, Ronald M.},
title = {Optimizing area under the ROC curve using semi-supervised learning},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.07.025},
doi = {10.1016/j.patcog.2014.07.025},
abstract = {Receiver operating characteristic (ROC) analysis is a standard methodology to evaluate the performance of a binary classification system. The area under the ROC curve (AUC) is a performance metric that summarizes how well a classifier separates two classes. Traditional AUC optimization techniques are supervised learning methods that utilize only labeled data (i.e., the true class is known for all data) to train the classifiers. In this work, inspired by semi-supervised and transductive learning, we propose two new AUC optimization algorithms hereby referred to as semi-supervised learning receiver operating characteristic (SSLROC) algorithms, which utilize unlabeled test samples in classifier training to maximize AUC. Unlabeled samples are incorporated into the AUC optimization process, and their ranking relationships to labeled positive and negative training samples are considered as optimization constraints. The introduced test samples will cause the learned decision boundary in a multi-dimensional feature space to adapt not only to the distribution of labeled training data, but also to the distribution of unlabeled test data. We formulate the semi-supervised AUC optimization problem as a semi-definite programming problem based on the margin maximization theory. The proposed methods SSLROC1 (1-norm) and SSLROC2 (2-norm) were evaluated using 34 (determined by power analysis) randomly selected datasets from the University of California, Irvine machine learning repository. Wilcoxon signed rank tests showed that the proposed methods achieved significant improvement compared with state-of-the-art methods. The proposed methods were also applied to a CT colonography dataset for colonic polyp classification and showed promising results.11Matlab code of the proposed methods will be released on http://clinicalcenter.nih.gov/drd/summers.html once the paper is published. HighlightsOptimizing area under the ROC curve using semi-supervised learning.A large margin maximization semi-supervised learning framework for AUC maximization.Closed-form solution based on semi-definite programming.Superior performance on 34 UCI machine learning datasets determined by power analysis.Showed efficacy on a CT colonography dataset for colonic polyp classification.},
journal = {Pattern Recogn.},
month = jan,
pages = {276–287},
numpages = {12},
keywords = {Transfer learning, Semidefinite programming, Semi-supervised learning, SVMROC, SSLROC, Receiver operating characteristic, RankBoost, AUC}
}

@inproceedings{10.5555/3504035.3504405,
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
title = {Non-discriminatory machine learning through convex fairness criteria},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Biased decision making by machine learning systems is increasingly recognized as an important issue. Recently, techniques have been proposed to learn non-discriminatory classifiers by enforcing constraints in the training phase. Such constraints are either non-convex in nature (posing computational difficulties) or don't have a clear probabilistic interpretation. Moreover, the techniques offer little understanding of the more subjective notion of fairness.In this paper, we introduce a novel technique to achieve non-discrimination without sacrificing convexity and probabilistic interpretation. Our experimental analysis demonstrates the success of the method on popular real datasets including ProPublica's COMPAS dataset. We also propose a new notion of fairness for machine learning and show that our technique satisfies this subjective fairness criterion.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {370},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/3293475.3293480,
author = {Qayyum, Shamaila and Qureshi, Ahsan},
title = {A Survey on Machine Learning Based Requirement Prioritization Techniques},
year = {2018},
isbn = {9781450365956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293475.3293480},
doi = {10.1145/3293475.3293480},
abstract = {Software is becoming a need of today's business and everyday tasks. However, building software that fulfils its user's need is not an easy task. It is deemed important to properly elicit and prioritise the requirements of users and these prioritized requirements should be implemented in the system to offer purposeful software to users. Requirements prioritization is a complex and trivial task. Lately machine learning had been used to automate the process of requirements prioritization. The purpose of this study is to identify and analyse the existing techniques of requirements prioritization that are based on machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Computational Intelligence and Intelligent Systems},
pages = {51–55},
numpages = {5},
keywords = {machine learning, Requirements Prioritization, Prigov, GDRank, CBRank, AHP},
location = {Phuket, Thailand},
series = {CIIS '18}
}

@article{10.1504/IJIIDS.2012.045848,
author = {Mar\'{\i}, &nbsp; and Espinosa, a. Eugenia Cabello and Salavert, Isidro Ramos and Guti\'{e}, Jorge Rafael and Pulido, rrez and G\'{o}, Abel and Llana, mez and Lim\'{o}, Rogelio and Cordero, n.},
title = {SPL variability management, cardinality and types: an MDA approach},
year = {2012},
issue_date = {March 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {2},
issn = {1751-5858},
url = {https://doi.org/10.1504/IJIIDS.2012.045848},
doi = {10.1504/IJIIDS.2012.045848},
abstract = {This paper presents a baseline-oriented modelling (BOM) approach to develop families of software products. BOM is a generic solution implemented as a framework that automatically generates software applications using executable architectural models by means of software product line (SPL) techniques. In order to cope with the variability problem, BOM considers its cardinality and type and implements two solutions: the BOM-EAGER and the BOM-LAZY approaches. BOM has been designed following the model-driven architecture (MDA) standard: all the SPL software artefacts are models, and model transformations enact the SPL production plan.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = mar,
pages = {129–153},
numpages = {25}
}

@inproceedings{10.1145/3440749.3442659,
author = {Doan, Duy Binh and Pham, Minh Tuan and Dang, Duc Long},
title = {Predicting RNA secondary structure based on machine learning and genetic algorithm},
year = {2021},
isbn = {9781450388863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440749.3442659},
doi = {10.1145/3440749.3442659},
abstract = {In recent years, RNA secondary structure prediction is an important issue in structural bioinformatics, and RNA pseudoknotted secondary structure prediction represents an NP-hard problem. Current RNA secondary structure prediction methods are mainly based on the minimum free energy algorithm. However, due to the complexity of biotic environment, a true RNA structure always keeps the balance of biological potential energy status, rather than the optimal folding status that meets the minimum energy. For short sequence RNA its equilibrium energy status for the RNA folding organism is close to the minimum free energy status. Nevertheless, in a longer sequence RNA, constant folding causes its biopotential energy balance to deviate far from the minimum free energy status. In this paper, we propose a novel RNA secondary structure prediction algorithm using a convolutional neural network model combined with a genetic algorithm method to improve the accuracy with large-scale RNA sequence and structure data...},
booktitle = {Proceedings of the 4th International Conference on Future Networks and Distributed Systems},
articleno = {52},
numpages = {12},
keywords = {RNA Secondary Structure, Neural Network, Minimum Free Energy, Machine Learning, Long Short-Term Memory, Genetic Algorithm, Energy-Based Filter, Energy Balance Status, Base Pairing Probability},
location = {St.Petersburg, Russian Federation},
series = {ICFNDS '20}
}

@inproceedings{10.1145/3383783.3383789,
author = {Lim, Tito C. and Torregosa, Jaedy O. and Pescadero, Aubrey Rose A. and Pangantihon, Rodrigo S.},
title = {De-husked Coconut Quality Evaluation using Image Processing and Machine Learning Techniques},
year = {2020},
isbn = {9781450372183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383783.3383789},
doi = {10.1145/3383783.3383789},
abstract = {Qualitative evaluation provides the basis for determining if the quality of products meets the target specifications. Manual evaluation of de-husked coconuts is still being performed by coconut farmers, however, it is time consuming and costly. Ergo this study aiming to replace the manual inspection, a prototype was developed for objective and automated quality evaluation of de-husked coconuts through the application of computer vision and machine learning, identifying good-quality de-husked coconuts from defective ones with respect to its RGB color space. JavaFX platform was utilized to create the system performing K-Nearest Neighbor and Arduino technology played a significant role in the hardware control of the device. The image samples were captured by a CMOS camera in an imaging chamber with invariant illumination on top of a conveyor belt. Image processing is done to get the required features of the sample and by comparing the average RGB value from the custom dataset, then the maturity level of the coconut is determined. With the accuracy of 86.67%, the system is able to evaluate de-husked coconuts which are good for further processing used in export and premature coconuts that are to be rejected.},
booktitle = {Proceedings of the 6th International Conference on Bioinformatics Research and Applications},
pages = {28–33},
numpages = {6},
keywords = {k-NN, OpenCV, Machine learning, JavaFX, Coconut quality},
location = {Seoul, Republic of Korea},
series = {ICBRA '19}
}

@inproceedings{10.1007/978-3-030-70210-6_43,
author = {Scurto, Hugo and Chemla–Romeu-Santos, Axel},
title = {Machine Learning for Computer Music Multidisciplinary Research: A Practical Case Study},
year = {2019},
isbn = {978-3-030-70209-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70210-6_43},
doi = {10.1007/978-3-030-70210-6_43},
abstract = {This paper presents a multidisciplinary case study of practice with machine learning for computer music. It builds on the scientific study of two machine learning models respectively developed for data-driven sound synthesis and interactive exploration. It details how the learning capabilities of the two models were leveraged to design and implement a musical interface focused on embodied musical interaction. It then describes how this interface was employed and applied to the composition and performance of \ae{}go, an improvisational piece with interactive sound and image for one performer. We discuss the outputs of our research and creation process, and expose our personal reflections and insights on transdisciplinary research opportunities framed by machine learning for computer music.},
booktitle = {Perception, Representations, Image, Sound, Music: 14th International Symposium, CMMR 2019, Marseille, France, October 14–18, 2019, Revised Selected Papers},
pages = {665–680},
numpages = {16},
keywords = {Transdisciplinarity, Performance, Composition, Interface design, Machine learning},
location = {Marseille, France}
}

@inproceedings{10.5555/3466184.3466252,
author = {Liu, Yang and Yan, Liang and Liu, Sheng and Jiang, Ting and Zhang, Feng and Wang, Yu and Wu, Shengnan},
title = {Enhancing input parameter estimation by machine learning for the simulation of large-scale logistics networks},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {The quality of large-scale logistics network simulation highly depends on the estimation of its key input parameters, which are usually influenced by various factors that are difficult to obtain. To tackle this challenge, this paper proposes a framework to estimate these parameters with high precision through machine learning, in which the impacting factors are divided into static and dynamic groups and used as features to train a learning model for estimation. To overcome the obstacle that dynamic factors are hard to obtain in some scenarios, the proposed framework employs unsupervised learning to analyze their patterns and extract time-invariant features for modeling. A validation study is conducted on the estimation of distribution center sorting times. The results proved our approach can generate more accurate estimation of input parameters, even with the shift of operational plans and absence of relevant data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {608–619},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1007/978-3-030-38081-6_7,
author = {Adibi, Pedram and Pranovi, Fabio and Raffaet\`{a}, Alessandra and Russo, Elisabetta and Silvestri, Claudio and Simeoni, Marta and Soares, Amilcar and Matwin, Stan},
title = {Predicting Fishing Effort and Catch Using Semantic Trajectories and Machine Learning},
year = {2019},
isbn = {978-3-030-38080-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38081-6_7},
doi = {10.1007/978-3-030-38081-6_7},
abstract = {In this paper we explore a unique, high-value spatio-temporal dataset that results from the fusion of three data sources: trajectories from fishing vessels (obtained from terrestrial Automatic Identification System, or AIS, data feed), the corresponding fish catch reports (i.e., the quantity and type of fish caught), and relevant environmental data. The result of that fusion is a set of semantic trajectories describing the fishing activities in Northern Adriatic Sea over two years. We present early results from an exploratory analysis of these semantic trajectories, as well as from initial predictive modeling using Machine Learning. Our goal is to predict the Catch Per Unit Effort (CPUE), an indicator of the fishing resources exploitation useful for fisheries management. Our predictive results are preliminary in both the temporal data horizon that we are able to explore and in the limited set of learning techniques that are employed on this task. We discuss several approaches that we plan to apply in the near future to learn from such data, evidence, and knowledge that will be useful for fisheries management. It is likely that other centers of intense fishing activities are in possession of similar data and could use the methods similar to the ones proposed here in their local context.},
booktitle = {Multiple-Aspect Analysis of Semantic Trajectories: First International Workshop, MASTER 2019, Held in Conjunction with ECML-PKDD 2019, W\"{u}rzburg, Germany, September 16, 2019, Proceedings},
pages = {83–99},
numpages = {17},
keywords = {AIS, Semantic trajectories, Machine Learning, Fisheries, Spatio-temporal data},
location = {W\"{u}rzburg, Germany}
}

@article{10.1007/s10766-017-0554-6,
author = {Cuomo, Salvatore and Michele, Pasquale and Nardo, Emanuel and Marcellino, Livia},
title = {Parallel Implementation of a Machine Learning Algorithm on GPU},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {46},
number = {5},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-017-0554-6},
doi = {10.1007/s10766-017-0554-6},
abstract = {The capability for understanding data passes through the ability of producing an effective and fast classification of the information in a time frame that allows to keep and preserve the value of the information itself and its potential. Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. A powerful tool is provided by self-organizing maps (SOM). The goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. Because of its time complexity, often using this method is a critical challenge. In this paper we propose a parallel implementation for the SOM algorithm, using parallel processor architecture, as modern graphics processing units by CUDA. Experimental results show improvements in terms of execution time, with a promising speed up, compared to the CPU version and the widely used package SOM_PAK.},
journal = {Int. J. Parallel Program.},
month = oct,
pages = {923–942},
numpages = {20},
keywords = {Self-organization map, Machine learning, GP-GPU, Fast data analysis and retrieval}
}

@article{10.1287/inte.2016.0862,
author = {Lee, Eva K. and Nakaya, Helder I. and Yuan, Fan and Querec, Troy D. and Burel, Greg and Pietz, Ferdinand H. and Benecke, Bernard A. and Pulendran, Bali},
title = {Machine Learning for Predicting Vaccine Immunogenicity},
year = {2016},
issue_date = {October 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {46},
number = {5},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2016.0862},
doi = {10.1287/inte.2016.0862},
abstract = {The ability to predict how different individuals will respond to vaccination and to understand what best protects individuals from infection greatly facilitates developing next-generation vaccines. It facilitates both the rapid design and evaluation of new and emerging vaccines and identifies individuals unlikely to be protected by vaccine. We describe a general-purpose machine-learning framework, DAMIP, for discovering gene signatures that can predict vaccine immunity and efficacy. DAMIP is a multiple-group, concurrent classifier that offers unique features not present in other models: a nonlinear data transformation to manage the curse of dimensionality and noise; a reserved-judgment region that handles fuzzy entities; and constraints on the allowed percentage of misclassifications.Using DAMIP, implemented results for yellow fever demonstrated that, for the first time, a vaccine's ability to immunize a patient could be successfully predicted with accuracy of greater than 90 percent within one week after vaccination. A gene identified by DAMIP, EIF2AK4, decrypted a seven-decade-old mystery of vaccination. Results for flu vaccine demonstrated DAMIP's applicability to both live-attenuated and inactivated vaccines. Results in a malaria study enabled targeted delivery to individual patients.Our project's methods and findings permit highlighting and probabilistically prioritizing hypothesis design to enhance biological discovery. Moreover, they guide the rapid development of better vaccines to fight emerging infections, and improve monitoring for poor responses in the elderly, infants, or others with weakened immune systems. In addition, the project's work should help with universal flu-vaccine design.},
journal = {Interfaces},
month = oct,
pages = {368–390},
numpages = {23},
keywords = {yellow fever, vaccine immunogenicity prediction, vaccine design for emerging infections, prophylactic medical countermeasures, multiple-group classification, malaria, machine learning, influenza, hypothesis generation, health security}
}

@inproceedings{10.1145/3411764.3445306,
author = {Xin, Doris and Wu, Eva Yiwei and Lee, Doris Jung-Lin and Salehi, Niloufar and Parameswaran, Aditya},
title = {Whither AutoML? Understanding the Role of Automation in Machine Learning Workflows},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445306},
doi = {10.1145/3411764.3445306},
abstract = {Efforts to make machine learning more widely accessible have led to a rapid increase in Auto-ML tools that aim to automate the process of training and deploying machine learning. To understand how Auto-ML tools are used in practice today, we performed a qualitative study with participants ranging from novice hobbyists to industry researchers who use Auto-ML tools. We present insights into the benefits and deficiencies of existing tools, as well as the respective roles of the human and automation in ML workflows. Finally, we discuss design implications for the future of Auto-ML tool development. We argue that instead of full automation being the ultimate goal of Auto-ML, designers of these tools should focus on supporting a partnership between the user and the Auto-ML tool. This means that a range of Auto-ML tools will need to be developed to support varying user goals such as simplicity, reproducibility, and reliability.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {83},
numpages = {16},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1007/978-3-030-77876-7_32,
author = {Krutikov, Vladimir N. and Meshechkin, Vladimir V. and Kagan, Elena S. and Kazakovtsev, Lev A.},
title = {Machine Learning Algorithms of Relaxation Subgradient Method with Space Extension},
year = {2021},
isbn = {978-3-030-77875-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77876-7_32},
doi = {10.1007/978-3-030-77876-7_32},
abstract = {In relaxation subgradient minimization methods, a descent direction, which is based on the subgradients obtained at the iteration, forms an obtuse angle with all subgradients in the neighborhood of the current minimum. Minimization along this direction enables us to go beyond this neighborhood and avoid method looping. To find the descent direction, we formulate a problem in a form of systems of inequalities and propose an algorithm with space extension close to the iterative least squares method for solving them. The convergence rate of the method is proportional to the valid value of the space extension parameter and limited by the characteristics of subgradient sets. Theoretical analysis of the learning algorithm with space extension enabled us to identify the components of the algorithm and alter them to use increased values of the extension parameter if possible. On this basis, we propose and substantiate a new learning method with space extension and corresponding subgradient method for nonsmooth minimization. Our computational experiment confirms their efficiency. Our approach can be used to develop new algorithms with space extension for relaxation subgradient minimization.},
booktitle = {Mathematical Optimization Theory and Operations Research: 20th International Conference, MOTOR 2021, Irkutsk, Russia, July 5–10, 2021, Proceedings},
pages = {477–492},
numpages = {16},
keywords = {Relaxation, Space extension, Subgradient methods},
location = {Irkutsk, Russia}
}

@inproceedings{10.1145/3442188.3445912,
author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
title = {Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445912},
doi = {10.1145/3442188.3445912},
abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {489–503},
numpages = {15},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{10.3233/AIS-210604,
author = {Marufuzzaman, Mohammad and Tumbraegel, Teresa and Rahman, Labonnah Farzana and Sidek, Lariyah Mohd},
title = {A machine learning approach to predict the activity of smart home inhabitant},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {13},
number = {4},
issn = {1876-1364},
url = {https://doi.org/10.3233/AIS-210604},
doi = {10.3233/AIS-210604},
abstract = {A smart home inhabitant performs a unique pattern or sequence of tasks repeatedly. Thus, a machine learning approach will be required to build an intelligent network of home appliances, and the algorithm should respond quickly to execute the decision. This study proposes a decision tree-based machine learning approach for predicting the activities using different appliances such as state, locations and time. A noise filter is employed to remove unwanted data and generate task sequences, and dual state properties of a home appliance are utilized to extract episodes from the sequence. An incremental decision tree approach was taken to reduce execution time. The algorithm was tested using a well-known smart home dataset from MavLab. The experimental results showed that the algorithm successfully extracted 689 predictions and their location at 90% accuracy, and the total execution time was 94&nbsp;s, which is less than that of existing methods. A hardware prototype was designed using Raspberry Pi 2 B to validate the proposed prediction system. The general-purpose input-output (GPIO) interfaces of Raspberry Pi 2 B were used to communicate with the prototype testbed and showed that the algorithm successfully predicted the next activities.},
journal = {J. Ambient Intell. Smart Environ.},
month = jan,
pages = {271–283},
numpages = {13},
keywords = {Raspberry Pi 2B, PPM, incremental tree, smart home, Multiagent}
}

@article{10.1145/3394659,
author = {Emami, Patrick and Pardalos, Panos M. and Elefteriadou, Lily and Ranka, Sanjay},
title = {Machine Learning Methods for Data Association in Multi-Object Tracking},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3394659},
doi = {10.1145/3394659},
abstract = {Data association is a key step within the multi-object tracking pipeline that is notoriously challenging due to its combinatorial nature. A popular and general way to formulate data association is as the NP-hard multi-dimensional assignment problem. Over the past few years, data-driven approaches to assignment have become increasingly prevalent as these techniques have started to mature. We focus this survey solely on learning algorithms for the assignment step of multi-object tracking, and we attempt to unify various methods by highlighting their connections to linear assignment and to the multi-dimensional assignment problem. First, we review probabilistic and end-to-end optimization approaches to data association, followed by methods that learn association affinities from data. We then compare the performance of the methods presented in this survey and conclude by discussing future research directions.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {69},
numpages = {34},
keywords = {machine learning, deep learning, data association, Multi-object tracking}
}

@article{10.1016/j.engappai.2019.103372,
author = {Yin, Linfei and Gao, Qi and Zhao, Lulin and Zhang, Bin and Wang, Tao and Li, Shengyuan and Liu, Hui},
title = {A review of machine learning for new generation smart dispatch in power systems},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.103372},
doi = {10.1016/j.engappai.2019.103372},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
numpages = {12},
keywords = {Parallel systems, Deep neural networks, Smart dispatch, Machine learning, 99-00, 00-01}
}

@article{10.1016/j.compbiomed.2019.02.006,
author = {Muller, Emily and Shock, Jonathan P. and Bender, Andreas and Kleeberger, Julian and H\"{o}gen, Tobias and Rosenfelder, Martin and Bah, Bubacarr and Lopez-Rolon, Alex},
title = {Outcome prediction with serial neuron-specific enolase and machine learning in anoxic-ischaemic disorders of consciousness},
year = {2019},
issue_date = {Apr 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {107},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.02.006},
doi = {10.1016/j.compbiomed.2019.02.006},
journal = {Comput. Biol. Med.},
month = apr,
pages = {145–152},
numpages = {8},
keywords = {Imputation, Clinical prediction modelling, Neurological outcome, Machine learning}
}

@article{10.1016/j.compbiomed.2017.11.001,
author = {Bisaso, Kuteesa R. and Anguzu, Godwin T. and Karungi, Susan A. and Kiragga, Agnes and Castelnuovo, Barbara},
title = {A survey of machine learning applications in HIV clinical research and care},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2017.11.001},
doi = {10.1016/j.compbiomed.2017.11.001},
abstract = {A wealth of genetic, demographic, clinical and biomarker data is collected from routine clinical care of HIV patients and exists in the form of medical records available among the medical care and research communities. Machine learning (ML) methods have the ability to identify and discover patterns in complex datasets and predict future outcomes of HIV treatment. We survey published studies that make use of ML techniques in HIV clinical research and care. An advanced search relevant to the use of ML in HIV research was conducted in the PubMed biomedical database. The survey outcomes of interest include data sources, ML techniques, ML tasks and ML application paradigms.A growing trend in application of ML in HIV research was observed. The application paradigm has diversified to include practical clinical application, but statistical analysis remains the most dominant application. There is an increase in the use of genomic sources of data and high performance non-parametric ML methods with a focus on combating resistance to antiretroviral therapy (ART). There is need for improvement in collection of health records data and increased training in ML so as to translate ML research into clinical application in HIV management. There is a steady growth in the application of ML in HIV clinical research.The application paradigm has diversified to include practical clinical application but statistical analysis remains the dominant application of ML in HIV clinical research.There is an increase in use of genomic sources of data.Classification tasks utilizing high performance non-parametric ML methods are predominant.Combating resistance to ART is the commonest clinical task addressed by ML.},
journal = {Comput. Biol. Med.},
month = dec,
pages = {366–371},
numpages = {6},
keywords = {Machine learning, HIV, Clinical research, Application paradigms}
}

@article{10.1007/s00521-020-05094-0,
author = {Wei, Xiaohui and Chen, Wanling and Li, Xiao},
title = {Exploring the financial indicators to improve the pattern recognition of economic data based on machine learning},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05094-0},
doi = {10.1007/s00521-020-05094-0},
abstract = {Various economic data in the financial market need to be pattern-recognized to improve the efficiency of economic data pattern recognition, further improve the accuracy of economic-related decisions, and promote stable economic development. Based on machine learning technology, this study establishes a statistical model by establishing a multiple regression model to extract financial indicators that have significant effects on the financing trade of listed companies. Moreover, this study provides a preliminary empirical model for judging whether a company conducts financing trade based on some company’s financial indicators and uses data to verify the consistency of the model. In addition, this study conducts research and demonstration of the algorithm model of this research through empirical research. The research results show that the model shows high reliability and validity in accurately identifying whether the enterprise has the characteristics of conducting financing trade.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {723–737},
numpages = {15},
keywords = {Pattern recognition, Data mining, Economic data, Machine learning}
}

@article{10.1287/mnsc.2016.2644,
author = {Ban, Gah-Yi and El Karoui, Noureddine and Lim, Andrew E. B.},
title = {Machine Learning and Portfolio Optimization},
year = {2018},
issue_date = {March 2018},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {64},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2016.2644},
doi = {10.1287/mnsc.2016.2644},
abstract = {The portfolio optimization model has limited impact in practice because of estimation issues when applied to real data. To address this, we adapt two machine learning methods, regularization and cross-validation, for portfolio optimization. First, we introduce performance-based regularization PBR, where the idea is to constrain the sample variances of the estimated portfolio risk and return, which steers the solution toward one associated with less estimation error in the performance. We consider PBR for both mean-variance and mean-conditional value-at-risk CVaR problems. For the mean-variance problem, PBR introduces a quartic polynomial constraint, for which we make two convex approximations: one based on rank-1 approximation and another based on a convex quadratic approximation. The rank-1 approximation PBR adds a bias to the optimal allocation, and the convex quadratic approximation PBR shrinks the sample covariance matrix. For the mean-CVaR problem, the PBR model is a combinatorial optimization problem, but we prove its convex relaxation, a quadratically constrained quadratic program, is essentially tight. We show that the PBR models can be cast as robust optimization problems with novel uncertainty sets and establish asymptotic optimality of both sample average approximation SAA and PBR solutions and the corresponding efficient frontiers. To calibrate the right-hand sides of the PBR constraints, we develop new, performance-based k-fold cross-validation algorithms. Using these algorithms, we carry out an extensive empirical investigation of PBR against SAA, as well as L1 and L2 regularizations and the equally weighted portfolio. We find that PBR dominates all other benchmarks for two out of three Fama-French data sets.This paper was accepted by Yinyu Ye, optimization.},
journal = {Manage. Sci.},
month = mar,
pages = {1136–1154},
numpages = {19},
keywords = {robust optimization, regularization, portfolio optimization, machine learning, cross-validation, conditional value-at-risk}
}

@inproceedings{10.1145/3365609.3365864,
author = {Xiong, Zhaoqi and Zilberman, Noa},
title = {Do Switches Dream of Machine Learning? Toward In-Network Classification},
year = {2019},
isbn = {9781450370202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365609.3365864},
doi = {10.1145/3365609.3365864},
abstract = {Machine learning is currently driving a technological and societal revolution. While programmable switches have been proven to be useful for in-network computing, machine learning within programmable switches had little success so far. Not using network devices for machine learning has a high toll, given the known power efficiency and performance benefits of processing within the network. In this paper, we explore the potential use of commodity programmable switches for in-network classification, by mapping trained machine learning models to match-action pipelines. We introduce IIsy, a software and hardware based prototype of our approach, and discuss the suitability of mapping to different targets. Our solution can be generalized to additional machine learning algorithms, using the methods presented in this work.},
booktitle = {Proceedings of the 18th ACM Workshop on Hot Topics in Networks},
pages = {25–33},
numpages = {9},
location = {Princeton, NJ, USA},
series = {HotNets '19}
}

@article{10.1016/j.procs.2021.09.170,
author = {Shafiq, Syed Imran and Sanin, Cesar and Szczebicki, Edward},
title = {Integrating Experience-Based Knowledge Representation and Machine Learning for Efficient Virtual Engineering Object Performance},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.170},
doi = {10.1016/j.procs.2021.09.170},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {3955–3965},
numpages = {11},
keywords = {Chatbot, Decisional DNA (DDNA), Set of Experience Knowledge Structure (SOEKS), Knowledge Representation}
}

@inproceedings{10.1145/3461648.3463842,
author = {Poroor, Jayaraj and Lal, Akash and Ghanta, Sandesh},
title = {Robust I/O-compute concurrency for machine learning pipelines in constrained cyber-physical devices},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463842},
doi = {10.1145/3461648.3463842},
abstract = {Cyberphysical systems have numerous industrial and commercial applications. Such systems are often built using low-resource devices that gather and process data, using machine-learning (ML) models, to make intelligent decisions and provide value to users. Programming such low-resource devices with an impoverished system runtime is often challenging.  This paper presents a new domain-specific language called PiCon for programming ML pipelines in low-resource devices. PiCon allows safe I/O-compute concurrency, ruling out a large class of errors, while providing a simple and sequential coding abstraction to the programmer. PiCon compiles to C code and easily interfaces with existing C/C++ code. Furthermore, the generated code does not rely on multi-threading support or dynamic memory allocation, dramatically reducing its footprint on the device. We present experience porting two real-world ML applications that demonstrate simplification in programmability, in addition to several safe-by-construction guarantees.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {1–11},
numpages = {11},
keywords = {Machine Learning, IoT, Embedded systems, Cyberphysical systems, Constrained devices},
location = {Virtual, Canada},
series = {LCTES 2021}
}

@article{10.1145/3185517,
author = {Dudley, John J. and Kristensson, Per Ola},
title = {A Review of User Interface Design for Interactive Machine Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3185517},
doi = {10.1145/3185517},
abstract = {Interactive Machine Learning (IML) seeks to complement human perception and intelligence by tightly integrating these strengths with the computational power and speed of computers. The interactive process is designed to involve input from the user but does not require the background knowledge or experience that might be necessary to work with more traditional machine learning techniques. Under the IML process, non-experts can apply their domain knowledge and insight over otherwise unwieldy datasets to find patterns of interest or develop complex data-driven applications. This process is co-adaptive in nature and relies on careful management of the interaction between human and machine. User interface design is fundamental to the success of this approach, yet there is a lack of consolidated principles on how such an interface should be implemented. This article presents a detailed review and characterisation of Interactive Machine Learning from an interactive systems perspective. We propose and describe a structural and behavioural model of a generalised IML system and identify solution principles for building effective interfaces for IML. Where possible, these emergent solution principles are contextualised by reference to the broader human-computer interaction literature. Finally, we identify strands of user interface research key to unlocking more efficient and productive non-expert interactive machine learning applications.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {8},
numpages = {37},
keywords = {interface design, Interactive machine learning}
}

@article{10.5555/2567709.2502619,
author = {Niyogi, Partha},
title = {Manifold regularization and semi-supervised learning: some theoretical analyses},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or "learns" it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning.},
journal = {J. Mach. Learn. Res.},
month = may,
pages = {1229–1250},
numpages = {22},
keywords = {semi-supervised learning, minimax rates, manifold regularization, graph Laplacian}
}

@inproceedings{10.1007/978-3-030-27615-7_17,
author = {Ehrlinger, Lisa and Haunschmid, Verena and Palazzini, Davide and Lettner, Christian},
title = {A DaQL to Monitor Data Quality in Machine Learning Applications},
year = {2019},
isbn = {978-3-030-27614-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27615-7_17},
doi = {10.1007/978-3-030-27615-7_17},
abstract = {Machine learning models can only be as good as the data used to train them. Despite this obvious correlation, there is little research about data quality measurement to ensure the reliability and trustworthiness of machine learning models. Especially in industrial settings, where sensors produce large amounts of highly volatile data, a one-time measurement of the data quality is not sufficient since errors in new data should be detected as early as possible. Thus, in this paper, we present DaQL (Data Quality Library), a generally-applicable tool to continuously monitor the quality of data to increase the prediction accuracy of machine learning models. We demonstrate and evaluate DaQL within an industrial real-world machine learning application at Siemens.},
booktitle = {Database and Expert Systems Applications: 30th International Conference, DEXA 2019, Linz, Austria, August 26–29, 2019, Proceedings, Part I},
pages = {227–237},
numpages = {11},
keywords = {Data quality, Machine learning, Trust},
location = {Linz, Austria}
}

@article{10.4018/IJOCI.2017010102,
author = {Tarik, Boudheb and Mahmoud, Djelloul Daouadji and Zakaria, Elberrichi},
title = {Classifying Web Pages by Aimed Nation Using Machine Learning},
year = {2017},
issue_date = {January 2017},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1947-9344},
url = {https://doi.org/10.4018/IJOCI.2017010102},
doi = {10.4018/IJOCI.2017010102},
abstract = {Classifying web pages is to automatically assign predefined class to them. It is one of the main applications of web mining. The authors' aim is to detect the targeted nation based on the web pages content. It is an original application. In this paper, the authors propose different web mining approaches using machine learning algorithms such as Na\"{\i}ve Bayes and Support Vector Machine in order classify them. They present detailed stages of the procedure. The best experimental result based on an original corpus created by their own means shows a very attention grabbing f-score of 85%.},
journal = {Int. J. Organ. Collect. Intell.},
month = jan,
pages = {20–35},
numpages = {16},
keywords = {Web Page Classification, Web Content Mining, Text Mining, Support Vector Machine, Supervised Learning}
}

@article{10.1016/j.cose.2019.101635,
author = {Alqahtani, Fatmah H. and Alsulaiman, Fawaz A.},
title = {Is image-based CAPTCHA secure against attacks based on machine learning? An experimental study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {88},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2019.101635},
doi = {10.1016/j.cose.2019.101635},
journal = {Comput. Secur.},
month = jan,
numpages = {13},
keywords = {User authentication, Security, Machine learning, Deep learning, CAPTCHAs}
}

@inproceedings{10.1145/3473714.3473788,
author = {Zhang, Jiamin},
title = {Potential energy saving estimation for retrofit building with ASHRAE-Great Energy Predictor III using machine learning},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473788},
doi = {10.1145/3473714.3473788},
abstract = {Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.},
booktitle = {Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics},
pages = {425–429},
numpages = {5},
keywords = {retrofit building, machine learning, green architecture, energy saving},
location = {Guangzhou, China},
series = {ICCIR '21}
}

@inproceedings{10.5555/3540261.3541371,
author = {Killamsetty, Krishnateja and Zhao, Xujiang and Chen, Feng and Iyer, Rishabh},
title = {RETRIEVE: coreset selection for efficient and robust semi-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Semi-supervised learning (SSL) algorithms have had great success in recent years in limited labeled data regimes. However, the current state-of-the-art SSL algorithms are computationally expensive and entail significant compute time and energy requirements. This can prove to be a huge limitation for many smaller companies and academic groups. Our main insight is that training on a subset of unlabeled data instead of entire unlabeled data enables the current SSL algorithms to converge faster, significantly reducing computational costs. In this work, we propose RETRIEVE1, a coreset selection framework for efficient and robust semi-supervised learning. RETRIEVE selects the coreset by solving a mixed discrete-continuous bi-level optimization problem such that the selected coreset minimizes the labeled set loss. We use a one-step gradient approximation and show that the discrete optimization problem is approximately submodular, enabling simple greedy algorithms to obtain the coreset. We empirically demonstrate on several real-world datasets that existing SSL algorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve a) faster training times, b) better performance when unlabeled data consists of Out-of-Distribution (OOD) data and imbalance. More specifically, we show that with minimal accuracy degradation, RETRIEVE achieves a speedup of around 3\texttimes{} in the traditional SSL setting and achieves a speedup of 5\texttimes{} compared to state-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1110},
numpages = {14},
series = {NIPS '21}
}

@article{10.1007/s10994-018-5707-3,
author = {Muggleton, Stephen H. and Schmid, Ute and Zeller, Christina and Tamaddoni-Nezhad, Alireza and Besold, Tarek},
title = {Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP},
year = {2018},
issue_date = {July      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {7},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5707-3},
doi = {10.1007/s10994-018-5707-3},
abstract = {During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell's, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper we provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. We present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment we directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie's sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. We believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.},
journal = {Mach. Learn.},
month = jul,
pages = {1119–1140},
numpages = {22},
keywords = {Ultra-strong machine learning, Inductive logic programming, Comprehensibility}
}

@inproceedings{10.1145/3411764.3445555,
author = {G. Mitchell, Elliot and M. Heitkemper, Elizabeth and Burgermaster, Marissa and E. Levine, Matthew and Miao, Yishen and L. Hwang, Maria and M. Desai, Pooja and Cassells, Andrea and N. Tobin, Jonathan and G. Tabak, Esteban and J. Albers, David and M. Smaldone, Arlene and Mamykina, Lena},
title = {From Reflection to Action: Combining Machine Learning with Expert Knowledge for Nutrition Goal Recommendations},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445555},
doi = {10.1145/3411764.3445555},
abstract = {Self-tracking can help personalize self-management interventions for chronic conditions like type 2 diabetes (T2D), but reflecting on personal data requires motivation and literacy. Machine learning (ML) methods can identify patterns, but a key challenge is making actionable suggestions based on personal health data. We introduce GlucoGoalie, which combines ML with an expert system to translate ML output into personalized nutrition goal suggestions for individuals with T2D. In a controlled experiment, participants with T2D found that goal suggestions were understandable and actionable. A 4-week in-the-wild deployment study showed that receiving goal suggestions augmented participants’ self-discovery, choosing goals highlighted the multifaceted nature of personal preferences, and the experience of following goals demonstrated the importance of feedback and context. However, we identified tensions between abstract goals and concrete eating experiences and found static text too ambiguous for complex concepts. We discuss implications for ML-based interventions and the need for systems that offer more interactivity, feedback, and negotiation.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {206},
numpages = {17},
keywords = {Personal Informatics, Machine learning, Goal setting, Diabetes self-management},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/3504035.3504522,
author = {Rustamov, Raif M. and Klosowski, James T.},
title = {Interpretable graph-based semi-supervised learning via flows},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {In this paper, we consider the interpretability of the foundational Laplacian-based semi-supervised learning approaches on graphs. We introduce a novel flow-based learning framework that subsumes the foundational approaches and additionally provides a detailed, transparent, and easily understood expression of the learning process in terms of graph flows. As a result, one can visualize and interactively explore the precise subgraph along which the information from labeled nodes flows to an unlabeled node of interest. Surprisingly, the proposed framework avoids trading accuracy for interpretability, but in fact leads to improved prediction accuracy, which is supported both by theoretical considerations and empirical results. The flow-based framework guarantees the maximum principle by construction and can handle directed graphs in an out-of-the-box manner.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {487},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1007/978-3-030-57521-2_5,
author = {Torra, Vicen\c{c} and Navarro-Arribas, Guillermo and Galv\'{a}n, Edgar},
title = {Explaining Recurrent Machine Learning Models: Integral Privacy Revisited},
year = {2020},
isbn = {978-3-030-57520-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-57521-2_5},
doi = {10.1007/978-3-030-57521-2_5},
abstract = {We have recently introduced a privacy model for statistical and machine learning models called integral privacy. A model extracted from a database or, in general, the output of a function satisfies integral privacy when the number of generators of this model is sufficiently large and diverse. In this paper we show how the maximal c-consensus meets problem can be used to study the databases that generate an integrally private solution. We also introduce a definition of integral privacy based on minimal sets in terms of this maximal c-consensus meets problem.},
booktitle = {Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2020, Tarragona, Spain, September 23–25, 2020, Proceedings},
pages = {62–73},
numpages = {12},
keywords = {Parameter selection, Clustering, Maximal c-consensus meets, Integral privacy},
location = {Tarragona, Spain}
}

@inproceedings{10.1007/978-3-030-41418-4_19,
author = {Khan, Muhammad Jahanzeb and Wang, Ruoyu and Sun, Daniel and Li, Guoqiang},
title = {Data Provenance Based System for&nbsp;Classification and Linear Regression in&nbsp;Distributed Machine Learning},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_19},
doi = {10.1007/978-3-030-41418-4_19},
abstract = {Nowadays, data provenance is widely used to increase the accuracy of machine learning models. However, facing the difficulties in information heredity, these models produce data association. Most of the studies in the field of data provenance are focused on specific domains. And there are only a few studies on a machine learning (ML) framework with distinct emphasis on the accurate partition of coherent and physical activities with implementation of ML pipelines for provenance. This paper presents a novel approach to usage of data provenance which is also called data provenance based system for classification and linear regression in distributed machine learning (DPMLR). To develop the comprehensive approach for data analysis and visualization based on a collective set of functions for various algorithms and provide the ability to run large scale graph analysis, we apply StellarGraph as our primary ML structure. The preliminary results on the complex data stream structure showed that the overall overhead is no more than 20%. It opens up opportunities for designing an integrated system which performs dynamic scheduling and network bounded synchronization based on the ML algorithm.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {279–295},
numpages = {17},
keywords = {Distributed computing, Pipeline, StellarGraph, Machine learning, Data provenance},
location = {Shenzhen, China}
}

@article{10.1504/ijahuc.2021.119852,
author = {Reddy, K. Harinadha},
title = {Machine learning soft computing fuzzy set to identify ubiquitous integrated network state at different AM operations},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {38},
number = {4},
issn = {1743-8225},
url = {https://doi.org/10.1504/ijahuc.2021.119852},
doi = {10.1504/ijahuc.2021.119852},
abstract = {Issues arising during the operation like interruption, disorder and uncertainty of network need to be solved. Among many conditions, island state needs to be controlled for providing proper safety and security integrated energy network. Also, converter devices and human being's functional operation in a domain of integrated energy resource environment demand the safe network conditions. The proposed fuzzy sets are associated to recognise the state of the said network, and also continuously updating by the variations of voltage, frequency control parameters. This paper presents a k-nearest neighbour machine learning (knn-ML) to obtain the upper limit (UL) and lower limit (LL) of parameters for continuous fuzzy-based state estimation. In a proposed knn-ML method, effective countenance neural network training has been addressed to get better and successful recognition and identification of integrated network state. Effective outcomes of the proposed work are: 1) continuous control of parameters at every abnormal mode (AM) of network function; 2) proper updating of the control at every conditional instants of network operation. Effective outcome has been obtained with knn-ML soft computing algorithm.},
journal = {Int. J. Ad Hoc Ubiquitous Comput.},
month = jan,
pages = {219–230},
numpages = {11},
keywords = {abnormal mode, machine learning, integrated network, island state, fuzzy control, fuzzy set}
}

@inbook{10.5555/3454287.3455164,
author = {Liu, Xuanqing and Si, Si and Zhu, Xiaojin and Li, Yang and Hsieh, Cho-Jui},
title = {A unified framework for data poisoning attack to graph-based semi-supervised learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases — poisoning regression tasks under ℓ2-norm constraint and classification tasks under ℓ0-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50% error).},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {877},
numpages = {11}
}

@article{10.1007/s00466-019-01716-0,
author = {Li, Hengyang and Kafka, Orion L. and Gao, Jiaying and Yu, Cheng and Nie, Yinghao and Zhang, Lei and Tajdari, Mahsa and Tang, Shan and Guo, Xu and Li, Gang and Tang, Shaoqiang and Cheng, Gengdong and Liu, Wing Kam},
title = {Clustering discretization methods for generation of material performance databases in machine learning and design optimization},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {64},
number = {2},
issn = {0178-7675},
url = {https://doi.org/10.1007/s00466-019-01716-0},
doi = {10.1007/s00466-019-01716-0},
abstract = {Mechanical science and engineering can use machine learning. However, data sets have remained relatively scarce; fortunately, known governing equations can supplement these data. This paper summarizes and generalizes three reduced order methods: self-consistent clustering analysis, virtual clustering analysis, and FEM-clustering analysis. These approaches have two-stage structures: unsupervised learning facilitates model complexity reduction and mechanistic equations provide predictions. These predictions define databases appropriate for training neural networks. The feed forward neural network solves forward problems, e.g., replacing constitutive laws or homogenization routines. The convolutional neural network solves inverse problems or is a classifier, e.g., extracting boundary conditions or determining if damage occurs. We will explain how these networks are applied, then provide a practical exercise: topology optimization of a structure (a) with non-linear elastic material behavior and (b) under a microstructural damage constraint. This results in microstructure-sensitive designs with computational effort only slightly more than for a conventional linear elastic analysis.},
journal = {Comput. Mech.},
month = aug,
pages = {281–305},
numpages = {25},
keywords = {Reduced order modeling, Multiscale design optimization, Materials database, Machine learning, Heterogeneous materials}
}

@article{10.1007/s10723-021-09561-3,
author = {Nawrocki, Piotr and Osypanka, Patryk},
title = {Cloud Resource Demand Prediction using Machine Learning in the Context of QoS Parameters},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {1570-7873},
url = {https://doi.org/10.1007/s10723-021-09561-3},
doi = {10.1007/s10723-021-09561-3},
abstract = {Predicting demand for computing resources in any system is a vital task since it allows the optimized management of resources. To some degree, cloud computing reduces the urgency of accurate prediction as resources can be scaled on demand, which may, however, result in excessive costs. Numerous methods of optimizing cloud computing resources have been proposed, but such optimization commonly degrades system responsiveness which results in quality of service deterioration. This paper presents a novel approach, using anomaly detection and machine learning to achieve cost-optimized and QoS-constrained cloud resource configuration. The utilization of these techniques enables our solution to adapt to different system characteristics and different QoS constraints. Our solution was evaluated using a system located in Microsoft’s Azure cloud environment, and its efficiency in other providers’ computing clouds was estimated as well. Experiment results demonstrate a cost reduction ranging from 51% to 85% (for PaaS/IaaS) over the tested period.},
journal = {J. Grid Comput.},
month = jun,
numpages = {20},
keywords = {Resource cost optimization, Quality of service, Machine learning, Anomaly detection, Resource usage prediction, Cloud computing}
}

@article{10.1177/1094342019842915,
author = {Dongarra, Jack and Tourancheau, Bernard and Endrei, Mark and Jin, Chao and Dinh, Minh Ngoc and Abramson, David and Poxon, Heidi and DeRose, Luiz and de Supinski, Bronis R},
title = {Statistical and machine learning models for optimizing energy in parallel applications},
year = {2019},
issue_date = {Nov 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {33},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342019842915},
doi = {10.1177/1094342019842915},
abstract = {Rising power costs and constraints are driving a growing focus on the energy efficiency of high performance computing systems. The unique characteristics of a particular system and workload and their effect on performance and energy efficiency are typically difficult for application users to assess and to control. Settings for optimum performance and energy efficiency can also diverge, so we need to identify trade-off options that guide a suitable balance between energy use and performance. We present statistical and machine learning models that only require a small number of runs to make accurate Pareto-optimal trade-off predictions using parameters that users can control. We study model training and validation using several parallel kernels and more complex workloads, including Algebraic Multigrid (AMG), Large-scale Atomic Molecular Massively Parallel Simulator, and Livermore Unstructured Lagrangian Explicit Shock Hydrodynamics. We demonstrate that we can train the models using as few as 12 runs, with prediction error of less than 10%. Our AMG results identify trade-off options that provide up to 45% improvement in energy efficiency for around 10% performance loss. We reduce the sample measurement time required for AMG by 90%, from 13 h to 74 min.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {1079–1097},
numpages = {19},
keywords = {high performance computing, machine learning, regression modeling, performance, Energy efficiency}
}

@article{10.1007/s11277-021-08274-w,
author = {Omana, J. and Moorthi, M.},
title = {Predictive Analysis and Prognostic Approach of Diabetes Prediction with Machine Learning Techniques},
year = {2021},
issue_date = {Nov 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08274-w},
doi = {10.1007/s11277-021-08274-w},
abstract = {Medical experts indulge in numerous strategies for efficient and predictive measures to model the health status of patients and formulate the patterns that are formed in test results. Most patients would dream of their betterments of their health conditions and thus preventing the progression of any disease. When diabetics is considered in the model, or highly intervening methodology would be required for pre-diabetic individuals. Hidden Markov models have been modified into variant models to derive predictions that accurately produce expected results by investigating patterns of clinical observations from a detailed sample of patient’s dataset. There are yet unanswered and concerning challenges to derive an absolute model for predicting diabetes. The datasets from which the patterns are derived from, still holds levels of in completeness, irregularity and obvious clinical interventions during the diagnosis. The Electronic Medical Records are not furnished with all requisite information in all conditions and scenarios. Due to these irregularities prediction has become highly challenging and there is increase in misclassification rate. Newton’s Divide Difference Method (NDDM) is a conventional model for filling the irregularity in electronic datasets through divided differences. The classical approach considers a polynomial approximation approach, thus leading to Runge Phenomenon. If the interval between data fields id higher, severity of finding the irregularities is even higher. By using this type of technique it helps in improving the accuracy thereby bringing in high level prediction without any error and misclassification. In this technique proposed, a novel approximation technique is implemented using the Euclidean distance parameter over the NDDM approximation to predict the outcomes or risk of Type 2 Diabetes Mellitus among patients. Real world entities in CPCSSN are considered for this study and proposed method is tested. The proposed method filled the irregularity in the data components of EMR with better approximations and the quality of prediction has improved significantly.},
journal = {Wirel. Pers. Commun.},
month = feb,
pages = {465–478},
numpages = {14},
keywords = {Machine learning algorithm, Approximation, Sparse data handling, Type 2 diabetes mellitus, Automated modelling, Prediction, Prognostic modelling}
}

@inproceedings{10.1145/3208788.3208800,
author = {Wang, Nanzhi and Li, Lin and Xiao, Linlong and Yang, Guocai and Zhou, Yue},
title = {Outcome prediction of DOTA2 using machine learning methods},
year = {2018},
isbn = {9781450364201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208788.3208800},
doi = {10.1145/3208788.3208800},
abstract = {With the wide spreading of network and capital inflows, Electronic Sport (ES) is developing rapidly in recent years and has become a competitive sport that cannot be ignored. Compared with traditional sports, the data of this industry is large in size and has the characteristics of easy-accessing and normalization. Based on these, data mining and machine learning methods can be applied to improve players' skills and help players make strategies. In this paper, a new approach predicting the outcome of an electronic sport DOTA2 was proposed. In earlier studies, the heroes' draft of a team was represented by unit vectors or its evolution, so the complex interactions among heroes were not captured. In our approach, the outcome prediction was performed in two steps. In the first step, Heroes in DOTA2 were quantified from 17 aspects in a more accurate way. In the second step, we proposed a new method to represent a heroes' draft. A priority table of 113 heroes was created based on the prior knowledge to support this method. The evaluation indexes of several machine learning methods on this task have been compared and analyzed in this paper. Experimental results demonstrate that our method was more effective and accurate than previous methods.},
booktitle = {Proceedings of 2018 International Conference on Mathematics and Artificial Intelligence},
pages = {61–67},
numpages = {7},
keywords = {machine learning, electronic sports, DOTA2},
location = {Chengdu, China},
series = {ICMAI '18}
}

@inproceedings{10.1145/3365438.3410966,
author = {Kessentini, Wael and Alizadeh, Vahid},
title = {Interactive metamodel/model co-evolution using unsupervised learning and multi-objective search},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410966},
doi = {10.1145/3365438.3410966},
abstract = {Metamodels evolve even more frequently than programming languages. This evolution process may result in a large number of instance models that are no longer conforming to the revised metamodel. On the one hand, the manual adaptation of models after the metamodels' evolution can be tedious, error-prone, and time-consuming. On the other hand, the automated co-evolution of metamodels/models is challenging, especially when new semantics is introduced to the metamodels. While some interactive techniques have been proposed, designers still need to explore a large number of possible revised models, which makes the interaction time-consuming. In this paper, we propose an interactive multi-objective approach that dynamically adapts and interactively suggests edit operations to designers based on three objectives: minimizing the deviation with the initial model, the number of non-conformities with the revised metamodel and the number of changes. The proposed approach proposes to the user few regions of interest by clustering the set of recommended co-evolution solutions of the multi-objective search. Thus, users can quickly select their preferred cluster and give feedback on a smaller number of solutions by eliminating similar ones. This feedback is then used to guide the search for the next iterations if the user is still not satisfied. We evaluated our approach on a set of metamodel/model co-evolution case studies and compared it to existing fully automated and interactive co-evolution techniques.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {68–78},
numpages = {11},
keywords = {search based software engineering, metamodel/model co-evolution, interactive multi-objective search},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1609/aaai.v33i01.33011511,
author = {Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao},
title = {Abduction-based explanations for Machine Learning models},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33011511},
doi = {10.1609/aaai.v33i01.33011511},
abstract = {The growing range of applications of Machine Learning (ML) in a multitude of settings motivates the ability of computing small explanations for predictions made. Small explanations are generally accepted as easier for human decision makers to understand. Most earlier work on computing explanations is based on heuristic approaches, providing no guarantees of quality, in terms of how close such solutions are from cardinality- or subset-minimal explanations. This paper develops a constraint-agnostic solution for computing explanations for any ML model. The proposed solution exploits abductive reasoning, and imposes the requirement that the ML model can be represented as sets of constraints using some target constraint reasoning system for which the decision problem can be answered with some oracle. The experimental results, obtained on well-known datasets, validate the scalability of the proposed approach as well as the quality of the computed solutions.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {186},
numpages = {9},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3368235.3369376,
author = {Chen, Lydia Y.},
title = {Opportunities and Challenges for Resource Management and Machine Learning Clusters},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3369376},
doi = {10.1145/3368235.3369376},
abstract = {The practice of collecting big performance data has changed how infrastructure providers model and manage the system in the past decade. There is a methodology shift from domain-knowledge based white-box models, e.g., queueing [1] and simulation[2], to black-box data-driven models, e.g., machine learning. Such a game change for resource management from workload characterization[3], dependability prediction [4,5] to sprinting policy[6], can be seen from major IT infastructure providers, e.g., IBM and Google.While applying higher order deep neural networks show promises in predicting performance [4,5], the scalability of such an approach is often limited. A plethoral of prior work focus on deriving complex and highly accurate models, such as deep neural networks, overlooking the constraints of computation efficiency and the scalability. Their applicability on resource management problems of the production systems is thus hindered. A crucial aspect to derive accurate and scalable predictive performance models lies on leveraging the domain expertise, white-box models, and black-box models. Examples of scalable ticket management services from IBM [4] and predicting job failures [5] at Google. Model driven computation sprinting [6] dynamically scales the frequency and the allocation of computing cores based on grey box models which outperforms deep neural networks. Aforementioned case studies strongly argue for the importance of combing domain-driven and data-driven modelsAt the same time, various of acceleration techniques are developed to reduce the computation overhead of (deep) machine learning models in small scale and isolated testbed. Managing the performance of clusters that are dominated by machine learning workloads remains challenging and calls for novel solutions. SlimML [9] accelerates the ML modeli training time by only processing critical data set at a slight cost of accuracy, whereas Dias [7] simultaneously explores the data dropping and frequency sprinting for ML clusters that support multiple priorities of different training workloads. Aforementioned studies point out the complexity of managing the accuracy-efficiency tradeoff of ML jobs in a cluster-like environment where jobs interfere each other via sharing the underlying resources and common data sets.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {165–166},
numpages = {2},
keywords = {sprinting, resource management, performance models, machine learning, dirty data, cloud},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3123939.3123979,
author = {Park, Jongse and Sharma, Hardik and Mahajan, Divya and Kim, Joon Kyung and Olds, Preston and Esmaeilzadeh, Hadi},
title = {Scale-out acceleration for machine learning},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123979},
doi = {10.1145/3123939.3123979},
abstract = {The growing scale and complexity of Machine Learning (ML) algorithms has resulted in prevalent use of distributed general-purpose systems. In a rather disjoint effort, the community is focusing mostly on high performance single-node accelerators for learning. This work bridges these two paradigms and offers CoSMIC, a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale. CoSMIC enables programmers to exploit scale-out acceleration using FPGAs and Programmable ASICs (P-ASICs) from a high-level and mathematical Domain-Specific Language (DSL). Nonetheless, CoSMIC does not require programmers to delve into the onerous task of system software development or hardware design. CoSMIC achieves three conflicting objectives of efficiency, automation, and programmability, by integrating a novel multi-threaded template accelerator architecture and a cohesive stack that generates the hardware and software code from its high-level DSL. CoSMIC can accelerate a wide range of learning algorithms that are most commonly trained using parallel variants of gradient descent. The key is to distribute partial gradient calculations of the learning algorithms across the accelerator-augmented nodes of the scale-out system. Additionally, CoSMIC leverages the parallelizability of the algorithms to offer multi-threaded acceleration within each node. Multi-threading allows CoSMIC to efficiently exploit the numerous resources that are becoming available on modern FPGAs/P-ASICs by striking a balance between multi-threaded parallelism and single-threaded performance. CoSMIC takes advantage of algorithmic properties of ML to offer a specialized system software that optimizes task allocation, role-assignment, thread management, and internode communication. We evaluate the versatility and efficiency of CoSMIC for 10 different machine learning applications from various domains. On average, a 16-node CoSMIC with UltraScale+ FPGAs offers 18.8\texttimes{} speedup over a 16-node Spark system with Xeon processors while the programmer only writes 22--55 lines of code. CoSMIC offers higher scalability compared to the state-of-the-art Spark; scaling from 4 to 16 nodes with CoSMIC yields 2.7\texttimes{} improvements whereas Spark offers 1.8\texttimes{}. These results confirm that the full-stack approach of CoSMIC takes an effective and vital step towards enabling scale-out acceleration for machine learning.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {367–381},
numpages = {15},
keywords = {scale-out, machine learning, distributed, cloud, accelerator},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/3449726.3459541,
author = {Faustmann, Georg and Mrkvicka, Christoph and Musliu, Nysret and Winter, Felix},
title = {Automated configuration of parallel machine dispatching rules by machine learning},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459541},
doi = {10.1145/3449726.3459541},
abstract = {Finding optimized machine schedules is a very important task that arises in many areas of industrial manufacturing. In practice dispatching rules are often used to create schedules within short run times. This paper investigates a method for automated specification of parameters for weighted dispatching rules. We define this problem as a machine learning task and propose a novel set of features to characterize problem instances. Experimental results with a practical dispatching rule show that our approach can obtain high quality solutions in short run times.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {125–126},
numpages = {2},
keywords = {parallel machine scheduling, dispatching rules, automatic configuration},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1007/s00500-019-04200-2,
author = {Di Noia, Antonio and Martino, Alessio and Montanari, Paolo and Rizzi, Antonello},
title = {Supervised machine learning techniques and genetic optimization for occupational diseases risk prediction},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04200-2},
doi = {10.1007/s00500-019-04200-2},
abstract = {Workers healthcare gained a lot of attention recently as many countries are increasingly concerning about welfare. This paper faces the problem of predicting occupational disease risks by means of computational intelligence and pattern recognition techniques. Specifically, three different machine learning approaches are compared: the first one is based on the k-means algorithm, in charge to determine a set of meaningful labelled clusters as the final model. The latter two are based on fully supervised techniques, namely Support Vector Machines and K-Nearest Neighbours. Real data regarding both the worker and the workplace by mixing numerical and categorical attributes have been used for testing. The three approaches are automatically tuned by means of genetic algorithms in order to simultaneously find the optimal hyperparameters for the classification systems and the optimal ad-hoc dissimilarity measure weights in order to maximize the classification performances. Computational results show that the three approaches are rather comparable in terms of performances, but a clustering-based approach allows a deeper knowledge discovery phase, helpful for further risk assessment and forecasting.},
journal = {Soft Comput.},
month = mar,
pages = {4393–4406},
numpages = {14},
keywords = {Computational intelligence, Pattern recognition, Cluster analysis, Support Vector Machine, Occupational diseases risk prediction, Predictive medicine}
}

@inproceedings{10.1145/3270101.3270107,
author = {Grieco, Gustavo and Dinaburg, Artem},
title = {Toward Smarter Vulnerability Discovery Using Machine Learning},
year = {2018},
isbn = {9781450360043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270101.3270107},
doi = {10.1145/3270101.3270107},
abstract = {A Cyber Reasoning System (CRS) is designed to automatically find and exploit software vulnerabilities in complex software. To be effective, CRSs integrate multiple vulnerability detection tools (VDTs), such as symbolic executors and fuzzers. Determining which VDTs can best find bugs in a large set of target programs, and how to optimally configure those VDTs, remains an open and challenging problem. Current solutions are based on heuristics created by security analysts that rely on experience, intuition and luck. In this paper, we present Central Exploit Organizer (CEO), a proof-of-concept tool to optimize VDT selection. CEO uses machine learning to optimize the selection and configuration of the most suitable vulnerability detection tool. We show that CEO can predict the relative effectiveness of a given vulnerability detection tool, configuration, and initial input. The estimation accuracy presents an improvement between $11%$ and $21%$ over random selection. We are releasing CEO and our dataset as open source to encourage further research.},
booktitle = {Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security},
pages = {48–56},
numpages = {9},
keywords = {vulnerability management, machine learning},
location = {Toronto, Canada},
series = {AISec '18}
}

@article{10.1016/j.jcp.2021.110185,
author = {Chen, G. and Chac\'{o}n, L. and Nguyen, T.B.},
title = {An unsupervised machine-learning checkpoint-restart algorithm using Gaussian mixtures for particle-in-cell simulations},
year = {2021},
issue_date = {Jul 2021},
publisher = {Academic Press Professional, Inc.},
address = {USA},
volume = {436},
number = {C},
issn = {0021-9991},
url = {https://doi.org/10.1016/j.jcp.2021.110185},
doi = {10.1016/j.jcp.2021.110185},
journal = {J. Comput. Phys.},
month = jul,
numpages = {18},
keywords = {Checkpoint restart, Particle-in-cell, Gaussian mixture model, Unsupervised machine learning}
}

@article{10.1016/j.procs.2019.01.007,
author = {SADGALI, I. and SAEL, N. and BENABBOU, F.},
title = {Performance of machine learning techniques in the detection of financial frauds},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {148},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.01.007},
doi = {10.1016/j.procs.2019.01.007},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {45–54},
numpages = {10},
keywords = {performance, machine-learning, financial fraud, Fraud detection}
}

@article{10.4018/IJSIR.2015100103,
author = {Gonsalves, Tad},
title = {Two Diverse Swarm Intelligence Techniques for Supervised Learning},
year = {2015},
issue_date = {October 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {4},
issn = {1947-9263},
url = {https://doi.org/10.4018/IJSIR.2015100103},
doi = {10.4018/IJSIR.2015100103},
abstract = {Particle Swarm Optimization PSO and Enhanced Fireworks Algorithm EFWA are two diverse optimization techniques of the Swarm Intelligence paradigm. The inspiration of the former comes from animate swarms like those of birds and fish efficiently hunting for prey, while that of the latter comes from inanimate swarms like those of fireworks illuminating the night sky. This novel study, aimed at extending the application of these two Swarm Intelligence techniques to supervised learning, compares and contrasts their performance in training a neural network to perform the task of classification on datasets. Both the techniques are found to be speedy and successful in training the neural networks. Further, their prediction accuracy is also found to be high. Except in the case of two datasets, the training and prediction accuracies of the Enhanced Fireworks Algorithm driven neural net are found to be superior to those of the Particle Swarm Optimization driven neural net.},
journal = {Int. J. Swarm. Intell. Res.},
month = oct,
pages = {55–66},
numpages = {12},
keywords = {Swarm Intelligence, Supervised Learning, Simple Fireworks Algorithm, Particle Swarm Optimization, Optimization, Fireworks Algorithm, Enhanced Fireworks Algorithm, Classification}
}

@inproceedings{10.5555/3437539.3437567,
author = {Xu, Nuo and Liu, Qi and Liu, Tao and Liu, Zihao and Guo, Xiaochen and Wen, Wujie},
title = {Stealing your data from compressed machine learning models},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {28},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.14778/3229863.3229876,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data integration and machine learning: a natural synergy},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229876},
doi = {10.14778/3229863.3229876},
abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2094–2097},
numpages = {4}
}

@inproceedings{10.1145/3394486.3411068,
author = {Vasileva, Mariya I.},
title = {The Dark Side of Machine Learning Algorithms: How and Why They Can Leverage Bias, and What Can Be Done to Pursue Algorithmic Fairness},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3411068},
doi = {10.1145/3394486.3411068},
abstract = {Machine learning and access to big data are revolutionizing the way many industries operate, providing analytics and automation to many aspects of real-world practical tasks that were previously thought to be necessarily manual. With the pervasiveness of artificial intelligence and machine learning over the past decade, and their epidemic spread in a variety of applications, algorithmic fairness has become a prominent open research problem. For instance, machine learning is used in courts to assess the probability that a defendant recommits a crime; in the medical domain to assist with diagnosis or predict predisposition to certain diseases; in social welfare systems; and autonomous vehicles. The decision making processes in these real-world applications have a direct effect on people's lives, and can cause harm to society if the machine learning algorithms deployed are not designed with considerations to fairness.The ability to collect and analyze large datasets for problems in many domains brings forward the danger of implicit data bias, which could be harmful. Data, especially big data, is often heterogeneous, generated by different subgroups with their owncharacteristics and behaviors. Furthermore, data collection strategies vary vastly across domains, and labelling of examples is performed by human annotators, thus causing the labelling process to amplify inherent biases the annotators might harbor. A model learned on biased data may not only lead to unfair and inaccurate predictions, but also significantly disadvantage certain subgroups, and lead to unfairness in downstream learning tasks. There aremultiple ways in which discriminatory bias can seep into data: for example, in medical domains, there are many instances in whichthe data used are skewed toward certain populations-which canhave dangerous consequences for the underrepresented communities [1]. Another example are large-scale datasets widely used in machine learning tasks, like ImageNet and Open Images: [2] shows that these datasets suffer from representation bias, and advocates for the need to incorporate geo-diversity and inclusion. Yet another example are the popular face recognition and generation datasets like CelebA and Flickr-Faces-HQ, where the ethnic and racial breakdown of example faces shows significant representation bias, evident in downstream tasks like face reconstruction from an obfuscated image [8].In order to be able to fight discriminatory use of machine learning algorithms that leverage such biases, one needs to first define the notion of algorithmic fairness. Broadly, fairness is the absence of any prejudice or favoritism towards an individual or a group based on their intrinsic or acquired traits in the context of decision making [3]. Fairness definitions fall under three broad types: individual fairness (whereby similar predictions are given to similar individuals [4, 5]), group fairness (whereby different groups are treated equally [4, 5]), and subgroup fairness (whereby a group fairness constraint is being selected, and the task is to determine whether the constraint holds over a large collection of subgroups [6, 7]). In this talk, I will discuss a formal definition of these fairness constraints, examine the ways in which machine learning algorithms can amplify representation bias, and discuss how bias in both the example set and label set of popular datasets has been misused in a discriminatory manner. I will touch upon the issues of ethics and accountability, and present open research directions for tackling algorithmic fairness at the representation level.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3586–3587},
numpages = {2},
keywords = {transparency, representation learning, fairness, bias in machine learning algorithms, accountability},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1016/j.eswa.2019.01.011,
author = {Ragab, Ahmed and El Koujok, Mohamed and Ghezzaz, Hakim and Amazouz, Mouloud and Ouali, Mohamed-Salah and Yacout, Soumaya},
title = {Deep understanding in industrial processes by complementing human expertise with interpretable patterns of machine learning},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.01.011},
doi = {10.1016/j.eswa.2019.01.011},
journal = {Expert Syst. Appl.},
month = may,
pages = {388–405},
numpages = {18},
keywords = {Causality analysis, Machine learning and pattern recognition, Fault tree analysis (FTA), Logical analysis of data (LAD), Fault detection and diagnosis (FDD)}
}

@article{10.1007/s10207-017-0381-1,
author = {Gonz\'{a}lez-Serrano, Francisco-Javier and Amor-Mart\'{\i}n, Adri\'{a}n and Casamay\'{o}n-Ant\'{o}n, Jorge},
title = {Supervised machine learning using encrypted training data},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {4},
issn = {1615-5262},
url = {https://doi.org/10.1007/s10207-017-0381-1},
doi = {10.1007/s10207-017-0381-1},
abstract = {Preservation of privacy in data mining and machine learning has emerged as an absolute prerequisite in many practical scenarios, especially when the processing of sensitive data is outsourced to an external third party. Currently, privacy preservation methods are mainly based on randomization and/or perturbation, secure multiparty computations and cryptographic methods. In this paper, we take advantage of the partial homomorphic property of some cryptosystems to train simple machine learning models with encrypted data. Our basic scenario has three parties: multiple Data Owners, which provide encrypted training examples; the Algorithm Owner (or Application), which processes them to adjust the parameters of its models; and a semi-trusted third party, which provides privacy and secure computation services to the Application in some operations not supported by the homomorphic cryptosystem. In particular, we focus on two issues: the use of multiple-key cryptosystems, and the impact of the quantization of real-valued input data required before encryption. In addition, we develop primitives based on the outsourcing of a reduced set of operations that allows to implement general machine learning algorithms using efficient dedicated hardware. As applications, we consider the training of classifiers using privacy-protected data and the tracking of a moving target using encrypted distance measurements.},
journal = {Int. J. Inf. Secur.},
month = aug,
pages = {365–377},
numpages = {13},
keywords = {protection, integrity, Security, Privacy protection, Machine learning, Homomorphic encryption, Classification}
}

@inproceedings{10.1145/3460120.3485255,
author = {Coleman, Benjamin and Shrivastava, Anshumali},
title = {A One-Pass Distributed and Private Sketch for Kernel Sums with Applications to Machine Learning at Scale},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485255},
doi = {10.1145/3460120.3485255},
abstract = {Differential privacy is a compelling privacy definition that explains the privacy-utility tradeoff via formal, provable guarantees. In machine learning, we often wish to release a function over a dataset while preserving differential privacy. Although there are general algorithms to solve this problem for any function, such methods can require hours to days to run on moderately sized datasets. As a result, most private algorithms address task-dependent functions for specific applications. In this work, we propose a general purpose private sketch, or small summary of the dataset, that supports machine learning tasks such as regression, classification, density estimation, and more. Our sketch is ideal for large-scale distributed settings because it is simple to implement, mergeable, and can be created with a one-pass streaming algorithm. At the heart of our proposal is the reduction of many machine learning objectives to kernel sums. Our sketch estimates these sums using randomized contingency tables that are indexed with locality-sensitive hashing. Existing alternatives for kernel sum estimation scale poorly, often exponentially slower with an increase in dimensions. In contrast, our sketch can quickly run on large high-dimensional datasets, such as the 65 million node Friendster graph, in a single pass that takes less than 20 minutes, which is otherwise infeasible with any known alternative. Exhaustive experiments show that the privacy-utility tradeoff of our method is competitive with existing algorithms, but at an order-of-magnitude smaller computational cost. We expect that our sketch will be practically useful for differential privacy in distributed, large-scale machine learning settings.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3252–3265},
numpages = {14},
keywords = {sketching, scaling, machine learning, locality-sensitive hashing, differential privacy},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@article{10.1007/s10664-020-09864-1,
author = {Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel C. and Traynor, Michael},
title = {Automated demarcation of requirements in textual specifications: a machine learning-based approach},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09864-1},
doi = {10.1007/s10664-020-09864-1},
abstract = {A simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. In principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. However, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. The result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. This is both tedious and time-consuming. We propose an automated approach for demarcating requirements in free-form requirements specifications. The approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles. We train and evaluate our approach over an independently labeled dataset comprised of 33 industrial requirements specifications. Over this dataset, our approach yields an average precision of 81.2% and an average recall of 95.7%. Compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16.4% in precision and 25.5% in recall. We collect and analyze expert feedback on the demarcations produced by our approach for industrial requirements specifications. The results indicate that experts find our approach useful and efficient in practice. We developed a prototype tool, named DemaRQ, in support of our approach. To facilitate replication, we make available to the research community this prototype tool alongside the non-proprietary portion of our training data.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5454–5497},
numpages = {44},
keywords = {Natural language processing, Machine learning, Requirements identification and classification, Textual requirements}
}

@inproceedings{10.1145/3448016.3452787,
author = {Zhang, Hantian and Chu, Xu and Asudeh, Abolfazl and Navathe, Shamkant B.},
title = {OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452787},
doi = {10.1145/3448016.3452787},
abstract = {Machine learning (ML) is increasingly being used to make decisions in our society. ML models, however, can be unfair to certain demographic groups (e.g., African Americans or females) according to various fairness metrics. Existing techniques for producing fair ML models either are limited to the type of fairness constraints they can handle (e.g., preprocessing) or require nontrivial modifications to downstream ML training algorithms (e.g., in-processing).We propose a declarative system OmniFair for supporting group fairness in ML. OmniFair features a declarative interface for users to specify desired group fairness constraints and supports all commonly used group fairness notions, including statistical parity, equalized odds, and predictive parity. OmniFair is also model-agnostic in the sense that it does not require modifications to a chosen ML algorithm. OmniFair also supports enforcing multiple user declared fairness constraints simultaneously while most previous techniques cannot. The algorithms in OmniFair maximize model accuracy while meeting the specified fairness constraints, and their efficiency is optimized based on the theoretically provable monotonicity property regarding the trade-off between accuracy and fairness that is unique to our system.We conduct experiments on commonly used datasets that exhibit bias against minority groups in the fairness literature. We show that OmniFair is more versatile than existing algorithmic fairness approaches in terms of both supported fairness constraints and downstream ML models. OmniFair reduces the accuracy loss by up to 94.8% compared with the second best method. OmniFair also achieves similar running time to preprocessing methods, and is up to 270x faster than in-processing methods.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2076–2088},
numpages = {13},
keywords = {group fairness, declarative systems, algorithmic bias},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {vulnerable configuration, vulnerabilities, testing, reasoning, pentesting, feature model, cybersecurity},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3437984.3458838,
author = {Zhu, K. and Zhao, W.Y. and Zheng, Z. and Guo, T.Y. and Zhao, P.Z. and Bai, J.J. and Yang, J. and Liu, X.Y. and Diao, L.S. and Lin, W.},
title = {DISC: A Dynamic Shape Compiler for Machine Learning Workloads},
year = {2021},
isbn = {9781450382984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437984.3458838},
doi = {10.1145/3437984.3458838},
abstract = {Many recent machine learning models show dynamic shape characteristics. However, existing AI compiler optimization systems suffer a lot from problems brought by dynamic shape models, including compilation overhead, memory usage, optimization pipeline and deployment complexity. This paper provides a compiler system to natively support optimization for dynamic shape workloads, named DISC. DISC enriches a set of IR to form a fully dynamic shape representation. It generates the runtime flow at compile time to support processing dynamic shape based logic, which avoids the interpretation overhead at runtime and enlarges the opportunity of host-device co-optimization. It addresses the kernel fusion problem of dynamic shapes with shape propagation and constraints collecting methods. This is the first work to demonstrate how to build an end-to-end dynamic shape compiler based on MLIR infrastructure. Experiments show that DISC achieves up to 3.3\texttimes{} speedup than TensorFlow/PyTorch, and 1.8\texttimes{} than Nimble.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning and Systems},
pages = {89–95},
numpages = {7},
keywords = {machine learning, dynamic shape, AI compiler},
location = {Online, United Kingdom},
series = {EuroMLSys '21}
}

@inproceedings{10.1145/3212734.3212798,
author = {Alistarh, Dan},
title = {A Brief Tutorial on Distributed and Concurrent Machine Learning},
year = {2018},
isbn = {9781450357951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3212734.3212798},
doi = {10.1145/3212734.3212798},
abstract = {The area of machine learning has made considerable progress over the past decade, enabled by the widespread availability of large datasets, as well as by improved algorithms and models. Given the large computational demands of machine learning workloads, parallelism, implemented either through single-node concurrency or through multi-node distribution, has been a third key ingredient to advances in machine learning.The goal of this tutorial is to provide the audience with an overview of standard distribution techniques in machine learning, with an eye towards the intriguing trade-offs between synchronization and communication costs of distributed machine learning algorithms, on the one hand, and their convergence, on the other.The tutorial will focus on parallelization strategies for the fundamental stochastic gradient descent (SGD) algorithm, which is a key tool when training machine learning models, from classical instances such as linear regression, to state-of-the-art neural network architectures.The tutorial will describe the guarantees provided by this algorithm in the sequential case, and then move on to cover both shared-memory and message-passing parallelization strategies, together with the guarantees they provide, and corresponding trade-offs. The presentation will conclude with a broad overview of ongoing research in distributed and concurrent machine learning. The tutorial will assume no prior knowledge beyond familiarity with basic concepts in algebra and analysis.},
booktitle = {Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing},
pages = {487–488},
numpages = {2},
keywords = {stochastic gradient descent, shared memory, message passing, machine learning, distributed systems},
location = {Egham, United Kingdom},
series = {PODC '18}
}

@inproceedings{10.1145/3299869.3300078,
author = {Chen, Lingjiao and Koutris, Paraschos and Kumar, Arun},
title = {Towards Model-based Pricing for Machine Learning in a Data Marketplace},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300078},
doi = {10.1145/3299869.3300078},
abstract = {Data analytics using machine learning (ML) has become ubiquitous in science, business intelligence, journalism and many other domains. While a lot of work focuses on reducing the training cost, inference runtime and storage cost of ML models, little work studies how to reduce the cost of data acquisition, which potentially leads to a loss of sellers' revenue and buyers' affordability and efficiency. In this paper, we propose a model-based pricing (MBP) framework, which instead of pricing the data, directly prices ML model instances. We first formally describe the desired properties of the MBP framework, with a focus on avoiding arbitrage. Next, we show a concrete realization of the MBP framework via a noise injection approach, which provably satisfies the desired formal properties. Based on the proposed framework, we then provide algorithmic solutions on how the seller can assign prices to models under different market scenarios (such as to maximize revenue). Finally, we conduct extensive experiments, which validate that the MBP framework can provide high revenue to the seller, high affordability to the buyer, and also operate on low runtime cost.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1535–1552},
numpages = {18},
keywords = {pricing, mechanism design, machine learning, data market},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3447548.3467413,
author = {LI, Qimai and Zhang, Xiaotong and Liu, Han and Dai, Quanyu and Wu, Xiao-Ming},
title = {Dimensionwise Separable 2-D Graph Convolution for Unsupervised and Semi-Supervised Learning on Graphs},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467413},
doi = {10.1145/3447548.3467413},
abstract = {Graph convolutional neural networks (GCN) have been the model of choice for graph representation learning, which is mainly due to the effective design of graph convolution that computes the representation of a node by aggregating those of its neighbors. However, existing GCN variants commonly use 1-D graph convolution that solely operates on the object link graph without exploring informative relational information among object attributes. This significantly limits their modeling capability and may lead to inferior performance on noisy and sparse real-world networks. In this paper, we explore 2-D graph convolution to jointly model object links and attribute relations for graph representation learning. Specifically, we propose a computationally efficient dimensionwise separable 2-D graph convolution (DSGC) for filtering node features. Theoretically, we show that DSGC can reduce intra-class variance of node features on both the object dimension and the attribute dimension to learn more effective representations. Empirically, we demonstrate that by modeling attribute relations, DSGC achieves significant performance gain over state-of-the-art methods for node classification and clustering on a variety of real-world networks. The source code for reproducing the experimental results is available at https://github.com/liqimai/DSGC.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {953–963},
numpages = {11},
keywords = {variance reduction, node clustering, node classification, graph convolution},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@book{10.5555/2930837,
author = {Sugiyama, Masashi},
title = {Introduction to Statistical Machine Learning},
year = {2015},
isbn = {9780128023501},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. Introduction to Statistical Machine Learning provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks. Provides the necessary background material to understand machine learning such as statistics, probability, linear algebra, and calculus. Complete coverage of the generative approach to statistical pattern recognition and the discriminative approach to statistical machine learning. Includes MATLAB/Octave programs so that readers can test the algorithms numerically and acquire both mathematical and practical skills in a wide range of data analysis tasks Discusses a wide range of applications in machine learning and statistics and provides examples drawn from image processing, speech processing, natural language processing, robot control, as well as biology, medicine, astronomy, physics, and materials. Table of Contents Part I: Introduction to Statistics and Probability 1. Random variables and probability distributions 2. Examples of discrete probability distributions 3. Examples of continuous probability distributions 4. Multi-dimensional probability distributions 5. Examples of multi-dimensional probability distributions 6. Random sample generation from arbitrary probability distributions 7. Probability distributions of the sum of independent random variables 8. Probability inequalities 9. Statistical inference 10. Hypothesis testing Part II: Generative Approach to Statistical Pattern Recognition 11. Fundamentals of statistical pattern recognition 12. Criteria for developing classifiers 13. Maximum likelihood estimation 14. Theoretical properties of maximum likelihood estimation 15. Linear discriminant analysis 16. Model selection for maximum likelihood estimation 17. Maximum likelihood estimation for Gaussian mixture model 18. Bayesian inference 19. Numerical computation in Bayesian inference 20. Model selection in Bayesian inference 21. Kernel density estimation 22. Nearest neighbor density estimation Part III: Discriminative Approach to Statistical Machine Learning 23. Fundamentals of statistical machine learning 24. Learning Models 25. Least-squares regression 26. Constrained least-squares regression 27. Sparse regression 28. Robust regression 29. Least-squares classification 30. Support vector classification 31. Ensemble classification 32. Probabilistic classification 33. Structured classification Part IV: Further Topics 34. Outlier detection 35. Unsupervised dimensionality reduction 36. Clustering 37. Online learning 38. Semi-supervised learning 39. Supervised dimensionality reduction 40. Transfer learning 41. Multi-task learning}
}

@inproceedings{10.1145/3314344.3332484,
author = {Ye, Teng and Johnson, Rebecca and Fu, Samantha and Copeny, Jerica and Donnelly, Bridgit and Freeman, Alex and Lima, Mirian and Walsh, Joe and Ghani, Rayid},
title = {Using machine learning to help vulnerable tenants in New York city},
year = {2019},
isbn = {9781450367141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314344.3332484},
doi = {10.1145/3314344.3332484},
abstract = {To keep housing affordable, the City of New York has implemented rent-stabilization policies to restrict the rate at which the rent of certain units can be increased every year. However, some landlords of these rent-stabilized units try to illegally force their tenants out in order to circumvent rent-stabilization laws and greatly increase the rent they can charge. To identify and help tenants who are vulnerable to such landlord harassment, the New York City Public Engagement Unit (NYC PEU) conducts targeted outreach to tenants to inform them of their rights and to assist them with serious housing challenges. In this paper, we1 collaborated with NYC PEU to develop machine learning models to better prioritize outreach and help to vulnerable tenants. Our best-performing model can potentially help TSU find 59% more buildings where tenants face landlord harassment than the current outreach method using the same resources. The results also highlight the factors that help predict the risk of experiencing tenant harassment, and provide a data-driven and comprehensive approach to improve the city's policy of proactive outreach to vulnerable tenants.},
booktitle = {Proceedings of the 2nd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {248–258},
numpages = {11},
keywords = {tenant harassment, social good, resource allocation, public policy, machine learning},
location = {Accra, Ghana},
series = {COMPASS '19}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00042,
author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
title = {Software engineering for machine learning: a case study},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00042},
doi = {10.1109/ICSE-SEIP.2019.00042},
abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components --- models may be "entangled" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {291–300},
numpages = {10},
keywords = {software engineering, process, data, AI},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1109/ICSE-Companion.2019.00096,
author = {Cheng, Liang and Zhang, Yang and Zhang, Yi and Wu, Chen and Li, Zhangtan and Fu, Yu and Li, Haisheng},
title = {Optimizing seed inputs in fuzzing with machine learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00096},
doi = {10.1109/ICSE-Companion.2019.00096},
abstract = {The success of a fuzzing campaign is heavily depending on the quality of seed inputs used for test generation. It is however challenging to compose a corpus of seed inputs that enable high code and behavior coverage of the target program, especially when the target program requires complex input formats such as PDF files. We present a machine learning based framework to improve the quality of seed inputs for fuzzing programs that take PDF files as input. Given an initial set of seed PDF files, our framework utilizes a set of neural networks to 1) discover the correlation between these PDF files and the execution in the target program, and 2) leverage such correlation to generate new seed files that more likely explore new paths in the target program. Our experiments on a set of widely used PDF viewers demonstrate that the improved seed inputs produced by our framework could significantly increase the code coverage of the target program and the likelihood of detecting program crashes.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {244–245},
numpages = {2},
keywords = {test case generation, recurrent neural networks, machine learning, fuzzing},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3140649.3140655,
author = {Hesamifard, Ehsan and Takabi, Hassan and Ghasemi, Mehdi and Jones, Catherine},
title = {Privacy-preserving Machine Learning in Cloud},
year = {2017},
isbn = {9781450352048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140649.3140655},
doi = {10.1145/3140649.3140655},
abstract = {Machine learning algorithms based on deep neural networks (NN) have achieved remarkable results and are being extensively used in different domains. On the other hand, with increasing growth of cloud services, several Machine Learning as a Service (MLaaS) are offered where training and deploying machine learning models are performed on cloud providers' infrastructure. However, machine learning algorithms require access to raw data which is often privacy sensitive and can create potential security and privacy risks. To address this issue, we develop new techniques to provide solutions for applying deep neural network algorithms to the encrypted data. In this paper, we show that it is feasible and practical to train neural networks using encrypted data and to make encrypted predictions, and also return the predictions in an encrypted form. We demonstrate applicability of the proposed techniques and evaluate its performance. The empirical results show that it provides accurate privacy-preserving training and classification.},
booktitle = {Proceedings of the 2017 on Cloud Computing Security Workshop},
pages = {39–43},
numpages = {5},
keywords = {privacy preserving, machine learning, homomorphic encryption},
location = {Dallas, Texas, USA},
series = {CCSW '17}
}

@inproceedings{10.1145/3035918.3054775,
author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
title = {Data Management in Machine Learning: Challenges, Techniques, and Systems},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054775},
doi = {10.1145/3035918.3054775},
abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1717–1722},
numpages = {6},
keywords = {machine learning, data management},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@article{10.1016/j.patrec.2016.02.001,
title = {Iris recognition through machine learning techniques},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {P2},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2016.02.001},
doi = {10.1016/j.patrec.2016.02.001},
abstract = {We provide a review of iris biometrics with machine learning techniques.We propose a taxonomy of machine learning techniques for iris recognition.We mention approaches ranging from neural networks to deep learning.We discuss some aspects related to the mentioned methods. Iris recognition is one of the most promising fields in biometrics. Notwithstanding this, there are not so many research works addressing it by machine learning techniques. In this survey, we especially focus on recognition, and leave the detection and feature extraction problems in the background. However, the kind of features used to code the iris pattern may significantly influence the complexity of the methods and their performance. In other words, complexity affects learning, and iris patterns require relatively complex feature vectors, even if their size can be optimized. A cross-comparison of these two parameters, feature complexity vs. learning effectiveness, in the context of different learning algorithms, would require an unbiased common benchmark. Moreover, at present it is still very difficult to reproduce techniques and experiments due to the lack of either sufficient implementation details or reliable shared code.},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {106–115},
numpages = {10}
}

@inproceedings{10.1007/978-3-030-61705-9_52,
author = {Villar, Jos\'{e} R. and Villar, Mario and Fa\~{n}ez, Mirko and de la Cal, Enrique and Sedano, Javier},
title = {Fall Detection Based on Local Peaks and&nbsp;Machine Learning},
year = {2020},
isbn = {978-3-030-61704-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61705-9_52},
doi = {10.1007/978-3-030-61705-9_52},
abstract = {This research focuses on Fall Detection (FD) using on-wrist wearable devices including tri-axial accelerometers performing FD autonomously. This type of approaches makes use of an event detection stage followed by some pre-processing and a final classification stage. The event detection stage is basically performed using thresholds or a combination of thresholds and finite state machines. In this research, we extend our previous work and propose an event detection method free of thresholds to tune or adapt to the user that reduces the number of false alarms; we also consider a mixture between the two approaches. Additionally, a set of features is proposed as an alternative to those used in previous research. The classification of the samples is performed using a Deep Learning Neural Network and the experimentation performs a comparison of this research to a published and well-known technique using the UMA Fall, one of the publicly available simulated fall detection data sets. Results show the improvements in the event detection using the new proposals.},
booktitle = {Hybrid Artificial Intelligent Systems: 15th International Conference, HAIS 2020, Gij\'{o}n, Spain, November 11-13, 2020, Proceedings},
pages = {631–643},
numpages = {13},
location = {Gij\'{o}n, Spain}
}

@article{10.1016/j.aei.2019.101027,
author = {Trappey, Amy J.C. and Trappey, Charles V. and Wu, Jheng-Long and Wang, Jack W.C.},
title = {Intelligent compilation of patent summaries using machine learning and natural language processing techniques},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2019.101027},
doi = {10.1016/j.aei.2019.101027},
journal = {Adv. Eng. Inform.},
month = jan,
numpages = {13},
keywords = {Patent analysis, Deep learning, Natural language processing, Machine learning, Artificial intelligence}
}

@inproceedings{10.5555/3408352.3408648,
author = {Cerina, L. and Santambrogio, M. D. and Franco, G. and Gallicchio, C. and Micheli, A.},
title = {Efficient embedded machine learning applications using echo state networks},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {The increasing role of Artificial Intelligence (AI) and Machine Learning (ML) in our lives brought a paradigm shift on how and where the computation is performed. Stringent latency requirements and congested bandwidth moved AI inference from Cloud space towards end-devices. This change required a major simplification of Deep Neural Networks (DNN), with memory-wise libraries or co-processors that perform fast inference with minimal power. Unfortunately, many applications such as natural language processing, time-series analysis and audio interpretation are built on a different type of Artifical Neural Networks (ANN), the so-called Recurrent Neural Networks (RNN), which, due to their intrinsic architecture, remains too complex and heavy to run efficiently on embedded devices. To solve this issue, the Reservoir Computing paradigm proposes sparse untrained nonlinear networks, the Reservoir, that can embed temporal relations without some of the hindrances of Recurrent Neural Networks training, and with a lower memory usage. Echo State Networks (ESN) and Liquid State Machines are the most notable examples. In this scenario, we propose a performance comparison of a ESN, designed and trained using Bayesian Optimization techniques, against current RNN solutions. We aim to demonstrate that ESN have comparable performance in terms of accuracy, require minimal training time, and they are more optimized in terms of memory usage and computational efficiency. Preliminary results show that ESN are competitive with RNN on a simple benchmark, and both training and inference time are faster, with a maximum speed-up of 2.35x and 6.60x, respectively.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1299–1302},
numpages = {4},
location = {Grenoble, France},
series = {DATE '20}
}

@article{10.5555/3122009.3242010,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic differentiation in machine learning: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5595–5637},
numpages = {43},
keywords = {differentiable programming, backpropagation}
}

@inproceedings{10.1007/978-3-030-62362-3_3,
author = {Shalit, Nadav and Fire, Michael and Ben-Elia, Eran},
title = {Imputation of Missing Boarding Stop Information in Smart Card Data with Machine Learning Methods},
year = {2020},
isbn = {978-3-030-62361-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62362-3_3},
doi = {10.1007/978-3-030-62362-3_3},
abstract = {With the increase in population densities and environmental awareness, public transport has become an important aspect of urban life. Consequently, large quantities of transportation data are generated, and mining data from smart card use has become a standardized method to understand the travel habits of passengers.Increase in available data and computation power demands more sophisticated methods to analyze big data. Public transport datasets, however, often lack data integrity. Boarding stop information may be missing either due to imperfect acquirement processes or inadequate reporting. As a result, large quantities of observations and even complete sections of cities might be absent from the smart card database. We have developed a machine (supervised) learning method to impute missing boarding stops based on ordinal classification. In addition, we present a new metric, Pareto Accuracy, to evaluate algorithms where classes have an ordinal nature. Results are based on a case study in the city of Beer Sheva utilizing one month of data. We show that our proposed method significantly outperforms schedule-based imputation methods and can improve the accuracy and usefulness of large-scale transportation data. The implications for data imputation of smart card information is further discussed.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2020: 21st International Conference, Guimaraes, Portugal, November 4–6, 2020, Proceedings, Part I},
pages = {17–27},
numpages = {11},
keywords = {Machine learning, Smart card, Boarding stop imputation},
location = {Guimaraes, Portugal}
}

@article{10.1155/2019/4708201,
author = {Thang, Vu Viet and Pashchenko, F. F. and Lashkari, Arash H.},
title = {Multistage System-Based Machine Learning Techniques for Intrusion Detection in WiFi Network},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {2090-7141},
url = {https://doi.org/10.1155/2019/4708201},
doi = {10.1155/2019/4708201},
abstract = {The aim of machine learning is to develop algorithms that can learn from data and solve specific problems in some context as human do. This paper presents some machine learning models applied to the intrusion detection system in WiFi network. Firstly, we present an incremental semisupervised clustering based on a graph. Incremental clustering or one-pass clustering is very useful when we work with data stream or dynamic data. In fact, for traditional clustering such as K-means, Fuzzy C-Means, DBSCAN, etc., many versions of incremental clustering have been developed. However, to the best of our knowledge, there is no incremental semisupervised clustering in the literature. Secondly, by combining a K-means algorithm and a measure of local density score, we propose a fast outlier detection algorithm, named FLDS. The complexity of FLDS is On1.5 while the results obtained are comparable with the algorithm LOF. Thirdly, we introduce a multistage system-based machine learning techniques for mining the intrusion detection data applied for the 802.11 WiFi network. Finally, experiments conducted on some data sets extracted from the 802.11 networks and UCI data sets show the effectiveness of our new proposed methods.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@article{10.3233/JIFS-189463,
author = {Zhu, Bin and Zhou, Jie and Satapathy, Suresh Chandra and Agrawal, Rashmi and Garc\'{\i}a D\'{\i}az, Vicente},
title = {Virtual design of urban planning based on GIS big data and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189463},
doi = {10.3233/JIFS-189463},
abstract = {In order to build a virtual urban planning model and improve the effect of urban planning, this paper builds a virtual urban planning design model based on GIS big data technology and machine learning algorithms, and proposes a solution that combines multiple features. With the development of polarized SAR in the direction of high resolution, a single feature often cannot fully express the detailed information of ground objects, resulting in poor classification results and low accuracy. The combination of multiple features can express feature information well. In addition, this paper uses the ELM method to plan SAR ground object classification, uses an extreme learning machine classification algorithm with fast learning speed and good classification effect, and uses ELM as a classifier. Finally, this paper designs experiments to explore the performance of the model constructed in this paper from two aspects: detection accuracy and planning score. The research results show that the model constructed in this paper meets the expected goals.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6263–6273},
numpages = {11},
keywords = {GIS, big data, machine learning, urban planning}
}

@article{10.1504/ijict.2021.115594,
author = {Wang, Hui},
title = {Research on personalised recommendation method of popular tourist attractions routes based on machine learning},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {4},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2021.115594},
doi = {10.1504/ijict.2021.115594},
abstract = {In view of the problems existing in the current tourist route recommendation methods, this paper proposes a personalised tourist attraction route recommendation method based on machine learning. The tourism heterogeneous information data is collected by using HTML parser and the captured data are used as machine learning training samples. The characteristics and user interest characteristics under the rating were extracted, and the target user interest characteristics were taken as the starting point, combining the features of scenic spots and tourist routes. Recommend scenic spots to users to realise personalised recommendation of popular scenic spots. The experimental results show that the method proposed in this paper has a recommendation accuracy of more than 95%, a recommendation time of between 115 ms-130 ms and a user satisfaction rate of over 89%. With high recommendation accuracy and user satisfaction, and less time to calculate the recommendation, it is a reliable method to recommend tourist routes.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {449–463},
numpages = {14},
keywords = {recommendation, tourist attractions routes, machine learning}
}

@inproceedings{10.1145/3377929.3389886,
author = {Picek, Stjepan and Jakobovic, Domagoj},
title = {Evolutionary computation and machine learning in cryptology},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389886},
doi = {10.1145/3377929.3389886},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1147–1173},
numpages = {27},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1007/978-3-030-58449-8_1,
author = {Belle, Vaishak},
title = {Symbolic Logic Meets Machine Learning: A Brief Survey in Infinite Domains},
year = {2020},
isbn = {978-3-030-58448-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58449-8_1},
doi = {10.1007/978-3-030-58449-8_1},
abstract = {The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence (AI). The deduction camp concerns itself with questions about the expressiveness of formal languages for capturing knowledge about the world, together with proof systems for reasoning from such knowledge bases. The learning camp attempts to generalize from examples about partial descriptions about the world. In AI, historically, these camps have loosely divided the development of the field, but advances in cross-over areas such as statistical relational learning, neuro-symbolic systems, and high-level control have illustrated that the dichotomy is not very constructive, and perhaps even ill-formed.In this article, we survey work that provides further evidence for the connections between logic and learning. Our narrative is structured in terms of three strands: logic versus learning, machine learning for logic, and logic for machine learning, but naturally, there is considerable overlap. We place an emphasis on the following “sore” point: there is a common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties. We report on results that challenge this view on the limitations of logic, and expose the role that logic can play for learning in infinite domains.},
booktitle = {Scalable Uncertainty Management: 14th International Conference, SUM 2020, Bozen-Bolzano, Italy, September 23–25, 2020, Proceedings},
pages = {3–16},
numpages = {14},
location = {Bozen-Bolzano, Italy}
}

@article{10.1016/j.micpro.2021.103953,
author = {Pandiaraj, K. and Sivakumar, P. and Prakash, K. Jeya},
title = {Machine learning based effective linear regression model for TSV layer assignment in 3DIC},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {83},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103953},
doi = {10.1016/j.micpro.2021.103953},
journal = {Microprocess. Microsyst.},
month = jun,
numpages = {10},
keywords = {Tsv, through silicon via, Ic- integrated circuit, Elrm- efficient linear regression model. ml- machine learning}
}

@inproceedings{10.1145/3009977.3010019,
author = {Chahal, Nidhi and Pippal, Meghna and Chaudhury, Santanu},
title = {Depth estimation from single image using machine learning techniques},
year = {2016},
isbn = {9781450347532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009977.3010019},
doi = {10.1145/3009977.3010019},
abstract = {In this paper, the problem of depth estimation from single monocular image is considered. The depth cues such as motion, stereo correspondences are not present in single image which makes the task more challenging. We propose a machine learning based approach for extracting depth information from single image. The deep learning is used for extracting features, then, initial depths are generated using manifold learning in which neighborhood preserving embedding algorithm is used. Then, fixed point supervised learning is applied for sequential labeling to obtain more consistent and accurate depth maps. The features used are initial depths obtained from manifold learning and various image based features including texture, color and edges which provide useful information about depth. A fixed point contraction mapping function is generated using which depth map is predicted for new structured input image. The transfer learning approach is also used for improvement in learning in a new task through the transfer of knowledge from a related task that has already been learned. The predicted depth maps are reliable, accurate and very close to ground truth depths which is validated using objective measures: RMSE, PSNR, SSIM and subjective measure: MOS score.},
booktitle = {Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {19},
numpages = {8},
keywords = {sequential labeling, monocular, manifold},
location = {Guwahati, Assam, India},
series = {ICVGIP '16}
}

@phdthesis{10.5555/AAI28263193,
author = {Krishnan, Vaishakh and Dhanak, Manhar and VanZwieten, James and Motta, Francis and Moslemian, Davood and Mireles-James, Jason},
advisor = {Gopal, Gaonkar,},
title = {Statistical Modeling of Ship Airwakes Including the Feasibility of Applying Machine Learning},
year = {2020},
isbn = {9798557036061},
publisher = {Florida Atlantic University},
address = {USA},
abstract = {Airwakes are shed behind the ship's superstructure and represent a highly turbulent and rapidly distorting flow field. This flow field severely affects pilot's workload and such helicopter shipboard operations. It requires both the one-point statistics of autospectrum and the two-point statistics of coherence (normalized cross-spectrum) for a relatively complete description. Recent advances primarily refer to generating databases of flow velocity points through experimental and computational fluid dynamics (CFD) investigations, numerically computing autospectra along with a few cases of cross-spectra and coherences, and developing a framework for extracting interpretive models of autospectra in closed form from a database along with an application of this framework to study the downwash effects. By comparison, relatively little is known about coherences. In fact, even the basic expressions of cross-spectra and coherences for three components of homogeneous isotropic turbulence (HIT) vary from one study to the other, and the related literature is scattered and piecemeal. Accordingly, this dissertation begins with a unified account of all the cross-spectra and coherences of HIT from first principles. Then, it presents a framework for constructing interpretive coherence models of airwake from a database on the basis of perturbation theory. For each velocity component, the coherence is represented by a separate perturbation series in which the basis function or the first term on the right-hand side of the series is represented by the corresponding coherence for HIT. The perturbation series coefficients are evaluated by satisfying the theoretical constraints and fitting a curve in a least squares sense on a set of numerically generated coherence points from a database. Although not tested against a specific database, the framework has a mathematical basis. Moreover, for assumed values of perturbation series constants, coherence results are presented to demonstrate how coherences of airwakes and such flow fields compare to those of HIT. Finally, the dissertation focuses on applying the machine learning approaches based on neural networks to an earlier developed algorithm for extracting autospectra in closed form with optimal usage of the database. To this end, the in situ airwake flow velocity points measured from the Navy YP676 ship are used for training and testing the neural network. The strengths and weaknesses of this machine learning approach are presented as well.},
note = {AAI28263193}
}

@article{10.1007/s00521-018-3582-2,
author = {Hou, Shudong and Liu, Heng and Sun, Quansen},
title = {Sparse regularized discriminative canonical correlation analysis for multi-view semi-supervised learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3582-2},
doi = {10.1007/s00521-018-3582-2},
abstract = {For multi-view data representation learning, recently the traditional unsupervised CCA method has been converted to supervised ways by introducing label information from samples. However, such supervised CCA variants require large numbers of labeled samples which hampers its practical application. In this paper, in order to mine the most discriminant information only from a few labeled samples, inspired by sparse representation we propose a novel sparse regularized discriminative CCA method to make use of the label information as much as possible. Through constructing sparse weighted matrices in multiple views, we incorporate the structure information into the original CCA framework to extract fused multi-view features which not only are the most correlated but also carry the important discriminative structure information. Our approach is evaluated on both handwritten dataset and face dataset. The experimental results and the comparisons with other related algorithms demonstrate its effectiveness and superiority.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7351–7359},
numpages = {9},
keywords = {Feature extraction, Dimension reduction, Semi-supervised learning, Sparse representation, Canonical correlation analysis}
}

@article{10.1007/s00521-019-04051-w,
author = {Vellido, Alfredo},
title = {The importance of interpretability and visualization in machine learning for applications in medicine and health care},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04051-w},
doi = {10.1007/s00521-019-04051-w},
abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {18069–18083},
numpages = {15},
keywords = {Health care, Medicine, Visualization, Machine learning, Explainability, Interpretability}
}

@article{10.1109/TASLP.2019.2921890,
author = {Jati, Arindam and Georgiou, Panayiotis},
title = {Neural Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning of Speaker Characteristics},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921890},
doi = {10.1109/TASLP.2019.2921890},
abstract = {Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding NPC, to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce “speaker embeddings” by learning to separate “same” versus “different” speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1577–1589},
numpages = {13}
}

@article{10.1016/j.procs.2020.03.240,
author = {Kumar, Raghavendra and Kumar, Pardeep and Kumar, Yugal},
title = {Time Series Data Prediction using IoT and Machine Learning Technique},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.240},
doi = {10.1016/j.procs.2020.03.240},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {373–381},
numpages = {9},
keywords = {Time series, Regression Model, ARIMA, Machine Learning}
}

@phdthesis{10.5555/AAI28548896,
author = {Dong, Weizhen and Bharat, Balasubramanian, and Fei, Hu, and Tim, Haskew, and Yu, Gan,},
advisor = {Shuhui, Li,},
title = {Artificial Intelligence and Machine Learning for Control and Operation of Electric Vehicles and Machine Drives},
year = {2021},
isbn = {9798544234470},
publisher = {The University of Alabama},
abstract = {Motor drive and charging system with batteries are two major parts in an electric vehicle (EV) powertrain system. This dissertation investigates the artificial intelligence-based control and operation of EV machine drives and the charging systems.There are several major challenges related the EV motor drive control such as machine parameter variations, magnetic saturations, accurate torque control, and optimal and efficient operation considering copper loss and iron loss. Regarding the charging and discharging control with DC/DC converters, the stable and robust voltage regulation under disturbances is required. The issues of how to smoothly handle the current/voltage constraints and the power limit still remain. This dissertation presents a novel machine learning strategy based on a neural network (NN) to achieve MTPA, flux-weakening, and MTPV for the most efficient IPM torque control over its full speed operating range. The NN is trained offline by using the LMBP (Levenberg-Marquardt backpropagation) algorithm, which avoids the disadvantages associated with the online NN training. A special technique is developed to generate NN training data, that is particularly suitable and favorable, to develop a high-performance NN-based IPM torque control system, and the impact of variable motor parameters is embedded into the NN system development and training. IPM machine modeling and parameter estimation are important for the controller design of high-efficient and high-performance motor drives. The accuracy of the magnetic modeling is a challenge dur to the magnetic saturation, cross saturation, iron loss, and temperature variations. The proposed ANN-based modeling method can capture the nonlinear areas of the model and generate accurate dq-axis flux linkages with saturation and iron loss considered. For the vehicle to grid (V2G) and vehicle to home (V2H) applications, the battery not only can be charged but also can provide power back to the load and systems through DC/DC converters. The ANN controller presented in this dissertation has a strong ability to track rapidly changing reference commands, maintain stable output voltage for a variable load, and manage maximum duty-ratio and current constraints properly. The presented control algorithm also has the ability of power sharing based on DG capabilities for DC microgrid applications.},
note = {AAI28548896}
}

@inproceedings{10.5555/3045390.3045396,
author = {Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
title = {Revisiting semi-supervised learning with graph embeddings},
year = {2016},
publisher = {JMLR.org},
abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {40–48},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3463914.3463922,
author = {Miao, Yuxin},
title = {Using Machine Learning Algorithms to Predict Diabetes Mellitus Based on PIMA Indians Diabetes Dataset},
year = {2021},
isbn = {9781450389327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463914.3463922},
doi = {10.1145/3463914.3463922},
abstract = {Currently, there are still a great amount of people suffering from diabetes mellitus (DM). Although advanced facilities and technologies could support the diagnosis of diabetes, complicated procedures are not supposed to be neglected. Actually, the causes of diabetes are various, involving glucose, blood pressure, skin thickness, insulin, BMI and age. Hence, in this study, a variety of machine learning algorithms are applied on PIMA Indians Diabetes dataset (PIDD) to construct the prediction model with higher accuracy. Once the model could be trained with better accuracy, it is possible to diagnose diabetes or even other ailments in the future. According to the results of this research, glucose, insulin and BMI have a higher correlation with diabetes. By comparison, the support vector machine (SVM) will obtain the highest accuracy of 85.06% based on the standardized data. After adjustment, this SVM still predicts the diagnosis of DM with the highest accuracy of 87.01%. In this paper, it seems that the Support Vector Classifier is demonstrated to obtain the highest accuracy for PIMA Indians Diabetes dataset (PIDD).},
booktitle = {Proceedings of the 2021 5th International Conference on Virtual and Augmented Reality Simulations},
pages = {47–53},
numpages = {7},
keywords = {medical diagnosis, SVM, PIMA Indians Diabetes dataset, Machine learning, Logistic Regression, KNN},
location = {Melbourne, VIC, Australia},
series = {ICVARS '21}
}

@inproceedings{10.1007/978-3-030-29400-7_1,
author = {Netti, Alessio and Kiziltan, Zeynep and Babaoglu, Ozalp and S\^{\i}rbu, Alina and Bartolini, Andrea and Borghesi, Andrea},
title = {Online Fault Classification in HPC Systems Through Machine Learning},
year = {2019},
isbn = {978-3-030-29399-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29400-7_1},
doi = {10.1007/978-3-030-29400-7_1},
abstract = {As High-Performance Computing (HPC) systems strive towards the exascale goal, studies suggest that they will experience excessive failure rates. For this reason, detecting and classifying faults in HPC systems as they occur and initiating corrective actions before they can transform into failures will be essential for continued operation. In this paper, we propose a fault classification method for HPC systems based on machine learning that has been designed specifically to operate with live streamed data. We cast the problem and its solution within realistic operating constraints of online use. Our results show that almost perfect classification accuracy can be reached for different fault types with low computational overhead and minimal delay. We have based our study on a local dataset, which we make publicly available, that was acquired by injecting faults to an in-house experimental HPC system.},
booktitle = {Euro-Par 2019: Parallel Processing: 25th International Conference on Parallel and Distributed Computing, G\"{o}ttingen, Germany, August 26–30, 2019, Proceedings},
pages = {3–16},
numpages = {14},
keywords = {Machine learning, Fault detection, Monitoring, Resiliency, Exascale systems, High-performance computing},
location = {G\"{o}ttingen, Germany}
}

@inproceedings{10.1145/3465416.3483291,
author = {Nyarko, Julian and Goel, Sharad and Sommers, Roseanna},
title = {Breaking Taboos in Fair Machine Learning: An Experimental Study},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483291},
doi = {10.1145/3465416.3483291},
abstract = {Many scholars, engineers, and policymakers believe that algorithmic fairness requires disregarding information about certain characteristics of individuals, such as their race or gender. Often, the mandate to “blind” algorithms in this way is conveyed as an unconditional ethical imperative—a minimal requirement of fair treatment—and any contrary practice is assumed to be morally and politically untenable. However, in some circumstances, prohibiting algorithms from considering information about race or gender can in fact lead to worse outcomes for racial minorities and women, complicating the rationale for blinding. In this paper, we conduct a series of randomized studies to investigate attitudes toward blinding algorithms, both among the general public as well as among computer scientists and professional lawyers. We find, first, that people are generally averse to the use of race and gender in algorithmic determinations of “pretrial risk”—the risk that criminal defendants pose to the public if released while awaiting trial. We find, however, that this preference for blinding shifts in response to a relatively mild intervention. In particular, we show that support for the use of race and gender in algorithmic decision-making increases substantially after respondents read a short passage about the possibility that blinding could lead to higher detention rates for Black and female defendants, respectively. Similar effect sizes are observed among the general public, computer scientists, and professional lawyers. These findings suggest that, while many respondents attest that they prefer blind algorithms, their preference is not based on an absolute principle. Rather, blinding is perceived as a way to ensure better outcomes for members of marginalized groups. Accordingly, in circumstances where blinding serves to disadvantage marginalized groups, respondents no longer view the exclusion of protected characteristics as a moral imperative, and the use of such information may become politically viable.},
booktitle = {Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {14},
numpages = {11},
location = {--, NY, USA},
series = {EAAMO '21}
}

@inproceedings{10.1007/978-3-030-73959-1_11,
author = {Lombardi, Michele and Baldo, Federico and Borghesi, Andrea and Milano, Michela},
title = {An Analysis of Regularized Approaches for Constrained Machine Learning},
year = {2020},
isbn = {978-3-030-73958-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73959-1_11},
doi = {10.1007/978-3-030-73959-1_11},
abstract = {Regularization-based approaches for injecting constraints in Machine Learning (ML) were introduced (see e.g. [2, 5, 8, 9]) to improve a predictive model via domain knowledge. Given the recent interest in ethical and trustworthy AI, however, several works are resorting to these approaches for enforcing desired properties over a ML model (e.g. fairness [1, 10]).},
booktitle = {Trustworthy AI - Integrating Learning, Optimization and Reasoning: First International Workshop, TAILOR 2020, Virtual Event, September 4–5, 2020, Revised Selected Papers},
pages = {112–119},
numpages = {8},
location = {Santiago de Compestela, Spain}
}

@inproceedings{10.1145/3471391.3471422,
author = {Bin Munir, Muhaimin and Alam, Fariha Raisa and Ishrak, Shadman and Hussain, Sonaila and Shalahuddin, Md. and Islam, Muhammad Nazrul},
title = {A Machine Learning Based Sign Language Interpretation System for Communication with Deaf-mute People},
year = {2021},
isbn = {9781450375979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471391.3471422},
doi = {10.1145/3471391.3471422},
abstract = {An utmost necessity of the deaf-mute people is to communicate with the non-mute people without the knowledge of sign language in different environments including office premises, shopping centers, educational institutions, and the like. Although several systems exist for teaching or learning sign language for the deaf-mute people, little attention has been paid to the development of useful and usable tools for effortless communication between mute and non-mute people. Therefore, the objective of this research is to develop an intelligent sign language interpretation system to connect with deaf-mute community, which will be used as a two way correspondence between speech impaired and regular speaking people. To attain this objective, firstly, a survey on recent vision-based gesture recognition approaches has been carried out. Secondly, an efficient and improved method for recognition of static hand posture and temporal gesture, focusing on finger-spelling method has been proposed which was developed using raspberry pi with the help of machine learning. Finally, the developed system was evaluated with 60 participants including the deaf-mute people and was found to be capable of successfully carrying out its functions with good performance.},
booktitle = {Proceedings of the XXI International Conference on Human Computer Interaction},
articleno = {4},
numpages = {9},
keywords = {support vector machine (SVM), machine learning, gesture recognition, finger-spelling, deaf-mute people, Sign language, Sign Language Interpreter, ASL},
location = {M\'{a}laga, Spain},
series = {Interacci\'{o}n '21}
}

@inproceedings{10.1145/3297280.3297411,
author = {Leotta, Maurizio and Olianas, Dario and Ricca, Filippo and Noceti, Nicoletta},
title = {How do implementation bugs affect the results of machine learning algorithms?},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297411},
doi = {10.1145/3297280.3297411},
abstract = {Applications based on Machine learning (ML) are growing in popularity in a multitude of different contexts such as medicine, bioinformatics, and finance. However, there is a lack of established approaches and strategies able to assure the reliability of this category of software. This has a big impact since nowadays our society relies on (potentially) unreliable applications that could cause, in extreme cases, catastrophic events (e.g., loss of life due to a wrong diagnosis of an ML-based cancer classifier).In this paper, as a preliminary step towards providing a solution to this big problem, we used automatic mutations to mimic realistic bugs in the code of two machine learning algorithms, Multilayer Perceptron and Logistic Regression, with the goal of studying the impact of implementation bugs on their behaviours.Unexpectedly, our experiments show that about 2/3 of the injected bugs are silent since they does not influence the results of the algorithms, while the bugs emerge as runtime errors, exceptions, or modified accuracy of the predictions only in the remaining cases. Moreover, we also discovered that about 1% of the bugs are extremely dangerous since they drastically affect the quality of the prediction only in rare cases and with specific datasets increasing the possibility of going unnoticed.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1304–1313},
numpages = {10},
keywords = {Oracle problem, accuracy, bug, machine learning, software quality assurance, testing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.5555/2567709.2502605,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
title = {Semi-supervised learning using greedy max-cut},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classification function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classification function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efficient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artificial and standard benchmark data sets where it obtains superior classification accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {771–800},
numpages = {30},
keywords = {semi-supervised learning, mixed integer programming, greedy max-cut, graph transduction, bivariate formulation}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {software product lines, reuse, reliability, failures, change},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1007/978-3-030-18305-9_19,
author = {Beretta, Elena and Santangelo, Antonio and Lepri, Bruno and Vetr\`{o}, Antonio and De Martin, Juan Carlos},
title = {The Invisible Power of Fairness. How Machine Learning Shapes Democracy},
year = {2019},
isbn = {978-3-030-18304-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-18305-9_19},
doi = {10.1007/978-3-030-18305-9_19},
abstract = {Many machine learning systems make extensive use of large amounts of data regarding human behaviors. Several researchers have found various discriminatory practices related to the use of human-related machine learning systems, for example in the field of criminal justice, credit scoring and advertising. Fair machine learning is therefore emerging as a new field of study to mitigate biases that are inadvertently incorporated into algorithms. Data scientists and computer engineers are making various efforts to provide definitions of fairness. In this paper, we provide an overview of the most widespread definitions of fairness in the field of machine learning, arguing that the ideas highlighting each formalization are closely related to different ideas of justice and to different interpretations of democracy embedded in our culture. This work intends to analyze the definitions of fairness that have been proposed to date to interpret the underlying criteria and to relate them to different ideas of democracy.},
booktitle = {Advances in Artificial Intelligence: 32nd Canadian Conference on Artificial Intelligence, Canadian AI 2019, Kingston, ON, Canada, May 28–31, 2019, Proceedings},
pages = {238–250},
numpages = {13},
keywords = {Machine learning, Fairness, Equity, Discrimination},
location = {Kingston, ON, Canada}
}

@article{10.1016/j.future.2019.02.006,
author = {Kastrati, Zenun and Imran, Ali Shariq},
title = {Performance analysis of machine learning classifiers on improved concept vector space models},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.02.006},
doi = {10.1016/j.future.2019.02.006},
journal = {Future Gener. Comput. Syst.},
month = jul,
pages = {552–562},
numpages = {11},
keywords = {Ontology, Deep learning, Document classification, iCVS, CVS, Document representation}
}

@article{10.1007/s11107-020-00895-8,
author = {Rahman, Sabidur and Ahmed, Tanjila and Ferdousi, Sifat and Bhaumik, Partha and Chowdhury, Pulak and Tornatore, Massimo and Das, Goutam and Mukherjee, Biswanath},
title = {Virtualized controller placement for multi-domain optical transport networks using machine learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {3},
issn = {1387-974X},
url = {https://doi.org/10.1007/s11107-020-00895-8},
doi = {10.1007/s11107-020-00895-8},
abstract = {Optical multi-domain transport networks are often controlled by a hierarchical distributed architecture of controllers. Optimal placement of these controllers is very important for efficient management and control. Traditional SDN controller placement methods focus mostly on controller placement in datacenter networks. But the problem of virtualized controller placement for multi-domain transport networks needs to be solved in the context of geographically distributed heterogeneous multi-domain networks. In this context, edge datacenters have enabled network operators to place virtualized controller instances closer to users, besides providing more candidate locations for controller placement. In this study, we propose a dynamic controller placement method for optical transport networks that considers the heterogeneity of optical controllers, resource limitations at edge hosting locations, and latency requirements. We also propose a machine-learning framework that helps the controller placement algorithm with proactive prediction (instead of traditional reactive threshold-based approach). Simulation studies, considering practical scenarios and temporal variation of load, show significant cost savings compared to traditional placement approaches.},
journal = {Photonic Netw. Commun.},
month = dec,
pages = {126–136},
numpages = {11},
keywords = {Machine learning, Edge computing, Network virtualization, Cost savings, Optical network controller, Optical transport network}
}

@article{10.1007/s11042-015-2818-8,
author = {Jarraya, Salma Kammoun and Hammami, Mohamed and Ben-Abdallah, Han\^{e}ne},
title = {Adaptive moving shadow detection and removal by new semi-supervised learning technique},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {18},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-015-2818-8},
doi = {10.1007/s11042-015-2818-8},
abstract = {The efficient application of current methods of shadow detection in video is hindered by the difficulty in defining their parameters or models and/or their application domain dependence. This paper presents a new shadow detection and removal method that aims to overcome these inefficiencies. It proposes a semi-supervised learning rule using a new variant of co-training technique for shadow detection and removal in uncontrolled scenes. The new variant both reduces the run-time through a periodical execution of a co-training process according to a novel temporal framework, and generates a more generic prediction model for an accurate classification. The efficiency of the proposed method is shown experimentally on a testbed of videos that were recorded by a static camera and that included several constraints, e.g., dynamic changes in the natural scene and various visual shadow features. The conducted experimental study produced quantitative and qualitative results that highlighted the robustness of our shadow detection method and its accuracy in removing cast shadows. In addition, the practical usefulness of the proposed method was evaluated by integrating it in a Highway Control and Management System software called RoadGuard.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {10949–10977},
numpages = {29},
keywords = {Shadow detection, Semi-supervised learning, Foreground segmentation, Co-training technique, Cast shadow removal}
}

@article{10.1007/s10115-013-0706-y,
author = {Triguero, Isaac and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {42},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-013-0706-y},
doi = {10.1007/s10115-013-0706-y},
abstract = {Semi-supervised classification methods are suitable tools to tackle training sets with large amounts of unlabeled data and a small quantity of labeled data. This problem has been addressed by several approaches with different assumptions about the characteristics of the input data. Among them, self-labeled techniques follow an iterative procedure, aiming to obtain an enlarged labeled data set, in which they accept that their own predictions tend to be correct. In this paper, we provide a survey of self-labeled methods for semi-supervised classification. From a theoretical point of view, we propose a taxonomy based on the main characteristics presented in them. Empirically, we conduct an exhaustive study that involves a large number of data sets, with different ratios of labeled data, aiming to measure their performance in terms of transductive and inductive classification capabilities. The results are contrasted with nonparametric statistical tests. Note is then taken of which self-labeled models are the best-performing ones. Moreover, a semi-supervised learning module has been developed for the Knowledge Extraction based on Evolutionary Learning software, integrating analyzed methods and data sets.},
journal = {Knowl. Inf. Syst.},
month = feb,
pages = {245–284},
numpages = {40},
keywords = {Semi-supervised learning, Self-training, Multi-view learning, Learning from unlabeled data, Co-training, Classification}
}

@article{10.1016/j.infsof.2020.106273,
author = {Singh, Jagsir and Singh, Jaswinder},
title = {Detection of malicious software by analyzing the behavioral artifacts using machine learning algorithms},
year = {2020},
issue_date = {May 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {121},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106273},
doi = {10.1016/j.infsof.2020.106273},
journal = {Inf. Softw. Technol.},
month = may,
numpages = {13},
keywords = {Static analysis, Random Forest, Machine learning algorithms, Malware, Dynamic analysis}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Semi-supervised learning, Self-paced learning, Nearest neighborhood graph, Graph Laplacian}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {visualization, test selection, test coverage, software product line testing, regression testing, decision support},
series = {ICSTW '11}
}

@article{10.1016/j.cosrev.2021.100376,
author = {Amutha, J. and Sharma, Sandeep and Sharma, Sanjay Kumar},
title = {Strategies based on various aspects of clustering in wireless sensor networks using classical, optimization and machine learning techniques: Review, taxonomy, research findings, challenges and future directions},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100376},
doi = {10.1016/j.cosrev.2021.100376},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {43},
keywords = {Reliability, Security, Routing, Machine learning, Optimization, Wireless Sensor Networks}
}

@article{10.1007/s10845-020-01570-5,
author = {Xu, Chengjun and Zhu, Guobin},
title = {Intelligent manufacturing Lie Group Machine Learning: real-time and efficient inspection system based on fog computing},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01570-5},
doi = {10.1007/s10845-020-01570-5},
abstract = {Due to the improvement of network infrastructure and the application of Internet of Things equipment, a large number of sensors are deployed in the industrial pipeline production, and the large size of data is generated. The most typical case in the production line is product inspection, that is, defect inspection. To implement an efficient and robust detection system, in this study, we propose a classification computing model based on Lie Group Machine Learning, which can find the possible defective products in production. Usually, a workshop has a lot of assembly lines. How to process large data on so many production lines in real-time and accurately is a difficult problem. To solve this problem, we use the concept of fog computing to design the system. By offloading the computation burden from the cloud server center to the fog nodes, the system obtains the ability to deal with extremely data. Our system has two obvious advantages. The first one is to apply Lie Group Machine Learning to fog computing environment to improve the computational efficiency and robustness of the system. The other is that without increasing any production costs, it can quickly detect products, reduce network latency, and reduce the load on bandwidth. The simulations prove that, compared with the existing methods, the proposed method has an average running efficiency increase of 52.57%, an average delay reduction of 42.13%, and an average accuracy increase of 27.86%.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {237–249},
numpages = {13},
keywords = {Inspection system, Fog computing, Lie group intrinsic mean, Lie Group Machine Learning}
}

@inproceedings{10.1145/3177540.3177554,
author = {Kahng, Andrew B.},
title = {Machine Learning Applications in Physical Design: Recent Results and Directions},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177540.3177554},
doi = {10.1145/3177540.3177554},
abstract = {In the late-CMOS era, semiconductor and electronics companies face severe product schedule and other competitive pressures. In this context, electronic design automation (EDA) must deliver "design-based equivalent scaling" to help continue essential industry trajectories. A powerful lever for this will be the use of machine learning techniques, both inside and "around" design tools and flows. This paper reviews opportunities for machine learning with a focus on IC physical implementation. Example applications include (1) removing unnecessary design and modeling margins through correlation mechanisms, (2) achieving faster design convergence through predictors of downstream flow outcomes that comprehend both tools and design instances, and (3) corollaries such as optimizing the usage of design resources licenses and available schedule. The paper concludes with open challenges for machine learning in IC physical design.},
booktitle = {Proceedings of the 2018 International Symposium on Physical Design},
pages = {68–73},
numpages = {6},
keywords = {physical design, machine learning},
location = {Monterey, California, USA},
series = {ISPD '18}
}

@article{10.1016/j.procs.2021.06.022,
author = {Domashova, Jenny and Kripak, Elena},
title = {Application of machine learning methods for risk analysis of unfavorable outcome of government procurement procedure in building and grounds maintenance domain},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.06.022},
doi = {10.1016/j.procs.2021.06.022},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {171–177},
numpages = {7},
keywords = {machine learning., cluster analysis methods, associative rules, government procurement, public procurement}
}

@article{10.1016/j.neunet.2015.06.004,
author = {Yang, Haiqin and Huang, Kaizhu and King, Irwin and Lyu, Michael R.},
title = {Maximum margin semi-supervised learning with irrelevant data},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.06.004},
doi = {10.1016/j.neunet.2015.06.004},
abstract = {Semi-supervised learning (SSL) is a typical learning paradigms training a model from both labeled and unlabeled data. The traditional SSL models usually assume unlabeled data are relevant to the labeled data, i.e., following the same distributions of the targeted labeled data. In this paper, we address a different, yet formidable scenario in semi-supervised classification, where the unlabeled data may contain irrelevant data to the labeled data. To tackle this problem, we develop a maximum margin model, named tri-class support vector machine (3C-SVM), to utilize the available training data, while seeking a hyperplane for separating the targeted data well. Our 3C-SVM exhibits several characteristics and advantages. First, it does not need any prior knowledge and explicit assumption on the data relatedness. On the contrary, it can relieve the effect of irrelevant unlabeled data based on the logistic principle and maximum entropy principle. That is, 3C-SVM approaches an ideal classifier. This classifier relies heavily on labeled data and is confident on the relevant data lying far away from the decision hyperplane, while maximally ignoring the irrelevant data, which are hardly distinguished. Second, theoretical analysis is provided to prove that in what condition, the irrelevant data can help to seek the hyperplane. Third, 3C-SVM is a generalized model that unifies several popular maximum margin models, including standard SVMs, Semi-supervised SVMs (S 3 VMs), and SVMs learned from the universum ( U -SVMs) as its special cases. More importantly, we deploy a concave-convex produce to solve the proposed 3C-SVM, transforming the original mixed integer programming, to a semi-definite programming relaxation, and finally to a sequence of quadratic programming subproblems, which yields the same worst case time complexity as that of S 3 VMs. Finally, we demonstrate the effectiveness and efficiency of our proposed 3C-SVM through systematical experimental comparisons.},
journal = {Neural Netw.},
month = oct,
pages = {90–102},
numpages = {13},
keywords = {Semi-supervised learning, Maximum margin classifier, Irrelevant data, Concave-convex procedure}
}

@article{10.1016/j.adhoc.2019.101913,
author = {Jagannath, Jithin and Polosky, Nicholas and Jagannath, Anu and Restuccia, Francesco and Melodia, Tommaso},
title = {Machine learning for wireless communications in the Internet of Things: A comprehensive survey},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2019.101913},
doi = {10.1016/j.adhoc.2019.101913},
journal = {Ad Hoc Netw.},
month = oct,
numpages = {46},
keywords = {Routing protocol, Medium access control, Spectrum sensing, Wireless ad hoc network, Internet of Things, Reinforcement learning, Deep learning, Machine learning}
}

@inproceedings{10.1145/2939672.2939696,
author = {Singh, Gursimran and Srikant, Shashank and Aggarwal, Varun},
title = {Question Independent Grading using Machine Learning: The Case of Computer Program Grading},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939696},
doi = {10.1145/2939672.2939696},
abstract = {Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system single s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {263–272},
numpages = {10},
keywords = {supervised learning, recruitment, question independent learning, one-class learning, feature engineering, automatic grading, MOOC},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1007/978-3-031-17587-9_2,
author = {He, Jiangpeng and Zhu, Fengqing},
title = {Unsupervised Continual Learning via&nbsp;Pseudo Labels},
year = {2021},
isbn = {978-3-031-17586-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17587-9_2},
doi = {10.1007/978-3-031-17587-9_2},
abstract = {Continual learning aims to learn new tasks incrementally using less computation and memory resources instead of retraining the model from scratch whenever new task arrives. However, existing approaches are designed in supervised fashion assuming all data from new tasks have been manually annotated, which are not practical for many real-life applications. In this work, we propose to use pseudo label instead of the ground truth to make continual learning feasible in unsupervised mode. The pseudo labels of new data are obtained by applying global clustering algorithm and we propose to use the model updated from last incremental step as the feature extractor. Due to the scarcity of existing work, we introduce a new benchmark experimental protocol for unsupervised continual learning of image classification task under class-incremental setting where no class label is provided for each incremental learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC) datasets by incorporating the pseudo label with various existing supervised approaches and show promising results in unsupervised scenario.},
booktitle = {Continual Semi-Supervised Learning: First International Workshop, CSSL 2021, Virtual Event, August 19–20, 2021, Revised Selected Papers},
pages = {15–32},
numpages = {18},
keywords = {Pseudo label, Unsupervised learning, Continual learning}
}

@phdthesis{10.5555/AAI28863221,
author = {Das, Anirban and J., Zaki, Mohammed and A., Varela, Carlos and Thomas, Brunschwiler,},
advisor = {Stacy, Patterson,},
title = {Resource-Aware Distributed Analytics and Machine Learning for Hybrid Edge-Cloud Systems},
year = {2021},
isbn = {9798351445472},
publisher = {Rensselaer Polytechnic Institute},
address = {USA},
abstract = {With more intelligent applications, data analytics and inference at the edge are proliferating as a complement to traditional computation done at a centralized cloud location. At the same time, distributed machine learning training at the edge of the network near the data producers is also gaining popularity, mainly due to benefits to security, privacy, and communication costs. However, edge devices are often resource-constrained, and further, there may be communication bottlenecks between the edge and the cloud. Successful solutions for these edge computing workloads must address challenges posed by constrained computation and communication resources.The first part of the thesis focuses on scheduling and task placement of data processing, analytics, and inference workloads. The goal is to provide some quality of service, for example, low latency or cost reduction in the context of edge-cloud architectures. We start with benchmarking the leading industry edge computing platforms that use the serverless computing paradigm as the medium of execution. We next consider serverless applications, consisting of a single-stage, and propose a framework to jointly execute such applications in the presence of an edge device and the public cloud. The aim is to decide whether to execute user jobs at the edge or the public cloud based on given latency or cost constraints. Finally, we consider a hybrid cloud scenario, where we consider a private cloud instead of a single edge device. Here, we study the problem of task placement and scheduling of multi-stage serverless applications between a private and the public cloud to minimize the cost of public cloud usage.  In the second part of the thesis, we consider machine learning training workloads in edge-cloud platforms. More specifically, we study federated learning in this part of the thesis. Like the first part of the thesis, we first conduct a feasibility study of federated learning algorithms on resource-constrained devices.  Next, we study an algorithm for horizontal federated learning in a hierarchical communication network. We analyze the convergence of the algorithm when there is a non-IID data distribution among the participants. Our analysis shows that the non-IID data distribution can have a significant impact on the algorithm convergence error. This insight paves the way for a more sophisticated algorithm design to diminish this performance gap. We then turn our focus towards vertical federated learning in a hierarchical network. We propose a new algorithm for model training where data is vertically partitioned across silos in the top tier and horizontally partitioned in the bottom tier among clients inside each silo. We present a theoretical analysis of our algorithm and show the dependence of the convergence rate on the number of vertical partitions, the number of local updates, and the number of clients in each hub.Lastly, we close with the summary and discussions on the future research directions and open questions of interest.},
note = {AAI28863221}
}

@article{10.1155/2021/7211419,
author = {Shokat, Sana and Riaz, Rabia and Rizvi, Sanam Shahla and Khan, Inayat and Paul, Anand and Yang, Zhongguo},
title = {Detection of Touchscreen-Based Urdu Braille Characters Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/7211419},
doi = {10.1155/2021/7211419},
abstract = {Revolution in technology is changing the way visually impaired people read and write Braille easily. Learning Braille in its native language can be more convenient for its users. This study proposes an improved backend processing algorithm for an earlier developed touchscreen-based Braille text entry application. This application is used to collect Urdu Braille data, which is then converted to Urdu text. Braille to text conversion has been done on Hindi, Arabic, Bangla, Chinese, English, and other languages. For this study, Urdu Braille Grade 1 data were collected with multiclass (39 characters of Urdu represented by class 1, Alif (ﺍ), to class 39, Bri Yay (ے). Total (N = 144) cases for each class were collected. The dataset was collected from visually impaired students from The National Special Education School. Visually impaired users entered the Urdu Braille alphabets using touchscreen devices. The final dataset contained (N = 5638) cases. Reconstruction Independent Component Analysis (RICA)-based feature extraction model is created for Braille to Urdu text classification. The multiclass was categorized into three groups (13 each), i.e., category-1 (1–13), Alif-Zaal (ﺫ - ﺍ), category-2 (14–26), Ray-Fay (ﻒ - ﺮ), and category-3 (27–39), Kaaf-Bri Yay (ے - ﻕ), to give better vision and understanding. The performance was evaluated in terms of true positive rate, true negative rate, positive predictive value, negative predictive value, false positive rate, total accuracy, and area under the receiver operating curve. Among all the classifiers, support vector machine has achieved the highest performance with a 99.73% accuracy. For comparisons, robust machine learning techniques, such as support vector machine, decision tree, and K-nearest neighbors were used. Currently, this work has been done on only Grade 1 Urdu Braille. In the future, we plan to enhance this work using Grade 2 Urdu Braille with text and speech feedback on touchscreen-based android phones.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {16}
}

@article{10.1007/s11277-020-07552-3,
author = {Sambyal, Nitigya and Saini, Poonam and Syal, Rupali},
title = {Microvascular Complications in Type-2 Diabetes: A Review of Statistical Techniques and Machine Learning Models},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {115},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07552-3},
doi = {10.1007/s11277-020-07552-3},
abstract = {Type 2 Diabetes (T2D) has become a serious concern as it contributes to 90% of the total diabetic population. The high glucose levels in diabetic patients result in damage to eyes, kidneys and nerves collectively known as microvascular complications. The paper presents a critical review of existing statistical and machine learning models with respect to such complications namely retinopathy, neuropathy and nephropathy. The statistical tests like chi-square, odds ratio, t-test, ANOVA enables inferential and descriptive analysis. On the other side, machine learning models like support vector machines, k-nearest neighbour, deep neural network, naive bayes also assures predictive analytics. It is worth mentioning that both analytics can reverse impaired glucose regulation early in its course resulting in the prevention of long-term complications associated with T2D. Despite variation in both analytic techniques, their integration in medical health practices can assist patients to effectively adapt a healthy lifestyle and reduce significant healthcare overheads for an individual's family as well as society.},
journal = {Wirel. Pers. Commun.},
month = nov,
pages = {1–26},
numpages = {26},
keywords = {Deep learning, Machine learning, Statistical analysis, Nephropathy, Neuropathy, Retinopathy, T2D, Microvascular complications}
}

@article{10.1007/s10515-019-00264-4,
author = {Cai, Cheng-Hao and Sun, Jing and Dobbie, Gillian},
title = {Automatic B-model repair using model checking and machine learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00264-4},
doi = {10.1007/s10515-019-00264-4},
abstract = {The B-method, which provides automated verification for the design of software systems, still requires users to manually repair faulty models. This paper proposes B-repair, an approach that supports automated repair of faulty models written in the B formal specification language. After discovering a fault in a model using the B-method, B-repair is able to suggest possible repairs for the fault, estimate the quality of suggested repairs and use a suitable repair to revise the model. The suggestion of repairs is produced using the Isolation method, which suggests changing the pre-conditions of operations, and the Revision method, which suggests changing the post-conditions of operations. The estimation of repair quality makes use of machine learning techniques that can learn the features of state transitions. After estimating the quality of suggested repairs, the repairs are ranked, and a best repair is selected according to the result of ranking and is used to revise the model. This approach has been evaluated using a set of finite state machines seeded with faults and a case study. The evaluation has revealed that B-repair is able to repair a large number of faults, including invariant violations, assertion violations and deadlock states, and gain high accuracies of repair. Using the combination of model checking and machine learning-guided techniques, B-repair saves development time by finding and repairing faults automatically during design.},
journal = {Automated Software Engg.},
month = sep,
pages = {653–704},
numpages = {52},
keywords = {Machine learning, Formal verification, Model checking, B-method, Model repair}
}

@article{10.1007/s42979-020-00303-y,
author = {Laguduva, Vishalini R. and Katkoori, Srinivas and Karam, Robert},
title = {Machine Learning Attacks and Countermeasures for PUF-Based IoT Edge Node Security},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {5},
url = {https://doi.org/10.1007/s42979-020-00303-y},
doi = {10.1007/s42979-020-00303-y},
abstract = {The Internet of things (IoT) ecosystem has grown exponentially with the convergence of various technologies such as deep learning, sensor systems, and advances in computing platforms. With such a highly pervasive nature of “smart” devices, the nature of data being collected and processed can be increasingly private and require safeguards to ensure the data’s integrity and security. Physically unclonable functions (PUFs) have emerged as a lightweight, viable security protocol in the Internet of Things (IoT) framework. Malicious modeling of PUF architectures has proven to be difficult due to the inherently stochastic nature of PUF architectures. In this work, we show that knowledge of the underlying PUF structure is unnecessary to clone a PUF. We tackle the problem of cloning PUF-based edge nodes in different settings such as unencrypted, encrypted, and obfuscated challenges in an IoT framework. We present a novel non-invasive, architecture-independent, machine learning attack for robust PUF designs and can handle encryption and obfuscation-based security measures on the transmitted challenge response pairs (CRPs). We show that the proposed framework can successfully clone different PUF architectures, including those encrypted using two (2) different encryption protocols in DES and AES and with varying degrees of obfuscation. We also show that the proposed approach outperforms a two-stage brute force attack model. Finally, we offer a machine learning-based countermeasure, a discriminator, which can distinguish cloned PUF devices and authentic PUFs with an average accuracy of 96%. The proposed discriminator can be used for rapidly authenticating millions of IoT nodes remotely from the cloud server.},
journal = {SN Comput. Sci.},
month = aug,
numpages = {13},
keywords = {Edge node security, Physically unclonable functions, Internet of things, Machine learning}
}

@inproceedings{10.1145/3240765.3243479,
author = {Marculescu, Diana and Stamoulis, Dimitrios and Cai, Ermao},
title = {Hardware-Aware Machine Learning: Modeling and Optimization},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1145/3240765.3243479},
doi = {10.1145/3240765.3243479},
abstract = {Recent breakthroughs in Machine Learning (ML) applications, and especially in Deep Learning (DL), have made DL models a key component in almost every modern computing system. The increased popularity of DL applications deployed on a wide-spectrum of platforms (from mobile devices to datacenters) have resulted in a plethora of design challenges related to the constraints introduced by the hardware itself. “What is the latency or energy cost for an inference made by a Deep Neural Network (DNN)?” “Is it possible to predict this latency or energy consumption before a model is even trained?” “If yes, how can machine learners take advantage of these models to design the hardware-optimal DNN for deployment?” From lengthening battery life of mobile devices to reducing the runtime requirements of DL models executing in the cloud, the answers to these questions have drawn significant attention. One cannot optimize what isn't properly modeled. Therefore, it is important to understand the hardware efficiency of DL models during serving for making an inference, before even training the model. This key observation has motivated the use of predictive models to capture the hardware performance or energy efficiency of ML applications. Furthermore, ML practitioners are currently challenged with the task of designing the DNN model, i.e., of tuning the hyper-parameters of the DNN architecture, while optimizing for both accuracy of the DL model and its hardware efficiency. Therefore, state-of-the-art methodologies have proposed hardware-aware hyper-parameter optimization techniques. In this paper, we provide a comprehensive assessment of state-of-the-art work and selected results on the hardware-aware modeling and optimization for ML applications. We also highlight several open questions that are poised to give rise to novel hardware-aware designs in the next few years, as DL applications continue to significantly impact associated hardware systems and platforms.},
booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
pages = {1–8},
numpages = {8},
location = {San Diego, CA, USA}
}

@inproceedings{10.1007/978-3-030-66981-2_3,
author = {Greco, Matteo and Spagnoletta, Michele and Appice, Annalisa and Malerba, Donato},
title = {Applying Machine Learning to Predict Closing Prices in Stock Market: A Case Study},
year = {2020},
isbn = {978-3-030-66980-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66981-2_3},
doi = {10.1007/978-3-030-66981-2_3},
abstract = {The stock’s closing price is the standard benchmark used by investors to track the stock performance over time. In particular, understanding the trend of the stock’s closing prices is of fundamental importance to choose investments carefully. In this paper, we address the task of forecasting closing prices of Exprivia S.p.A.’s stock by comparing the performance of both traditional and deep machine learning methods. Preliminary experiments show that the multi-variate setting can significantly outperform the univariate one and that deep learning can gain accuracy compared to traditional machine learning methods in the considered task.},
booktitle = {Mining Data for Financial Applications: 5th ECML PKDD Workshop, MIDAS 2020, Ghent, Belgium, September 18, 2020, Revised Selected Papers},
pages = {32–39},
numpages = {8},
keywords = {Stock’s closing price forecasting, Machine learning, Deep learning, Time series analysis},
location = {Ghent, Belgium}
}

@article{10.1186/s13677-019-0128-9,
author = {Moreno-Vozmediano, Rafael and Montero, Rub\'{e}n S. and Huedo, Eduardo and Llorente, Ignacio M.},
title = {Efficient resource provisioning for elastic Cloud services based on machine learning techniques},
year = {2019},
issue_date = {Dec 2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {8},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-019-0128-9},
doi = {10.1186/s13677-019-0128-9},
abstract = {Automated resource provisioning techniques enable the implementation of elastic services, by adapting the available resources to the service demand. This is essential for reducing power consumption and guaranteeing QoS and SLA fulfillment, especially for those services with strict QoS requirements in terms of latency or response time, such as web servers with high traffic load, data stream processing, or real-time big data analytics. Elasticity is often implemented in cloud platforms and virtualized data-centers by means of auto-scaling mechanisms. These make automated resource provisioning decisions based on the value of specific infrastructure and/or service performance metrics. This paper presents and evaluates a novel predictive auto-scaling mechanism based on machine learning techniques for time series forecasting and queuing theory. The new mechanism aims to accurately predict the processing load of a distributed server and estimate the appropriate number of resources that must be provisioned in order to optimize the service response time and fulfill the SLA contracted by the user, while attenuating resource over-provisioning in order to reduce energy consumption and infrastructure costs. The results show that the proposed model obtains a better forecasting accuracy than other classical models, and makes a resource allocation closer to the optimal case.},
journal = {J. Cloud Comput.},
month = dec,
articleno = {128},
numpages = {18},
keywords = {Machine learning, Elasticity, Cloud computing, Auto-scaling}
}

@inproceedings{10.1145/3297280.3297398,
author = {Draa, Ismat Chaib and Bouquillon, Fabien and Niar, Smail and Strugeon, Emmanuelle Grislin-Le},
title = {Machine learning for improving mobile user satisfaction},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297398},
doi = {10.1145/3297280.3297398},
abstract = {Optimizing energy consumption in modern mobile handheld devices plays a crucial role as lowering power consumption impacts system's autonomy and system reliability. Recent mobile platforms have an increasing number of sensors and processing components. Added to the increasing popularity of power-hungry applications, battery life in mobile devices is an important issue. However, we think that the utilization of the large amount of data from the various sensors can be beneficial to detect the changing device context, the user needs or the running application requirements in terms of resources. When these information are used properly, an efficient control of power knobs can be implemented thus reducing the energy consumption. This paper presents URBOC, for User Request Based Optimization Component. This component is an extension of our previous framework [7] ENOrMOUS. This framework was able to identify and classify the user contexts in order to understand user habits, preferences and needs which allow to improve the operating system power scheme. In this paper, we extend the use of ENOrMOUS by allowing the user to send requests in order to extend the battery life up to a specific time. Machine Learning (ML) and data mining algorithms have been used to obtain an efficient trade-off between power consumption reduction opportunities and user requests satisfaction. The proposed solution increases battery life by up to seven hours depending on user requests vs. the out-of-the-box operating system power manager with a negligible overhead.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1200–1207},
numpages = {8},
keywords = {data mining algorithms, mobile power consumption, mobile systems, neural networks, run-time analysis},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1155/2021/4116497,
author = {Ran, JingFei and Ali, Sikandar},
title = {Intelligent Method of Supply Chain Circulation Industry Structure Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/4116497},
doi = {10.1155/2021/4116497},
abstract = {In the deepening of supply chain competition, whether the structure of supply chain industry is reasonable and scientific has been severely tested. For warehousing, purchase and distribution channels, and customers, it largely determines whether the structure of supply chain is stable and efficient. The rationality of structure can determine the value of supply chain. By analyzing these four levels, this paper judges whether the supply chain structure is reasonable; the judgment standard is based on the three popular machine learning models, Stochastic Forest, XGBoost, and Support Vector Machine. The three models are based on a large number of real data environments. Through data simulation and parameter optimization, four supply chain characteristics are put into the model for simulation training for many times, and the three error numbers of MAE, RMSE, and MAPE of the model are analyzed to judge the reliability of the model. On this basis, through the combination of models, it is determined that the average percentage error of the combination of the three models is higher than that of the other pairwise combinations, reaching 0.937, which completes the expectation of intelligent prediction of supply chain structure.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@inproceedings{10.5555/3491440.3491627,
author = {Saha, Avirup and Sheshadri, Shreyas and Datta, Samik and Ganguly, Niloy and Makhija, Disha and Patel, Priyank},
title = {Understanding the success of graph-based semi-supervised learning using partially labelled Stochastic block model},
year = {2021},
isbn = {9780999241165},
abstract = {With the proliferation of learning scenarios with an abundance of instances, but limited amount of high-quality labels, semi-supervised learning algorithms came to prominence. Graph-based Semi-Supervised Learning (G-SSL) algorithms, of which Label Propagation (LP) is a prominent example, are particularly well-suited for these problems. The premise of LP is the existence of homophily in the graph, but beyond that nothing is known about the efficacy of LP. In particular, there is no characterisation that connects the structural constraints, volume and quality of the labels to the accuracy of LP. In this work, we draw upon the notion of recovery from the literature on community detection, and provide guarantees on accuracy for partially-labelled graphs generated from the Partially-Labelled Stochastic Block Model (PLSBM). Extensive experiments performed on synthetic data verify the theoretical findings.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {187},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1007/978-3-030-93620-4_9,
author = {Barry, Mariam and Bifet, Albert and Chiky, Raja and Montiel, Jacob and Tran, Vinh-Thuy},
title = {Challenges of Machine Learning for Data Streams in the Banking Industry},
year = {2021},
isbn = {978-3-030-93619-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93620-4_9},
doi = {10.1007/978-3-030-93620-4_9},
abstract = {Banking Information Systems continuously generate large quantities of data as inter-connected streams (transactions, events logs, time series, metrics, graphs, process, etc.). Such data streams need to be processed online to deal with critical business applications such as real-time fraud detection, network security attack prevention or predictive maintenance on information system infrastructure. Many algorithms have been proposed for data stream learning, however, most of them do not deal with the important challenges and constraints imposed by real-world applications. In particular, when we need to train models incrementally from heterogeneous data mining and deployment them within complex big data architecture. Based on banking applications and lessons learned in production environments of BNP Paribas - a major international banking group and leader in the Eurozone - we identified the most important current challenges for mining IT data streams. Our goal is to highlight the key challenges faced by data scientists and data engineers within complex industry settings for building or deploying models for real word streaming applications. We provide future research directions on Stream Learning that will accelerate the adoption of online learning models for solving real-word problems. Therefore bridging the gap between research and industry communities. Finally, we provide some recommendations to tackle some of these challenges.},
booktitle = {Big Data Analytics: 9th International Conference, BDA 2021, Virtual Event, December 15-18, 2021, Proceedings},
pages = {106–118},
numpages = {13},
keywords = {Challenges, Production, IT, Streaming, Banking}
}

@phdthesis{10.5555/AAI28647365,
author = {Ma, Runchao and Samuel, Burer, and Weiyu, Xu, and Kurt, Anstreicher, and Tianbao, Yang,},
advisor = {Qihang, Lin,},
title = {First-Order Methods for Constrained Optimization with New Iteration Complexity and Applications in Machine Learning},
year = {2021},
isbn = {9798544292418},
publisher = {The University of Iowa},
abstract = {In recent years, there has been a surge of interest in the topic of machine learning with a large collection of statistical models being developed and analyzed. There has also been substantial research on optimization algorithms applied to machine learning problems. Despite the existence of data-driven constraints in many statistical models, such as Neyman-Pearson classification and constrained lasso, the focus of most new developments on optimization for machine learning has been either on unconstrained problems or simply constrained problems in which the projection mapping onto the feasible set has a closed-form solution. This thesis seeks to fill in the gap by studying first-order methods for optimization problems with data-driven nonlinear constraints. A number of first-order algorithms are proposed, and the corresponding computational complexity results are established. In this thesis, we investigate provable first-order methods for optimization problems with functional constraints under different assumptions. The level-set method, presented in Chapter 2, is used to solve a convex constrained optimization where the objective function and the constraints are defined as summation of finitely many loss functions. We propose a new variant of the feasible level-set method, which can generate a feasible solution path. To construct an efficient oracle used in the level-set method, a novel saddle-point reformulation of the level-set function using convex conjugate and perspective of the loss functions is analyzed. In Chapter 3, we examine a class of convex constrained optimization problems where an error bound condition holds. Different from the existing works where growth parameters in the error bound condition are either known or estimated in advance, our methods do not require knowing those growth parameters and any other problem-dependent parameters. In Chapter 4, we investigate the non-convex non-smooth constrained optimization where the objective function and the constraint functions are weakly convex. Quadratically regularized subgradient methods are proposed. The computational complexity of our methods for finding a nearly stationary point is established under a uniform Slater's condition. In Chapter 5, we propose inexact proximal-point penalty methods for solving smooth constrained optimization where the objective function is non-convex, and the constraint functions can also be either convex or non-convex. The uniform Slater's condition assumption discussed in Chapter 4 is relaxed and the computational complexity of the proposed method for finding an ε-stationary point is established.},
note = {AAI28647365}
}

@article{10.1016/j.eswa.2019.112843,
author = {Hoque Tania, Marzia and Lwin, Khin T. and Shabut, Antesar M. and Najlah, Mohammad and Chin, Jeannette and Hossain, M.A.},
title = {Intelligent image-based colourimetric tests using machine learning framework for lateral flow assays},
year = {2020},
issue_date = {Jan 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112843},
doi = {10.1016/j.eswa.2019.112843},
journal = {Expert Syst. Appl.},
month = jan,
numpages = {22},
keywords = {Colourimetric tests, Deep learning, Machine learning, Feature selection, Histogram thresholding, Image processing}
}

@inproceedings{10.1007/978-3-030-30215-3_8,
author = {Tsikhanovich, Maksim and Magdon-Ismail, Malik and Ishaq, Muhammad and Zikas, Vassilis},
title = {PD-ML-Lite: Private Distributed Machine Learning from Lightweight Cryptography},
year = {2019},
isbn = {978-3-030-30214-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30215-3_8},
doi = {10.1007/978-3-030-30215-3_8},
abstract = {Privacy arises to a major issue in distributed learning. Current approaches that do not use a trusted external authority either reduce the accuracy of the learning algorithm (e.g., by adding noise), or incur a high performance penalty. We propose a methodology for private distributed ML from light-weight cryptography (in short, PD-ML-Lite). We apply our methodology to two major ML algorithms, namely non-negative matrix factorization (NMF) and singular value decomposition (SVD). Our protocols are communication optimal, achieve the same accuracy as their non-private counterparts, and satisfy a notion of privacy—which we define—that is both intuitive and measurable. We use light cryptographic tools (multi-party secure sum and normed secure sum) to build learning algorithms rather than wrap complex learning algorithms in a heavy multi-party computation (MPC) framework.We showcase our algorithms’ utility and privacy for NMF on topic modeling and recommender systems, and for SVD on principal component regression, and low rank approximation.},
booktitle = {Information Security: 22nd International Conference, ISC 2019, New York City, NY, USA, September 16–18, 2019, Proceedings},
pages = {149–167},
numpages = {19},
location = {New York City, NY, USA}
}

@inproceedings{10.1145/3209889.3209893,
author = {Makrynioti, Nantia and Vasiloglou, Nikolaos and Pasalic, Emir and Vassalos, Vasilis},
title = {Modelling Machine Learning Algorithms on Relational Data with Datalog},
year = {2018},
isbn = {9781450358286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209889.3209893},
doi = {10.1145/3209889.3209893},
abstract = {The standard process of data science tasks is to prepare features inside a database, export them as a denormalized data frame and then apply machine learning algorithms. This process is not optimal for two reasons. First, it requires denormalization of the database that can convert a small data problem into a big data problem. The second shortcoming is that it assumes that the machine learning algorithm is disentangled from the relational model of the problem. That seems to be a serious limitation since the relational model contains very valuable domain expertise. In this paper we explore the use of convex optimization and specifically linear programming, for modelling machine learning algorithms on relational data in an integrated way with data processing operators. We are using SolverBlox, a framework that accepts as an input Datalog code and feeds it into a linear programming solver. We demonstrate the expression of common machine learning algorithms and present use case scenarios where combining data processing with modelling of optimization problems inside a database offers significant advantages.},
booktitle = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
articleno = {5},
numpages = {4},
location = {Houston, TX, USA},
series = {DEEM'18}
}

@article{10.1016/j.neucom.2019.03.079,
author = {Xie, Jin and Liu, Sanyang and Dai, Hao},
title = {Manifold regularization based distributed semi-supervised learning algorithm using extreme learning machine over time-varying network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.079},
doi = {10.1016/j.neucom.2019.03.079},
journal = {Neurocomput.},
month = aug,
pages = {24–34},
numpages = {11},
keywords = {Extreme learning machine (ELM), Zero-gradient-sum (ZGS), Time-varying network, Manifold regularization (MR), Semi-supervised learning (SSL), Distributed learning (DL)}
}

@article{10.1155/2021/1496364,
author = {Baothman, Fatmah Abdulrahman and Gupta, Deepak},
title = {A Machine Learning Approach for Improving the Movement of Humanoid NAO’s Gaits},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/1496364},
doi = {10.1155/2021/1496364},
abstract = {A humanoid robot’s development requires an incredible combination of interdisciplinary work from engineering to mathematics, software, and machine learning. NAO is a humanoid bipedal robot designed to participate in football competitions against humans by 2050, and speed is crucial for football sports. Therefore, the focus of the paper is on improving NAO speed. This paper is aimed at testing the hypothesis of whether the humanoid NAO walking speed can be improved without changing its physical configuration. The applied research method compares three classification techniques: artificial neural network (ANN), Na\"{\i}ve Bayes, and decision tree to measure and predict NAO’s best walking speed, then select the best method, and enhance it to find the optimal average velocity speed. According to Aldebaran documentation, the real NAO’s robot default walking speed is 9.52 cm/s. The proposed work was initiated by studying NAO hardware platform limitations and selecting Nao’s gait 12 parameters to measure the accuracy metrics implemented in the three classification models design. Five experiments were designed to model and trace the changes for the 12 parameters. The preliminary NAO’s walking datasets open-source available at GitHub, the NAL, and RoboCup datasheets are implemented. All generated gaits’ parameters for both legs and feet in the experiments were recorded using the Choregraphe software. This dataset was divided into 30% for training and 70% for testing each model. The recorded gaits’ parameters were then fed to the three classification models to measure and predict NAO’s walking best speed. After 500 training cycles for the Na\"{\i}ve Bayes, the decision tree, and ANN, the RapidMiner scored 48.20%, 49.87%, and 55.12%, walking metric speed rate, respectively. Next, the emphasis was on enhancing the ANN model to reach the optimal average velocity walking speed for the real NAO. With 12 attributes, the maximum accuracy metric rate of 65.31% was reached with only four hidden layers in 500 training cycles with a 0.5 learning rate for the best walking learning process, and the ANN model predicted the optimal average velocity speed of 51.08% without stiffness: V1=22.62 cm/s, V2=40 cm/s, and V=30 cm/s. Thus, the tested hypothesis holds with the ANN model scoring the highest accuracy rate for predicting NAO’s robot walking state speed by taking both legs to gauge joint 12 parameter values.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.1145/3345768.3355920,
author = {Edalat, Yalda and Obraczka, Katia},
title = {Dynamically Tuning IEEE 802.11's Contention Window Using Machine Learning},
year = {2019},
isbn = {9781450369046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345768.3355920},
doi = {10.1145/3345768.3355920},
abstract = {The IEEE 802.11's binary exponential backoff (BEB) algorithm plays a critical role in the throughput performance and fair channel allocation of IEEE 802.11 networks. In particular, one of BEB algorithm's parameters, the Contention Window determines how long a node needs to wait before it (re)transmits data. Consequently, choosing adequate values of the Contention Window is crucial for IEEE 802.11's performance. In this paper, we introduce a simple, yet effective machine learning approach to adjust the value of IEEE 802.11's Contention Window based on present- as well as recent past network contention conditions. Using a wide range of network scenarios and conditions, we show that our approach outperforms both 802.11's BEB as well as an existing contention window adjustment technique that only considers the last two transmissions. Our results indicate that our contention window adaptation algorithm is able to deliver consistently higher average throughput, lower end-to-end delay, as well as improved fairness.},
booktitle = {Proceedings of the 22nd International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {19–26},
numpages = {8},
keywords = {machine learning, ieee 802.11, fixed-share, experts, contention window, backoff},
location = {Miami Beach, FL, USA},
series = {MSWIM '19}
}

@article{10.1007/s10270-020-00856-9,
author = {Pilarski, Sebastian and Staniszewski, Martin and Bryan, Matthew and Villeneuve, Frederic and Varr\'{o}, D\'{a}niel},
title = {Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00856-9},
doi = {10.1007/s10270-020-00856-9},
abstract = {The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {685–709},
numpages = {25},
keywords = {Gas turbine engines, Code generation, Automated deployment, Neural networks, Machine learning, Prediction-at-runtime}
}

@inproceedings{10.1007/978-3-030-75762-5_15,
author = {Lin, Chia-Chih and Chen, Ming-Syan},
title = {Attack Is the Best Defense: A Multi-Mode Poisoning PUF Against Machine Learning Attacks},
year = {2021},
isbn = {978-3-030-75761-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75762-5_15},
doi = {10.1007/978-3-030-75762-5_15},
abstract = {Resistance to modeling attacks is an important issue for Physical Unclonable Functions (PUFs). Deep learning, the state-of-the-art modeling attack, has recently been shown to be able to break many newly developed PUFs. Since then, many more complex PUF structures or challenge obfuscations have been proposed to resist deep learning attacks. However, the proposed methods typically focus on increasing the nonlinearity of PUF structure and challenge-response mapping. In this paper, we explore another direction with a multi-mode poisoning approach for a classic PUF (MMP PUF) in which each working mode is a simple add-on function for a classic PUF. By dividing the original challenge space for each working mode, the proposed MMP PUF generates a multi-modal challenge-response dataset that poisons machine learning algorithms. To validate the idea, we design two working mode types, challenge shift and response flip, as examples with widely-used delay-based Arbiter PUF. Experimental results show that our approach respectively achieves 74.37%, 68.08%, and 50.09% accuracy for dual-mode shift, quad-mode circular shift and dual-mode flip with deep learning models trained on over 3 million challenge-response pairs.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part I},
pages = {176–187},
numpages = {12},
keywords = {Physical Unclonable Function (PUF), Modeling attacks, Machine learning, Deep neural network, Poisoning attack}
}

@inproceedings{10.1007/978-3-030-68787-8_33,
author = {Kajatin, Roland and Nalpantidis, Lazaros},
title = {Image Segmentation of Bricks in Masonry Wall Using a Fusion of Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-68786-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68787-8_33},
doi = {10.1007/978-3-030-68787-8_33},
abstract = {Autonomous mortar raking requires a computer vision system which is able to provide accurate segmentation masks of close-range images of brick walls. The goal is to detect and ultimately remove the mortar, leaving the bricks intact, thus automating this construction-related task. This paper proposes such a vision system based on the combination of machine learning algorithms. The proposed system fuses the individual segmentation outputs of eight classifiers by means of a weighted voting scheme and then performing a threshold operation to generate the final binary segmentation. A novel feature of this approach is the fusion of several segmentations using a low-cost commercial off-the-shelf hardware setup. The close-range brick wall segmentation capabilities of the system are demonstrated on a total of about 9 million data points.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10-15, 2021, Proceedings, Part VII},
pages = {446–461},
numpages = {16},
keywords = {Deep learning, Machine learning, Construction robotics, Image segmentation}
}

@inproceedings{10.1007/978-3-030-33607-3_12,
author = {Shepperd, Martin and Guo, Yuchen and Li, Ning and Arzoky, Mahir and Capiluppi, Andrea and Counsell, Steve and Destefanis, Giuseppe and Swift, Stephen and Tucker, Allan and Yousefi, Leila},
title = {The Prevalence of Errors in Machine Learning Experiments},
year = {2019},
isbn = {978-3-030-33606-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33607-3_12},
doi = {10.1007/978-3-030-33607-3_12},
abstract = {Context: Conducting experiments is central to research machine learning research to benchmark, evaluate and compare learning algorithms. Consequently it is important we conduct reliable, trustworthy experiments.Objective: We investigate the incidence of errors in a sample of machine learning experiments in the domain of software defect prediction. Our focus is simple arithmetical and statistical errors.Method: We analyse 49 papers describing 2456 individual experimental results from a previously undertaken systematic review comparing supervised and unsupervised defect prediction classifiers. We extract the confusion matrices and test for relevant constraints, e.g., the marginal probabilities must sum to one. We also check for multiple statistical significance testing errors.Results: We find that a total of 22 out of 49 papers contain demonstrable errors. Of these 7 were statistical and 16 related to confusion matrix inconsistency (one paper contained both classes of error).Conclusions: Whilst some errors may be of a relatively trivial nature, e.g., transcription errors their presence does not engender confidence. We strongly urge researchers to follow open science principles so errors can be more easily be detected and corrected, thus as a community reduce this worryingly high error rate with our computational experiments.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2019: 20th International Conference, Manchester, UK, November 14–16, 2019, Proceedings, Part I},
pages = {102–109},
numpages = {8},
keywords = {Classifier, Computational experiment, Reliability, Error},
location = {Manchester, United Kingdom}
}

@inproceedings{10.5555/3157096.3157364,
author = {Krishnamurthy, Akshay and Agarwal, Alekh and Dud\'{\i}k, Miroslav},
title = {Contextual semibandits via supervised learning oracles},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2396–2404},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1007/978-3-030-26250-1_30,
author = {Burton, Simon and Gauerhof, Lydia and Sethy, Bibhuti Bhusan and Habli, Ibrahim and Hawkins, Richard},
title = {Confidence Arguments for Evidence of Performance in Machine Learning for Highly Automated Driving Functions},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_30},
doi = {10.1007/978-3-030-26250-1_30},
abstract = {Due to their ability to efficiently process unstructured and highly dimensional input data, machine learning algorithms are being applied to perception tasks for highly automated driving functions. The consequences of failures and insufficiencies in such algorithms are severe and a convincing assurance case that the algorithms meet certain safety requirements is therefore required. However, the task of demonstrating the performance of such algorithms is non-trivial, and as yet, no consensus has formed regarding an appropriate set of verification measures. This paper provides a framework for reasoning about the contribution of performance evidence to the assurance case for machine learning in an automated driving context and applies the evaluation criteria to a pedestrian recognition case study.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {365–377},
numpages = {13},
keywords = {Safety Assurance, Machine learning, Highly automated driving},
location = {Turku, Finland}
}

@article{10.1155/2021/8545686,
author = {Wu, Liqin and Pan, Meisen and Ahmed, Syed Hassan},
title = {English Grammar Detection Based on LSTM-CRF Machine Learning Model},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/8545686},
doi = {10.1155/2021/8545686},
abstract = {Deep learning and neural network have been widely used in the field of speech, vocabulary, text, pictures, and other information processing fields, which has achieved excellent research results. Neural network algorithm and prediction model were used in this paper for the study and exploration of English grammar. Aiming at the application requirements of English grammar accuracy and standardization, we proposed a machine learning model based on LSTM-CRF to detect and analyze English grammar. This paper briefly summarized the development trend of deep learning and neural network algorithm and designed the structure pattern of radial basis function neural network in grammar semantic detection and analysis on the basis of deep learning artificial neural network theory. Based on the morphological features of English grammar, a grammar database was established according to the rules of English word segmentation. In this paper, we proposed an improved conditional random field CRF (Conditional Random Field) network model based on LSTM (Long Short-Term Memory) neural network. It can improve the problem that the traditional machine learning model relies on feature point selection in English grammar detection. The machine learning model based on LSTM-CRF was used to recognize English grammar text entities. The results show that the English grammar detection system based on the LSTM-CRF model can simplify the process structure in the recognition process, reduce the unnecessary operation cycle, and improve the overall detection accuracy.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {10}
}

@inproceedings{10.1609/aaai.v33i01.33014790,
author = {Redko, Ievgen and Laclau, Charlotte},
title = {On fair cost sharing games in machine learning},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014790},
doi = {10.1609/aaai.v33i01.33014790},
abstract = {Machine learning and game theory are known to exhibit a very strong link as they mutually provide each other with solutions and models allowing to study and analyze the optimal behaviour of a set of agents. In this paper, we take a closer look at a special class of games, known as fair cost sharing games, from a machine learning perspective. We show that this particular kind of games, where agents can choose between selfish behaviour and cooperation with shared costs, has a natural link to several machine learning scenarios including collaborative learning with homogeneous and heterogeneous sources of data. We further demonstrate how the game-theoretical results bounding the ratio between the best Nash equilibrium (or its approximate counterpart) and the optimal solution of a given game can be used to provide the upper bound of the gain achievable by the collaborative learning expressed as the expected risk and the sample complexity for homogeneous and heterogeneous cases, respectively. We believe that the established link can spur many possible future implications for other learning scenarios as well, with privacy-aware learning being among the most noticeable examples.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {588},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s10845-019-01510-y,
author = {Gonz\'{a}lez Rodr\'{\i}guez, Germ\'{a}n and Gonzalez-Cava, Jose M. and M\'{e}ndez P\'{e}rez, Juan Albino},
title = {An intelligent decision support system for production planning based on machine learning},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-019-01510-y},
doi = {10.1007/s10845-019-01510-y},
abstract = {This paper presents a new methodology to solve a Closed-Loop Supply Chain (CLSC) management problem through a decision-making system based on fuzzy logic built on machine learning. The system will provide decisions to operate a production plant integrated in a CLSC to meet the production goals with the presence of uncertainties. One of the main contributions of the proposal is the ability to reject the effects that the imbalances in the rest of the chain have on the inventories of raw materials and finished products. For this, an intelligent algorithm will be in charge of the supervision of the plant operation and task-reprogramming to ensure the achievement of the process goals. Fuzzy logic and machine learning techniques are combined to design the tool. The method was tested on an industrial hospital laundry with satisfactory results, thus highlighting the potential of this proposal for its incorporation into the Industry 4.0 framework.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {1257–1273},
numpages = {17},
keywords = {Decision support system, Operation management, Machine learning, Intelligent manufacturing, Artificial intelligence}
}

@inproceedings{10.5555/3326943.3327108,
author = {Krijthe, Jesse H. and Loog, Marco},
title = {The pessimistic limits and possibilities of margin-based losses in semi-supervised learning},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Consider a classification problem where we have both labeled and unlabeled data available. We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing, it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1795–1804},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1016/j.neucom.2014.12.091,
author = {Veredas, Francisco J. and Luque-Baena, Rafael M. and Mart\'{\i}n-Santos, Francisco J. and Morilla-Herrera, Juan C. and Morente, Laura},
title = {Wound image evaluation with machine learning},
year = {2015},
issue_date = {September 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.091},
doi = {10.1016/j.neucom.2014.12.091},
abstract = {A pressure ulcer is a clinical pathology of localized damage to the skin and underlying tissue caused by pressure, shear or friction. Diagnosis, care and treatment of pressure ulcers can result in extremely expensive costs for health systems. A reliable diagnosis supported by precise wound evaluation is crucial in order to succeed on the treatment decision and, in some cases, to save the patient s life. However, current clinical evaluation procedures, focused mainly on visual inspection, do not seem to be accurate enough to accomplish this important task. This paper presents a computer-vision approach based on image processing algorithms and supervised learning techniques to help detect and classify wound tissue types that play an important role in wound diagnosis. The system proposed involves the use of the k-means clustering algorithm for image segmentation and compares three different machine learning approaches-neural networks, support vector machines and random forest decision trees-to classify effectively each segmented region as the appropriate tissue type. Feature selection based on a wrapper approach with recursive feature elimination is shown to be effective in keeping the efficacy of the classifiers up and significantly reducing the number of necessary predictors. Results obtained show high performance rates from classifiers based on fitted neural networks, random forest models and support vector machines (overall accuracy on a testing set 95% CI], respectively: 81.87% 80.03%, 83.61%]; 87.37% 85.76%, 88.86%]; 88.08% 86.51%, 89.53%]), with significant differences found between the three machine learning approaches. This study seeks to provide, using standard classification algorithms, a consistent and robust methodological framework as a basis for the development of reliable computational systems to support ulcer diagnosis.},
journal = {Neurocomput.},
month = sep,
pages = {112–122},
numpages = {11},
keywords = {Wound evaluation, Medical imaging, Machine vision, Computer-aided diagnosis}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {variability extraction, reverse engineering, relational concept analysis, complex software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/PIMRC.2019.8904143,
author = {Sun, Chengjian and Yang, Chenyang},
title = {Learning to Optimize with Unsupervised Learning: Training Deep Neural Networks for URLLC},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PIMRC.2019.8904143},
doi = {10.1109/PIMRC.2019.8904143},
abstract = {Learning the optimized solution as a function of environmental parameters by deep neural networks (DNN) is effective in solving numerical optimization in real time for time-sensitive resource allocation in wireless systems. Existing works of learning to optimize train the DNN with labels, which are generated by solving the optimization problems. The learned solution are often inaccurate and hence cannot be employed to ensure the stringent quality of service. In this paper, we propose a framework to learn the latent function with unsupervised deep learning, where the property that the optimal solution should satisfy is used as the "supervision signal" implicitly. The framework is applicable to both variable and functional optimization problems with constraints, which are respectively formulated to optimize variables and functions of concern. We take a variable optimization problem in ultra-reliable and low-latency communications as an example, which demonstrates that the ultra-high reliability can be supported by the DNN without supervision labels.},
booktitle = {2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)},
pages = {1–7},
numpages = {7},
location = {Istanbul, Turkey}
}

@article{10.1145/3376898,
author = {Chouldechova, Alexandra and Roth, Aaron},
title = {A snapshot of the frontiers of fairness in machine learning},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3376898},
doi = {10.1145/3376898},
abstract = {A group of industry, academic, and government experts convene in Philadelphia to explore the roots of algorithmic bias.},
journal = {Commun. ACM},
month = apr,
pages = {82–89},
numpages = {8}
}

@inproceedings{10.1007/978-3-030-52200-1_30,
author = {Florescu, Dorian and England, Matthew},
title = {A Machine Learning Based Software Pipeline to Pick the Variable Ordering for Algorithms with Polynomial Inputs},
year = {2020},
isbn = {978-3-030-52199-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52200-1_30},
doi = {10.1007/978-3-030-52200-1_30},
abstract = {We are interested in the application of Machine Learning (ML) technology to improve mathematical software. It may seem that the probabilistic nature of ML tools would invalidate the exact results prized by such software, however, the algorithms which underpin the software often come with a range of choices which are good candidates for ML application. We refer to choices which have no effect on the mathematical correctness of the software, but do impact its performance.In the past we experimented with one such choice: the variable ordering to use when building a Cylindrical Algebraic Decomposition (CAD). We used the Python library Scikit-Learn (sklearn) to experiment with different ML models, and developed new techniques for feature generation and hyper-parameter selection.These techniques could easily be adapted for making decisions other than our immediate application of CAD variable ordering. Hence in this paper we present a software pipeline to use sklearn to pick the variable ordering for an algorithm that acts on a polynomial system. The code described is freely available online.},
booktitle = {Mathematical Software – ICMS 2020: 7th International Conference, Braunschweig, Germany, July 13–16, 2020, Proceedings},
pages = {302–311},
numpages = {10},
keywords = {Variable ordering, Cylindrical algebraic decomposition, Mathematical software, Scikit-learn, Machine learning},
location = {Braunschweig, Germany}
}

@article{10.1016/j.eswa.2016.03.024,
author = {Dornaika, Fadi and Moujahid, Abdelmalik and El Merabet, Youssef and Ruichek, Yassine},
title = {Building detection from orthophotos using a machine learning approach},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.03.024},
doi = {10.1016/j.eswa.2016.03.024},
abstract = {Automatic building detection in orthophotos via a machine learning approach.Flexible framework that exploits supervised learning.Applying the covariance descriptor to the building detection problem.An extended performance study of several combination segmentation-descriptor.Classification performance is obtained with K-NN, Partial Least Square and SVM. Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {130–142},
numpages = {13},
keywords = {Supervised learning, Orthophotos, Image segmentation, Image descriptors, Classifier, Automatic building detection and delineation}
}

@inproceedings{10.1145/3437120.3437322,
author = {T. Michailidis, Emmanouel and G. Kogias, Dimitrios and Voyiatzis, Ioannis},
title = {A Review on Hardware Security Countermeasures for IoT: Emerging Mechanisms and Machine Learning Solutions},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437322},
doi = {10.1145/3437120.3437322},
abstract = {The evolution of the Internet of Things (IoT) ecosystem necessitates the mitigation of security threats not only in the software domain but also in the hardware. However, owing to the inherent characteristics and limitations of the IoT devices, designing hardware security solutions is a complex and non-trivial process. This paper surveys recent research works, in which countermeasures against potential threats of hardware security of IoT devices have been proposed. In this context, employing machine-learning (ML) methods is also expected to lead to effective solutions. Towards this end, this paper provides an overview of ML-based countermeasures. Finally, open issues in these relevant fields are given, stimulating further research.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {268–271},
numpages = {4},
keywords = {Machine Learning, Internet of Things (IoT), Hardware security},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1145/3322640.3326705,
author = {Tolan, Song\"{u}l and Miron, Marius and G\'{o}mez, Emilia and Castillo, Carlos},
title = {Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia},
year = {2019},
isbn = {9781450367547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322640.3326705},
doi = {10.1145/3322640.3326705},
abstract = {In this paper we study the limitations of Machine Learning (ML) algorithms for predicting juvenile recidivism. Particularly, we are interested in analyzing the trade-off between predictive performance and fairness. To that extent, we evaluate fairness of ML models in conjunction with SAVRY, a structured professional risk assessment framework, on a novel dataset originated in Catalonia. In terms of accuracy on the prediction of recidivism, the ML models slightly outperform SAVRY; the results improve with more data or more features available for training (AUCROC of 0.64 with SAVRY vs. AUCROC of 0.71 with ML models). However, across three fairness metrics used in other studies, we find that SAVRY is in general fair, while the ML models tend to discriminate against male defendants, foreigners, or people of specific national groups. For instance, foreigners who did not recidivate are almost twice as likely to be wrongly classified as high risk by ML models than Spanish nationals. Finally, we discuss potential sources of this unfairness and provide explanations for them, by combining ML interpretability techniques with a thorough data analysis. Our findings provide an explanation for why ML techniques lead to unfairness in data-driven risk assessment, even when protected attributes are not used in training.},
booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law},
pages = {83–92},
numpages = {10},
keywords = {risk assessment, machine learning, criminal recidivism, algorithmic fairness, algorithmic bias},
location = {Montreal, QC, Canada},
series = {ICAIL '19}
}

@inproceedings{10.5555/3361338.3361469,
author = {Jayaraman, Bargav and Evans, David},
title = {Evaluating differentially private machine learning in practice},
year = {2019},
isbn = {9781939133069},
publisher = {USENIX Association},
address = {USA},
abstract = {Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, ε, about how much information is leaked by a mechanism. When used in privacy-preserving machine learning, the goal is typically to limit what can be inferred from the model about individual training records. However, the calibration of the privacy budget is not well understood. Implementations of privacy-preserving machine learning often select large values of ε in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, relaxed definitions of differential privacy are often used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is no way to obtain privacy for free--relaxed definitions of differential privacy that reduce the amount of noise needed to improve utility also increase the measured privacy leakage. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs for complex learning tasks: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models.},
booktitle = {Proceedings of the 28th USENIX Conference on Security Symposium},
pages = {1895–1912},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {SEC'19}
}

@inproceedings{10.1145/3311927.3323139,
author = {Zimmermann-Niefield, Abigail and Turner, Makenna and Murphy, Bridget and Kane, Shaun K. and Shapiro, R. Benjamin},
title = {Youth Learning Machine Learning through Building Models of Athletic Moves},
year = {2019},
isbn = {9781450366908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311927.3323139},
doi = {10.1145/3311927.3323139},
abstract = {Machine Learning-based (ML) technologies impact many facets of our lives. Given ML's ubiquity, and the ways it offers creative computational possibilities distinct from programming, we believe it could be a powerful tool for youth to leverage in making, creativity, and play. We investigate how youth with no programming experience can incorporate ML classifiers into athletic practice by building models of their own physical activity. In this paper, we describe a design experiment exploring how to introduce youth to making ML models within the context of their athletic interests. We present AlpacaML, an iOS application that connects to wearable sensors and allows young people to model physical movement using an ML classifier, and detail its use in a three-hour workshop with middle- and high-school athletes. We found the youth were able to collect data, build models, test and evaluate models, and quickly iterate on this process. We finish with a discussion of why this is a promising direction for the incorporation of Machine Learning into novice youth making, exploration, and play.},
booktitle = {Proceedings of the 18th ACM International Conference on Interaction Design and Children},
pages = {121–132},
numpages = {12},
keywords = {Wearable Technologies, Sports, Modeling, Mobile, Machine Learning, K12, Education},
location = {Boise, ID, USA},
series = {IDC '19}
}

@inproceedings{10.1109/ICRA.2019.8793622,
author = {Wang, Guangming and Wang, Hesheng and Liu, Yiling and Chen, Weidong},
title = {Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA.2019.8793622},
doi = {10.1109/ICRA.2019.8793622},
abstract = {A new unsupervised learning method of depth and ego-motion using multiple masks from monocular video is proposed in this paper. The depth estimation network and the ego-motion estimation network are trained according to the constraints of depth and ego-motion without truth values. The main contribution of our method is to carefully consider the occlusion of the pixels generated when the adjacent frames are projected to each other, and the blank problem generated in the projection target imaging plane. Two fine masks are designed to solve most of the image pixel mismatch caused by the movement of the camera. In addition, some relatively rare circumstances are considered, and repeated masking is proposed. To some extent, the method is to use a geometric relationship to filter the mismatched pixels for training, making unsupervised learning more efficient and accurate. The experiments on KITTI dataset show our method achieves good performance in terms of depth and ego-motion. The generalization capability of our method is demonstrated by training on the low-quality uncalibrated bike video dataset and evaluating on KITTI dataset, and the results are still good.},
booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
pages = {4724–4730},
numpages = {7},
location = {Montreal, QC, Canada}
}

@article{10.1016/j.envsoft.2019.03.006,
author = {Frey, Ulrich J. and Klein, Martin and Deissenroth, Marc},
title = {Modelling complex investment decisions in Germany for renewables with different machine learning algorithms},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {118},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2019.03.006},
doi = {10.1016/j.envsoft.2019.03.006},
journal = {Environ. Model. Softw.},
month = aug,
pages = {61–75},
numpages = {15},
keywords = {Germany, Machine learning, Deep learning, Renewable energy, Solar installation, Investment decisions}
}

@article{10.1016/j.asoc.2019.02.017,
author = {Maryan, Corey and Hoque, Md Tamjidul and Michael, Christopher and Ioup, Elias and Abdelguerfi, Mahdi},
title = {Machine learning applications in detecting rip channels from images},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {78},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.02.017},
doi = {10.1016/j.asoc.2019.02.017},
journal = {Appl. Soft Comput.},
month = may,
pages = {84–93},
numpages = {10},
keywords = {Meta-learner, Principal component analysis, Support vector machine, Deep learning, Rip channel}
}

@article{10.1155/2021/4456222,
author = {Wu, Bo and Zheng, Changlong and Wu, Wenqing},
title = {An Analysis of the Effectiveness of Machine Learning Theory in the Evaluation of Education and Teaching},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/4456222},
doi = {10.1155/2021/4456222},
abstract = {Artificial intelligence was first proposed in the 1950s, when it was only a forward-looking concept. If machines can have the same learning ability as human beings and the computing power of computers themselves, this concept has been placed high hopes. Until about 2010, with the explosion of data volume and the improvement of computer performance, machine learning has become a leader in breaking through the bottleneck of artificial intelligence. Research on machine learning in education and teaching has attracted much attention. From the above research status, we can see that in the current period of the vigorous development of machine learning, many applications are still not perfect and ordinary education and teaching evaluation is difficult to meet people’s requirements, so how to gradually improve its effectiveness is a significant goal with research significance and practical interests. However, in the environment of colleges and universities, prediction information and evaluation methods have important application value and development space in education and teaching. In this context, according to the theory of machine science, the effectiveness of several conventional prediction and evaluation methods is analyzed. In this paper, machine learning theory is used to study college students’ performance prediction and credit evaluation, as well as teaching quality evaluation and comprehensive ability evaluation in colleges and universities. Questionnaire survey is used to investigate and analyze the results. The effectiveness of machine theory in teaching is analyzed. It is found that machine learning has great advantages in education and teaching evaluation. It builds models in complex computing environment and is not affected by human factors; the effectiveness of prediction and evaluation is significant.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@article{10.1145/3350422,
author = {Charles, Subodha and Ahmed, Alif and Ogras, Umit Y. and Mishra, Prabhat},
title = {Efficient Cache Reconfiguration Using Machine Learning in NoC-Based Many-Core CMPs},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3350422},
doi = {10.1145/3350422},
abstract = {Dynamic cache reconfiguration (DCR) is an effective technique to optimize energy consumption in many-core architectures. While early work on DCR has shown promising energy saving opportunities, prior techniques are not suitable for many-core architectures since they do not consider the interactions and tight coupling between memory, caches, and network-on-chip (NoC) traffic. In this article, we propose an efficient cache reconfiguration framework in NoC-based many-core architectures. The proposed work makes three major contributions. First, we model a distributed directory based many-core architecture similar to Intel Xeon Phi architecture. Next, we propose an efficient cache reconfiguration framework that considers all significant components, including NoC, caches, and main memory. Finally, we propose a machine learning--based framework that can reduce the exploration time by an order of magnitude with negligible loss in accuracy. Our experimental results demonstrate 18.5% energy savings on average compared to base cache configuration.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {60},
numpages = {23},
keywords = {machine learning, Cache reconfiguration}
}

@article{10.1016/j.artmed.2019.02.006,
author = {Lee, Su-Dong and Lee, Ji-Hyung and Choi, Young-Geun and You, Hee-Cheon and Kang, Ja-Heon and Jun, Chi-Hyuck},
title = {Machine learning models based on the dimensionality reduction of standard automated perimetry data for glaucoma diagnosis},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {94},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2019.02.006},
doi = {10.1016/j.artmed.2019.02.006},
journal = {Artif. Intell. Med.},
month = mar,
pages = {110–116},
numpages = {7},
keywords = {Visual field clustering, Dimensionality reduction, Machine learning classifier, Glaucoma}
}

@article{10.1007/s11227-019-02894-7,
author = {Gong, Yunlu and Jia, Lianguo},
title = {Research on SVM environment performance of parallel computing based on large data set of machine learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-02894-7},
doi = {10.1007/s11227-019-02894-7},
abstract = {The support vector machine (SVM) algorithm is widely used in various fields because of its good classification effect, simplicity and practicability. However, the support vector machine calculates the support vector by quadratic programming, and the solution of quadratic programming will calculate the n-order matrix. When the amount of data is large, the calculation and storage of the n-order matrix will make the optimization speed very slow, even lead to memory overflow and interrupt operation. Using the big data computing platform Spark to improve the support vector machine algorithm can solve the above problems, but it’s not competent for multi-classification problems. Therefore, this paper starts with constructing multiple classifiers, combines the Spark framework of big data programming model and the classification characteristics of support vector machine to realize a parallel one-to-many SVM optimization algorithm based on large data sets and compares them through UCI data sets. In the experiments, the one-to-many support vector machine improved by Spark is obviously better than the one-to-many support vector machine in the single-machine environment. The simulation results show that the proposed algorithm has better performance.},
journal = {J. Supercomput.},
month = sep,
pages = {5966–5983},
numpages = {18},
keywords = {Machine learning, Large data set, Parallel computing, SVM environment}
}

@inproceedings{10.1007/978-3-030-64148-1_14,
author = {Figalist, Iris and Elsner, Christoph and Bosch, Jan and Olsson, Helena Holmstr\"{o}m},
title = {An End-to-End Framework for Productive Use of Machine Learning in Software Analytics and Business Intelligence Solutions},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_14},
doi = {10.1007/978-3-030-64148-1_14},
abstract = {Nowadays, machine learning (ML) is an integral component in a wide range of areas, including software analytics (SA) and business intelligence (BI). As a result, the interest in custom ML-based software analytics and business intelligence solutions is rising. In practice, however, such solutions often get stuck in a prototypical stage because setting up an infrastructure for deployment and maintenance is considered complex and time-consuming. For this reason, we aim at structuring the entire process and making it more transparent by deriving an end-to-end framework from existing literature for building and deploying ML-based software analytics and business intelligence solutions. The framework is structured in three iterative cycles representing different stages in a model’s lifecycle: prototyping, deployment, update. As a result, the framework specifically supports the transitions between these stages while also covering all important activities from data collection to retraining deployed ML models. To validate the applicability of the framework in practice, we compare it to and apply it in a real-world ML-based SA/BI solution.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {217–233},
numpages = {17},
keywords = {Business intelligence, Software analytics, Machine learning},
location = {Turin, Italy}
}

@article{10.1016/j.cogsys.2018.09.006,
author = {Song, Yue-gang and Cao, Qi-lin and Zhang, Chen},
title = {Towards a new approach to predict business performance using machine learning},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2018.09.006},
doi = {10.1016/j.cogsys.2018.09.006},
journal = {Cogn. Syst. Res.},
month = dec,
pages = {1004–1012},
numpages = {9},
keywords = {Finical ratios, Business prediction, Machine learning}
}

@article{10.1007/s00146-021-01282-1,
author = {Sapienza, Salvatore and Vedder, Anton},
title = {Principle-based recommendations for big data and machine learning in food safety: the P-SAFETY model},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {1},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-021-01282-1},
doi = {10.1007/s00146-021-01282-1},
abstract = {Big data and Machine learning Techniques are reshaping the way in which food safety risk assessment is conducted. The ongoing ‘datafication’ of food safety risk assessment activities and the progressive deployment of probabilistic models in their practices requires a discussion on the advantages and disadvantages of these advances. In particular, the low level of trust in EU food safety risk assessment framework highlighted in 2019 by an EU-funded survey could be exacerbated by novel methods of analysis. The variety of processed data raises unique questions regarding the interplay of multiple regulatory systems alongside food safety legislation. Provisions aiming to preserve the confidentiality of data and protect personal information are juxtaposed to norms prescribing the public disclosure of scientific information. This research is intended to provide guidance for data governance and data ownership issues that unfold from the ongoing transformation of the technical and legal domains of food safety risk assessment. Following the reconstruction of technological advances in data collection and analysis and the description of recent amendments to food safety legislation, emerging concerns are discussed in light of the individual, collective and social implications of the deployment of cutting-edge Big Data collection and analysis techniques. Then, a set of principle-based recommendations is proposed by adapting high-level principles enshrined in institutional documents about Artificial Intelligence to the realm of food safety risk assessment. The proposed set of recommendations adopts Safety, Accountability, Fairness, Explainability, Transparency as core principles (SAFETY), whereas Privacy and data protection are used as a meta-principle.},
journal = {AI Soc.},
month = oct,
pages = {5–20},
numpages = {16},
keywords = {Data ownership, Data governance, Risk assessment, Food safety, Machine learning, Big data}
}

@inproceedings{10.1007/978-3-030-22419-6_19,
author = {Lee, Corinne and Sood, Suraj and Hancock, Monte and Higgins, Tyler and Sproul, Kristy and Hadgis, Antoinette and Joe-Yen, Stefan},
title = {Biomimicry and Machine Learning in the Context of Healthcare Digitization},
year = {2019},
isbn = {978-3-030-22418-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22419-6_19},
doi = {10.1007/978-3-030-22419-6_19},
abstract = {The healthcare industry is inundated with elder, disabled, and partial-use care issues such as falls in homes with no available aid present. This article’s thesis is that bio-support in the form of biomimetic artificial intelligence (AI) is not yet fully-exploited within the stated problem space.This article summarizes analyses conducted using Hancock’s “Knowledge-Based Expert System” (KBES) on two datasets from the popular machine learning website Kaggle. The first dataset contains various numeric, health-related data from 400 anonymized patients diagnosed with chronic kidney disease (CKD). The second contains the same kind of data, but for 569 patients diagnosed with either malignant or benign forms of breast cancer.In the last place, the potential for a “Holacratic” health analytics organization will be assessed. Said organization would be akin to Tapscott’s “Global Solutions Network” (GSN), which was defined as a digital group of public or private individuals with the following four features:Diversity in stakeholders who collectively represent at least two of the four pillars of society (government, private sector, civil society, individuals)Multinational or global presenceAt least partial digitality with respect to its communications tools and platformsProgressive goals related to the creation of public goodsIt will be argued that the group working on this article (known colloquially as the Sirius Project) successfully addresses both the criteria of being and need for a Holacratic GSN.},
booktitle = {Augmented Cognition: 13th International Conference, AC 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings},
pages = {273–283},
numpages = {11},
keywords = {Biomimicry, Machine learning, Breast cancer, Chronic kidney disease, Benefit corporation},
location = {Orlando, FL, USA}
}

@inbook{10.1145/3310205.3310213,
title = {Machine learning and probabilistic data cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310213},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.5555/3327144.3327316,
author = {Jakab, Tomas and Gupta, Ankush and Bilen, Hakan and Vedaldi, Andrea},
title = {Unsupervised learning of object landmarks through conditional image generation},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets — faces, people, 3D objects, and digits — without any modifications.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4020–4031},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1016/j.compeleceng.2019.106497,
author = {Tout, Hanine and Kara, Nadjia and Talhi, Chamseddine and Mourad, Azzam},
title = {Proactive machine learning-based solution for advanced manageability of multi-persona mobile computing},
year = {2019},
issue_date = {Dec 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {80},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2019.106497},
doi = {10.1016/j.compeleceng.2019.106497},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {18},
keywords = {Artificial intelligence, Machine learning, Dynamic programming, Optimization, Offloading, Mobile cloud computing, Multi-persona mobile computing, Mobile device}
}

@article{10.1007/s11277-020-07781-6,
author = {Kumar, Rohit and Venkanna, U. and Tiwari, Vivek},
title = {A Time Granular Analysis of Software Defined Wireless Mesh Based IoT (SDWM-IoT) Network Traffic Using Supervised Learning},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {116},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07781-6},
doi = {10.1007/s11277-020-07781-6},
abstract = {The ceaseless increase in the number of the wireless Internet of Things (IoT) devices has resulted in the need of different traffic engineering techniques to manage the massive network traffic. Wireless Mesh Networks (WMNs) are an important constituent part of the wireless IoT networks, and are helpful to route the IoT networks’ traffic over long distances. The WMN devices are powerful in comparison to the IoT sensor devices, and are suitable to run the traffic engineering algorithms. To further improve the performance of the WMNs, Software Defined Networking can be used. Its unique features like global visibility, agility, etc., guarantee the optimal network management. As granularity plays an important role in data analysis and none of the existing works has discussed a time granularity based network data analysis, this work tries to offer a time granular analysis of Software Defined Wireless Mesh based IoT (SDWM-IoT) network’s traffic using supervised learning approaches. A time granular analysis helps to explore the functional traits of the data at the Coarse, Medium, and Fine granularity levels. This assists in divulging and understanding the hidden characteristics and behaviour of the SDWM-IoT network’s data based on varying time granularity, respectively. Some well known supervised learning algorithms are used to offer an in-depth analysis of the traffic, and to draw the relevant conclusions. Different variants of Decision Tree, Support Vector Machine and K-Nearest Neighbour (KNN) are used to analyze the traffic and achieve a reliable accuracy rate of more than 90%. Among all the variants, fine-KNN produces the best accuracy for most of the traffic classes with a rate of more than 98%. In addition to this, a tenfold cross-validation technique is also used to prevent the the chances of over-fitting.},
journal = {Wirel. Pers. Commun.},
month = feb,
pages = {2083–2109},
numpages = {27},
keywords = {Supervised machine learning, Wireless mesh networks (WMNs), Software defined networking (SDN), Traffic classification (TC), Traffic engineering (TE), Internet of things (IoT), Time granularity}
}

@inproceedings{10.1145/3388440.3412428,
author = {Mondal, Abhijit and Kordi, Misagh and Bansal, Mukul S.},
title = {A Supervised Machine Learning Approach for Distinguishing Between Additive and Replacing Horizontal Gene Transfers},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412428},
doi = {10.1145/3388440.3412428},
abstract = {Horizontal gene transfer is one of the most important drivers of microbial gene and genome evolution. Despite its central role in microbial evolution, several aspects of horizontal gene transfer remain poorly understood. In particular, transfers can be either additive or replacing depending on whether the transferred gene adds itself as a new gene in the recipient genome or replaces an existing homologous gene. However, despite recent efforts, there do not yet exist effective computational approaches for classifying inferred transfers as being additive or replacing.In this work, we address this gap by devising a novel supervised machine learning approach for classifying transfers as being either additive or replacing. Our approach is based on phylogenetic reconciliation, a standard computational technique for inferring transfers. Our classifier, named ARTra, uses as features the classifications provided by several simple reconciliation-based classification rules, along with topological information from the gene tree, and ensembles them to produce a more accurate classification.ARTra is efficient and robust and significantly improves upon the classification accuracy of the only existing computational approach for this problem. We demonstrate the accuracy of ARTra by applying it to a wide range of simulated datasets and to a large biological dataset. Our results show that ARTra performs well over a broad range of evolutionary conditions and on real data, and that it does so even when trained only on a narrow range of such conditions and only using simulated data.An open-source implementation of ARTra is freely available from https://compbio.engr.uconn.edu/software/ARTra/.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {16},
numpages = {11},
keywords = {xenologous gene displacement, supervised machine learning, replacing transfer, microbial gene family evolution, classification, additive transfer, Horizontal gene transfer, DTRL reconciliation},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3365871.3365872,
author = {Power, Alexander and Kotonya, Gerald},
title = {Providing Fault Tolerance via Complex Event Processing and Machine Learning for IoT Systems},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365872},
doi = {10.1145/3365871.3365872},
abstract = {Fault-tolerance (FT) support is a key challenge for ensuring dependable Internet of Things (IoT) systems. Many existing FT-support mechanisms in IoT are static, tightly coupled, inflexible implementations that struggle to adapt in dynamic IoT environments. This paper proposes Complex Patterns of Failure (CPoF), an approach to providing reactive and proactive FT using Complex Event Processing (CEP) and Machine Learning (ML). Error-detection strategies are defined as nondeterministic finite automata (NFA) and implemented via CEP systems. Reactive-FT support is monitored and learned from to train ML models that proactively handle imminent future occurrences of known errors. We evaluated CPoF on an indoor agriculture system with experiments that used time and error correlations to preempt battery-depletion failures. We trained predictive models to learn from reactive-FT support and provide preemptive error recovery.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {1},
numpages = {7},
keywords = {machine learning, internet of things, fault tolerance, dependability, complex event processing, automata},
location = {Bilbao, Spain},
series = {IoT '19}
}

@inproceedings{10.1145/3478384.3478407,
author = {Bakogiannis, Konstantinos and Andreopoulou, Areti and Georgaki, Anastasia},
title = {The development of a dance-musification model with the use of machine learning techniques under COVID-19 restrictions},
year = {2021},
isbn = {9781450385695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478384.3478407},
doi = {10.1145/3478384.3478407},
abstract = {Interactive technologies enable dancers to control the music in real-time with their movement. This paper presents the design and development of a model which takes as input a dancer’s movement and outputs music, structurally related to dance, with the use of machine learning techniques. Both the technical and artistic aspects of the model development are described in detail. In particular, the paper compares the use of machine learning techniques to traditional coding, in interactive dance and music applications. Moreover, it describes the significant discrimination between movement sonification and dance musification and explains why the model presented here falls into the second category. Special focus is given to the implications of the COVID-19 restrictions regarding the established collaboration with the dancer.},
booktitle = {Proceedings of the 16th International Audio Mostly Conference},
pages = {81–88},
numpages = {8},
keywords = {music composition for dance, interactive technology, interactive machine learning, interactive dance, choreomusicology, algorithmic music},
location = {virtual/Trento, Italy},
series = {AM '21}
}

@article{10.1007/s00521-018-3874-6,
author = {Asif, Amina and Dawood, Muhammad and Jan, Bismillah and Khurshid, Javaid and DeMaria, Mark and Minhas, Fayyaz ul Amir Afsar},
title = {PHURIE: hurricane intensity estimation from infrared satellite imagery using machine learning},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3874-6},
doi = {10.1007/s00521-018-3874-6},
abstract = {Automated prediction of hurricane intensity from satellite infrared imagery is a challenging problem with implications in weather forecasting and disaster planning. In this work, a novel machine learning-based method for estimation of intensity or maximum sustained wind speed of tropical cyclones over their life cycle is presented. The approach is based on a support vector regression model over novel statistical features of infrared images of a hurricane. Specifically, the features characterize the degree of uniformity in various temperature bands of a hurricane. Performance of several machine learning methods such as ordinary least squares regression, backpropagation neural networks and XGBoost regression has been compared using these features under different experimental setups for the task. Kernelized support vector regression resulted in the lowest prediction error between true and predicted hurricane intensities (approximately 10 knots or 18.5&nbsp;km/h), which is better than previously proposed techniques and comparable to SATCON consensus. The performance of the proposed scheme has also been analyzed with respect to errors in annotation of center of the hurricane and aircraft reconnaissance data. The source code and webserver implementation of the proposed method called PHURIE (PIEAS HURricane Intensity Estimator) is available at the URL: .},
journal = {Neural Comput. Appl.},
month = may,
pages = {4821–4834},
numpages = {14},
keywords = {Support vector regression, Machine learning-based forecasting, Tropical cyclones, Hurricane intensity prediction}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Code reviews, Action research, Machine learning, Measurement}
}

@article{10.1007/s11263-011-0442-2,
author = {Leordeanu, Marius and Sukthankar, Rahul and Hebert, Martial},
title = {Unsupervised Learning for Graph Matching},
year = {2012},
issue_date = {January   2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {96},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-011-0442-2},
doi = {10.1007/s11263-011-0442-2},
abstract = {Graph matching is an essential problem in computer vision that has been successfully applied to 2D and 3D feature matching and object recognition. Despite its importance, little has been published on learning the parameters that control graph matching, even though learning has been shown to be vital for improving the matching rate. In this paper we show how to perform parameter learning in an unsupervised fashion, that is when no correct correspondences between graphs are given during training. Our experiments reveal that unsupervised learning compares favorably to the supervised case, both in terms of efficiency and quality, while avoiding the tedious manual labeling of ground truth correspondences. We verify experimentally that our learning method can improve the performance of several state-of-the art graph matching algorithms. We also show that a similar method can be successfully applied to parameter learning for graphical models and demonstrate its effectiveness empirically.},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {28–45},
numpages = {18},
keywords = {Unsupervised learning, Semi-supervised learning, Parameter learning, MAP inference, Graph matching, Feature matching}
}

@article{10.1109/TCBB.2018.2858808,
author = {Wassan, Jyotsna Talreja and Wang, Haiying and Browne, Fiona and Zheng, Huiru},
title = {A Comprehensive Study on Predicting Functional Role of Metagenomes Using Machine Learning Methods},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2858808},
doi = {10.1109/TCBB.2018.2858808},
abstract = {"Metagenomics" is the study of genomic sequences obtained directly from environmental microbial communities with the aim to linking their structures with functional roles. The field has been aided in the unprecedented advancement through high-throughput omics data sequencing. The outcome of sequencing are biologically rich data sets. Metagenomic data consisting of microbial species which outnumber microbial samples, lead to the "curse of dimensionality" in datasets. Hence, the focus in metagenomics studies has moved towards developing efficient computational models using Machine Learning ML, reducing the computational cost. In this paper, we comprehensively assessed various ML approaches to classifying high-dimensional human microbiota effectively into their functional phenotypes. We propose the application of embedded feature selection methods, namely, Extreme Gradient Boosting and Penalized Logistic Regression to determine important microbial species. The resultant feature set enhanced the performance of one of the most popular state-of-the-art methods, Random Forest RF over metagenomic studies. Experimental results indicate that the proposed method achieved best results in terms of accuracy, area under the Receiver Operating Characteristic curve ROC-AUC, and major improvement in processing time. It outperformed other feature selection methods of filters or wrappers over RF and classifiers such as Support Vector Machine SVM, Extreme Learning Machine ELM, and k- Nearest Neighbors k-NN.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {751–763},
numpages = {13}
}

@article{10.1016/j.pmcj.2019.05.001,
author = {Nawrocki, P. and Sniezynski, B. and Slojewski, H.},
title = {Adaptable mobile cloud computing environment with code transfer based on machine learning},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2019.05.001},
doi = {10.1016/j.pmcj.2019.05.001},
journal = {Pervasive Mob. Comput.},
month = jul,
pages = {49–63},
numpages = {15},
keywords = {Energy optimization, Code offloading, Mobile cloud computing, Machine learning}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Word localization, Weakly supervised learning, Sentiment analysis, Convolutional neural network, Class activation mapping}
}

@inproceedings{10.1007/978-3-030-71158-0_9,
author = {Isaak, Nicos and Michael, Loizos},
title = {Blending NLP and Machine Learning for the Development of Winograd Schemas},
year = {2020},
isbn = {978-3-030-71157-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71158-0_9},
doi = {10.1007/978-3-030-71158-0_9},
abstract = {The Winograd Schema Challenge (WSC), a novel litmus test for machine intelligence, has been proposed to advance the field of AI. Over the last decade, AI researchers have become increasingly interested in this challenge. While a common and trivial task for humans, studies have shown that the WSC is still difficult for current AI systems. Tackling the challenge would likely require access to a sufficiently rich set of Winograd schema examples, which are currently limited in their number and too cumbersome to create completely manually. Towards addressing these limitations, we propose a machine-driven approach for the development of large numbers of schemas. Our empirical evaluation suggests that our developed system, which blends the advantages of Machine Learning and Natural Language Processing, is able to automatically develop Winograd schemas autonomously, or considerably help humans in the development task.},
booktitle = {Agents and Artificial Intelligence: 12th International Conference, ICAART 2020, Valletta, Malta, February 22–24, 2020, Revised Selected Papers},
pages = {188–214},
numpages = {27},
keywords = {Deep learning, machine learning, Schema development, Winograd Schema Challenge},
location = {Valletta, Malta}
}

@inproceedings{10.5555/3504035.3505123,
author = {Arendt, Dustin and Grace, Emily and Volkova, Svitlana},
title = {Interactive machine learning at scale with CHISSL},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {We demonstrate CHISSL a scalable client-server system for real-time interactive machine learning. Our system is capable of incorporating user feedback incrementally and immediately without a pre-defined prediction task. Computation is partitioned between a lightweight web-client and a heavyweight server. The server relies on representation learning and off-the-shelf agglomerative clustering to find a dendrogram, which we use to quickly approximate distances in the representation space. The client, using only this dendrogram, incorporates user feedback via transduction. Distances and predictions for each unlabeled instance are updated incrementally and deterministically, with O(n) space and time complexity. Our algorithm is implemented in a functional prototype, designed to be easy to use by non-experts. The prototype organizes the large amounts of data into recommendations. This allows the user to interact with actual instances by dragging and dropping to provide feedback in an intuitive manner. We applied CHISSL to several domains including cyber, social media, and geo-temporal analysis.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1088},
numpages = {2},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11277-021-08167-y,
author = {Radhika, S. and Rangarajan, P.},
title = {Fuzzy Based Sleep Scheduling Algorithm with Machine Learning Techniques to Enhance Energy Efficiency in Wireless Sensor Networks},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {118},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08167-y},
doi = {10.1007/s11277-021-08167-y},
abstract = {Wireless sensor networks, generally, are grouped into clusters to collect information effectively. Such grouping of nodes helps immensely to elongate the life of Wireless Sensor Networks. Message exchanges between nodes for consecutive and periodic clustering overload the sensor nodes and cause a shortfall of energy. Additional overhead during cluster formation, instability in energy use and the difficulty of information sharing during clustering, uncertain network structure, etc. are the current clustering problems. There is also a need to enhance intra-cluster transmission and to find effective methods to extend the network's lifespan. This paper aims to reduce the energy loss of nodes by reducing the message transmission overhead and simplifying the creation and upgrading of clusters to improve the lifespan of the network. A clustering strategy where the cluster is regularly restructured to decrease the overhead on cluster head nodes is also proposed in the paper. The suggested approach reduces data transmission using machine learning by the cluster member nodes and reduces the energy consumption of individual sensor nodes by implementing a suitable active/sleep schedule. To calculate the cluster update cycle and sleep cycle, it also makes use of the advantages of fuzzy logic by selecting appropriate fuzzy descriptors such as average data rate, distance from the head node to the sink and the remaining energy. The proposed approach optimizes the energy utilization of cluster heads and node members thereby enhancing the lifespan of the sensor network.},
journal = {Wirel. Pers. Commun.},
month = jun,
pages = {3025–3044},
numpages = {20},
keywords = {Sleep scheduling, Wireless sensor networks, Energy enhancement, Network lifetime, Machine learning, Fuzzy inference, Clustering}
}

@inproceedings{10.1145/3500931.3500962,
author = {Wu, Zhixuan and Mao, Yuwei and Li, Qiyu},
title = {Procedural Game Map Generation using Multi-leveled Cellular Automata by Machine learning},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500962},
doi = {10.1145/3500931.3500962},
abstract = {The concept of Procedural Content Generation (PCG) has been intensively applied in the game industry for its capability of producing infinite game maps without any human effort. Innumerable games, such as Minecraft, Terraria, and No Man's Sky, successfully employed this technique to create unpredictable yet playful gaming experiences. While randomness is essential to adding engaging elements to a game, complete randomness may hurt the outcome of map generations by making a chaotic scene. To address this issue, this paper introduces an effective way of "tweaking" the randomness to generate flexible, endless, natural-looking game maps by machine learning.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {168–172},
numpages = {5},
keywords = {medical deep learning, Roguelike Game, Procedural Content Generation, Cellular Automata, Cave Generation Algorithm},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.5555/3524938.3525060,
author = {Calder, Jeff and Cook, Brendan and Thorpe, Matthew and Slep\v{c}ev, Dejan},
title = {Poisson learning: graph based semi-supervised learning at very low label rates},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a new framework, called Poisson learning, for graph based semi-supervised learning at very low label rates. Poisson learning is motivated by the need to address the degeneracy of Laplacian semi-supervised learning in this regime. The method replaces the assignment of label values at training points with the placement of sources and sinks, and solves the resulting Poisson equation on the graph. The outcomes are provably more stable and informative than those of Laplacian learning. Poisson learning is efficient and simple to implement, and we present numerical experiments showing the method is superior to other recent approaches to semi-supervised learning at low label rates on MNIST, FashionMNIST, and Cifar-10. We also propose a graph-cut enhancement of Poisson learning, called Poisson MBO, that gives higher accuracy and can incorporate prior knowledge of relative class sizes.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {122},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1145/3285002.3285005,
author = {Bishop, Matt and Gates, Carrie and Levitt, Karl},
title = {Augmenting Machine Learning with Argumentation},
year = {2018},
isbn = {9781450365970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285002.3285005},
doi = {10.1145/3285002.3285005},
abstract = {The information security community is haunted by the failure of an appropriate break-the-glass access control at the United States Center for Disease Control that led to an estimated additional 1.2 million deaths in North America in 2036. In this paper we review what caused the security failures in this system and argue that, by combining human intelligence with multiple technological approaches to create a system that emphasizes human approaches to guide analysis, the failures that occurred will not recur. We also leverage people and technologies to identify and fill gaps in the training data to minimize the threat of unexpected events. While we use this scenario as our running example, we note that our approach is generalizable to a broader problem space where machine learning approaches have been deployed to make decisions.},
booktitle = {Proceedings of the New Security Paradigms Workshop},
pages = {1–11},
numpages = {11},
keywords = {pandemic, break the glass, argumentation},
location = {Windsor, United Kingdom},
series = {NSPW '18}
}

