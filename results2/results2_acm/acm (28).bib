@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@inproceedings{10.5555/3524938.3525368,
author = {Jacot, Arthur and \c{S}im\c{s}ek, Berfin and Spadaro, Francesco and Hongler, Cl\'{e}ment and Gabriel, Franck},
title = {Implicit regularization of random feature models},
year = {2020},
publisher = {JMLR.org},
abstract = {Random Feature (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with P features, N data points, and a ridge λ, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an effective ridge λ˜. We show that λ˜ &gt; λ and λ˜ ↘ λ monotonically as P grows, thus revealing the implicit regularization effect of finite RF sampling. We then compare the risk (i.e. test error) of the λ-KRR predictor with the average risk of the λ-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average λ-RF predictor and λ˜-KRR predictor.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {430},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.5555/3455716.3455816,
author = {Valera, Isabel and Pradier, Melanie F. and Lomeli, Maria and Ghahramani, Zoubin},
title = {General latent feature models for heterogeneous datasets},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {100},
numpages = {49}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {bias, fairness, feature selection, machine learning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1007/978-3-030-91387-8_8,
author = {Tsuchiya, Takeshi and Mochizuki, Ryuichi and Hirose, Hiroo and Yamada, Tetsuyasu and Koyanagi, Keiichi and Minh, Quang Tran},
title = {Selective Combination and Management of Distributed Machine Learning Models},
year = {2021},
isbn = {978-3-030-91386-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91387-8_8},
doi = {10.1007/978-3-030-91387-8_8},
abstract = {This study presents a method for selecting and combining feature models constructed by the machine learning on the processing task capability. The evaluation of combining the feature models shows that the processing task capability can be improved by selecting and reaching feature models based on their similarity to the vector of queries without combining all feature models. Then, we discuss a method for constructing logical the R-Tree algorithm on the distributed fog nodes. For future work, we will implement the proposed method on various types of data.},
booktitle = {Future Data and Security Engineering: 8th International Conference, FDSE 2021, Virtual Event, November 24–26, 2021, Proceedings},
pages = {113–124},
numpages = {12},
keywords = {Fog computing model, Distributed future model, Similarity of future models}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@article{10.1016/j.eswa.2020.114022,
author = {Rauber, Thomas Walter and da Silva Loca, Antonio Luiz and Boldt, Francisco de Assis and Rodrigues, Alexandre Loureiros and Varej\~{a}o, Fl\'{a}vio Miguel},
title = {An experimental methodology to evaluate machine learning methods for fault diagnosis based on vibration signals},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114022},
doi = {10.1016/j.eswa.2020.114022},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {18},
keywords = {Fault detection, CWRU bearing fault database, Performance criteria, Classification, Pattern recognition, Machine learning}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@inproceedings{10.1145/3463677.3463762,
author = {Bojjireddy, Sirisha and Chun, Soon Ae and Geller, James},
title = {Machine Learning Approach to Detect Fake News, Misinformation in COVID-19 Pandemic},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463762},
doi = {10.1145/3463677.3463762},
abstract = {Fake news is false information about current events, intentionally created to mislead readers. The spread of such fake news has the potential to create a negative impact on individuals and society. With today’s straightforward creation of social media posts, there has been an increasing amount of fake news, compared to traditional media in the past. We present one of the most serious societal issue of misinformation, specifically using Presidential Election and COVID-19 health related fake news. We present multi-dimensional approaches that organizations and individuals could utilize for detecting fake news, ranging from human/social approaches, to technical approaches to organizational trust/policy approaches. The Machine Learning approach as a technical solution is presented for automating the detection of fake news and misleading contents. A fake news detection web application is presented to make it easy for end users to determine whether an article is legitimate or fake.},
booktitle = {Proceedings of the 22nd Annual International Conference on Digital Government Research},
pages = {575–578},
numpages = {4},
keywords = {Covid-19 misinformation, Fake news, machine learning, misinformation},
location = {Omaha, NE, USA},
series = {dg.o '21}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.5555/3045390.3045510,
author = {Zhang, Aonan and Paisley, John},
title = {Markov latent feature models},
year = {2016},
publisher = {JMLR.org},
abstract = {We introduce Markov latent feature models (MLFM), a sparse latent feature model that arises naturally from a simple sequential construction. The key idea is to interpret each state of a sequential process as corresponding to a latent feature, and the set of states visited between two null-state visits as picking out features for an observation. We show that, given some natural constraints, we can represent this stochastic process as a mixture of recurrent Markov chains. In this way we can perform correlated latent feature modeling for the sparse coding problem. We demonstrate two cases in which we define finite and infinite latent feature models constructed from first-order Markov chains, and derive their associated scalable inference algorithms. We show empirical results on a genome analysis task and an image denoising task.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1129–1137},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3336191.3371877,
author = {Deldjoo, Yashar and Di Noia, Tommaso and Merra, Felice Antonio},
title = {Adversarial Machine Learning in Recommender Systems (AML-RecSys)},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371877},
doi = {10.1145/3336191.3371877},
abstract = {Recommender systems (RS) are an integral part of many online services aiming to provide an enhanced user-oriented experience. Machine learning (ML) models are nowadays broadly adopted in modern state-of-the-art approaches to recommendation, which are typically trained to maximize a user-centred utility (e.g., user satisfaction) or a business-oriented one (e.g., profitability or sales increase). They work under the main assumption that users' historical feedback can serve as proper ground-truth for model training and evaluation. However, driven by the success in the ML community, recent advances show that state-of-the-art recommendation approaches such as matrix factorization (MF) models or the ones based on deep neural networks can be vulnerable to adversarial perturbations applied on the input data. These adversarial samples can impede the ability for training high-quality MF models and can put the driven success of these approaches at high risk.As a result, there is a new paradigm of secure training for RS that takes into account the presence of adversarial samples into the recommendation process. We present adversarial machine learning in Recommender Systems (AML-RecSys), which concerns the study of effective ML techniques in RS to fight against an adversarial component. AML-RecSys has been proposed in two main fashions within the RS literature: (i) adversarial regularization, which attempts to combat against adversarial perturbation added to input data or model parameters of a RS and, (ii) generative adversarial network (GAN)-based models, which adopt a generative process to train powerful ML models. We discuss a theoretical framework to unify the two above models, which is performed via a minimax game between an adversarial component and a discriminator. Furthermore, we explore various examples illustrating the successful application of AML to solve various RS tasks. Finally, we present a global taxonomy/overview of the academic literature based on several identified dimensions, namely (i) research goals and challenges, (ii) application domains and (iii) technical overview.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {869–872},
numpages = {4},
keywords = {adversarial machine learning, deep learning, recommender systems, security},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1007/s00779-020-01414-2,
author = {Tegen, Agnes and Davidsson, Paul and Persson, Jan A.},
title = {Activity recognition through interactive machine learning in a dynamic sensor setting},
year = {2020},
issue_date = {Feb 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-020-01414-2},
doi = {10.1007/s00779-020-01414-2},
abstract = {The advances in Internet of things lead to an increased number of devices generating and streaming data. These devices can be useful data sources for activity recognition by using machine learning. However, the set of available sensors may vary over time, e.g. due to mobility of the sensors and technical failures. Since the machine learning model uses the data streams from the sensors as input, it must be able to handle a varying number of input variables, i.e. that the feature space might change over time. Moreover, the labelled data necessary for the training is often costly to acquire. In active learning, the model is given a budget for requesting labels from an oracle, and aims to maximize accuracy by careful selection of what data instances to label. It is generally assumed that the role of the oracle only is to respond to queries and that it will always do so. In many real-world scenarios however, the oracle is a human user and the assumptions are simplifications that might not give a proper depiction of the setting. In this work we investigate different interactive machine learning strategies, out of which active learning is one, which explore the effects of an oracle that can be more proactive and factors that might influence a user to provide or withhold labels. We implement five interactive machine learning strategies as well as hybrid versions of them and evaluate them on two datasets. The results show that a more proactive user can improve the performance, especially when the user is influenced by the accuracy of earlier predictions. The experiments also highlight challenges related to evaluating performance when the set of classes is changing over time.},
journal = {Personal Ubiquitous Comput.},
month = jun,
pages = {273–286},
numpages = {14},
keywords = {Interactive machine learning, Activity recognition, Internet of things, Active learning, Machine learning}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@article{10.1007/s42979-020-00171-6,
author = {Tsuchiya, Takeshi and Mochizuki, Ryuichi and Hirose, Hiroo and Yamada, Tetsuyasu and Koyanagi, Keiichi and Minh Tran, Quang},
title = {Distributed Data Platform for Machine Learning Using the Fog Computing Model},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {3},
url = {https://doi.org/10.1007/s42979-020-00171-6},
doi = {10.1007/s42979-020-00171-6},
abstract = {In this paper, we propose a machine learning-driven information platform for distributed data management without data accumulation using a fog computing model. It helps in analyzing the features of managed contents through several kinds of methods and their synchronization among the distributed nodes. Moreover, the feature model generated based on the features of all contents is achieved by combining the respective feature models distributed to some nodes. In other words, a flexible feature model is created by the combining feature models adapting to targets arbitrarily. In this paper, we propose the basic ideas and some basic functions for the data management platform among distributed nodes. Our evaluation confirms the effectiveness of combining the feature models using datasets.},
journal = {SN Comput. Sci.},
month = may,
numpages = {9},
keywords = {Fog computing model, Distributed machine learning, Cloud computing}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3084226.3084276,
author = {Zhou, Shulin and Li, Shanshan and Liu, Xiaodong and Xu, Xiangyang and Zheng, Si and Liao, Xiangke and Xiong, Yun},
title = {Easier Said Than Done: Diagnosing Misconfiguration via Configuration Constraints Analysis: A Study of the Variance of Configuration Constraints in Source Code},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084276},
doi = {10.1145/3084226.3084276},
abstract = {Misconfigurations have drawn tremendous attention for their increasing prevalence and severity, and the main causes are the complexity of configurations as well as the lack of domain knowledge for software. To diagnose misconfigurations, one typical approach is to find out the conditions that configuration options should satisfy, which we refer to as configuration constraints. Current researches only handled part of the situations of configuration constraints in source code, which provide only limited help for misconfiguration diagnosis. To better extract configuration constraints, we conduct a comprehensive manual study on the existence and variance of the configuration constraints in source code from five pieces of popular open-source software. We summarized several findings from different aspects, including the general statistics about configuration constraints, the general features for specific configurations, and the obstacles in extraction of configuration constraints. Based on the findings, we propose several suggestions to maximize the automation of constraints extraction.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {196–201},
numpages = {6},
keywords = {Misconfiguration, configuration constraints, misconfiguration diagnosis},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.5555/3122009.3208008,
author = {Perrone, Valerio and Jenkins, Paul A. and Span\`{o}, Dario and Teh, Yee Whye},
title = {Poisson random fields for dynamic feature models},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4626–4670},
numpages = {45},
keywords = {Bayesian nonparametrics, Markov chain Monte Carlo, Poisson random field, indian buffet process, topic model}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {change, failures, reliability, reuse, software product lines},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {decision support, regression testing, software product line testing, test coverage, test selection, visualization},
series = {ICSTW '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@article{10.1007/s10270-020-00856-9,
author = {Pilarski, Sebastian and Staniszewski, Martin and Bryan, Matthew and Villeneuve, Frederic and Varr\'{o}, D\'{a}niel},
title = {Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00856-9},
doi = {10.1007/s10270-020-00856-9},
abstract = {The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {685–709},
numpages = {25},
keywords = {Prediction-at-runtime, Machine learning, Neural networks, Automated deployment, Code generation, Gas turbine engines}
}

@article{10.1016/j.compbiomed.2021.104517,
author = {Kreimeyer, Kory and Dang, Oanh and Spiker, Jonathan and Mu\~{n}oz, Monica A. and Rosner, Gary and Ball, Robert and Botsis, Taxiarchis},
title = {Feature engineering and machine learning for causality assessment in pharmacovigilance: Lessons learned from application to the FDA Adverse Event Reporting System},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104517},
doi = {10.1016/j.compbiomed.2021.104517},
journal = {Comput. Biol. Med.},
month = aug,
numpages = {9},
keywords = {Pharmacovigilance, Case classification, Clinical natural language processing, Decision support, Causality assessment}
}

@inproceedings{10.1145/3442391.3442408,
author = {Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Uta, Mathias and Tran, Thi Ngoc Trang and Atas, M\"{u}sl\"{u}m},
title = {An Overview of Recommender Systems and Machine Learning in Feature Modeling and Configuration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442408},
doi = {10.1145/3442391.3442408},
abstract = {Recommender systems support decisions in various domains ranging from simple items such as books and movies to more complex items such as financial services, telecommunication equipment, and software systems. In this context, recommendations are determined, for example, on the basis of analyzing the preferences of similar users. In contrast to simple items which can be enumerated in an item catalog, complex items have to be represented on the basis of variability models (e.g., feature models) since a complete enumeration of all possible configurations is infeasible and would trigger significant performance issues. In this paper, we give an overview of a potential new line of research which is related to the application of recommender systems and machine learning techniques in feature modeling and configuration. In this context, we give examples of the application of recommender systems and machine learning and discuss future research issues.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {8},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.eswa.2014.10.003,
author = {Kurtulmu\c{s}, F. and \"{U}nal, H.},
title = {Discriminating rapeseed varieties using computer vision and machine learning},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.003},
doi = {10.1016/j.eswa.2014.10.003},
abstract = {We developed an expert system for detection of rapeseed variety using computer vision and machine learning.Scanner images can be used to discriminate rapeseed variety.Developed algorithm determines successfully variety of rapeseed with an accuracy rate of 99.24%. Rapeseed is widely cultivated throughout the world for the production of animal feed, vegetable fat for human consumption, and biodiesel. Since the seeds are evaluated in many areas for sowing and oilseed processing, they must be identified quickly and accurately for selection of a correct variety. An affordable method based on computer vision and machine learning was proposed to classify the seven rapeseed varieties. Different types of feature sets, feature models, and machine learning classifiers were investigated to obtain the best predictive model for rapeseed classification. The training and test sets were used to tune the model parameters during the training epochs by varying the complexity of the predictive models with grid-search and K-fold cross validation. After obtaining optimized models for each level of complexity, a dedicated validation set was used to validate predictive models. The developed computer vision system provided an overall accuracy rate of 99.24% for the best predictive model in discriminating rapeseed variety.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1880–1891},
numpages = {12},
keywords = {Computer vision, Machine learning, Rapeseed, Variety discrimination}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.future.2019.06.022,
author = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Zhao, Ming and Rehman, Zia ur},
title = {A comparative analysis of machine learning models for quality pillar assessment of SaaS services by multi-class text classification of users’ reviews},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.06.022},
doi = {10.1016/j.future.2019.06.022},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {341–371},
numpages = {31},
keywords = {SaaS, Quality pillars, User reviews, Text classification, Machine learning approaches}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-540-88582-5_50,
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
title = {Enhancing Software Product Line Maintenance with Source Code Mining},
year = {2008},
isbn = {9783540885818},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-88582-5_50},
doi = {10.1007/978-3-540-88582-5_50},
abstract = {Large-scale reuse and accelerated software development have been some of the key attractions behind software product lines. Various strategies and processes have been developed to facilitate product line development, maintenance, and evolution. However, experiences with software product lines also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and effort needed to manage and maintain product lines increase and quality degrades as product lines evolve. Without proper methods and tools to support the evolution, the cost can outweigh the benefits.This paper describes an approach to simplifying the maintenance of software product lines and improving software quality by integrating traditional software maintenance practices with pattern-based source code mining for defect detection and correction. Our case studies were performed in an industrial setting where the evolution of multiple mobile phone models of a product line was investigated.},
booktitle = {Proceedings of the Third International Conference on Wireless Algorithms, Systems, and Applications},
pages = {538–547},
numpages = {10},
keywords = {Product Line, Reuse, Software Maintenance},
location = {Dallas, Texas},
series = {WASA '08}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3292522.3326027,
author = {Reis, Julio C. S. and Correia, Andr\'{e} and Murai, Fabr\'{\i}cio and Veloso, Adriano and Benevenuto, Fabr\'{\i}cio},
title = {Explainable Machine Learning for Fake News Detection},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326027},
doi = {10.1145/3292522.3326027},
abstract = {Recently, there have been many research efforts aiming to understand fake news phenomena and to identify typical patterns and features of fake news. Yet, the real discriminating power of these features is still unknown: some are more general, but others perform well only with specific data. In this work, we conduct a highly exploratory investigation that produced hundreds of thousands of models from a large and diverse set of features. These models are unbiased in the sense that their features are randomly chosen from the pool of available features. While the vast majority of models are ineffective, we were able to produce a number of models that yield highly accurate decisions, thus effectively separating fake news from actual stories. Specifically, we focused our analysis on models that rank a randomly chosen fake news story higher than a randomly chosen fact with more than 0.85 probability. For these models we found a strong link between features and model predictions, showing that some features are clearly tailored for detecting certain types of fake news, thus evidencing that different combinations of features cover a specific region of the fake news space. Finally, we present an explanation of factors contributing to model decisions, thus promoting civic reasoning by complementing our ability to evaluate digital content and reach warranted conclusions.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {17–26},
numpages = {10},
keywords = {civic reasoning, fake news, features, social media},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1186/s13638-020-01709-1,
author = {Shinkuma, Ryoichi and Nishio, Takayuki and Inagaki, Yuichi and Oki, Eiji},
title = {Data assessment and prioritization in mobile networks for real-time prediction of spatial information using machine learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-020-01709-1},
doi = {10.1186/s13638-020-01709-1},
abstract = {A new framework of data assessment and prioritization for real-time prediction of spatial information is presented. The real-time prediction of spatial information is promising for next-generation mobile networks. Recent developments in machine learning technology have enabled prediction of spatial information, which will be quite useful for smart mobility services including navigation, driving assistance, and self-driving. Other key enablers for forming spatial information are image sensors in mobile devices like smartphones and tablets and in vehicles such as cars and drones and real-time cognitive computing like automatic number/license plate recognition systems and object recognition systems. However, since image data collected by mobile devices and vehicles need to be delivered to the server in real time to extract input data for real-time prediction, the uplink transmission speed of mobile networks is a major impediment. This paper proposes a framework of data assessment and prioritization that reduces the uplink traffic volume while maintaining the prediction accuracy of spatial information. In our framework, machine learning is used to estimate the importance of each data element and to predict spatial information under the limitation of available data. A numerical evaluation using an actual vehicle mobility dataset demonstrated the validity of the proposed framework. Two extension schemes in our framework, which use the ensemble of importance scores obtained from multiple feature selection methods, are also presented to improve its robustness against various machine learning and feature selection methods. We discuss the performance of those schemes through numerical evaluation.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = may,
numpages = {19},
keywords = {Spatial information, Real-time prediction, Mobile crowdsensing, Data assessment, Machine learning, Feature selection}
}

@phdthesis{10.5555/2518534,
author = {Menon, Aditya Krishna},
advisor = {Elkan, Charles},
title = {Latent feature models for dyadic prediction},
year = {2013},
isbn = {9781267998538},
publisher = {University of California at San Diego},
address = {USA},
abstract = {Following the Netflix prize, the collaborative filtering problem has gained significant attention within machine learning, spawning novel models and theoretical analyses. In parallel, the growth of social media has driven research in link prediction, with the aim of determining whether two individuals in a network are likely to know each other. Both problems involve the prediction of label (star ratings or friendship) between a pair of entities (user-movie or user-user). We call this general problem dyadic prediction. The problem arises in several other guises: predicting student responses to test questions, military disputes between nations, and clickthrough rates of webpages on ads, to name a few. In general, each such domain employs a markedly different approach, obscuring the underlying similarity of the problems being solved. This dissertation aims to explore the use of a single general method, based on latent feature modelling, for generic dyadic prediction problems. To this end, we make three contributions. First, we propose a generic  framework  with which to analyze dyadic prediction problems. This lets one reason about seemingly disparate problems in a unified manner. Second, we propose a  model  based on the log-linear framework, which is applicable to each of the aforementioned problems. The model learns  latent features  from dyadic data, and estimates a probability distribution over labels. Third, we systematically explore  applications  of our latent feature model to domains such as collaborative filtering, link prediction, and clickthrough rate prediction. In all cases, we show performance comparable or superior to existing state-of-the-art methods. For clickthrough rate prediction, ours represents the first application of latent feature modelling to the problem, demonstrating the value in a single framework with which to reason about these problems. We also show that latent feature modelling is scalable to datasets with hundreds of millions of observations on a single machine (the Netflix prize dataset), and hundreds of billions of observations on a small cluster (Yahoo! ad click data). We conclude with a discussion of future research directions, including transferring information from one network to another, and adapting to domains with extreme label sparsity.},
note = {AAI3557100}
}

@article{10.1007/s10846-016-0449-6,
author = {Liu, Weihui and Chen, Diansheng and Steil, Jochen},
title = {Analytical Inverse Kinematics Solver for Anthropomorphic 7-DOF Redundant Manipulators with Human-Like Configuration Constraints},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {86},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-016-0449-6},
doi = {10.1007/s10846-016-0449-6},
abstract = {It is a common belief that service robots shall move in a human-like manner to enable natural and convenient interaction with a human user or collaborator. In particular, this applies to anthropomorphic 7-DOF redundant robot manipulators that have a shoulder-elbow-wrist configuration. On the kinematic level, human-like movement then can be realized by means of selecting a redundancy resolution for the inverse kinematics (IK), which realizes human-like movement through respective nullspace preferences. In this paper, key positions are introduced and defined as Cartesian positions of the manipulator's elbow and wrist joints. The key positions are used as constraints on the inverse kinematics in addition to orientation constraints at the end-effector, such that the inverse kinematics can be calculated through an efficient analytical scheme and realizes human-like configurations. To obtain suitable key positions, a correspondence method named wrist-elbow-in-line is derived to map key positions of human demonstrations to the real robot for obtaining a valid analytical inverse kinematics solution. A human demonstration tracking experiment is conducted to evaluate the end-effector accuracy and human-likeness of the generated motion for a 7-DOF Kuka-LWR arm. The results are compared to a similar correspondance method that emphasizes only the wrist postion and show that the subtle differences between the two different correspondence methods may lead to significant performance differences. Furthermore, the wrist-elbow-in-line method is validated as more stable in practical application and extended for obstacle avoidance.},
journal = {J. Intell. Robotics Syst.},
month = apr,
pages = {63–79},
numpages = {17},
keywords = {Correspondance problem, Human-like motion, Inverse kinematics, Redundancy resolution}
}

@article{10.1007/s11042-018-6667-0,
author = {Liu, Hongbin and Chang, Faliang and Liu, Chunsheng},
title = {Multi-target tracking with hierarchical data association using main-parts and spatial-temporal feature models},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6667-0},
doi = {10.1007/s11042-018-6667-0},
abstract = {Multi-target tracking in complex scenes is challenging because target appearance features generate partial or significant variations frequently. In order to solve the problem, we propose a multi-target tracking method with hierarchical data association using main-parts and spatial-temporal feature models. In our tracking framework, target feature models and tracklets are initialized when the new targets appear. Main-parts feature model is presented to represent target with partial or no appearance variations. It is established by partitioning a target template into several parts and formulating appearance variation densities of these parts. For the target with significant appearance variations, the tracker learns its global spatial-temporal feature model by integrating appearance with histogram of optical flow features. During tracking, tracklet confidence is exploited to implement hierarchical data association. According to different tracklet confidence values, main-parts and global data association are respectively performed by employing main-parts and spatial-temporal feature models. As a result, our approach uses the Hungarian algorithm to obtain optimal associated pairs between target tracklets and detections. Finally, target feature models and tracklets are updated by the association detections for subsequently tracking. Experiments conducted on CAVIAR, Parking Lot and MOT15 datasets verify the effectiveness and improvement of our multi-target tracking method.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {29161–29181},
numpages = {21},
keywords = {Data association, Appearance variation, Main-parts, Spatial-temporal feature, Multi-target tracking}
}

@inproceedings{10.1007/978-3-030-58811-3_40,
author = {Araldi, Alessandro and Venerandi, Alessandro and Fusco, Giovanni},
title = {Count Regression and Machine Learning Approach for Zero-Inflated Over-Dispersed Count Data. Application to Micro-Retail Distribution and Urban Form},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_40},
doi = {10.1007/978-3-030-58811-3_40},
abstract = {This paper investigates the relationship between urban form and the spatial distribution of micro-retail activities. In the last decades, several works demonstrated how configurational properties of the street network and morphological descriptors of the urban built environment are significantly related to store distribution. However, two main challenges still need to be addressed. On the one side, the combined effect of different urban form properties should be considered providing a holistic study of the urban form and its relationship to retail patterns. On the other, analytical approaches should consider the discrete, skewed and zero-inflated nature of the micro-retail distribution. To overcome these limitations, this work compares two sophisticated modelling procedure: Penalised Count Regression and Machine Learning approaches. While the former is specifically conceived to account for retail count distribution, the latter can capture non-linear behaviours in the data. The two modelling procedures are implemented on the same large dataset of street-based measures describing the urban form of the French Riviera. The outcomes of the two modelling approaches are compared in terms of prediction performance and selection frequencies of the most recurrent variables among the implemented models.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {550–565},
numpages = {16},
keywords = {Retail distribution, Urban form, Street-network configuration, Feature selection, Penalised models, Machine Learning},
location = {Cagliari, Italy}
}

@article{10.1109/TPAMI.2014.2321387,
author = {Gershman, Samuel J. and Frazier, Peter I. and Blei, David M.},
title = {Distance Dependent Infinite Latent Feature Models},
year = {2015},
issue_date = {Feb. 2015},
publisher = {IEEE Computer Society},
address = {USA},
volume = {37},
number = {2},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2014.2321387},
doi = {10.1109/TPAMI.2014.2321387},
abstract = {Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the &lt;italic&gt;distance dependent Indian buffet process&lt;/italic&gt; (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. We develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on real-world non-exchangeable data.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = feb,
pages = {334–345},
numpages = {12}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {ArgoUML SPL benchmark, dynamic feature localization, spectrum-based localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1504/ijsnet.2020.109720,
author = {Cheng, Haosu and Liu, Jianwei and Xu, Tongge and Ren, Bohan and Mao, Jian and Zhang, Wei},
title = {Machine learning based low-rate DDoS attack detection for SDN enabled IoT networks},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {34},
number = {1},
issn = {1748-1279},
url = {https://doi.org/10.1504/ijsnet.2020.109720},
doi = {10.1504/ijsnet.2020.109720},
abstract = {The software-defined network (SDN) enabled internet of things (IoT) architecture is deployed in many industrial systems. The ability of SDN to intelligently route traffic and use underutilised network resources, enables IoT networks to cope with data onslaught smoothly. SDN also eliminates bottlenecks and helps to process IoT data efficiently without placing a larger strain on the network. The SDN-based IoT network is vulnerable to DDoS attack in a sophisticated usage environment. The SDN-based IoT network behaviours are different from traditional networks, which makes the detection of low-traffic DDoS attacks more difficult. In this paper, we propose a learning-based detection approach that deploys learning algorithms and utilizes stateful and stateless features from Openflow packages to identify attack traffics in SDN control and data planes. Our prototype approach and experiment results show that our system identified the low-rate DDoS attack traffic accurately with relatively low system performance overheads.},
journal = {Int. J. Sen. Netw.},
month = jan,
pages = {56–69},
numpages = {13},
keywords = {IoT, internet of things, software-defined networking, industrial system, low-rate distributed denial-of-service, machine learning}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3220162.3220188,
author = {Ma, Xiaofeng and Yang, Yan and Zhou, Zhurong},
title = {Using Machine Learning Algorithm to Predict Student Pass Rates In Online Education},
year = {2018},
isbn = {9781450364577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220162.3220188},
doi = {10.1145/3220162.3220188},
abstract = {In online education, the quality evaluation of education is crucial importance to schools and even to the entire range of educational institutions. There are many ways to evaluate online education. Taking the prediction of student pass rates as an example, many researchers have used machine Learning algorithms to predict student pass rates and find out important student features affecting learning. However, they did not establish feature model for online education that predicts the student pass rates and introduce deep neural network (DNN) algorithms - a new method in machine learning into online education. Therefore, this study first explores how to establish a feature model that predicts the student pass rates for online education, and then uses the grid search (GS) algorithm to optimize the decision tree algorithm (DT) and support vector machine (SVM) algorithm to improve the prediction accuracy. Finally, compared the improved algorithm with the DNN algorithm, we find a suitable algorithm for student pass rate prediction. The purpose of this study is to improve the quality of online teaching by predicting the student pass rates, increasing students' academic performance and strengthening online educational management.},
booktitle = {Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing},
pages = {156–161},
numpages = {6},
keywords = {Decision Tree Algorithm, Deep Neural Network, Feature Model, Grid Search Algorithm, Support Vector Machine Algorithm},
location = {Shenzhen, China},
series = {ICMSSP '18}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/2999611.2999747,
author = {Hayashi, Kohei and Fujimaki, Ryohei},
title = {Factorized asymptotic Bayesian inference for latent feature models},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models (LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hessian matrix of a complete log-likelihood, which is required to derive a "factorized information criterion" (FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models. FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1214–1222},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1109/TMM.2015.2477242,
author = {Hasan, Mahmudul and Roy-Chowdhury, Amit K.},
title = {A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models},
year = {2015},
issue_date = {Nov. 2015},
publisher = {IEEE Press},
volume = {17},
number = {11},
issn = {1520-9210},
url = {https://doi.org/10.1109/TMM.2015.2477242},
doi = {10.1109/TMM.2015.2477242},
abstract = {Most of the research on human activity recognition has focused on learning a static model, considering that all the training instances are labeled and present in advance, while in streaming videos new instances continuously arrive and are not labeled. Moreover, these methods generally use application- specific hand-engineered and static feature models, which are not suitable for continuous learning. Some recent approaches on activity recognition use deep-learning-based hierarchical feature models, but the large size of these networks constrain them from being used in continuous learning scenarios. In this work, we propose a continuous activity learning framework for streaming videos by intricately tying together deep hybrid feature models and active learning. This allows us to automatically select the most suitable features and take the advantage of incoming unlabeled instances to improve the existing model incrementally. Given the segmented activities from streaming videos, we learn features in an unsupervised manner using deep hybrid networks, which have the ability to take the advantage of both the local hand-engineered features and the deep model in an efficient way. Additionally, we use active learning to train the activity classifier using a reduced amount of manually labeled instances. Retraining the models with a huge amount of accumulated examples is computationally expensive and not suitable for continuous learning. Hence, we propose a method to select the best subset of these examples to update the models incrementally. We conduct rigorous experiments on four challenging human activity datasets to demonstrate the effectiveness of our framework.},
journal = {Trans. Multi.},
month = nov,
pages = {1909–1922},
numpages = {14}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/MODELS-C.2019.00083,
author = {Sch\"{o}ne, Ren\'{e} and Mey, Johannes and Ren, Boqi and A\ss{}mann, Uwe},
title = {Bridging the gap between smart home platforms and machine learning using relational reference attribute grammars},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00083},
doi = {10.1109/MODELS-C.2019.00083},
abstract = {Machine learning improves the development of self-adaptive systems, i.e., to react to unforeseen situations. However, state-of-the-art smart home platforms lack the ability to integrate machine learning models directly into their knowledge base. While there are machine learning approaches for smart home environments, they are usually not integrated nor represented in the middleware of smart environments. Instead, they rely on data obtained previously and use it in a form prepared for a single use case. To remedy this, our aim is to extend existing middleware platforms to integrate machine learning algorithms as first-class concepts of the knowledge base. To achieve this, we employ relational Reference Attribute Grammars to design and implement an integrated runtime model, where machine learning models can be represented and related to elements of the extended knowledge base, e.g., physical entities, location, users, and activities. Consequently, this enables using a state-of-the-art middleware to build a self-adaptive system, which integrates machine learning algorithms enabling both context-awareness and self-awareness. To showcase the feasibility of our approach, we implemented a small smart home scenario using openHAB as a middleware, in which the system learns the preferences of a user using neural networks.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {533–542},
numpages = {10},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@article{10.1016/j.patrec.2015.07.023,
author = {Lazzarini, Nicola and Nanni, Loris and Fantozzi, Carlo and Pietracaprina, Andrea and Pucci, Geppino and Seccia, Teresa Maria and Rossi, Gian Paolo},
title = {Heterogeneous machine learning system for improving the diagnosis of primary aldosteronism},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2015.07.023},
doi = {10.1016/j.patrec.2015.07.023},
abstract = {Novel classifier for the diagnosis of Aldosterone-Producing Adenoma.Improved accuracy (as measured by EUC metric) with respect to state of art.Feature correlation, missing values and class imbalance all effectively managed.Validation through a novel, robust technique over a large dataset of patients. We develop a novel classifier for the diagnosis of Aldosterone-Producing Adenoma (APA), which induces Primary Aldosteronism, the most common endocrine cause of curable hypertension. The classifier considerably improves upon the state of the art, and it is devised and tested on a large dataset of patients, each described by several demographic and biochemical features. As customary in real-world datasets, ours is affected by feature correlation, missing values, and class imbalance. We make explicit provisions for dealing with all of these issues through an ensemble of ensembles, that is, a multilevel fusion of different component classifiers. Using the Wilcoxon signed-rank test at 0.05 significance level, we show that our ensemble significantly outperforms the state-of-the-art classifier and any individual component in the ensemble. Our experiments employ a "leave-one-out-clinical" cross validation as patients were treated in 15 different specialized centers for hypertension; in each fold, 14 centers are used for training and 1 as the test set. Our classifier is available at http://www.dei.unipd.it/node/2357 (MATLAB code).},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {124–130},
numpages = {7},
keywords = {Class imbalance, Ensemble of classifiers, Feature correlation, Missing values, Primary aldosteronism}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@inproceedings{10.5555/1158337.1158678,
author = {Czarnecki, Krzysztof and Peter Kim, Chang Hwan and Kalleberg, Karl Trygve},
title = {Feature Models are Views on Ontologies},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {41–51},
numpages = {11},
series = {SPLC '06}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/2034117.2034151,
author = {Nguyen, Canh Hao and Mamitsuka, Hiroshi},
title = {Kernels for link prediction with latent feature models},
year = {2011},
isbn = {9783642237829},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Predicting new links in a network is a problem of interest in many application domains. Most of the prediction methods utilize information on the network's entities such as nodes to build a model of links. Network structures are usually not used except for the networks with similarity or relatedness semantics. In this work, we use network structures for link prediction with a more general network type with latent feature models. The problem is the difficulty to train these models directly for large data. We propose a method to solve this problem using kernels and cast the link prediction problem into a binary classification problem. The key idea is not to infer latent features explicitly, but to represent these features implicitly in the kernels, making the method scalable to large networks. In contrast to the other methods for latent feature models, our method inherits all the advantages of kernel framework: optimality, efficiency and nonlinearity. We apply our method to real data of protein-protein interactions to show the merits of our method.},
booktitle = {Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II},
pages = {517–532},
numpages = {16},
location = {Athens, Greece},
series = {ECML PKDD'11}
}

@article{10.1016/j.jss.2010.11.920,
author = {Xie, Xiaoyuan and Ho, Joshua W. K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
title = {Testing and validating machine learning classifiers by metamorphic testing},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.11.920},
doi = {10.1016/j.jss.2010.11.920},
abstract = {Abstract: Machine learning algorithms have provided core functionality to many application domains - such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no ''test oracle'' to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique ''metamorphic testing'', which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.},
journal = {J. Syst. Softw.},
month = apr,
pages = {544–558},
numpages = {15},
keywords = {Machine learning, Metamorphic testing, Oracle problem, Test oracle, Validation, Verification}
}

@article{10.1162/COMJ_a_00316,
author = {Herremans, Dorien and S\"{o}rensen, Kenneth and Martens, David},
title = {Classification and generation of composer-specific music using global feature models and variable neighborhood search},
year = {2015},
issue_date = {Fall 2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {39},
number = {3},
issn = {0148-9267},
url = {https://doi.org/10.1162/COMJ_a_00316},
doi = {10.1162/COMJ_a_00316},
abstract = {In this article a number of musical features are extracted from a large musical database and these were subsequently used to build four composer-classification models. The first two models, an if-then rule set and a decision tree, result in an understanding of stylistic differences between Bach, Haydn, and Beethoven. The other two models, a logistic regression model and a support vector machine classifier, are more accurate. The probability of a piece being composed by a certain composer given by the logistic regression model is integrated into the objective function of a previously developed variable neighborhood search algorithm that can generate counterpoint. The result is a system that can generate an endless stream of contrapuntal music with composer-specific characteristics that sounds pleasing to the ear. This system is implemented as an Android app called FuX.},
journal = {Comput. Music J.},
month = sep,
pages = {71–91},
numpages = {21}
}

@inproceedings{10.1109/ICPR.2006.207,
author = {Langs, Georg and Peloschek, Philipp and Donner, Rene and Reiter, Michael and Bischof, Horst},
title = {Active Feature Models},
year = {2006},
isbn = {0769525210},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPR.2006.207},
doi = {10.1109/ICPR.2006.207},
abstract = {In this paper active feature models are proposed. They utilize local texture features and a statistical shape model for the reliable localization of landmarks in images. They are related to active appearance models, but instead of modelling the entire texture of an object they represent image texture by means of local descriptors. The approach has advantages with complex image data like anatomical structures that exhibit high texture variation with limited relevance for the recognition of the object location. Experimental results and the comparison to AAMs on different data sets indicate that active feature models can improve search speed and result accuracy, considerably.},
booktitle = {Proceedings of the 18th International Conference on Pattern Recognition - Volume 01},
pages = {417–420},
numpages = {4},
series = {ICPR '06}
}

@inproceedings{10.5555/2041790.2041811,
author = {Gamez, Nadia and Fuentes, Lidia and Arag\"{u}ez, Miguel A.},
title = {Autonomic computing driven by feature models and architecture in FamiWare},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A wireless sensor network is an example of a system that should be able to adapt its sensor nodes to some context changes with minimum human intervention. This means that the architecture of the middleware for sensors must encapsulate a dynamic mechanism to allow reconfiguration. We present a novel approach to achieve self-adaptation based on software product lines and on the autonomic computing paradigm for the FamiWare middleware. FamiWare uses feature models to represent the potential middleware configurations at runtime. Each configuration is automatically mapped to the corresponding architectural representation of a specific middleware product. Following the autonomic computing principles, FamiWare defines a reconfiguration mechanism that switches from one architectural configuration to another by means of executing a plan. This is possible thanks to the loosely coupled architecture of FamiWare based on an event-based publish and subscribe mechanism. We evaluate our work by showing that the resource consumption and the overhead are not so critical compared with the benefits of providing this self-adaptation mechanism.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {164–179},
numpages = {16},
keywords = {autonomic computing, event-based architectures, feature models, middleware, models at runtime, product lines architectures},
location = {Essen, Germany},
series = {ECSA'11}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1007/978-3-642-23783-6_33,
author = {Nguyen, Canh Hao and Mamitsuka, Hiroshi},
title = {Kernels for link prediction with latent feature models},
year = {2011},
isbn = {9783642237829},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-23783-6_33},
doi = {10.1007/978-3-642-23783-6_33},
abstract = {Predicting new links in a network is a problem of interest in many application domains. Most of the prediction methods utilize information on the network's entities such as nodes to build a model of links. Network structures are usually not used except for the networks with similarity or relatedness semantics. In this work, we use network structures for link prediction with a more general network type with latent feature models. The problem is the difficulty to train these models directly for large data. We propose a method to solve this problem using kernels and cast the link prediction problem into a binary classification problem. The key idea is not to infer latent features explicitly, but to represent these features implicitly in the kernels, making the method scalable to large networks. In contrast to the other methods for latent feature models, our method inherits all the advantages of kernel framework: optimality, efficiency and nonlinearity. We apply our method to real data of protein-protein interactions to show the merits of our method.},
booktitle = {Proceedings of the 2011th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II},
pages = {517–532},
numpages = {16},
location = {Athens, Greece},
series = {ECMLPKDD'11}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.5555/1795114.1795132,
author = {Doshi-Velez, Finale and Ghahramani, Zoubin},
title = {Correlated non-parametric latent feature models},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We are often interested in explaining data through a set of hidden factors or features. When the number of hidden features is unknown, the Indian Buffet Process (IBP) is a nonparametric latent feature model that does not bound the number of active features in dataset. However, the IBP assumes that all latent features are uncorrelated, making it inadequate for many realworld problems. We introduce a framework for correlated non-parametric feature models, generalising the IBP. We use this framework to generate several specific models and demonstrate applications on realworld datasets.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {143–150},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}

@inproceedings{10.5555/3042573.3042725,
author = {Zhu, Jun},
title = {Max-margin nonparametric latent feature models for link prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a max-margin nonparametric latent feature relational model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1179–1186},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.1145/2695664.2695907,
author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han\^{e}ne},
title = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695907},
doi = {10.1145/2695664.2695907},
abstract = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1602–1609},
numpages = {8},
keywords = {evaluation, feature model, measurement, software product lines},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/2984093.2984237,
author = {Miller, Kurt T. and Griffiths, Thomas L. and Jordan, Michael I.},
title = {Nonparametric latent feature models for link prediction},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As the availability and importance of relational data—such as the friendships summarized on a social networking website—increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems},
pages = {1276–1284},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/ICDM.2010.140,
author = {Noto, Keith and Brodley, Carly and Slonim, Donna},
title = {Anomaly Detection Using an Ensemble of Feature Models},
year = {2010},
isbn = {9780769542560},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDM.2010.140},
doi = {10.1109/ICDM.2010.140},
abstract = {We present a new approach to semi-supervised anomaly detection. Given a set of training examples believed to come from the same distribution or class, the task is to learn a model that will be able to distinguish examples in the future that do not belong to the same class. Traditional approaches typically compare the position of a new data point to the set of ``normal'' training data points in a chosen representation of the feature space. For some data sets, the normal data may not have discernible positions in feature space, but do have consistent relationships among some features that fail to appear in the anomalous examples. Our approach learns to predict the values of training set features from the values of other features. After we have formed an ensemble of predictors, we apply this ensemble to new data points. To combine the contribution of each predictor in our ensemble, we have developed a novel, information-theoretic anomaly measure that our experimental results show selects against noisy and irrelevant features. Our results on 47 data sets show that for most data sets, this approach significantly improves performance over current state-of-the-art feature space distance and density-based approaches.},
booktitle = {Proceedings of the 2010 IEEE International Conference on Data Mining},
pages = {953–958},
numpages = {6},
keywords = {anomaly detection, feature selection, machine learning},
series = {ICDM '10}
}

@article{10.5555/2590483.2590486,
author = {Karata\c{s}, Ahmet Serkan and Do\u{g}ru, Ali H. and O\u{g}uzt\"{u}z\"{u}n, Halit and Tolun, Mehmet},
title = {USING CONTEXT INFORMATION FOR STAGED CONFIGURATION OF FEATURE MODELS},
year = {2011},
issue_date = {April 2011},
publisher = {IOS Press},
address = {NLD},
volume = {15},
number = {2},
issn = {1092-0617},
abstract = {Since their introduction feature models have been widely accepted and used in Software Product Lines. However, feature models may become too large easily, which complicates the management of the model. One of the reasons of complexity arises from the constraints imposed by the context on the product family. In this article we propose a strategy that uses context information for performing staged configurations on feature models. We introduce an organization structure for the context variability model and use this organization to determine the configurations for specification stages. This approach enables the elimination of context-related variability from the feature model, thus results in a reduction in the complexity of the model and enables to focus on the functional features of the product family.},
journal = {J. Integr. Des. Process Sci.},
month = apr,
pages = {37–51},
numpages = {15},
keywords = {Context Variability, Software Product Lines, Staged Configurations}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/11510888_24,
author = {Scalzo, Fabien and Piater, Justus},
title = {Unsupervised learning of visual feature hierarchies},
year = {2005},
isbn = {3540269231},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11510888_24},
doi = {10.1007/11510888_24},
abstract = {We propose an unsupervised, probabilistic method for learning visual feature hierarchies. Starting from local, low-level features computed at interest point locations, the method combines these primitives into high-level abstractions. Our appearance-based learning method uses local statistical analysis between features and Expectation-Maximization to identify and code spatial correlations. Spatial correlation is asserted when two features tend to occur at the same relative position of each other. This learning scheme results in a graphical model that constitutes a probabilistic representation of a flexible visual feature hierarchy. For feature detection, evidence is propagated using Belief Propagation. Each message is represented by a Gaussian mixture where each component represents a possible location of the feature. In experiments, the proposed approach demonstrates efficient learning and robust detection of object models in the presence of clutter and occlusion and under view point changes.},
booktitle = {Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition},
pages = {243–252},
numpages = {10},
location = {Leipzig, Germany},
series = {MLDM'05}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/3540261.3540993,
author = {Winter, Robin and No\'{e}, Frank and Clevert, Djork-Arn\'{e}},
title = {Permutation-invariant variational autoencoder for graph-level representation learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g., node clustering). Despite its wide range of possible applications, graph-level unsupervised representation learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {732},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {Software product lines, Synthetic biology, Reverse engineering, BioBricks}
}

@inproceedings{10.5555/2888116.2888147,
author = {Wu, Ga and Sanner, Scott and Oliveira, Rodrigo F. S. C.},
title = {Bayesian model averaging naive bayes (BMA-NB): averaging over an exponential number of feature models in linear time},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Naive Bayes (NB) is well-known to be a simple but effective classifier, especially when combined with feature selection. Unfortunately, feature selection methods are often greedy and thus cannot guarantee an optimal feature set is selected. An alternative to feature selection is to use Bayesian model averaging (BMA), which computes a weighted average over multiple predictors; when the different predictor models correspond to different feature sets, BMA has the advantage over feature selection that its predictions tend to have lower variance on average in comparison to any single model. In this paper, we show for the first time that it is possible to exactly evaluate BMA over the exponentiallysized powerset of NB feature models in linear-time in the number of features; this yields an algorithm about as expensive to train as a single NB model with all features, but yet provably converges to the globally optimal feature subset in the asymptotic limit of data. We evaluate this novel BMA-NB classifier on a range of datasets showing that it never underperforms NB (as expected) and sometimes offers performance competitive (or superior) to classifiers such as SVMs and logistic regression while taking a fraction of the time to train.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {3094–3100},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/1614108.1614112,
author = {Bromberg, Ilana and Morris, Jeremy and Fosler-Lussier, Eric},
title = {Joint versus independent phonological feature models within CRF phone recognition},
year = {2007},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent.},
booktitle = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers},
pages = {13–16},
numpages = {4},
location = {Rochester, New York},
series = {NAACL-Short '07}
}

@article{10.1016/j.compbiomed.2021.104954,
author = {Haque, Fahmida and Ibne Reaz, Mamun Bin and Chowdhury, Muhammad E.H. and Md Ali, Sawal Hamid and Ashrif A Bakar, Ahmad and Rahman, Tawsifur and Kobashi, Syoji and Dhawale, Chitra A. and Sobhan Bhuiyan, Mohammad Arif},
title = {A nomogram-based diabetic sensorimotor polyneuropathy severity prediction using Michigan neuropathy screening instrumentations},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104954},
doi = {10.1016/j.compbiomed.2021.104954},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {11},
keywords = {DSPN, Machine learning, Nomogram, MNSI, Diagnosis, Severity grading}
}

@article{10.5555/3546258.3546413,
author = {Lin, Licong and Dobriban, Edgar},
title = {What causes the test error? going beyond bias-variance via ANOVA},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Modern machine learning methods are often overparametrized, allowing adaptation to the data at a fine level. This can seem puzzling; in the worst case, such models do not need to generalize. This puzzle inspired a great amount of work, arguing when overparametrization reduces test error, in a phenomenon called "double descent". Recent work aimed to understand in greater depth why overparametrization is helpful for generalization. This lead to discovering the unimodality of variance as a function of the level of parametrization, and to decomposing the variance into that arising from label noise, initialization, and randomness in the training data to understand the sources of the error.In this work we develop a deeper understanding of this area. Specifically, we propose using the analysis of variance (ANOVA) to decompose the variance in the test error in a symmetric way, for studying the generalization performance of certain two-layer linear and non-linear networks. The advantage of the analysis of variance is that it reveals the effects of initialization, label noise, and training data more clearly than prior approaches. Moreover, we also study the monotonicity and unimodality of the variance components. While prior work studied the unimodality of the overall variance, we study the properties of each term in the variance decomposition.One of our key insights is that often, the interaction between training samples and initialization can dominate the variance; surprisingly being larger than their marginal effect. Also, we characterize "phase transitions" where the variance changes from unimodal to monotone. On a technical level, we leverage advanced deterministic equivalent techniques for Haar random matrices, that--to our knowledge--have not yet been used in the area. We verify our results in numerical simulations and on empirical data examples.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {155},
numpages = {82},
keywords = {test error, ANOVA, double descent, ridge regression, random matrix theory}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Breast cancer, Mass segmentation, Mass detection, Dual-view mammogram analysis, Active learning, Computer-aided diagnosis},
location = {Strasbourg, France}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/2976248.2976308,
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
title = {Infinite latent feature models and the Indian buffet process},
year = {2005},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
pages = {475–482},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'05}
}

@article{10.1016/j.eswa.2008.02.071,
author = {Tsai, Tsung-Hsien and Lee, Chi-Kang and Wei, Chien-Hung},
title = {Neural network based temporal feature models for short-term railway passenger demand forecasting},
year = {2009},
issue_date = {March, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.02.071},
doi = {10.1016/j.eswa.2008.02.071},
abstract = {Accurate forecasts are the base for correct decisions in revenue management. This paper addresses two novel neural network structures for short-term railway passenger demand forecasting. An idea to render information at suitable places rather than mixing all available information at the beginning in neural network operations is proposed. The first proposed network structure is multiple temporal units neural network (MTUNN), which deals with distinctive input information via designated connections in the network. The second proposed network structure is parallel ensemble neural network (PENN), which deals with different input information in several individual models. The outputs of the individual models are then integrated to obtain final forecasts. Conventional multi-layer perceptron (MLP) is also constructed for comparison purposes. The results show that both MTUNN and PENN outperform conventional MLP in the study. On average, MTUNN can obtain 8.1% improvement of MSE and 4.4% improvement of MAPE in comparison with MLP. PENN can achieve 10.5% improvement of MSE and 3.3% improvement of MAPE in comparison with MLP.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {3728–3736},
numpages = {9},
keywords = {Divide-and-conquer, Neural networks, Railway passenger demand, Short-term forecasting, Temporal features}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Feature diagram, Feature-based configuration, Multi-view, Separation of concerns, Software product line engineering}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {adaptive pattern classification, auditory periphery model, machine perception, neural responses, radial basis functions, self-organizing maps, unsupervised learning}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Latent semantic analysis, Natural language processing, Requirements reuse, Software engineering, Unsupervised learning}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Software Product Line, Feature location, Refactoring},
location = {Hammamet, Tunisia}
}

@article{10.5555/3546258.3546460,
author = {Liu, Huafeng and Jing, Liping and Wen, Jingxuan and Xu, Pengyu and Wang, Jiaqi and Yu, Jian and Ng, Michael K.},
title = {Interpretable deep generative recommendation models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {User preference modeling in recommendation system aims to improve customer experience through discovering users' intrinsic preference based on prior user behavior data. This is a challenging issue because user preferences usually have complicated structure, such as inter-user preference similarity and intra-user preference diversity. Among them, inter-user similarity indicates different users may share similar preference, while intra-user diversity indicates one user may have several preferences. In literatures, deep generative models have been successfully applied in recommendation systems due to its exibility on statistical distributions and strong ability for non-linear representation learning. However, they suffer from the simple generative process when handling complex user preferences. Meanwhile, the latent representations learned by deep generative models are usually entangled, and may range from observed-level ones that dominate the complex correlations between users, to latent-level ones that characterize a user's preference, which makes the deep model hard to explain and unfriendly for recommendation. Thus, in this paper, we propose an Interpretable Deep Generative Recommendation Model (InDGRM) to characterize inter-user preference similarity and intra-user preference diversity, which will simultaneously disentangle the learned representation from observed-level and latent-level. In InDGRM, the observed-level disentanglement on users is achieved by modeling the user-cluster structure (i.e., inter-user preference similarity) in a rich multimodal space, so that users with similar preferences are assigned into the same cluster. The observed-level disentanglement on items is achieved by modeling the intra-user preference diversity in a prototype learning strategy, where different user intentions are captured by item groups (one group refers to one intention). To promote disentangled latent representations, InDGRM adopts structure and sparsity-inducing penalty and integrates them into the generative procedure, which has ability to enforce each latent factor focus on a limited subset of items (e.g., one item group) and benefit latent-level disentanglement. Meanwhile, it can be efficiently inferred by minimizing its penalized upper bound with the aid of local variational optimization technique. Theoretically, we analyze the generalization error bound of InDGRM to guarantee its performance. A series of experimental results on four widely-used benchmark datasets demonstrates the superiority of InDGRM on recommendation performance and interpretability.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {202},
numpages = {54},
keywords = {recommendation system, collaborative filtering, deep generative model, interpretable machine learning, latent factor model}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Semi-supervised learning, Mutual consistency, Cycled pseudo label},
location = {Strasbourg, France}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {rl-as-inference, tempered inference, self-paced learning, reinforcement learning, curriculum learning}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {self-supervised learning, once and for all, multi-modal, co-training},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Systematic literature review, Software product line, Product line engineering, Product derivation}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Requirements engineering, Automotive, Test case selection and prioritization, Regression testing, Use case driven development, Product Line Engineering}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Community detection, Information retrieval, Feature extraction, Software product line}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Quality, Product line model, Defect, Software product line, Feature model}
}

@article{10.1145/3477127.3477128,
author = {Antoniadi, Anna Markella and Galvin, Miriam and Heverin, Mark and Hardiman, Orla and Mooney, Catherine},
title = {Prediction of quality of life in people with ALS: on the road towards explainable clinical decision support},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3477127.3477128},
doi = {10.1145/3477127.3477128},
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease that causes a rapid decline in motor functions and has a fatal trajectory. ALS is currently incurable, so the aim of the treatment is mostly to alleviate symptoms and improve quality of life (QoL) for the patients. The goal of this study is to develop a Clinical Decision Support System (CDSS) to alert clinicians when a patient is at risk of experiencing low QoL. The source of data was the Irish ALS Registry and interviews with the 90 patients and their primary informal caregiver at three time-points. In this dataset, there were two different scores to measure a person's overall QoL, based on the McGill QoL (MQoL) Questionnaire and we worked towards the prediction of both. We used Extreme Gradient Boosting (XGBoost) for the development of the predictive models, which was compared to a logistic regression baseline model. Additionally, we used Synthetic Minority Over-sampling Technique (SMOTE) to examine if that would increase model performance and SHAP (SHapley Additive explanations) as a technique to provide local and global explanations to the outputs as well as to select the most important features. The total calculated MQoL score was predicted accurately using three features - age at disease onset, ALSFRS-R score for orthopnoea and the caregiver's status pre-caregiving - with a F1-score on the test set equal to 0.81, recall of 0.78, and precision of 0.84. The addition of two extra features (caregiver's age and the ALSFRS-R score for speech) produced similar outcomes (F1-score 0.79, recall 0.70 and precision 0.90).},
journal = {SIGAPP Appl. Comput. Rev.},
month = jul,
pages = {5–17},
numpages = {13},
keywords = {amyotrophic lateral sclerosis, clinical decision support system, explainable artificial intelligence, machine learning, quality of life}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.compbiomed.2021.104737,
author = {Tavolara, Thomas E. and Gurcan, Metin N. and Segal, Scott and Niazi, M.K.K.},
title = {Identification of difficult to intubate patients from frontal face images using an ensemble of deep learning models},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104737},
doi = {10.1016/j.compbiomed.2021.104737},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {9},
keywords = {Endotracheal intubation, Deep learning, Machine learning, Airway management, Image analysis}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Real-world case studies, Variability modeling, Constraint classification, Multi-stage and multi-step configuration process, Automated configuration, Product line engineering, Cyber-physical systems}
}

@article{10.1016/j.comcom.2019.02.007,
author = {Schwenk, G. and Pabst, R. and M\"{u}ller, K.R.},
title = {Classification of structured validation data using stateless and stateful features},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {138},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.02.007},
doi = {10.1016/j.comcom.2019.02.007},
journal = {Comput. Commun.},
month = apr,
pages = {54–66},
numpages = {13},
keywords = {Stateless and stateful features, Interpretable learning, Structured data, Supervised learning, Feature modeling, Quality of service, Mobile communication}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {sub-Gaussian biclustering, stochastic block model, spectral clustering, regularization of random graphs, graphon clustering, community detection, bipartite networks}
}

@article{10.1016/j.eswa.2020.114095,
author = {Tariq, Mehreen and Iqbal, Sajid and Ayesha, Hareem and Abbas, Ishaq and Ahmad, Khawaja Tehseen and Niazi, Muhammad Farooq Khan},
title = {Medical image based breast cancer diagnosis: State of the art and future directions},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114095},
doi = {10.1016/j.eswa.2020.114095},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {34},
keywords = {Breast cancer detection and diagnosis, Transfer learning (TL), Deep learning (DL), Machine learning (ML), Computer aided diagnosis (CAD)}
}

@inproceedings{10.1145/3474085.3475387,
author = {Sun, Jingxian and Zhang, Lichao and Zha, Yufei and Gonzalez-Garcia, Abel and Zhang, Peng and Huang, Wei and Zhang, Yanning},
title = {Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475387},
doi = {10.1145/3474085.3475387},
abstract = {The target representation learned by convolutional neural networks plays an important role in Thermal Infrared (TIR) tracking. Currently, most of the top-performing TIR trackers are still employing representations learned by the model trained on the RGB data. However, this representation does not take into account the information in the TIR modality itself, limiting the performance of TIR tracking.To solve this problem, we propose to distill representations of the TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a large amount of unlabeled paired RGB-TIR data. We take advantage of the two-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal distillation working on two components of the tracker. Specifically, we use one branch as a teacher module to distill the representation learned by the model into the other branch. Benefiting from the powerful model in the RGB modality, the cross-modal distillation can learn the TIR-specific representation for promoting TIR tracking. The proposed approach can be incorporated into different baseline trackers conveniently as a generic and independent component. Furthermore, the semantic coherence of paired RGB and TIR images is utilized as a supervised signal in the distillation loss for cross-modal knowledge transfer. In practice, three different approaches are explored to generate paired RGB-TIR patches with the same semantics for training in an unsupervised way. It is easy to extend to an even larger scale of unlabeled training data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR dataset demonstrate that our proposed cross-modal distillation method effectively learns TIR-specific target representations transferred from the RGB modality. Our tracker outperforms the baseline tracker by achieving absolute gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision respectively. Code and models are available at https://github.com/zhanglichao/cmdTIRtracking.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2262–2270},
numpages = {9},
keywords = {TIR tracking, convolutional neural network, knowledge distillation, unsupervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Variable selection, Replicated measurements, Regularization, Reinke's edema, Classification, Acoustic features}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Feature Selection, Feature Model, Decision Sequence},
series = {SPLC '11}
}

@inproceedings{10.5555/3524938.3525209,
author = {Emami, Melikasadat and Sahraee-Ardakan, Mojtaba and Pandit, Parthe and Rangan, Sundeep and Fletcher, Alyson K.},
title = {Generalization error of generalized linear models in high dimensions},
year = {2020},
publisher = {JMLR.org},
abstract = {At the heart of machine learning lies the question of generalizability of learned rules over previously unseen data. While over-parameterized models based on neural networks are now ubiquitous in machine learning applications, our understanding of their generalization capabilities is incomplete and this task is made harder by the nonconvexity of the underlying learning problems. We provide a general framework to characterize the asymptotic generalization error for single-layer neural networks (i.e., generalized linear models) with arbitrary non-linearities, making it applicable to regression as well as classification problems. This framework enables analyzing the effect of (i) overparameterization and non-linearity during modeling; (ii) choices of loss function, initialization, and regularizer during learning; and (iii) mismatch between training and test distributions. As examples, we analyze a few special cases, namely linear regression and logistic regression. We are also able to rigorously and analytically explain the double descent phenomenon in generalized linear models.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {271},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@inproceedings{10.5555/3023638.3023692,
author = {Quadrianto, Novi and Sharmanska, Viktoriia and Knowles, David A. and Ghahramani, Zoubin},
title = {The supervised IBP: neighbourhood preserving infinite latent feature models},
year = {2013},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We propose a probabilistic model to infer supervised latent variables in the Hamming space from observed data. Our model allows simultaneous inference of the number of binary latent variables, and their values. The latent variables preserve neighbourhood structure of the data in a sense that objects in the same semantic concept have similar latent values, and objects in different concepts have dissimilar latent values. We formulate the supervised infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the inferred supervised latent variables can be directly used to perform a nearest neighbour search for the purpose of retrieval. We introduce a new application of dynamically extending hash codes, and show how to effectively couple the structure of the hash codes with continuously growing structure of the neighbourhood preserving infinite latent feature space.},
booktitle = {Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
pages = {527–536},
numpages = {10},
location = {Bellevue, WA},
series = {UAI'13}
}

@inproceedings{10.5555/3540261.3540912,
author = {d'Ascoli, St\'{e}phane and Gabri\'{e}, Marylou and Sagun, Levent and Biroli, Giulio},
title = {On the interplay between data structure and loss function in classification problems},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the central puzzles in modern machine learning is the ability of heavily overparametrized models to generalize well. Although the low-dimensional structure of typical datasets is key to this behavior, most theoretical studies of overparametrization focus on isotropic inputs. In this work, we instead consider an analytically tractable model of structured data, where the input covariance is built from independent blocks allowing us to tune the saliency of low-dimensional structures and their alignment with respect to the target function.Using methods from statistical physics, we derive a precise asymptotic expression for the train and test error achieved by random feature models trained to classify such data, which is valid for any convex loss function. We study in detail how the data structure affects the double descent curve, and show that in the over-parametrized regime, its impact is greater for logistic loss than for mean-squared loss: the easier the task, the wider the gap in performance at the advantage of the logistic loss. Our insights are confirmed by numerical experiments on MNIST and CIFAR10.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {651},
numpages = {12},
series = {NIPS '21}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Accuracy, Self-paced learning, Extreme learning machine, Classification}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Soft computing, Receiver operator characteristic (ROC) curve, Gene expression, Fuzzy classification, DNA microarrays, Area under the curve (AUC)}
}

@inproceedings{10.5555/3020948.3021018,
author = {Shah, Amar and Ghahramani, Zoubin},
title = {Markov beta processes for time evolving dictionary learning},
year = {2016},
isbn = {9780996643115},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We develop Markov beta processes (MBP) as a model suitable for data which can be represented by a sparse set of latent features which evolve over time. Most time evolving nonparametric latent feature models in the literature vary feature usage, but maintain a constant set of features over time. We show that being able to model features which themselves evolve over time results in the MBP outperforming other beta process based models. Our construction utilizes Poisson process operations, which leave each transformed beta process marginally beta process distributed. This allows one to analytically marginalize out latent beta processes, exploiting conjugacy when we couple them with Bernoulli processes, leading to a surprisingly elegant Gibbs MCMC scheme considering the expressiveness of the prior. We apply the model to the task of denoising and interpolating noisy image sequences and in predicting time evolving gene expression data, demonstrating superior performance to other beta process based methods.},
booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {676–685},
numpages = {10},
location = {Jersey City, New Jersey, USA},
series = {UAI'16}
}

@article{10.1007/s10618-011-0234-x,
author = {Noto, Keith and Brodley, Carla and Slonim, Donna},
title = {FRaC: a feature-modeling approach for semi-supervised and unsupervised anomaly detection},
year = {2012},
issue_date = {July      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-011-0234-x},
doi = {10.1007/s10618-011-0234-x},
abstract = {Anomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called "normal" instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection. The unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them. Many real-world machine learning tasks, including many fraud and intrusion detection tasks, are unsupervised because it is impractical (or impossible) to verify all of the training data. We recently presented FRaC, a new approach for semi-supervised anomaly detection. FRaC is based on using normal instances to build an ensemble of feature models, and then identifying instances that disagree with those models as anomalous. In this paper, we investigate the behavior of FRaC experimentally and explain why FRaC is so successful. We also show that FRaC is a superior approach for the unsupervised as well as the semi-supervised anomaly detection task, compared to well-known state-of-the-art anomaly detection methods, LOF and one-class support vector machines, and to an existing feature-modeling approach.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {109–133},
numpages = {25},
keywords = {Unsupervised learning, Anomaly detection}
}

@article{10.1007/BF03037551,
author = {De Vries, Arjen P. and Windhouwer, Menzo and Apers, Peter M. G. and Kersten, Martin},
title = {Information access in multimedia databases based on feature models},
year = {2000},
issue_date = {Dec 2000},
publisher = {Ohmsha},
address = {JPN},
volume = {18},
number = {4},
issn = {0288-3635},
url = {https://doi.org/10.1007/BF03037551},
doi = {10.1007/BF03037551},
abstract = {With the increasing popularity of the WWW, the main challenge in computer science has become content-based retrieval of multimedia objects. Access to multimedia objects in databases has long been limited to the information provided in manually assigned keywords. Now, with the integration of feature-detection algorithms in database systems software, content-based retrieval can be fully integrated with query processing. We describe our experimentation platform under development, making database technology available to multimedia. Our approach is based on the new notion of feature databases. Its architecture fully integrates traditional query processing and content-based retrieval techniques.},
journal = {New Gen. Comput.},
month = dec,
pages = {323–339},
numpages = {17},
keywords = {Information Access, Multimedia Databases, Query Processing, Content-Based Retrieval}
}

@inproceedings{10.1109/SPLC.2008.28,
author = {Chae, Wonseok and Blume, Matthias},
title = {Building a Family of Compilers},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.28},
doi = {10.1109/SPLC.2008.28},
abstract = {We have developed and maintained a set of closely related compilers. Although much of their code is duplicated and shared, they have been maintained separately because they are treated as different compilers. Even if they were merged together, the combined code would become too complicated to serve as the base for another extension. We describe our experience to address this problem by adopting the product line engineering paradigm to build a family of compilers. This paradigm encourages developers to focus on developing a set of compilers rather than on developing one particular compiler. We show engineering activities for a family of compilers from product line analysis through product line architecture design to product line component design. Then, we present how to build particular compilers from core assets resulting from the previous activities and how to take advantage of modern programming language technology to organize this task. Our experience demonstrates that the product line engineering as a developing paradigm can ease the construction of a family of compilers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {307–316},
numpages = {10},
keywords = {standard ml, product line engineering, module system, feature-oriented, compilers},
series = {SPLC '08}
}

@article{10.1016/j.eswa.2021.115659,
author = {Karadal, Can Haktan and Kaya, M. Cagri and Tuncer, Turker and Dogan, Sengul and Acharya, U. Rajendra},
title = {Automated classification of remote sensing images using multileveled MobileNetV2 and DWT techniques},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115659},
doi = {10.1016/j.eswa.2021.115659},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {15},
keywords = {MobilNetV2, Multilevel feature generation, INCA, Remote sensing image classification}
}

@article{10.1007/s11042-020-10174-3,
author = {Lingwal, Surabhi and Bhatia, Komal Kumar and Tomer, Manjeet Singh},
title = {Image-based wheat grain classification using convolutional neural network},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {28–29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10174-3},
doi = {10.1007/s11042-020-10174-3},
abstract = {India is among the largest cultivators and consumers of wheat grains leading to apparent demand for identifying the quality and varietal distribution of wheat to fulfill the specific requirements of food industries. Moreover, with the variations in prices of distinct varieties in different parts of the country, it becomes a vital need for the customers as well as for the cultivators to identify and classify the grains based upon specific end products, demand, and prices of individual variety. The growth of Machine Learning and Computer Vision in agriculture, facilitate the development of such techniques that can successfully identify the classes based on visual features and representation. In this paper, a model has been developed from scratch for the classification of fifteen different varieties of wheat consists of 15000 images based on their visual traits using Convolutional Neural Network. The model has been produced under a different set of hyper-parameters tuned to develop the best model that can classify the varieties of wheat grains with high accuracy and minimum loss. The performance of the different models are compared in terms of classification accuracy and categorical cross-entropy loss. The model which is found best, successfully classifies the wheat varieties with 94.88% training accuracy and 97.53% test accuracy while on the other side reduces loss to 15% for training and 8% for the test set. Hence, the developed model can be deployed for the classification of different grain varieties, plant diseases, plant varieties, and several other fields under agriculture.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {35441–35465},
numpages = {25},
keywords = {Deep learning, Image processing, Convolutional neural network, Classification, Wheat crops}
}

@article{10.1145/3312739,
author = {Taha, Ayman and Hadi, Ali S.},
title = {Anomaly Detection Methods for Categorical Data: A Review},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3312739},
doi = {10.1145/3312739},
abstract = {Anomaly detection has numerous applications in diverse fields. For example, it has been widely used for discovering network intrusions and malicious events. It has also been used in numerous other applications such as identifying medical malpractice or credit fraud. Detection of anomalies in quantitative data has received a considerable attention in the literature and has a venerable history. By contrast, and despite the widespread availability use of categorical data in practice, anomaly detection in categorical data has received relatively little attention as compared to quantitative data. This is because detection of anomalies in categorical data is a challenging problem. Some anomaly detection techniques depend on identifying a representative pattern then measuring distances between objects and this pattern. Objects that are far from this pattern are declared as anomalies. However, identifying patterns and measuring distances are not easy in categorical data compared with quantitative data. Fortunately, several papers focussing on the detection of anomalies in categorical data have been published in the recent literature. In this article, we provide a comprehensive review of the research on the anomaly detection problem in categorical data. Previous review articles focus on either the statistics literature or the machine learning and computer science literature. This review article combines both literatures. We review 36 methods for the detection of anomalies in categorical data in both literatures and classify them into 12 different categories based on the conceptual definition of anomalies they use. For each approach, we survey anomaly detection methods, and then show the similarities and differences among them. We emphasize two important issues, the number of parameters each method requires and its time complexity. The first issue is critical, because the performance of these methods are sensitive to the choice of these parameters. The time complexity is also very important in real applications especially in big data applications. We report the time complexity if it is reported by the authors of the methods. If it is not, then we derive it ourselves and report it in this article. In addition, we discuss the common problems and the future directions of the anomaly detection in categorical data.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {38},
numpages = {35},
keywords = {unsupervised learning, supervised learning, semi-supervised learning, outliers detection, novelty detection, nominal data, mixed data, intrusion detection systems, holo entropy, data mining, Shannon entropy, Computational complexity}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {software product lines, services, health watcher system, cloud platforms, cloud computing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3412841.3441940,
author = {Antoniadi, Anna Markella and Galvin, Miriam and Heverin, Mark and Hardiman, Orla and Mooney, Catherine},
title = {Development of an explainable clinical decision support system for the prediction of patient quality of life in amyotrophic lateral sclerosis},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441940},
doi = {10.1145/3412841.3441940},
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative and currently incurable disease. It causes a rapid decline in motor functions and has a fatal trajectory. The aim of the treatment is mostly to alleviate symptoms and improve the patient's quality of life (QoL). The goal of this study is to develop a Clinical Decision Support System (CDSS) in order to alert clinicians when a patient is at risk of experiencing a low QoL, so that they are better supported. The source of the data was the Irish ALS Registry and interviews with the 90 patients and their primary informal caregiver at three time-points. In this dataset, there were two different scores to measure a person's overall QoL, based on the McGill QoL (MQoL) Questionnaire and we worked towards the prediction of both. The method we used for the development of the predictive models was Extreme Gradient Boosting (XGBoost), which was compared to a logistic regression baseline model. We used the SHAP (SHapley Additive exPlanations) values as a technique to provide local and global explanations to the outputs as well as to select the most important features. The total calculated MQoL score was predicted accurately by three features, with a F1-score on the test set equal to 0.81, a recall score of 0.78, and a precision score of 0.84, while, the addition of two features produced similar outcomes (0.79, 0.70 and 0.90 respectively). The three most important features were the age at disease onset, ALSFRS score for orthopnoea and the caregiver's status pre-caregiving.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {594–602},
numpages = {9},
keywords = {amyotrophic lateral sclerosis, clinical decision support system, explainable artificial intelligence, machine learning, quality of life},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Cloud service, Feature model, Reinforcement learning, Adaptation},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.5555/2934046.2934098,
author = {Shanahan, James G. and Baldwin, James F. and Martin, Trevor P.},
title = {Constructive induction of fuzzy Cartesian granule feature models using genetic programming},
year = {1999},
isbn = {1558606114},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The G_DACG (Genetic Discovery of Additive Cartesian Granule feature models) constructive induction algorithm is presented as a means of automatically identifying rule-based Cartesian granule feature models from example data. G_DACG combines the powerful search capabilities of genetic programming with a rather novel and cheap fitness function based upon the semantic separation of learnt concepts expressed in terms of fuzzy sets extracted over Cartesian granule features. G_DACG is illustrated on a variety of artificial and real world problems.},
booktitle = {Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 2},
pages = {1237},
numpages = {1},
location = {Orlando, Florida},
series = {GECCO'99}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {two-Stage sampled distribution regression, multi-instance learning, minimax optimality, mean embedding, Kernel ridge regression}
}

@article{10.5555/3546258.3546455,
author = {Bouchard-C\^{o}t\'{e}, Alexandre and Roth, Andrew},
title = {Particle-Gibbs sampling for Bayesian feature allocation models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Bayesian feature allocation models are a popular tool for modelling data with a combinatorial latent structure. Exact inference in these models is generally intractable and so practitioners typically apply Markov Chain Monte Carlo (MCMC) methods for posterior inference. The most widely used MCMC strategies rely on a single variable Gibbs update of the feature allocation matrix. These updates can be inefficient as features are typically strongly correlated. To overcome this problem we have developed a block sampler that can update an entire row of the feature allocation matrix in a single move. In the context of feature allocation models, naive block Gibbs sampling is impractical for models with a large number of features as the computational complexity scales exponentially in the number of features. We develop a Particle Gibbs (PG) sampler that targets the same distribution as the row wise Gibbs updates, but has computational complexity that only grows linearly in the number of features. We compare the performance of our proposed methods to the standard Gibbs sampler using synthetic and real data from a range of feature allocation models. Our results suggest that row wise updates using the PG methodology can significantly improve the performance of samplers for feature allocation models.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {197},
numpages = {105},
keywords = {sequential monte Carlo, particle Gibbs sampler, Gibbs sampler, Indian buffet process, Bayesian feature allocation}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {transfer learning, reinforcement learning, curriculum learning}
}

@inproceedings{10.5555/3045118.3045261,
author = {Wang, Yi and Li, Bin and Wang, Yang and Chen, Fang},
title = {Metadata dependent mondrian processes},
year = {2015},
publisher = {JMLR.org},
abstract = {Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1339–1347},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1016/j.pmcj.2021.101474,
author = {Madoery, Pablo G. and Detke, Ramiro and Blanco, Lucas and Comerci, Sandro and Fraire, Juan and Gonzalez&nbsp;Montoro, Aldana and Bellassai, Juan Carlos and Britos, Grisel and Ojeda, Silvia and Finochietto, Jorge M.},
title = {Feature selection for proximity estimation in COVID-19 contact tracing apps based on Bluetooth Low Energy (BLE)},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2021.101474},
doi = {10.1016/j.pmcj.2021.101474},
journal = {Pervasive Mob. Comput.},
month = oct,
numpages = {13},
keywords = {COVID-19, Bluetooth, Contact tracing, Proximity estimation, Machine learning, Feature selection}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.5555/3540261.3540904,
author = {Cao, Yuan and Gu, Quanquan and Belkin, Mikhail},
title = {Risk bounds for over-parameterized maximum margin classification on sub-Gaussian mixtures},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can fit the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this "benign overfitting" phenomenon of the maximum margin classifier for linear classification problems. Specifically, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classifier in the over-parameterized setting. Our results precisely characterize the condition under which benign overfitting can occur in linear classification problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {643},
numpages = {12},
series = {NIPS '21}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Non-negative matrix factorization, Multi-view clustering, Semi-supervised learning, Graph-regularization}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {structured prediction, structural SVM, direct loss minimization, CRF}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.comnet.2021.108613,
author = {D’hooge, Laurens and Verkerken, Miel and Wauters, Tim and Volckaert, Bruno and De Turck, Filip},
title = {Hierarchical feature block ranking for data-efficient intrusion detection modeling},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108613},
doi = {10.1016/j.comnet.2021.108613},
journal = {Comput. Netw.},
month = dec,
numpages = {17},
keywords = {Network security, Intrusion detection, Hybrid feature selection}
}

@article{10.5555/3546258.3546329,
author = {Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
title = {Phase diagram for two-layer ReLU neural networks at infinite-width limit},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {How neural network behaves during the training over different choices of hyperparameters is an important question in the study of neural networks. In this work, inspired by the phase diagram in statistical mechanics, we draw the phase diagram for the two-layer ReLU neural network at the infinite-width limit for a complete characterization of its dynamical regimes and their dependence on hyperparameters related to initialization. Through both experimental and theoretical approaches, we identify three regimes in the phase diagram, i.e., linear regime, critical regime and condensed regime, based on the relative change of input weights as the width approaches infinity, which tends to 0, O(1) and +∞, respectively. In the linear regime, NN training dynamics is approximately linear similar to a random feature model with an exponential loss decay. In the condensed regime, we demonstrate through experiments that active neurons are condensed at several discrete orientations. The critical regime serves as the boundary between above two regimes, which exhibits an intermediate nonlinear behavior with the mean-field model as a typical example. Overall, our phase diagram for the two-layer ReLU NN serves as a map for the future studies and is a first step towards a more systematical investigation of the training behavior and the implicit regularization of NNs of different structures.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {71},
numpages = {47},
keywords = {two-layer ReLU neural network, infinite-width limit, phase diagram, dynamical regime, condensation}
}

@article{10.1016/j.neucom.2016.02.078,
author = {Wei, Leyi and Bowen, Zhang and Zhiyong, Chen and Gao, Xing and Liao, Minghong},
title = {Exploring local discriminative information from evolutionary profiles for cytokinereceptor interaction prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {217},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.02.078},
doi = {10.1016/j.neucom.2016.02.078},
abstract = {Cytokinereceptor interaction is one of the most important types of proteinprotein interactions that are widely involved in cellular regulatory processes. Knowledge of cytokinereceptor interactions facilitates to deeply understand several physiological functions. In post-genomic era of sequence explosion, there is an increasing demand for developing machine learning based computational methods for the fast and accurate cytokinereceptor interaction prediction. However, the major problem lying on existing machine learning based methods is that the overall prediction accuracy is relatively low. To improve the accuracy, a crucial step is to establish a well-defined feature representation algorithm. Motivated on this perspective, we propose a novel feature representation method by integrating local information embedded in evolutionary profiles with the Pse-PSSM and AAC-PSSM-AC feature models. We further develop an improved prediction method, namely CRI-Pred, based on the proposed feature set using the Random Forest classifier. Experimental results evaluated with the jackknife test show that the CRI-Pred predictor outperforms the state-of-the-art methods, 5.1% higher in terms of the overall accuracy. This indicates the effectiveness and superiority of CRI-Pred. A webserver that implements CRI-Pred is now freely available at http://server.malab.cn/CRIPred/Index.html to the public to use in practical applications.},
journal = {Neurocomput.},
month = dec,
pages = {37–45},
numpages = {9},
keywords = {Cytokinereceptor interaction prediction, Evolutionary profiles, Feature representation algorithm, Local discriminative information, Machine learning method}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Stochastic searching on the line, Interpretable machine learning, Weighted tsetlin machines, Regression tsetlin machines, Tsetlin machines},
location = {Kitakyushu, Japan}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {hinge loss minimization, linear regression, curriculum learning}
}

@article{10.5555/2503308.2188400,
author = {Frank, Mario and Streich, Andreas P. and Basin, David and Buhmann, Joachim M.},
title = {Multi-assignment clustering for boolean data},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {459–489},
numpages = {31},
keywords = {Boolean data, clustering, latent feature models, multi-assignments, overlapping clusters, role mining}
}

@inproceedings{10.5555/3524938.3525321,
author = {Hasanzadeh, Arman and Hajiramezanali, Ehsan and Boluki, Shahin and Zhou, Mingyuan and Duffield, Nick and Narayanan, Krishna and Qian, Xiaoning},
title = {Bayesian graph neural networks with adaptive connection sampling},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates oversmoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters as in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boosting the performance of GNNs in semi-supervised node classification, making them less prone to oversmoothing and over-fitting with more robust prediction.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {383},
numpages = {11},
series = {ICML'20}
}

@article{10.5555/2188385.2188400,
author = {Frank, Mario and Streich, Andreas P. and Basin, David and Buhmann, Joachim M.},
title = {Multi-assignment clustering for boolean data},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {459–489},
numpages = {31},
keywords = {Boolean data, clustering, latent feature models, multi-assignments, overlapping clusters, role mining}
}

@article{10.1016/j.patrec.2021.07.024,
author = {Liang, Maohan and Zhan, Yang and Liu, Ryan Wen},
title = {MVFFNet: Multi-view feature fusion network for imbalanced ship classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.07.024},
doi = {10.1016/j.patrec.2021.07.024},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {26–32},
numpages = {7},
keywords = {Ship classification, Multi-view feature fusion, Imbalanced data, CAE, BiGRU}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {structural typing, software product line engineering, delta-oriented programming},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Latent features, Deep autoencoder, Sparse speech dataset, Speech disfluency classification, Hybrid Deep Ensemble}
}

@article{10.1016/j.asoc.2020.106586,
author = {Mahmoud, Reem A. and Hajj, Hazem and Karameh, Fadi N.},
title = {A systematic approach to multi-task learning from time-series data},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106586},
doi = {10.1016/j.asoc.2020.106586},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {16},
keywords = {Learning with limited labels (lwLL), Personalized modeling, Multi-task learning (MTL), Deep learning, Markov chains, Time-series}
}

@article{10.1007/s00607-019-00776-7,
author = {Cagliero, Luca and Garza, Paolo and Attanasio, Giuseppe and Baralis, Elena},
title = {Training ensembles of faceted classification models for quantitative stock trading},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-019-00776-7},
doi = {10.1007/s00607-019-00776-7},
abstract = {Forecasting the stock markets is among the most popular research challenges in finance. Several quantitative trading systems based on supervised machine learning approaches have been presented in literature. Recently proposed solutions train classification models on historical stock-related datasets. Training data include a variety of features related to different facets (e.g., stock price trends, exchange volumes, price volatility, news and public mood). To increase the accuracy of the predictions, multiple models are often combined together using ensemble methods. However, understanding which models should be combined together and how to effectively handle features related to different facets within different models are still open research questions. In this paper we investigate the use of ensemble methods to combine faceted classification models for supporting stock trading. To this aim, separate classification models are trained on each subset of features belonging to the same facet. They produce trading signals tailored to a specific facet. Signals are then combined together and filtered to generate a unified, multi-faceted recommendation. The experimental validation, performed on different markets and in different conditions, shows that, in many cases, some of the faceted models perform as good as or better than models trained on a mix of different features. An ensemble of the faceted recommendations makes the generated trading signals more profitable yet robust to draw-down periods.},
journal = {Computing},
month = may,
pages = {1213–1225},
numpages = {13},
keywords = {Quantitative stock trading, Classification, Ensemble methods, Financial application, 68U35}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Locally linear coding, Manifold learning, Self-paced learning, Pattern classification, Semi-supervised classification}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Reweight instance, Self-paced learning, Multi-label classification}
}

@inproceedings{10.1145/3411764.3445315,
author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Wortman Vaughan, Jennifer Wortman and Wallach, Hanna},
title = {Manipulating and Measuring Model Interpretability},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445315},
doi = {10.1145/3411764.3445315},
abstract = {With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {237},
numpages = {52},
keywords = {machine-assisted decision making, interpretability, human-centered machine learning},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/3540261.3540636,
author = {Ardeshir, Navid and Sanford, Clayton and Hsu, Daniel},
title = {Support vector machines and linear regression coincide with very high-dimensional features},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training example used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the ℓ1 variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general ℓp case.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {375},
numpages = {12},
series = {NIPS '21}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Deep Learning, Machine Learning, Echocardiography, Echocardiogram}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {Image processing, Tissue, Bias field, Segmentation, Brain tumor, MRI}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Valence, Speech elicitation, Machine learning, Paralinguistics, Digital medicine, Digital phenotyping}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Batch training, Typicality, Curriculum learning},
location = {Bratislava, Slovakia}
}

@inproceedings{10.5555/3524938.3525151,
author = {d'Ascoli, St\'{e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
title = {Double trouble in double descent: bias and variance(s) in the lazy regime},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep neural networks can achieve remarkable generalization performances while interpolating the training data; rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a "double descent" curve -- a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the context of high-dimensional random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond it which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. Following up on (Geiger et al., 2019a), we demonstrate that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We quantify how they are suppressed by averaging the outputs of independently initialized estimators, and compare this ensembling procedure with overparametrization and regularization. Finally, we present numerical experiments on a standard deep learning setup to show that our results are relevant to the lazy regime of deep neural networks.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {213},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@article{10.1145/3291044,
author = {Xuan, Junyu and Lu, Jie and Zhang, Guangquan},
title = {A Survey on Bayesian Nonparametric Learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291044},
doi = {10.1145/3291044},
abstract = {Bayesian (machine) learning has been playing a significant role in machine learning for a long time due to its particular ability to embrace uncertainty, encode prior knowledge, and endow interpretability. On the back of Bayesian learning’s great success, Bayesian nonparametric learning (BNL) has emerged as a force for further advances in this field due to its greater modelling flexibility and representation power. Instead of playing with the fixed-dimensional probabilistic distributions of Bayesian learning, BNL creates a new “game” with infinite-dimensional stochastic processes. BNL has long been recognised as a research subject in statistics, and, to date, several state-of-the-art pilot studies have demonstrated that BNL has a great deal of potential to solve real-world machine-learning tasks. However, despite these promising results, BNL has not created a huge wave in the machine-learning community. Esotericism may account for this. The books and surveys on BNL written by statisticians are overcomplicated and filled with tedious theories and proofs. Each is certainly meaningful but may scare away new researchers, especially those with computer science backgrounds. Hence, the aim of this article is to provide a plain-spoken, yet comprehensive, theoretical survey of BNL in terms that researchers in the machine-learning community can understand. It is hoped this survey will serve as a starting point for understanding and exploiting the benefits of BNL in our current scholarly endeavours. To achieve this goal, we have collated the extant studies in this field and aligned them with the steps of a standard BNL procedure—from selecting the appropriate stochastic processes through manipulation to executing the model inference algorithms. At each step, past efforts have been thoroughly summarised and discussed. In addition, we have reviewed the common methods for implementing BNL in various machine-learning tasks along with its diverse applications in the real world as examples to motivate future studies.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {13},
numpages = {36},
keywords = {Bayesian-learning, Data science, machine-learning}
}

@article{10.1007/s00180-020-00987-z,
author = {Beaulac, C\'{e}dric and Rosenthal, Jeffrey S.},
title = {BEST: a decision tree algorithm that handles missing values},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {3},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-020-00987-z},
doi = {10.1007/s00180-020-00987-z},
abstract = {The main contribution of this paper is the development of a new decision tree algorithm. The proposed approach allows users to guide the algorithm through the data partitioning process. We believe this feature has many applications but in this paper we demonstrate how to utilize this algorithm to analyse data sets containing missing values. We tested our algorithm against simulated data sets with various missing data structures and a real data set. The results demonstrate that this new classification procedure efficiently handles missing values and produces results that are slightly more accurate and more interpretable than most common procedures without any imputations or pre-processing.},
journal = {Comput. Stat.},
month = sep,
pages = {1001–1026},
numpages = {26},
keywords = {Classification and regression tree, Missing data, Applied machine learning, Interpretable models, Variable importance analysis}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Network Intrusion Detection, Multiple Kernel Learning, Multiclass Classification, Machine Learning, Kernel Selection, KDD Cup 1999, Fractional Polynomial Kernels, Extreme Learning Machine, Ensemble Learning, Cyber security, Adaptive Boosting}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {reference architecture, multi product line, cross-domain reference architecture, actor, Software product line},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.eswa.2019.04.035,
author = {McWhirter, Paul R. and Hussain, Abir and Al-Jumeily, Dhiya and Steele, Iain A. and Vellasco, Marley M.B.R.},
title = {Classifying Periodic Astrophysical Phenomena from non-survey optimized variable-cadence observational data},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.04.035},
doi = {10.1016/j.eswa.2019.04.035},
journal = {Expert Syst. Appl.},
month = oct,
pages = {94–115},
numpages = {22},
keywords = {Astronomical time-series, Light curve analysis, Period analysis, Variable stars, Binary stars, Random forest classification}
}

@article{10.1007/s10772-015-9327-z,
author = {Johnson, David O. and Kang, Okim},
title = {Automatic prosodic tone choice classification with Brazil's intonation model},
year = {2016},
issue_date = {March     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-015-9327-z},
doi = {10.1007/s10772-015-9327-z},
abstract = {This paper examines the performance of automatically classifying five tone choices (i.e., falling, rising, rising-falling, falling-rising, and neutral) of Brazil's intonation model. We tested two machine learning classifiers (neural network and boosting ensemble) in two configurations (multi-class and pairwise coupling) and a rule-based classifier. Three sets of acoustic features built from the TILT and B\'{e}zier pitch contour models and a new four-point pitch contour model we introduced here were investigated. Tone choices are one of the key elements of Brazil's prosodic intonation model. We found the rule-based classifier, which was built on our four-point model, achieved better results than the others with an accuracy of 75.1 % and a Cohen's kappa coefficient of 0.73. This research proves that it is possible to classify tone choices with an accuracy reaching close to the percentage of agreement between two human analysts. The findings further concluded that our four-point model was better for classifying Brazil's tone choices than both of the TILT or B\'{e}zier models.},
journal = {Int. J. Speech Technol.},
month = mar,
pages = {95–109},
numpages = {15},
keywords = {B\'{e}zier model, Brazil's prosodic intonation model, Machine learning, TILT model, ToBI, Tone choice classification}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Computer-aided diagnosis, Self-paced learning, Low-rank, Feature extraction, Multi-modal fusion}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@inbook{10.5555/3454287.3454560,
author = {L\"{o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan S.},
title = {Putting an end to end-to-end: gradient-isolated learning of representations},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {273},
numpages = {13}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Learning with privileged information, Self-paced learning, Curriculum learning}
}

@inproceedings{10.5555/3042573.3042733,
author = {McDowell, Luke K. and Aha, David W.},
title = {Semi-supervised collective classification via hybrid label regularization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for collective classification (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of label regularization to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1243–1250},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {webly-supervised learning, web label, video understanding, prior knowledge, noisy data, concept detection, big data},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Unsupervised domain adaptation, Person re-identification}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Self-Paced learning, Multiple instance boost learning, Multiple instance learning}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Unlabeled data, Recurring concept drift, Ensemble learning, Data stream classification}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Evaluation, Hierarchical classification, Artificial datasets}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Cross-domain errors, Domain adaptation, Transfer learning},
location = {Taipei, Taiwan}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.5555/2946645.3053445,
author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
title = {Augmentable gamma belief networks},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5656–5699},
numpages = {44},
keywords = {Bayesian nonparametrics, Poisson factor analysis, deep learning, multilayer representation, topic modeling, unsupervised learning}
}

@inproceedings{10.5555/2986459.2986640,
author = {Zhu, Jun and Chen, Ning and Xing, Eric P.},
title = {Infinite latent SVM for classification and multi-task learning},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unlike existing nonparametric Bayesian models, which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, we study nonparametric Bayesian inference with regularization on the desired posterior distributions. While priors can indirectly affect posterior distributions through Bayes' theorem, imposing posterior regularization is arguably more direct and in some cases can be much easier. We particularly focus on developing infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets. Our results appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
pages = {1620–1628},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Soft weighting, Non-convexity, Self-paced learning, Multi-task clustering}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@article{10.1016/j.knosys.2019.104923,
author = {Tuncer, Turker and Dogan, Sengul and P\l{}awiak, Pawe\l{} and Rajendra Acharya, U.},
title = {Automated arrhythmia detection using novel hexadecimal local pattern and multilevel wavelet transform with ECG signals},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {186},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104923},
doi = {10.1016/j.knosys.2019.104923},
journal = {Know.-Based Syst.},
month = dec,
numpages = {19},
keywords = {Hexadecimal local pattern, Multilevel DWT, ECG classification, Pattern recognition, Biomedical engineering}
}

@article{10.1016/j.asoc.2021.107112,
author = {Almaghrabi, Fatima and Xu, Dong-Ling and Yang, Jian-Bo},
title = {An evidential reasoning rule based feature selection for improving trauma outcome prediction},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107112},
doi = {10.1016/j.asoc.2021.107112},
journal = {Appl. Soft Comput.},
month = may,
numpages = {15},
keywords = {Imbalance classes, ReliefF, Random forest, Evidential reasoning rule, Trauma, Feature selection}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Clustering, Process discovery, Process mining, Configuration workflow, Variability}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@inproceedings{10.5555/3042573.3042690,
author = {Hu, Yuening and Zhai, Ke and Williamson, Sinead and Boyd-Graber, Jordan},
title = {Modeling images using transformed indian buffet processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {899–906},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {unsupervised learning, convolutional neural network, clustering, Large-scale person re-identification}
}

@inproceedings{10.5555/3540261.3541062,
author = {Wang, Ziyu and Zhou, Yuhao and Ren, Tongzheng and Zhu, Jun},
title = {Scalable quasi-Bayesian inference for instrumental variable regression},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent years have witnessed an upsurge of interest in employing flexible machine learning models for instrumental variable (IV) regression, but the development of uncertainty quantification methodology is still lacking. In this work we present a scalable quasi-Bayesian procedure for IV regression, building upon the recently developed kernelized IV models. Contrary to Bayesian modeling for IV, our approach does not require additional assumptions on the data generating process, and leads to a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods. Our algorithm can be further extended to work with neural network models. We analyze the theoretical properties of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of our method.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {801},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1109/IJCB48548.2020.9304937,
author = {Fondje, Cedric Nimpa and Hu, Shuowen and Short, Nathaniel J. and Riggan, Benjamin S.},
title = {Cross-Domain Identification for Thermal-to-Visible Face Recognition},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IJCB48548.2020.9304937},
doi = {10.1109/IJCB48548.2020.9304937},
abstract = {Recent advances in domain adaptation, especially those applied to heterogeneous facial recognition, typically rely upon restrictive Euclidean loss functions (e.g., L2 norm) which perform best when images from two different domains (e.g., visible and thermal) are co-registered and temporally synchronized. This paper proposes a novel domain adaptation framework that combines a new feature mapping sub-network with existing deep feature models, which are based on modified network architectures (e.g., VGG16 or Resnet50). This framework is optimized by introducing new cross-domain identity and domain invariance lossfunctions for thermal-to-visible face recognition, which alleviates the requirement for precisely co-registered and synchronized imagery. We provide extensive analysis of both features and loss functions used, and compare the proposed domain adaptation framework with state-of-the-art feature based domain adaptation models on a difficult dataset containing facial imagery collected at varying ranges, poses, and expressions. Moreover, we analyze the viability of the proposed framework for more challenging tasks, such as non-frontal thermal-to-visible face recognition.},
booktitle = {2020 IEEE International Joint Conference on Biometrics (IJCB)},
pages = {1–9},
numpages = {9},
location = {Houston, TX, USA}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {Automatic configuration, Artificial intelligence, Feature model, Information dashboards, Meta-model, Domain engineering, SPL}
}

@inproceedings{10.1007/978-3-030-91560-5_1,
author = {Long, Chao and Zhu, Yanmin and Liu, Haobing and Yu, Jiadi},
title = {Efficient Feature Interactions Learning with Gated Attention Transformer},
year = {2021},
isbn = {978-3-030-91559-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91560-5_1},
doi = {10.1007/978-3-030-91560-5_1},
abstract = {Click-through rate (CTR) prediction plays a key role in many domains, such as online advising and recommender system. In practice, it is necessary to learn feature interactions (i.e., cross features) for building an accurate prediction model. Recently, several self-attention based transformer methods are proposed to learn feature interactions automatically. However, those approaches are hindered by two drawbacks. First, Learning high-order feature interactions by using self-attention will generate many repetitive cross features because k-order cross features are generated by crossing (k–1)-order cross features and (k–1)-order cross features. Second, introducing useless cross features (e.g., repetitive cross features) will degrade model performance. To tackle these issues but retain the strong ability of the Transformer, we combine the vanilla attention mechanism with the gated mechanism and propose a novel model named Gated Attention Transformer. In our method, k-order cross features are generated by crossing (k–1)-order cross features and 1-order features, which uses the vanilla attention mechanism instead of the self-attention mechanism and is more explainable and efficient. In addition, as a supplement of the attention mechanism that distinguishes the importance of feature interactions at the vector-wise level, we further use the gated mechanism to distill the significant feature interactions at the bit-wise level. Experiments on two real-world datasets demonstrate the superiority and efficacy of our proposed method.},
booktitle = {Web Information Systems Engineering – WISE 2021: 22nd International Conference on Web Information Systems Engineering, WISE 2021, Melbourne, VIC, Australia, October 26–29, 2021, Proceedings, Part II},
pages = {3–17},
numpages = {15},
keywords = {Feature interactions, CTR prediction, Transformer},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Deep learning, Classification, Vehicle, Road traffic, Environmental noise}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@article{10.1016/j.neucom.2019.02.035,
author = {Zhang, Jiashuai and Miao, Jianyu and Zhao, Kun and Tian, Yingjie},
title = {Multi-task feature selection with sparse regularization to extract common and task-specific features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {340},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.02.035},
doi = {10.1016/j.neucom.2019.02.035},
journal = {Neurocomput.},
month = may,
pages = {76–89},
numpages = {14},
keywords = {ADMM, Non-convex, Sparse regularization, Multi-task feature learning}
}

@inproceedings{10.1007/978-3-030-23502-4_14,
author = {Sondur, Sanjeev and Kant, Krishna},
title = {Towards Automated Configuration of Cloud Storage Gateways: A Data Driven Approach},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_14},
doi = {10.1007/978-3-030-23502-4_14},
abstract = {Cloud storage gateways (CSGs) are an essential part of enterprises to take advantage of the scale and flexibility of cloud object store. A CSG provides clients the impression of a locally configured large size block-based storage device, which needs to be mapped to remote cloud storage which is invariably object based. Proper configuration of the cloud storage gateway is extremely challenging because of numerous parameters involved and interactions among them. In this paper, we study this problem for a commercial CSG product that is typical of offerings in the market. We explore how machine learning techniques can be exploited both for the forward problem (i.e. predicting performance from the configuration parameters) and backward problem (i.e. predicting configuration parameter values from the target performance). Based on extensive testing with real world customer workloads, we show that it is possible to achieve excellent prediction accuracy while ensuring that the model is not overfitted to the data.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {192–207},
numpages = {16},
keywords = {Machine learning, Configuration management, Performance, Object store, Cloud storage gateway},
location = {San Diego, CA, USA}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {variability mining, software product lines, reverse engineering, feature location, benchmark},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {network-based knowledge, log-sum penalty, gene selection, Regularization}
}

@article{10.1007/s10845-021-01827-7,
author = {Ning, Fangwei and Shi, Yan and Cai, Maolin and Xu, Weiqing},
title = {Part machining feature recognition based on a deep learning method},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01827-7},
doi = {10.1007/s10845-021-01827-7},
abstract = {Machining feature recognition is a key step in computer-aided process planning to improve the level of design and manufacturing, production efficiency, and competitiveness. Although the traditional feature recognition method using a graph-based approach has advantages in feature logic expression, the calculation process is inefficient. Deep learning is a new technology that can automatically learn complex mapping relationships and high-level data features from a large amount of data. Therefore, this classification technology has been successfully and widely used in various fields. This study examined a three-dimensional convolutional neural network combined with a graph-based approach, taking advantage of deep learning technology and traditional feature recognition methods. First, the convex and concave machining features of a part were determined using an attributed adjacency graph. Then, the machining features were separated using the bounding box method and voxelized. Subsequently, a stretching and zooming method was proposed to obtain the training data. After training, the test and comparison results demonstrated the high accuracy rate of the proposed method and the improvement in recognition efficiency. The proposed method could also identify convex features, which further improved the recognition range.},
journal = {J. Intell. Manuf.},
month = sep,
pages = {809–821},
numpages = {13},
keywords = {CAPP, Convolution neural network, STEP, Feature recognition, Deep learning}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Self-sampling, Robustness, Loss function, Boosting}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Multi-modal, Self-paced learning, Curriculum learning, Image classification}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {hub genes, differentially expressed genes, co-expression network, cervical cancer}
}

@inproceedings{10.1145/3459930.3471160,
author = {William, Femi and Zhu, Feng},
title = {CNN models for eye state classification using EEG with temporal ordering},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3471160},
doi = {10.1145/3459930.3471160},
abstract = {Most studies on eye states (open or closed) detection apply machine learning techniques on subject dependent eye state datasets, but subject independent data with large physiological variation between individuals has not been well explored. Temporal ordering information is important to predict eye state because EEG is a time sequence dataset. In this research, we keep the temporal ordering of the data in place. We create multiple CNN network models and select optimal filters and depth. Our CNN feature models are effective on both subject dependent and subject independent eye state EEG classifications. We got the best subject dependent result with 4 layers of CNNs with an accuracy rate of 96.51% on dataset I and 100% on dataset II. For subject's independent studies, we got the best classification accuracy of 80.47% on dataset I and we got 90.15% on dataset II.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {80},
numpages = {8},
keywords = {CNN, EEG, deep learning, eye state, models subject independent},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {deep learning, computer vision, machine learning, natural language}
}

@article{10.1007/s11761-019-00272-y,
author = {Alghofaily, Bayan I. and Ding, Chen},
title = {Data mining service recommendation based on dataset features},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-019-00272-y},
doi = {10.1007/s11761-019-00272-y},
abstract = {Quality of service (QoS)-based web service selection has been studied in the service computing community for some time. However, characteristics of the input dataset that is going to be processed by the web service are not usually considered in the selection process, even though they might have impact on QoS values of the service, e.g. latency on processing a bigger dataset is higher than that on a smaller dataset, one service takes longer time to process a certain dataset than another service. To address this issue, in this work, we take into consideration the dataset features in the QoS-based service recommendation process and we focus on data mining services because their QoS values could be highly dependent on dataset features. We propose two approaches for data mining service recommendations and compare their performances. In the first approach, we use a meta-learning algorithm to incorporate dataset features in the recommendation process and study the use of different machine learning algorithms (both classification models and regression models) as meta-learners in recommending data mining services for the given dataset. We also investigate the impact of the number of dataset features on the performance of the meta-learners. In the second approach, we propose a novel technique of using factor analysis for web service recommendation. We use decomposition technique to identify latent features of the input dataset and then recommend services by exploiting these latent variables. Our proposed approach of web service recommendation based on latent features was shown to be a more robust model with an accuracy of 85% compared to meta-feature-based recommendation.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {261–277},
numpages = {17},
keywords = {Web services, Service recommendation, Machine learning, Data mining}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Classification in data streams, Non-stationary environments, Learning automata, Weak estimators}
}

@inproceedings{10.5555/3495724.3495981,
author = {d'Ascoli, St\'{e}phane and Sagun, Levent and Biroli, Giulio},
title = {Triple descent and the two kinds of overfitting: where &amp; why do they appear?},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A recent line of research has highlighted the existence of a "double descent" phenomenon in deep learning, whereby increasing the number of training examples N causes the generalization error of neural networks to peak when N is of the same order as the number of parameters P. In earlier works, a similar phenomenon was shown to exist in simpler models such as linear regression, where the peak instead occurs when N is equal to the input dimension D. Since both peaks coincide with the interpolation threshold, they are often conflated in the litterature. In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is then governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent. As shown previously, the nonlinear peak at N = P is a true divergence caused by the extreme sensitivity of the output function to both the noise corrupting the labels and the initialization of the random features (or the weights in neural networks). This peak survives in the absence of noise, but can be suppressed by regularization. In contrast, the linear peak at N = D is solely due to overfitting the noise in the labels, and forms earlier during training. We show that this peak is implicitly regularized by the nonlinearity, which is why it only becomes salient at high noise and is weakly affected by explicit regularization. Throughout the paper, we compare analytical results obtained in the random feature model with the outcomes of numerical experiments involving deep neural networks.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {257},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.knosys.2012.11.009,
author = {Wu, Yu-Chieh},
title = {Integrating statistical and lexical information for recognizing textual entailments in text},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2012.11.009},
doi = {10.1016/j.knosys.2012.11.009},
abstract = {Recognizing textual entailment is to infer that a given text span follows from the meaning of a given hypothesis. To have better recognition capability, it is necessary to employ deep text processing units such as syntactic parsers and semantic taggers. However, these resources are not usually available in other non-English languages. In this paper, we present a light-weight Chinese textual entailment recognition system using part-of-speech information only. We designed two different feature models from training data and employed the well-known kernel method to learn to predict testing data. One feature set abstracts the generic statistics between the text pairs, while the other set directly models lexical features based on the traditional bag-of-words model. The ability of the proposed feature models not only brings additional statistical information from their datasets but also helps to enhance the prediction capability. To validate this, we conducted the experiments on the novel benchmark corpus - NTCIR-RITE-2011. The empirical results demonstrate that our method achieves the best results in comparison to the other competitors. In terms of accuracy, our method achieves 54.77% for the NTCIR RITE MC task.},
journal = {Know.-Based Syst.},
month = mar,
pages = {27–35},
numpages = {9},
keywords = {Kernel methods, Machine learning, Natural language processing, Text mining, Textual entailment}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {XNOR-Net, Ball detection, Deep learning, Convolution neural network},
location = {Nagoya, Japan}
}

@article{10.1007/s10618-019-00616-4,
author = {Clark, Jessica and Provost, Foster},
title = {Unsupervised dimensionality reduction versus supervised regularization for classification from sparse data},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-019-00616-4},
doi = {10.1007/s10618-019-00616-4},
abstract = {Unsupervised matrix-factorization-based dimensionality reduction (DR) techniques are popularly used for feature engineering with the goal of improving the generalization performance of predictive models, especially with massive, sparse feature sets. Often DR is employed for the same purpose as supervised regularization and other forms of complexity control: exploiting a bias/variance tradeoff to mitigate overfitting. Contradicting this practice, there is consensus among existing expert guidelines that supervised regularization is a superior way to improve predictive performance. However, these guidelines are not always followed for this sort of data, and it is not unusual to find DR used with no comparison to modeling with the full feature set. Further, the existing literature does not take into account that DR and supervised regularization are often used in conjunction. We experimentally compare binary classification performance using DR features versus the original features under numerous conditions: using a total of 97 binary classification tasks, 6 classifiers, 3 DR techniques, and 4 evaluation metrics. Crucially, we also experiment using varied methodologies to tune and evaluate various key hyperparameters. We find a very clear, but nuanced result. Using state-of-the-art hyperparameter-selection methods, applying DR does not add value beyond supervised regularization, and can often diminish performance. However, if regularization is not done well (e.g., one just uses the default regularization parameter), DR does have relatively better performance--but these approaches result in lower performance overall. These latter results provide an explanation for why practitioners may be continuing to use DR without undertaking the necessary comparison to using the original features. However, this practice seems generally wrongheaded in light of the main results, if the goal is to maximize generalization performance.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {871–916},
numpages = {46},
keywords = {Sparse data, Experimental comparison, Dimensionality reduction, Data mining, Binary classification}
}

@article{10.1016/j.eswa.2016.04.008,
author = {Corr\^{e}a, D\'{e}bora C. and Rodrigues, Francisco Ap.},
title = {A survey on symbolic data-based music genre classification},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {60},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.04.008},
doi = {10.1016/j.eswa.2016.04.008},
abstract = {Music is present in everyday life and used for a wide range of objectives. Musical databases have considerably increased in number and size over the past years, therefore, the development of accurate tools for music information retrieval (MIR) has become an important topic in computer science. The increasing theoretical advances in machine learning algorithms together with the abundance of recordings available in digital audio formats, the growing quality and accessibility of on-line symbolic music data, and availability of tools and toolboxes for the extraction of musical properties have motivated many studies on machine learning and MIR. Relevant problems in MIR include classification of songs into genres, which enables the summarization of common features (or patterns) shared by different songs. The automatic classification of music genres plays a fundamental role in the context of music indexing and retrieval, so that websites and device music engines can manage and label music content. Most studies have dealt with such an issue by extracting music characteristics from the audio content, and some have provided overviews of audio features and classification algorithms for music genre classification. However, precise high-level musical information can be extracted from symbolic data (e.g. digital music scores), known to be closely related to the way humans perceive music. A number of approaches use such musical information to process, retrieve and classify music content. This manuscript provides an overview of the most important approaches that deal with music genre classification and consider the symbolic representation of music data. Current issues inherent to such a music format, as well the main algorithms adopted for the modeling of the music feature space are presented.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {190–210},
numpages = {21},
keywords = {Classification algorithms, Music descriptors, Music information retrieval, Musical genres, Symbolic music data}
}

@inproceedings{10.5555/3042817.3042962,
author = {Broderick, Tamara and Kulis, Brian and Jordan, Michael I.},
title = {MAD-bayes: MAP-based asymptotic derivations from bayes},
year = {2013},
publisher = {JMLR.org},
abstract = {The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis &amp; Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–226–III–234},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@inproceedings{10.1109/SPLC.2008.11,
author = {Niu, Nan and Easterbrook, Steve},
title = {On-Demand Cluster Analysis for Product Line Functional Requirements},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.11},
doi = {10.1109/SPLC.2008.11},
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {87–96},
numpages = {10},
keywords = {requirements clustering, overlapping clustering, information-theoretic clustering, functional requirements profiles},
series = {SPLC '08}
}

@article{10.1016/j.asoc.2021.107242,
author = {Yang, Yun and Guo, Jing and Ye, Qiongwei and Xia, Yuelong and Yang, Po and Ullah, Amin and Muhammad, Khan},
title = {A weighted multi-feature transfer learning framework for intelligent medical decision making},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107242},
doi = {10.1016/j.asoc.2021.107242},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {11},
keywords = {Transformative computing, Distribution variances, Ensemble learning, Transfer learning, Medical decision making}
}

@inproceedings{10.5555/1779178.1779251,
author = {Leitner, Raimund},
title = {Learning 3D object recognition from an unlabelled and unordered training set},
year = {2007},
isbn = {3540768572},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes an unsupervised learning technique for object recognition from an unlabelled and unordered set of training images. It enables the robust recognition of complex 3D objects in cluttered scenes, under scale changes and partial occlusion. The technique uses a matching based on the consistency of two different descriptors characterising the appearance and shape of local features. The variation of each local feature with viewing direction is modeled by a multi-view feature model. These multi-view feature models can be matched directly to the features found in a test image. This avoids a matching to all training views as necessary for approaches based on canonical views.The proposed approach is tested with real world objects and compared to a supervised approach using features characterised by SIFT descriptors (Scale Invariant Feature Transform). These experiments show that the performance of our unsupervised technique is equal to that of a supervised SIFT object recognition approach.},
booktitle = {Proceedings of the 3rd International Conference on Advances in Visual Computing - Volume Part I},
pages = {644–651},
numpages = {8},
keywords = {unsupervised learning, object recognition},
location = {Lake Tahoe, NV, USA},
series = {ISVC'07}
}

@article{10.1007/s10845-019-01502-y,
author = {Kim, Myeongso and Lee, Minyoung and An, Minjeong and Lee, Hongchul},
title = {Effective automatic defect classification process based on CNN with stacking ensemble model for TFT-LCD panel},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-019-01502-y},
doi = {10.1007/s10845-019-01502-y},
abstract = {The classification of defect types during LCD panel production is very important because it is closely related to deciding whether a defect panel is restorable. But since defect areas are very small compared to the panel area, it is hard to classify defect types by images. Therefore, we need to eliminate the background pattern of the panel, but this is not an easy task because the brightness and saturation of the background varies, even in a single image. In this paper, we propose an indicator that can distinguish between defect and background area, which is robust to brightness change and minor noises. With this indicator, we got useful defect information and images with patterns eliminated to make a more efficient defect classifier. The convolutional neural network with stacked ensemble techniques played a great role in improving defect classification performance, when various information from image preprocessing was combined.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {1165–1174},
numpages = {10},
keywords = {Pattern elimination, Convolutional neural network, Defect classification}
}

@inproceedings{10.5555/3540261.3541878,
author = {Latorre, Fabian and Dadi, Leello and Rolland, Paul and Cevher, Volkan},
title = {The effect of the intrinsic dimension on the generalization of quadratic classifiers},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It has been recently observed that neural networks, unlike kernel methods, enjoy a reduced sample complexity when the distribution is isotropic (i.e., when the covariance matrix is the identity). We find that this sensitivity to the data distribution is not exclusive to neural networks, and the same phenomenon can be observed on the class of quadratic classifiers (i.e., the sign of a quadratic polynomial) with a nuclear-norm constraint. We demonstrate this by deriving an upper bound on the Rademacher Complexity that depends on two key quantities: (i) the intrinsic dimension, which is a measure of isotropy, and (ii) the largest eigenvalue of the second moment (covariance) matrix of the distribution. Our result improves the dependence on the dimension over the best previously known bound and precisely quantifies the relation between the sample complexity and the level of isotropy of the distribution.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1617},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-41505-1_9,
author = {del R\'{\i}o, Iria},
title = {Native Language Identification on L2 Portuguese},
year = {2020},
isbn = {978-3-030-41504-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41505-1_9},
doi = {10.1007/978-3-030-41505-1_9},
abstract = {This study advances on Native Language Identification (NLI) for L2 Portuguese. We use texts from the NLI-PT dataset corresponding to five native languages: Chinese, English, German, Italian, and Spanish. We include the same L1s as in previous works, and more texts per language. We investigate the impact of different lexical representations, the use of syntactic dependencies and the performance of diverse classification methods. Our best model achieves an accuracy of 0.66 including lexical features, and of 0.61 excluding them. Both results improve previous works on NLI for L2 Portuguese.},
booktitle = {Computational Processing of the Portuguese Language: 14th International Conference, PROPOR 2020, Evora, Portugal, March 2–4, 2020, Proceedings},
pages = {87–97},
numpages = {11},
keywords = {Native Language Identification, L2 Portuguese, Second language acquisition},
location = {Evora, Portugal}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Soft weighting, Multi-view clustering, Self-paced learning}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@inproceedings{10.1145/3299819.3299826,
author = {Dai, Pengcheng and Song, Changxiong and Lin, Huiping and Jia, Pei and Xu, Zhipeng},
title = {Cluster-Based Destination Prediction in Bike Sharing System},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299826},
doi = {10.1145/3299819.3299826},
abstract = {Destination prediction not only helps to understand users' behavior, but also provides basic information for destination-related customized service. This paper studies the destination prediction in the public bike sharing system, which is now blooming in many cities as an environment friendly short-distance transportation solution. Due to the large number of bike stations (e.g. more than 800 stations of Citi Bike in New York City), the accuracy and effectiveness of destination prediction becomes a problem, where clustering algorithm is often used to reduce the number of destinations. However, grouping bike stations according to their location is not effective enough. The contribution of the paper lies in two aspects: 1) Proposes a Compound Stations Clustering method that considers not only the geographic location but also the usage pattern; 2) Provide a framework that uses feature models and corresponding labels for machine learning algorithms to predict destination for on-going trips. Experiments are conducted on real-world data sets of Citi Bike in New York City through the year of 2017 and results show that our method outperforms baselines in accuracy.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {1–8},
numpages = {8},
keywords = {Machine Learning, Destination Prediction, Clustering, Bike Sharing System},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Meta-modeling, Machine Learning, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {99-00, 00-01, Local structure preservation, Latent representation learning, Hypergraph learning, Unsupervised feature selection}
}

@article{10.1007/s10489-020-01754-9,
author = {Lagopoulos, Athanasios and Tsoumakas, Grigorios},
title = {Content-aware web robot detection},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01754-9},
doi = {10.1007/s10489-020-01754-9},
abstract = {Web crawlers account for more than a third of the total web traffic and they are threatening the security, privacy and veracity of web applications and their users. Businesses in finance, ticketing, and publishing, as well as websites with rich and unique content are the ones mostly affected by their actions. To deal with this problem, we present a novel web robot detection approach that takes advantage of the content of a website based on the assumption that human web users are interested in specific topics, while web robots crawl the web randomly. Our approach extends the typical user session representation of log-based features with a novel set of features that capture the semantics of the content of the requested resources. In addition, we contribute a new real-world dataset, which we make publicly available, towards alleviating the scarcity of open data in this field. Empirical results on this dataset validate our assumption and show that our approach outranks state-of-the-art methods for web robot detection.},
journal = {Applied Intelligence},
month = nov,
pages = {4017–4028},
numpages = {12},
keywords = {Latent dirichlet allocation, Supervised learning, Semantics, Crawler, Web robot}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Combinatorial optimization, Multi-armed bandits, Machine learning, A/B testing, Continuous experimentation}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {unsupervised learning, domain adaptation, deep learning, Person re-identification}
}

@inproceedings{10.5555/3540261.3541667,
author = {Ghalebikesabi, Sahra and Ter-Minassian, Lucile and Diaz-Ordaz, Karla and Holmes, Chris},
title = {On locality of local explanation models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1406},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@inproceedings{10.1007/978-3-030-30244-3_21,
author = {Chawla, Piyush and Esteves, Diego and Pujar, Karthik and Lehmann, Jens},
title = {SimpleLSTM: A Deep-Learning Approach to Simple-Claims Classification},
year = {2019},
isbn = {978-3-030-30243-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30244-3_21},
doi = {10.1007/978-3-030-30244-3_21},
abstract = {The information on the internet suffers from noise and corrupt knowledge that may arise due to human and mechanical errors. To further exacerbate this problem, an ever-increasing amount of fake news on social media, or internet in general, has created another challenge to drawing correct information from the web. This huge sea of data makes it difficult for human fact checkers and journalists to assess all the information manually. In recent years Automated Fact-Checking has emerged as a branch of natural language processing devoted to achieving this feat. In this work, we give an overview of recent approaches, emphasizing on the key challenges faced during the development of such frameworks. We test existing solutions to perform claim classification on simple-claims and introduce a new model dubbed SimpleLSTM, which outperforms baselines by 11%, 10.2% and 18.7% on FEVER-Support, FEVER-Reject and 3-Class datasets respectively. The data, metadata and code are released as open-source and will be available at .},
booktitle = {Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 3–6, 2019, Proceedings, Part II},
pages = {244–255},
numpages = {12},
keywords = {Fact-checking, Trustworthiness, Evidence extraction},
location = {Vila Real, Portugal}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Text, Linear discriminant analysis, Fisher, Eigenvectors, Classification, Arabic}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Noise identification, Meta-learning, Complexity measures, Characterization measures}
}

@article{10.1016/j.eswa.2021.115234,
author = {Ma, Liang and Ding, Yu and Wang, Zili and Wang, Chao and Ma, Jian and Lu, Chen},
title = {An interpretable data augmentation scheme for machine fault diagnosis based on a sparsity-constrained generative adversarial network},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115234},
doi = {10.1016/j.eswa.2021.115234},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Raw vibration signal, Machine fault diagnosis, Mechanism interpretation, Data augmentation, Generative adversarial networks}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@article{10.5555/2567709.2502612,
author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
title = {Bayesian Canonical correlation analysis},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combination of large dimensionalities and small sample sizes. The existing methods have not been particularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment.},
journal = {J. Mach. Learn. Res.},
month = apr,
pages = {965–1003},
numpages = {39},
keywords = {variational Bayesian approximation, inter-battery factor analysis, group-wise sparsity, canonical correlation analysis, Bayesian modeling}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Machine learning, Risk optimization, Product line configuration, Variability modeling, Multi-cloud services},
location = {Rhodes, Greece}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@article{10.1007/s11042-018-6548-6,
author = {Zhao, Lijun and Zhang, Wei and Tang, Ping},
title = {Analysis of the inter-dataset representation ability of deep features for high spatial resolution remote sensing image scene classification},
year = {2019},
issue_date = {Apr 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {8},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6548-6},
doi = {10.1007/s11042-018-6548-6},
abstract = {Recently, scene based classification has become a new trend for very high spatial resolution remote sensing image interpretation. With the advent of deep learning, the pretrained convolutional neural networks (CNNs) have been proved effective as feature extractors for scene classification tasks in the remote sensing domain, but the potential characteristics and capabilities of such deep features have not been sufficiently analyzed and fully understood. Facing with complex remote sensing scenes with huge intra-class variations, it is still not clear about the limitation of these powerful deep features in exploring essential invariant attributes of remote sensing scenes of the same kind but, in most cases, from separate sources. Therefore, this paper makes an intensive investigation in the feature representation ability of such deep features from the aspect of inter-dataset scene classification of remote sensing images. Four well-known pretrained CNN models and three different commonly used datasets are selected and summarized. Firstly, deep features extracted from various intermediate layers of these models are compared. Then, the inter-dataset feature representation ability is evaluated using cross-classification of different datasets and discussed in terms of imaging spatial resolution, image size, model structure, and time efficiency. Finally, several instructive findings are revealed and conclusions are drawn regarding the strength and weakness of the CNN features in the application of remote sensing image scene classification.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {9667–9689},
numpages = {23},
keywords = {Scene classification, Remote sensing image, Inter-dataset feature representation, Deep learning features, Convolutional neural networks (CNNs)}
}

@article{10.5555/3455716.3455743,
author = {Williamson, Sinead A. and Zhang, Michael Minyi and Damien, Paul},
title = {A new class of time dependent latent factor models with applications},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time.This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process -- a probability measure on the space of random, unbounded binary matrices -- finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {27},
numpages = {24},
keywords = {Bayesian nonparametrics, latent factor models, time dependence}
}

@inproceedings{10.1145/3395035.3425645,
author = {Chen, Yifan and Jie, Zhuoni and Gunes, Hatice},
title = {Automatic Analysis of Facilitated Taste-liking},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425645},
doi = {10.1145/3395035.3425645},
abstract = {This paper focuses on: (i) Automatic recognition of taste-liking from facial videos by comparatively training and evaluating models with engineered features and state-of-the-art deep learning architectures, and (ii) analysing the classification results along the aspects of facilitator type, and the gender, ethnicity, and personality of the participants. To this aim, a new beverage tasting dataset acquired under different conditions (human vs. robot facilitator and priming vs. non-priming facilitation) is utilised. The experimental results show that: (i) The deep spatiotemporal architectures provide better classification results than the engineered feature models; (ii) the classification results for all three classes of liking, neutral and disliking reach F1 scores in the range of 71% - 91%; (iii) the personality-aware network that fuses participants' personality information with that of facial reaction features provides improved classification performance; and (iv) classification results vary across participant gender, but not across facilitator type and participant ethnicity.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {292–300},
numpages = {9},
keywords = {taste-liking, personality, facial reactions, engineered features, deep spatiotemporal networks, affective computing},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1016/j.eswa.2017.10.034,
author = {Bazaoui, Abir and Barhoumi, Walid and Ahmed, Amr and Zagrouba, Ezzeddine},
title = {Modeling clinician medical-knowledge in terms of med-level features for semantic content-based mammogram retrieval},
year = {2018},
issue_date = {March 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.10.034},
doi = {10.1016/j.eswa.2017.10.034},
abstract = {Generation of the computational clinician medical-knowledge models.Inferring semantic concepts from low-level features in terms of med-level ones.New variation of reduced shearlet coefficients for multi-resolution mammogram characterization.Creation of a ground-truth of MIAS dataset containing enriched Q/A radiologists.The proposed method outperforms compared CBMIR methods. The huge volume of variability in real-world medical images such as on dimensionality, modality and shape, makes necessary efficient medical image retrieval systems for assisting physicians to perform more accurate diagnoses. However, the major limitation of these systems is the semantic gap, which is the difference between low-level features of images and their high-level semantics in a given situation. This paper deals with this problem and proposes a content-based image retrieval method based on med-level descriptors. These descriptors are automatically generated from low-level image features by exploiting the semantic concepts based on the clinician medical-knowledge. In fact, the proposed method is based on three main steps: (1) low-level feature extraction, (2) med-level model extraction and (3) online retrieval based on med-level feature vectors. The main contributions reside firstly in the integration of clinician medical-knowledge in terms of med-level features without needing radiologists interaction. Secondly, the determination of the query high-level features can be performed through the predicted query med-level descriptors, in addition to retrieve the most relevant images to the query one. Proposed method was validated in the context of mammogram retrieval, on the MIAS dataset, and the results prove its effectiveness and its superiority to the compared methods.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {11–20},
numpages = {10},
keywords = {Breast cancer diagnosis, CBMIR, Clinician medical-knowledge, Low-level features, Med-level model}
}

@article{10.1007/s11042-019-7217-0,
author = {Chutani, Shaveta and Goyal, Anjali},
title = {A review of forensic approaches to digital image Steganalysis},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7217-0},
doi = {10.1007/s11042-019-7217-0},
abstract = {Traditional or Binary Steganalysis brands a digital object such as an image as stego or innocent only but modern day information security requires deeper insight about the embedded message. Forensic Steganalysis is the systemic application of different research techniques to gather further in-depth knowledge about the hidden secret information. Once an image is categorised as being stego, additional investigations are carried out to find the steganographic algorithm used to insert the covert message, estimate the length of such message and finally the stego key used to embed the message in various pixels of the stego image. A lot more literature is available on the review of the traditional steganalysis techniques as compared to forensic steganalysis. The present paper gives details about important forensic techniques as available in the steganalysis literature. The techniques presented and discussed relate to digital image domain for determination of the embedding algorithm, estimation of the secret message payload and stego key determination. The paper describes and compares different features of these forensic techniques. Discussions about significant performance metrics and evaluation parameters used in all phases further elaborate the comparative perspective. We identify potential challenges and explore areas of future work to boost the capabilities of present forensic steganalyzers.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18169–18204},
numpages = {36},
keywords = {Forensic Steganalysis, Multi-class classification, Payload estimation, Quantitative Steganalysis, Stego-key attack}
}

@article{10.1145/3439950,
author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
title = {Deep Learning for Anomaly Detection: A Review},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3439950},
doi = {10.1145/3439950},
abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages, and disadvantages and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {38},
numpages = {38},
keywords = {outlier detection, one-class classification, novelty detection, deep learning, Anomaly detection}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hern\'{a}n and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Hybrid Systems, Clustering, Classification, Class Imbalance}
}

@article{10.1016/j.neucom.2016.09.108,
author = {Ji, Ping and Gao, Xianhe and Hu, Xueyou},
title = {Automatic image annotation by combining generative and discriminant models},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {236},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.108},
doi = {10.1016/j.neucom.2016.09.108},
abstract = {Generative model based image annotation methods have achieved good annotation performance. However, due to the problem of semantic gap, these methods always suffer from the images with similar visual features but different semantics. It seems promising to separate these images from semantic relevant ones by using discriminant models, since they have shown excellent generalization performance. Motivated to gain the benefits of both generative and discriminative approaches, in this paper, we propose a novel image annotation approach which combine the generative and discriminative models through local discriminant topics in the neighborhood of the unlabeled image. Singular Value Decomposition(SVD) is applied to group the images of the neighborhood into different topics according to their semantic labels. The semantic relevant images and the irrelevant ones are always assigned into different topics. By exploiting the discriminant information between different topics, Support Vector Machine(SVM) is applied to classify the unlabeled image into the relevant topic, from which the more accurate annotation will be obtained by reducing the bad influence of irrelevant images. The experiments on the ECCV 2002 [3] and NUS-WIDE [34] benchmark show that our method outperforms state-of-the-art annotation models.},
journal = {Neurocomput.},
month = may,
pages = {48–55},
numpages = {8},
keywords = {Multimedia, Image annotation, Discriminant model, Content Analysis}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {task assignment, recommendation, machine learning, configuration assistance, Bug report triage}
}

@inproceedings{10.1109/ICASSP.2018.8461818,
author = {Tang, Qingming and Wang, Weiran and Livescu, Karen},
title = {Acoustic Feature Learning Using Cross-Domain Articulatory Measurements},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2018.8461818},
doi = {10.1109/ICASSP.2018.8461818},
abstract = {Previous work has shown that it is possible to improve speech recognition by learning acoustic features from paired acoustic-articulatory data, for example by using canonical correlation analysis (CCA) or its deep extensions. One limitation of this prior work is that the learned feature models are difficult to port to new datasets or domains, and articulatory data is not available for most speech corpora. In this work we study the problem of acoustic feature learning in the setting where we have access to an external, domain-mismatched dataset of paired speech and articulatory measurements, either with or without labels. We develop methods for acoustic feature learning in these settings, based on deep variational CCA and extensions that use both source and target domain data and labels. Using this approach, we improve phonetic recognition accuracies on both TIMIT and Wall Street Journal and analyze a number of design choices.},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {4849–4853},
numpages = {5},
location = {Calgary, AB, Canada}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {transfer learning, pseudo-labelling, mixed-kernels neural networks, integrated gradients, image perturbations, domain-specific, Semi-supervised learning, Neural networks, CT scans},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1007/978-3-030-89095-7_73,
author = {Sun, Ruiqi and Zhang, Qin and Guo, Jiamin and Chai, Hui and Li, Yueyang},
title = {Human Action Recognition Using Skeleton Data from Two-Stage Pose Estimation Model},
year = {2021},
isbn = {978-3-030-89094-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89095-7_73},
doi = {10.1007/978-3-030-89095-7_73},
abstract = {This paper presents a method of human action recognition based on the key points of skeleton, aiming to guide the robot to follow a leader in complex environments. We propose a two-stage human pose estimation model which combines the Single Shot Detector (SSD) algorithm based on ResNet with Convolutional Pose Machines (CPMs) to obtain the key points positions of the human skeleton in 2D images. Based on the position information, we construct structure vectors. Feature models consisting of eight angle features and four modulus ratio features are then extracted as the representation of actions. Finally, multi-classification SVM is used to classify the feature models for action recognition. The experimental results demonstrate the validity of the two-stage human pose estimation model to accomplish the task of human action recognition. Our method achieves 97% recognition accuracy on the self-collected dataset composed of six command actions.},
booktitle = {Intelligent Robotics and Applications: 14th International Conference, ICIRA 2021, Yantai, China, October 22–25, 2021, Proceedings, Part I},
pages = {769–779},
numpages = {11},
keywords = {Multi-classification SVM, Convolutional pose machines, Key points position of skeleton, Human action recognition},
location = {Yantai, China}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Weakly supervised learning, Self-paced larning, Object detection}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Multi-class classification, Decomposition strategy, One-vs-one, Heterogeneous ensemble, Dynamic selection}
}

@article{10.5555/2627435.2638592,
author = {Zhu, Jun and Chen, Ning and Xing, Eric P.},
title = {Bayesian inference with posterior regularization and applications to infinite latent SVMs},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1799–1847},
numpages = {49},
keywords = {posterior regularization, multi-task learning, large-margin learning, classification, Bayesian nonparametrics, Bayesian inference}
}

@article{10.1007/s00034-019-01144-8,
author = {Ramya, R. and Mala, K. and Selva Nidhyananthan, S.},
title = {3D Facial Expression Recognition Using Multi-channel Deep Learning Framework},
year = {2020},
issue_date = {Feb 2020},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {39},
number = {2},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-019-01144-8},
doi = {10.1007/s00034-019-01144-8},
abstract = {Facial expression offers an important way of detecting the affective state of a human being. It plays a major role in various fields such as the estimation of students’ attention level in online education, intelligent transportation systems and interactive games. This paper proposes a facial expression recognition system in which two channels of featured images are used to represent a 3D facial scan. Features are extracted from the local binary pattern and local directional pattern using a fine-tuned pre-trained AlexNet and a shallow convolutional neural network. The feature sets are then fused together using canonical correlation analysis. The fused feature set is fed into a multi-support vector machine (mSVM) classifier to classify the expressions into seven basic categories: anger, disgust, fear, happiness, neutral, sadness and surprise. Experiments were carried out on the Bosphorus database using tenfold cross-validation with mutually exclusive training and testing samples. The results show an average accuracy of 87.69% using an mSVM classifier with a polynomial kernel and demonstrate that the system performs better by characterizing the peculiarities in facial expressions than alternative state-of-the-art approaches.},
journal = {Circuits Syst. Signal Process.},
month = feb,
pages = {789–804},
numpages = {16},
keywords = {Support vector machines, Machine learning, Emotion recognition, Deep learning, Convolutional neural networks, Affective computing}
}

@inproceedings{10.1007/978-3-319-48279-8_42,
author = {Tomai, Emmett and Lopez, Luis},
title = {Towards a Model-Learning Approach to Interactive Narrative Intelligence for Opportunistic Storytelling},
year = {2016},
isbn = {978-3-319-48278-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-48279-8_42},
doi = {10.1007/978-3-319-48279-8_42},
abstract = {Opportunistic storytelling is an approach to interactive narrative where game play is the ordinary activity that underlies notable story events, and the AI challenge is to tell a story about what the player is doing, that meets authorial goals. In this preliminary work, we describe a game and AI system that motivates the need for event prediction within the game world, and provides the opportunity for automated machine learning of such a predictive model. We report results showing how different feature models can be learned and compared in this context, towards automating model selection.},
booktitle = {Interactive Storytelling: 9th International Conference on Interactive Digital Storytelling, ICIDS 2016, Los Angeles, CA, USA, November 15–18, 2016, Proceedings},
pages = {428–432},
numpages = {5},
keywords = {Intelligent narrative technologies, Artificial intelligence, Machine learning, Game design},
location = {Los Angeles, USA}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@article{10.1016/j.artmed.2019.101788,
author = {Sangaiah, Arun Kumar and Arumugam, Maheswari and Bian, Gui-Bin},
title = {An intelligent learning approach for improving ECG signal classification and arrhythmia analysis},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2019.101788},
doi = {10.1016/j.artmed.2019.101788},
journal = {Artif. Intell. Med.},
month = mar,
numpages = {14},
keywords = {Cardiac arrhythmia, HMM (Hidden Markov Model), Feature extraction, Devoted wavelet, Signal to noise ratio (SNR), Electromyography (EMG), Power line interference (PLI), Baseline wander (BW), Noise suppression, ECG}
}

@article{10.1016/j.inffus.2016.01.002,
author = {Cotelo, J.M. and Cruz, F.L. and Enr\'{\i}quez, F. and Troyano, J.A.},
title = {Tweet categorization by combining content and structural knowledge},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {31},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2016.01.002},
doi = {10.1016/j.inffus.2016.01.002},
abstract = {We explore the idea of integrating both textual and structural information.Using only structural information gives similar results to ones yielded BoW model.Complementing textual content with structural information achieves the best results.A proper combination scheme is critical when integrating both types of models.Experimental results show that our combination proposal is quite effective. Display Omitted Twitter is a worldwide social media platform where millions of people frequently express ideas and opinions about any topic. This widespread success makes the analysis of tweets an interesting and possibly lucrative task, being those tweets rarely objective and becoming the targeting for large-scale analysis. In this paper, we explore the idea of integrating two fundamental aspects of a tweet, the proper textual content and its underlying structural information, when addressing the tweet categorization task. Thus, not only we analyze textual content of tweets but also analyze the structural information provided by the relationship between tweets and users, and we propose different methods for effectively combining both kinds of feature models extracted from the different knowledge sources. In order to test our approach, we address the specific task of determining the political opinion of Twitter users within their political context, observing that our most refined knowledge integration approach performs remarkably better (about 5 points above) than the textual-based classic model.},
journal = {Inf. Fusion},
month = sep,
pages = {54–64},
numpages = {11},
keywords = {Twitter, Tweet categorization, Knowledge combination, Ensemble learning}
}

@inproceedings{10.5555/3540261.3541325,
author = {Tripuraneni, Nilesh and Adlam, Ben and Pennington, Jeffrey},
title = {Overparameterization improves robustness to covariate shift in high dimensions},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A significant obstacle in the development of robust machine learning models is covariate shift, a form of distribution shift that occurs when the input distributions of the training and test sets differ while the conditional label distributions remain the same. Despite the prevalence of covariate shift in real-world applications, a theoretical understanding in the context of modern machine learning has remained lacking. In this work, we examine the exact high-dimensional asymptotics of random feature regression under covariate shift and present a precise characterization of the limiting test error, bias, and variance in this setting. Our results motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. We find that overparameterized models exhibit enhanced robustness to covariate shift, providing one of the first theoretical explanations for this ubiquitous empirical phenomenon. Additionally, our analysis reveals an exact linear relationship between the in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent observation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1064},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.neucom.2019.02.025,
author = {Wang, Yanyun and Song, Chunfeng and Huang, Yan and Wang, Zhenyu and Wang, Liang},
title = {Learning view invariant gait features with Two-Stream GAN},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {339},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.02.025},
doi = {10.1016/j.neucom.2019.02.025},
journal = {Neurocomput.},
month = apr,
pages = {245–254},
numpages = {10},
keywords = {Gait recognition, Cross-veiw, Two-Stream GAN}
}

@inproceedings{10.1007/978-3-030-92273-3_11,
author = {Lv, Xuerui and Zhang, Li},
title = {Feature Fusion Learning Based on&nbsp;LSTM and&nbsp;CNN Networks for&nbsp;Trend Analysis of&nbsp;Limit Order Books},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_11},
doi = {10.1007/978-3-030-92273-3_11},
abstract = {In recent years, deep learning has been successfully applied to analyzing financial time series. In this paper, we propose a novel feature fusion learning (FFL) method to analyze the trend of high-frequency limit order books (LOBs). The proposed FFL method combines a convolutional neural network (CNN) and two long short-term memory (LSTM) models. The CNN module uses a kind of up-sampling techniques to enhance basic features and the two LSTM modules can extract time-related information from time-insensitive and time-sensitive features. In addition, two fusion rules (majority voting and weighted summation) are designed to fuse different feature models. Experiments are conducted on the benchmark dataset FI-2010. Experimental results show that FFL can go beyond the performance of every sub-model and outperform the state-of-the-art model on the prediction performance of LOBs.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {125–137},
numpages = {13},
keywords = {Convolutional neural network, Long short-term memory, Limit order books, Feature fusion learning},
location = {Sanur, Bali, Indonesia}
}

@article{10.5555/1953048.2021039,
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
title = {The Indian Buffet Process: An Introduction and Review},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {1185–1224},
numpages = {40}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@inproceedings{10.1145/3459637.3482142,
author = {Chen, John and Wang, Qihan and Kyrillidis, Anastasios},
title = {Mitigating Deep Double Descent by Concatenating Inputs},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482142},
doi = {10.1145/3459637.3482142},
abstract = {The double descent curve is one of the most intriguing properties of deep neural networks. It contrasts the classical bias-variance curve with the behavior of modern neural networks, occurring where the number of samples nears the number of parameters. In this work, we explore the connection between the double descent phenomena and the number of samples in the deep neural network setting. In particular, we propose a construction which augments the existing dataset by artificially increasing the number of samples. This construction empirically mitigates the double descent curve in this setting. We reproduce existing work on deep double descent, and observe a smooth descent into the overparameterized region for our construction. This occurs both with respect to the model size, and with respect to the number epochs.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2930–2934},
numpages = {5},
keywords = {overparameterization, neural networks, deep double descent},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/1553374.1553498,
author = {Streich, Andreas P. and Frank, Mario and Basin, David and Buhmann, Joachim M.},
title = {Multi-assignment clustering for Boolean data},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553498},
doi = {10.1145/1553374.1553498},
abstract = {Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each object can be assigned to multiple clusters. Using a deterministic annealing scheme, our method decomposes the observed data into the contributions of individual clusters and infers their parameters.Experiments on synthetic Boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to state-of-the-art approaches. We also apply our method to an important problem in computer security known as role mining. Experiments on real-world access control data show performance gains in generalization to new employees against other multi-assignment methods. In challenging situations with high noise levels, our approach maintains its good performance, while alternative state-of-the-art techniques lack robustness.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {969–976},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {99-00, 00-01, Convergence, Non-convex optimization, Machine learning, Self-paced learning}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1007/s10489-017-1048-3,
author = {Koh, Joel E. and Ng, Eddie Y. and Bhandary, Sulatha V. and Laude, Augustinus and Acharya, U. Rajendra},
title = {Automated detection of retinal health using PHOG and SURF features extracted from fundus images},
year = {2018},
issue_date = {May       2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {5},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-017-1048-3},
doi = {10.1007/s10489-017-1048-3},
abstract = {Many health-related problems arise with aging. One of the diseases that is prevalent among the elderly is the loss of sight. Various eye diseases, namely age-related macular degeneration (AMD), diabetic retinopathy (DR), and glaucoma are the prime causes of vision loss as we grow old. Nevertheless, early detection of such eye diseases can impede the progression of this problem. Therefore, the elderly are encouraged to attend regular eye checkups for early detection of eye diseases. However, it is time-consuming and laborious to conduct a mass eye screening session frequently. Hence, we proposed a novel approach to develop an automated retinal health screening system in this work. This paper discusses a retinal screening system to automatically differentiate normal image from abnormal (AMD, DR, and glaucoma) fundus images. The fundus images are subjected to the pyramid histogram of oriented gradients (PHOG) and speeded up robust features (SURF) techniques. Then, the extracted data are subjected to adaptive synthetic sampling to balance the number of data in the two classes (normal and abnormal). Subsequently, we employed the canonical correlation analysis approach to fuse the highly-correlated features extracted from the two (PHOG and SURF) descriptors. We have achieved 96.21% accuracy, 95.00% sensitivity, and 97.42% specificity with ten-fold cross-validation strategy using k-nearest neighbor (kNN) classifier. This novel algorithm has high potential in the diagnosis of normal eyes during the mass eye screening session or in polyclinics quickly and reliably. Hence, the patients having abnormal eyes can be sent to the main hospitals which will reduce the workload for the ophthalmologists.[Figure not available: see fulltext.]},
journal = {Applied Intelligence},
month = may,
pages = {1379–1393},
numpages = {15},
keywords = {SURF, PSO, PHOG, Glaucoma, Fundus, Eye, Diabetes retinopathy, Classifier, AMD}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Penalized maximum likelihood estimation, Macaque visual cortex, High-dimensional estimation, Graphical lasso, Gaussian graphical model, Functional connectivity, False discovery rate, Bayesian inference}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Genetic Algorithm (GA), GA-SVM, GA-ANN, Brain tumors}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Dimensionality reduction, Cascade classification, Retinal vessel segmentation, Fundus image}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Privacy, Evolutionary partitioning, Decomposition, Data mining, Classification, Anonymization}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Discriminate subspace learning, Set based image classification, Sparse projection learning, Query set}
}

@article{10.1016/j.compeleceng.2021.107215,
author = {Ardagna, Claudio A. and Bellandi, Valerio and Damiani, Ernesto and Bezzi, Michele and Hebert, Cedric},
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107215},
doi = {10.1016/j.compeleceng.2021.107215},
journal = {Comput. Electr. Eng.},
month = jul,
numpages = {10},
keywords = {Security and privacy, Machine learning, Big Data Analytics, Artificial intelligence}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {99-00, 00-01, Diversity and consistency learning, Spectral embedding, Multi-view clustering}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.cviu.2019.102901,
author = {Kim, Eu Young and Shin, Seung Yeon and Lee, Soochahn and Lee, Kyong Joon and Lee, Kyoung Ho and Lee, Kyoung Mu},
title = {Triplanar convolution with shared 2D kernels for 3D classification and shape retrieval},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {193},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2019.102901},
doi = {10.1016/j.cviu.2019.102901},
journal = {Comput. Vis. Image Underst.},
month = apr,
numpages = {12},
keywords = {Computer vision, Deep learning, Medical image, 3D vision, 65D17, 65D05, 41A10, 41A05}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.5555/1314498.1314553,
author = {Saar-Tsechansky, Maytal and Provost, Foster},
title = {Handling Missing Values when Applying Classification Models},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper first compares several different methods---predictive value imputation, the distribution-based imputation used by C4.5, and using reduced models---for applying classification trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1623–1657},
numpages = {35}
}

@article{10.1109/TCBB.2018.2824332,
author = {Yang, Runtao and Zhang, Chengjin and Gao, Rui and Zhang, Lina and Song, Qing},
title = {Predicting FAD Interacting Residues with Feature Selection and Comprehensive Sequence Descriptors},
year = {2019},
issue_date = {Nov.-Dec. 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2824332},
doi = {10.1109/TCBB.2018.2824332},
abstract = {The function of a flavoprotein is determined to a great extent by the binding sites on its surface that interacts with flavin adenine dinucleotide (FAD). Malfunction or dysregulation of FAD binding leads to a series of diseases. Therefore, accurately identifying FAD interacting residues (FIRs) provides insights into the molecular mechanisms of flavoprotein-related biological processes and disease progression. In this paper, a new computational method is proposed for identifying FIRs from protein sequences. Various sequence-derived discriminative features are explored. We analyze the distinctions of these features between FIRs and non-FIRs. We also investigate the predictive capabilities of both individual features and combinations of features. A relief algorithm followed by incremental feature selection (relief-IFS) is then adopted to search the optimal features. Finally, a random forest (RF) module is used to predict FIRs based on the optimal features. Using a 5-fold cross-validation test, the proposed method performs well, with a sensitivity of 0.847, a specificity of 0.933, an accuracy of 0.890, and a Matthews correlation coefficient (MCC) of 0.782, thereby outperforming previous methods. These results indicate that our method is relatively successful at predicting FIRs.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {2046–2056},
numpages = {11}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Recommendation system, Noise ranking, Noise filters, Label noise, Ensemble filters}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.5555/3540261.3540864,
author = {Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Lhoest, Quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry and Kashirin, Maxim and Borzunov, Alexander and Moral, Albert Villanova del and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolff, Thomas and Pekhimenko, Gennady},
title = {Distributed deep learning in open collaborations},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with 40 participants.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {603},
numpages = {19},
series = {NIPS '21}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {regression, multi-task learning, Self-paced learning, Machine learning, Alzheimer's disease},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1007/978-3-540-76858-6_62,
author = {Leitner, Raimund},
title = {Learning 3D Object Recognition from an Unlabelled and Unordered Training Set},
year = {2007},
isbn = {978-3-540-76857-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-76858-6_62},
doi = {10.1007/978-3-540-76858-6_62},
abstract = {This paper proposes an unsupervised learning technique for object recognition from an unlabelled and unordered set of training images. It enables the robust recognition of complex 3D objects in cluttered scenes, under scale changes and partial occlusion. The technique uses a matching based on the consistency of two different descriptors characterising the appearance and shape of local features. The variation of each local feature with viewing direction is modeled by a multi-view feature model. These multi-view feature models can be matched directly to the features found in a test image. This avoids a matching to all training views as necessary for approaches based on canonical views.The proposed approach is tested with real world objects and compared to a supervised approach using features characterised by SIFT descriptors (Scale Invariant Feature Transform). These experiments show that the performance of our unsupervised technique is equal to that of a supervised SIFT object recognition approach.},
booktitle = {Advances in Visual Computing: Third International Symposium, ISVC 2007, Lake Tahoe, NV, USA, November 26-28, 2007, Proceedings, Part I},
pages = {644–651},
numpages = {8},
keywords = {unsupervised learning, object recognition},
location = {Lake Tahoe, NV, USA}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@article{10.1007/s00500-017-2708-2,
author = {Yin, Chunyong and Xia, Lian and Zhang, Sun and Sun, Ruxia and Wang, Jin},
title = {Improved clustering algorithm based on high-speed network data stream},
year = {2018},
issue_date = {July      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-017-2708-2},
doi = {10.1007/s00500-017-2708-2},
abstract = {With the continuous development of network technology, the attack has become the biggest threat to the stable operation of the network. Intrusion detection technology is a proactive safety protection measure which provides real-time monitoring of internal attacks, external attacks, and misuse. Traditional intrusion detection system is short of adaptability due to the complication and scale of the network. The main problem is that the real-time performance of the network is poor and the reliability is not high. This paper designs the intrusion detection mechanism combined with data stream clustering algorithm and intrusion detection system to solve the problem in processing a large amount of high-speed data streams. The performance of processing data streams is improved through the clustering algorithm based on density and the sliding window and the experiments show that the intrusion detection efficiency is higher than DenStream algorithm.},
journal = {Soft Comput.},
month = jul,
pages = {4185–4195},
numpages = {11},
keywords = {Intrusion detection, High-speed network, Data stream, Clustering algorithm}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Sparsity, Regularization, Pattern analysis, Neuroimaging, NPAIRS resampling, Model interpretation, Machine learning, Kernel methods, Classification}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {id3, Sentiment classification, ID3 algorithm, English sentiment classification, English document opinion mining, Decision tree}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {self-paced learning, random forest, classification, bootstrap, Lung cancer}
}

@article{10.1007/s10044-016-0558-7,
author = {Vital, Jessica P. and Faria, Diego R. and Dias, Gon\c{c}alo and Couceiro, Micael S. and Coutinho, Fernanda and Ferreira, Nuno M.},
title = {Combining discriminative spatiotemporal features for daily life activity recognition using wearable motion sensing suit},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-016-0558-7},
doi = {10.1007/s10044-016-0558-7},
abstract = {Motion sensing plays an important role in the study of human movements, motivated by a wide range of applications in different fields, such as sports, health care, daily activity, action recognition for surveillance, assisted living and the entertainment industry. In this paper, we describe how to classify a set of human movements comprising daily activities using a wearable motion capture suit, denoted as FatoXtract. A probabilistic integration of different classifiers recently proposed is employed herein, considering several spatiotemporal features, in order to classify daily activities. The classification model relies on the computed confidence belief from base classifiers, combining multiple likelihoods from three different classifiers, namely Na\"{\i}ve Bayes, artificial neural networks and support vector machines, into a single form, by assigning weights from an uncertainty measure to counterbalance the posterior probability. In order to attain an improved performance on the overall classification accuracy, multiple features in time domain (e.g., velocity) and frequency domain (e.g., fast Fourier transform), combined with geometrical features (joint rotations), were considered. A dataset from five daily activities performed by six participants was acquired using FatoXtract. The dataset provided in this work was designed to be extremely challenging since there are high intra-class variations, the duration of the action clips varies dramatically, and some of the actions are quite similar (e.g., brushing teeth and waving, or walking and step). Reported results, in terms of both precision and recall, remained around 85 %, showing that the proposed framework is able to successfully classify different human activities.},
journal = {Pattern Anal. Appl.},
month = nov,
pages = {1179–1194},
numpages = {16},
keywords = {Pattern recognition, Motion capture suit, Human movement analysis, Feature extraction, Classification methods}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Clustering, Distance, Density, Single link, Mutual neighbors}
}

@inproceedings{10.1007/978-3-319-11656-3_20,
author = {Habibzadeh, Mehdi and Krzy\.{z}ak, Adam and Fevens, Thomas},
title = {Comparative Study of Feature Selection for White Blood Cell Differential Counts in Low Resolution Images},
year = {2014},
isbn = {9783319116556},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-11656-3_20},
doi = {10.1007/978-3-319-11656-3_20},
abstract = {Features that are widely used in digital image analysis and pattern recognition tasks are from three main categories: shape, intensity, and texture invariant features. For computer-aided diagnosis in medical imaging for many specific types of medical problem, the most effective choice of a subset of these features through feature selection is still an open problem. In this work, we consider the problem of white blood cell (leukocyte) recognition into their five primary types: Neutrophils, Lymphocytes, Eosinophils, Monocytes and Basophils using a Support Vector Machine classifier. For features, we use four main intensity histogram calculations, set of 11 invariant moments, the relative area, co-occurrence and run-length matrices, dual tree complex wavelet transform, Haralick and Tamura features. Global sensitivity analysis using Sobol's RS-HDMR which can deal with independent and dependent input variables is used to assess dominate discriminatory power and the reliability of feature models in presence of high dimensional input feature data to build an efficient feature selection. Both the numerical and empirical results of experiments are compared with forward sequential feature selection. Finally, the results obtained from the preliminary analysis of white blood cell classification are presented in confusion matrices and interpreted using Cohen's kappa (  \"{\i} ) with the classification framework being validated with experiments conducted on poor quality white blood cell images.},
booktitle = {Proceedings of the 6th IAPR TC 3 International Workshop on Artificial Neural Networks in Pattern Recognition - Volume 8774},
pages = {216–227},
numpages = {12}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {thompson sampling, stochastic point location, searching on the line, probabilistic bisection search, deceptive environment}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Density, Dissimilarity, Agglomerative}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.scico.2016.03.009,
author = {\v{S}tuikys, Vytautas and Burbaite, Renata and Bespalova, Kristina and Ziberkas, Giedrius},
title = {Model-driven processes and tools to design robot-based generative learning objects for computer science education},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2016.03.009},
doi = {10.1016/j.scico.2016.03.009},
abstract = {In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {48–71},
numpages = {24},
keywords = {Model transformation, Generative learning objects (GLOs), GLO design tool, Feature models, Educational robots}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {variability, software reuse, similarities, requirements, product line},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3275219.3275234,
author = {Liu, Wenbin and Chen, Ningjiang and Li, Hua and Tang, Yusi and Liang, Birui},
title = {A Fair Scheduling Algorithm for Adaptive Heterogeneous Resources in Data Centers},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275234},
doi = {10.1145/3275219.3275234},
abstract = {The resource scheduling problem of data center clusters has always been a hot topic in the field of cloud computing. Existing research efforts focus on fairness, resource utilization and energy efficiency, and lack of research on heterogeneous clustering issues. To solve the problem that the traditional DRF algorithm does not consider the classification of machine performance and task type, this paper proposes a fair scheduling algorithm X-DRF that adapts to heterogeneous resources in the data center. The algorithm mainly classifies the performance of physical machines, increases the machine performance scoring factor, and increases the training and job type judgment classification of the XGBoost model. The experiments show that CPU utilization and memory usage increased by 10% and 6%, respectively. The normalized ratio is increased by about 3% compared to the original DRF system. Therefore, the presented fair scheduling algorithm for heterogeneous resources is more fair and reasonable in terms of resource allocation.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {15},
numpages = {6},
keywords = {Resource scheduling, Mesos, Machine learning, Heterogeneous cluster, Fairness, Data center},
location = {Beijing, China},
series = {Internetware '18}
}

@article{10.1007/s11227-020-03604-4,
author = {Long, Leijin and He, Feng and Liu, Hongjiang},
title = {The use of remote sensing satellite using deep learning in emergency monitoring of high-level landslides disaster in Jinsha River},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03604-4},
doi = {10.1007/s11227-020-03604-4},
abstract = {In order to monitor the high-level landslides frequently occurring in Jinsha River area of Southwest China, and protect the lives and property safety of people in mountainous areas, the data of satellite remote sensing images are combined with various factors inducing landslides and transformed into landslide influence factors, which provides data basis for the establishment of landslide detection model. Then, based on the deep belief networks (DBN) and convolutional neural network (CNN) algorithm, two landslide detection models DBN and convolutional neural-deep belief network (CDN) are established to monitor the high-level landslide in Jinsha River. The influence of the model parameters on the landslide detection results is analyzed, and the accuracy of DBN and CDN models in dealing with actual landslide problems is compared. The results show that when the number of neurons in the DBN is 100, the overall error is the minimum, and when the number of learning layers is 3, the classification error is the minimum. The detection accuracy of DBN and CDN is 97.56% and 97.63%, respectively, which indicates that both DBN and CDN models are feasible in dealing with landslides from remote sensing images. This exploration provides a reference for the study of high-level landslide disasters in Jinsha River.},
journal = {J. Supercomput.},
month = aug,
pages = {8728–8744},
numpages = {17},
keywords = {Disaster monitoring, DBN, CNN, High-level landslide, Remote sensing technology}
}

@article{10.1504/IJBRA.2018.092685,
title = {Subspace module extraction from MI-based co-expression network},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/IJBRA.2018.092685},
doi = {10.1504/IJBRA.2018.092685},
abstract = {Most of the existing methods in literature have used proximity measures in the construction of co-expression networks CEN consisting of functional gene modules. This work describes the construction of co-expression network using mutual information MI as a proximity measure with non-linear correlation. The network modules are extracted that are defined over a subset of samples. This method has been tested on several publicly available datasets and the subspace network modules obtained have been validated in terms of both internal and external measures.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {207–234},
numpages = {28}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@article{10.1007/s11280-018-0622-x,
author = {Wen, Guoqiu and Zhu, Yonghua and Cai, Zhiguo and Zheng, Wei},
title = {Self-tuning clustering for high-dimensional data},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0622-x},
doi = {10.1007/s11280-018-0622-x},
abstract = {Spectral clustering is an important component of clustering method, via tightly relying on the affinity matrix. However, conventional spectral clustering methods 1). equally treat each data point, so that easily affected by the outliers; 2). are sensitive to the initialization; 3). need to specify the number of cluster. To conquer these problems, we have proposed a novel spectral clustering algorithm, via employing an affinity matrix learning to learn an intrinsic affinity matrix, using the local PCA to resolve the intersections; and further taking advantage of a robust clustering that is insensitive to initialization to automatically generate clusters without an input of number of cluster. Experimental results on both artificial and real high-dimensional datasets have exhibited our proposed method outperforms the clustering methods under comparison in term of four clustering metrics.},
journal = {World Wide Web},
month = nov,
pages = {1563–1573},
numpages = {11},
keywords = {Spectral clustering, Multi-manifold clustering, Local PCA, High-dimensional data}
}

@inproceedings{10.1145/3308558.3316498,
author = {Li, Shuaiji and Huang, Tao and Qin, Zhiwei and Zhang, Fanfang and Chang, Yinhong},
title = {Domain Generation Algorithms detection through deep neural network and ensemble},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3316498},
doi = {10.1145/3308558.3316498},
abstract = {Digital threats such as backdoors, trojans, info-stealers and bots can be especially damaging nowadays as they actively steal information or allow remote control for nefarious purposes. A common attribute amongst such malware is the need for network communication and many of them use domain generation algorithms (DGAs) to pseudo-randomly generate numerous domains to communicate with each other to avoid being take-down by blacklisting method. DGAs are constantly evolving and these generated domains are mixed with benign queries in network communication traffic each day, which raises a high demand for an efficient real-time DGA classifier on domains in DNS log. Previous works either rely on group contextual/statistical features or extra host-based information and thus need long time window, or depend on lexical features extracted from domain strings to build real-time classifiers, or directly build an end-to-end deep neural network to make prediction from domain strings. Pros and cons exist for either way in experiments. In this paper, we propose several new real-time detection models and frameworks which utilize meta-data generated from domains and combine the advantages of a deep neural network model and a lexical features based model using the ensemble technique. Our proposed model obtains performance higher than all state-of-art methods so far to the best knowledge of the authors, with both precision and recall at 99.8% on a widely used public dataset.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {189–196},
numpages = {8},
keywords = {URL classification, Recurrent neural network, Lexical features, Ensemble, Deep neural network, DGA},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Diversity, Clustering, Graph kernels, Testing, Verification and validation, Model-driven engineering},
location = {Ulm, Germany}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.5555/3170715.3170804,
author = {Pota, Marco and Scalco, Elisa and Sanguineti, Giuseppe and Farneti, Alessia and Cattaneo, Giovanni Mauro and Rizzo, Giovanna and Esposito, Massimo},
title = {Early prediction of radiotherapy-induced parotid shrinkage and toxicity based on CT radiomics and fuzzy classification},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {81},
number = {C},
issn = {0933-3657},
abstract = {This paper aims at classifying patients, under radiotherapy for head and neck cancer, at risk of parotid gland shrinkage and 12 months xerostomia.Knowledge is extracted by means of Likelihood-Fuzzy Analysis, representing statistical information by fuzzy rule-based models.Clinical features, dosimetric parameters, and measures obtained by texture analysis of CT imagesare extracted to characterize real patients.For parotid shrinkage,we detectgood predictors and the models to use at different treatment stages, showing predictor-outcome dependencies.For 12-months toxicity, some possible predictors are detected, and the relation between final parotid shrinkage rate and 12-months xerostomia is evaluated. MotivationPatients under radiotherapy for head-and-neck cancer often suffer of long-term xerostomia, and/or consistent shrinkage of parotid glands. In order to avoid these drawbacks, adaptive therapy can be planned for patients at risk, if the prediction is obtained timely, before or during the early phase of treatment. Artificial intelligence can address the problem, by learning from examples and building classification models. In particular, fuzzy logic has shown its suitability for medical applications, in order to manage uncertain data, and to build transparent rule-based classifiers.In previous works, clinical, dosimetric and image-based features were considered separately, to find different possible predictors of parotid shrinkage. On the other hand, a few works reported possible image-based predictors of xerostomia, while the combination of different types of features has been little addressed. ObjectiveThis paper proposes the application of a novel machine learning approach, based on both statistics and fuzzy logic, aimed at the classification of patients at risk of i) parotid gland shrinkage and ii) 12-months xerostomia. Both problems are addressed with the aim of individuating predictors and models to classify respective outcomes. MethodsKnowledge is extracted from a real dataset of radiotherapy patients, by means of a recently developed method named Likelihood-Fuzzy Analysis, based on the representation of statistical information by fuzzy rule-based models. This method enables to manage heterogeneous variables and missing data, and to obtain interpretable fuzzy models presenting good generalization power (thus high performance), and to measure classification confidence.Numerous features are extracted to characterize patients, coming from different sources, i.e. clinical features, dosimetric parameters, and radiomics-based measures obtained by texture analysis of Computed Tomography images. A learning approach based on the composition of simple models in a more complicated one allows to consider the features separately, in order to identify predictors and models to use when only some data source is available, and obtaining more accurate results when more information can be combined. ResultsRegarding parotid shrinkage, a number of good predictors is detected, some already known and confirmed here, and some others found here, in particular among radiomics-based features. A number of models are also designed, some using single features and others involving models composition to improve classification accuracy. In particular, the best model to be used at the initial treatment stage, and another one applicable at the half treatment stage are identified.Regarding 12-months toxicity, some possible predictors are detected, in particular among radiomics-based features. Moreover, the relation between final parotid shrinkage rate and 12-months xerostomia is evaluated.The method is compared to the nave Bayes classifier, which reveals similar results in terms of classification accuracy and best predictors.The interpretable fuzzy rule-based models are explicitly presented, and the dependence between predictors and outcome is explained, thus furnishing in some cases helpful insights about the considered problems. ConclusionThanks to the performance and interpretability of the fuzzy classification method employed, predictors of both parotid shrinkage and xerostomia are detected, and their influence on each outcome is revealed. Moreover, models for predicting parotid shrinkage at initial and half radiotherapy stages are found.},
journal = {Artif. Intell. Med.},
month = sep,
pages = {41–53},
numpages = {13},
keywords = {Xerostomia, Rule-based systems, Radiomics, Parotid gland, Fuzzy logic, Classification}
}

@inproceedings{10.1145/3448139.3448151,
author = {Jensen, Emily and Umada, Tetsumichi and Hunkins, Nicholas C. and Hutt, Stephen and Huggins-Manley, A. Corinne and D'Mello, Sidney K.},
title = {What You Do Predicts How You Do: Prospectively Modeling Student Quiz Performance Using Activity Features in an Online Learning Environment},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448151},
doi = {10.1145/3448139.3448151},
abstract = {Students using online learning environments need to effectively self-regulate their learning. However, with an absence of teacher-provided structure, students often resort to less effective, passive learning strategies versus constructive ones. We consider the potential benefits of interventions that promote retrieval practice – retrieving learned content from memory – which is an effective strategy for learning and retention. The goal is to nudge students towards completing short, formative quizzes when they are likely to succeed on those assessments. Towards this goal, we developed a machine-learning model using data from 32,685 students who used an online mathematics platform over an entire school year to prospectively predict scores on three-item assessments (N = 210,020) from interaction patterns up to 9 minutes before the assessment as well as Item Response Theory (IRT) estimates of student ability and quiz difficulty. These models achieved a student-independent correlation of 0.55 between predicted and actual scores on the assessments and outperformed IRT-only predictions (r = 0.34). Model performance was largely independent of the length of the analyzed window preceding a quiz. We discuss potential for future applications of the models to trigger dynamic interventions that aim to encourage students to engage with formative assessments rather than more passive learning strategies.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {121–131},
numpages = {11},
keywords = {Retrieval Practice, Predicting Student Performance, Online Learning, Machine Learning, Item Response Theory, Formative assessment},
location = {Irvine, CA, USA},
series = {LAK21}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.sigpro.2013.04.007,
author = {Wei, Xin and Li, Chunguang},
title = {Bayesian mixtures of common factor analyzers: Model, variational inference, and applications},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
number = {11},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2013.04.007},
doi = {10.1016/j.sigpro.2013.04.007},
abstract = {Recently, a representative approach, named mixtures of common factor analyzers (MCFA), was proposed for clustering high-dimensional observed data. Existing model-parameter estimation methods for this approach is based on the maximum likelihood criterion and performed by the expectation-maximization algorithm. In this paper, we consider the MCFA from a Bayesian perspective and propose the Bayesian mixtures of common factor analyzers (BMCFA) model, which replaces the deterministic model parameters in the MCFA by stochastic variables. Then we present a variational inference algorithm for this BMCFA model. Moreover, the proposed BMCFA model and the associated variational inference algorithm are used for clustering the high-dimensional synthetic data, the wine data from the UCI machine learning repository and the gene expression data. Experimental results illustrate that the BMCFA has good generalization capacities, automatically determining the appropriate number of clusters from high-dimensional observed data.},
journal = {Signal Process.},
month = nov,
pages = {2894–2905},
numpages = {12},
keywords = {Variational inference, Dimension reduction, Clustering, Bayesian mixtures of common factor analyzers}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Local discriminant bases, Speech encoding, Speech ABR}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@article{10.1016/j.patrec.2012.10.007,
author = {Whalen, Sean and Peisert, Sean and Bishop, Matt},
title = {Multiclass classification of distributed memory parallel computations},
year = {2013},
issue_date = {February, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2012.10.007},
doi = {10.1016/j.patrec.2012.10.007},
abstract = {High Performance Computing (HPC) is a field concerned with solving large-scale problems in science and engineering. However, the computational infrastructure of HPC systems can also be misused as demonstrated by the recent commoditization of cloud computing resources on the black market. As a first step towards addressing this, we introduce a machine learning approach for classifying distributed parallel computations based on communication patterns between compute nodes. We first provide relevant background on message passing and computational equivalence classes called dwarfs and describe our exploratory data analysis using self organizing maps. We then present our classification results across 29 scientific codes using Bayesian networks and compare their performance against Random Forest classifiers. These models, trained with hundreds of gigabytes of communication logs collected at Lawrence Berkeley National Laboratory, perform well without any a priori information and address several shortcomings of previous approaches.},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {322–329},
numpages = {8},
keywords = {Self-organizing maps, Random forests, Multiclass classification, High performance computing, Communication patterns, Bayesian networks}
}

@article{10.1007/s00500-015-1942-8,
author = {Bostani, Hamid and Sheikhan, Mansour},
title = {Hybrid of binary gravitational search algorithm and mutual information for feature selection in intrusion detection systems},
year = {2017},
issue_date = {May       2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {9},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1942-8},
doi = {10.1007/s00500-015-1942-8},
abstract = {Intrusion detection systems (IDSs) play an important role in the security of computer networks. One of the main challenges in IDSs is the high-dimensional input data analysis. Feature selection is a solution to overcoming this problem. This paper presents a hybrid feature selection method using binary gravitational search algorithm (BGSA) and mutual information (MI) for improving the efficiency of standard BGSA as a feature selection algorithm. The proposed method, called MI-BGSA, used BGSA as a wrapper-based feature selection method for performing global search. Moreover, MI approach was integrated into the BGSA, as a filter-based method, to compute the feature---feature and the feature---class mutual information with the aim of pruning the subset of features. This strategy found the features considering the least redundancy to the selected features and also the most relevance to the target class. A two-objective function based on maximizing the detection rate and minimizing the false positive rate was defined as a fitness function to control the search direction of the standard BGSA. The experimental results on the NSL-KDD dataset showed that the proposed method can reduce the feature space dramatically. Moreover, the proposed algorithm found better subset of features and achieved higher accuracy and detection rate as compared to the some standard wrapper-based and filter-based feature selection methods.},
journal = {Soft Comput.},
month = may,
pages = {2307–2324},
numpages = {18},
keywords = {Mutual information, Intrusion detection system, Hybrid model, Feature selection, Binary gravitational search algorithm, Anomaly detection}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@inproceedings{10.1145/3291280.3291795,
author = {Khader, Mariam and Awajan, Arafat and Al-Naymat, Ghazi},
title = {Sentiment Analysis Based on MapReduce: A survey},
year = {2018},
isbn = {9781450365680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291280.3291795},
doi = {10.1145/3291280.3291795},
abstract = {Sentiment analysis is the process of analyzing people's sentiments, opinions, evaluations and emotions by studying their written text. It attracts the interest of many researchers, since it is useful for many applications, ranging from decision making to product evaluation to mention a few. Sentiment analysis can be conducted using machine-learning techniques, lexicon-based techniques or hybrid techniques that combines both. As people are more reliant on social networks such as Twitter, this has become a valuable source for sentiment analysis. However, the existence of big data frameworks require adaptation of these techniques to run within such frameworks. This paper reviews sentiment analysis techniques, focusing on the MapReduce-based analysis techniques. We found that the Na\"{\i}ve Bayes algorithm was the most used machine learning technique for extracting sentiments from big datasets because of its high accuracy rates. However, the dictionary-based techniques achieved better results in terms of execution time.},
booktitle = {Proceedings of the 10th International Conference on Advances in Information Technology},
articleno = {11},
numpages = {8},
keywords = {Sentiment Analysis, Na\"{\i}ve Bayes, MapReduce Framework, Machine Learning, Dictionary Based Analysis, Big Data},
location = {Bangkok, Thailand},
series = {IAIT '18}
}

@article{10.5555/2627435.2638570,
author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
title = {Gibbs max-margin topic models with data augmentation},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the "augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time effciency. The classification performance is also improved over competitors on binary, multi-class and multi-label classification tasks.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1073–1110},
numpages = {38},
keywords = {support vector machines, supervised topic models, regularized Bayesian inference, max-margin learning, Gibbs classifiers}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Vector space model, R, Model-Driven Engineering, Model comparison, Hierarchical clustering}
}

@article{10.1145/2700481,
author = {Bouguessa, Mohamed and Romdhane, Lotfi Ben},
title = {Identifying Authorities in Online Communities},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2700481},
doi = {10.1145/2700481},
abstract = {Several approaches have been proposed for the problem of identifying authoritative actors in online communities. However, the majority of existing methods suffer from one or more of the following limitations: (1) There is a lack of an automatic mechanism to formally discriminate between authoritative and nonauthoritative users. In fact, a common approach to authoritative user identification is to provide a ranked list of users expecting authorities to come first. A major problem of such an approach is the question of where to stop reading the ranked list of users. How many users should be chosen as authoritative? (2) Supervised learning approaches for authoritative user identification suffer from their dependency on the training data. The problem here is that labeled samples are more difficult, expensive, and time consuming to obtain than unlabeled ones. (3) Several approaches rely on some user parameters to estimate an authority score. Detection accuracy of authoritative users can be seriously affected if incorrect values are used. In this article, we propose a parameterless mixture model-based approach that is capable of addressing the three aforementioned issues in a single framework. In our approach, we first represent each user with a feature vector composed of information related to its social behavior and activity in an online community. Next, we propose a statistical framework, based on the multivariate beta mixtures, in order to model the estimated set of feature vectors. The probability density function is therefore estimated and the beta component that corresponds to the most authoritative users is identified. The suitability of the proposed approach is illustrated on real data extracted from the Stack Exchange question-answering network and Twitter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {23},
keywords = {unsupervised learning, multivariate beta, mixture model, authoritative users, Online communities}
}

@article{10.1016/j.neucom.2015.07.152,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng},
title = {Hessian regularization by patch alignment framework},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {204},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.152},
doi = {10.1016/j.neucom.2015.07.152},
abstract = {In recent years, semi-supervised learning has played a key part in large-scale image management, where usually only a few images are labeled. To address this problem, many representative works have been reported, including transductive SVM, universum SVM, co-training and graph-based methods. The prominent method is the patch alignment framework, which unifies the traditional spectral analysis methods. In this paper, we propose Hessian regression based on the patch alignment framework. In particular, we construct a Hessian using the patch alignment framework and apply it to regression problems. To the best of our knowledge, there is no report on Hessian construction from the patch alignment viewpoint. Compared with the traditional Laplacian regularization, Hessian can better match the data and then leverage the performance. To validate the effectiveness of the proposed method, we conduct human face recognition experiments on a celebrity face dataset. The experimental results demonstrate the superiority of the proposed solution in human face classification.},
journal = {Neurocomput.},
month = sep,
pages = {183–188},
numpages = {6},
keywords = {Semi-supervised learning, Patch alignment, Least squares, Hessian}
}

@article{10.1016/j.compag.2018.04.023,
author = {Sharif, Muhammad and Khan, Muhammad Attique and Iqbal, Zahid and Azam, Muhammad Faisal and Lali, M. Ikram Ullah and Javed, Muhammad Younus},
title = {Detection and classification of citrus diseases in agriculture based on optimized weighted segmentation and feature selection},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {150},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.04.023},
doi = {10.1016/j.compag.2018.04.023},
journal = {Comput. Electron. Agric.},
month = jul,
pages = {220–234},
numpages = {15},
keywords = {Features fusion, Feature extraction, Feature selection, Citrus diseases, Citrus fruits}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {transfer learning, pseudo-labeling, negative learning, image classification, deep-metric learning, class-aware alignment, adversarial unsupervised domain adaptation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1007/978-3-030-87234-2_73,
author = {Shahzadi, Iram and Lattermann, Annika and Linge, Annett and Zwanenburg, Alexander and Baldus, Christian and Peeken, Jan C. and Combs, Stephanie E. and Baumann, Michael and Krause, Mechthild and Troost, Esther G. C. and L\"{o}ck, Steffen},
title = {Do We Need Complex Image Features to Personalize Treatment of Patients with Locally Advanced Rectal Cancer?},
year = {2021},
isbn = {978-3-030-87233-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87234-2_73},
doi = {10.1007/978-3-030-87234-2_73},
abstract = {Radiomics has shown great potential for outcome prognosis and presents a promising approach for improving personalized cancer treatment. In radiomic analyses, features of different complexity are extracted from clinical imaging datasets, which are correlated to the endpoints of interest using machine-learning approaches. However, it is generally unclear if more complex features have a higher prognostic value and show a robust performance in external validation. Therefore, in this study, we developed and validated radiomic signatures for outcome prognosis after neoadjuvant radiochemotherapy in locally advanced rectal cancer (LARC) using computed tomography (CT) and T2-weighted magnetic resonance imaging (MRI) of two independent institutions (training/validation: 94/28 patients). For the prognosis of tumor response and freedom from distant metastases (FFDM), we used different imaging features extracted from the gross tumor volume: less complex morphological and first-order (MFO) features, more complex second-order texture (SOT) features, and both feature classes combined. Analyses were performed for both imaging modalities separately and combined. Performance was assessed by the area under the curve (AUC) and the concordance index (CI) for tumor response and FFDM, respectively. Overall, radiomic features showed prognostic value for both endpoints. Combining MFO and SOT features led to equal or higher performance in external validation compared to MFO and SOT features alone. The best results were observed after combining MRI and CT features (AUC = 0.76, CI = 0.65). In conclusion, promising biomarker signatures combining MRI and CT were developed for outcome prognosis in LARC. Further external validation is pending before potential clinical application.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part VII},
pages = {775–785},
numpages = {11},
keywords = {Biomarkers, Distant metastases, Tumor response, Rectal cancer},
location = {Strasbourg, France}
}

@article{10.1016/j.patcog.2015.10.020,
author = {Zhang, Xuefeng and Chen, Bo and Liu, Hongwei and Zuo, Lei and Feng, Bo},
title = {Infinite max-margin factor analysis via data augmentation},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {52},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.10.020},
doi = {10.1016/j.patcog.2015.10.020},
abstract = {This paper addresses the Bayesian estimation of the discriminative probabilistic latent models, especially the mixture models. We develop the max-margin factor analysis (MMFA) model, which utilizes the latent variable support vector machine (LVSVM) as the classification criterion in the latent space to learn a discriminative subspace with max-margin constraint. Furthermore, to deal with multimodally distributed data, we further extend MMFA to infinite Gaussian mixture model and develop the infinite max-margin factor analysis (iMMFA) model, via the consideration of Dirichlet process mixtures (DPM). It jointly learns clustering, max-margin classifiers and the discriminative latent space in a united framework to improve the prediction performance. Moreover, both of MMFA and iMMFA are natural to handle outlier rejection problem, since the observations are described by a single or a mixture of Gaussian distributions. Additionally, thanks to the conjugate property, the parameters in the two models can be inferred efficiently via the simple Gibbs sampler. Finally, we implement our models on synthesized and real-world data, including multimodally distributed datasets and measured radar echo data, to validate the classification and rejection performance of the proposed models. Jointly learning FA and SVM, MMFA is proposed to get a discriminative subspace.Clustering the dataset in the subspace by DPM, MMFA is extended to iMMFA.Thanks to the jointly learning framework, they gain good prediction performance.Having the data description ability, the proposed models can reject outlier samples.In Bayesian framework, parameters can be inferred efficiently by the Gibbs sampler.},
journal = {Pattern Recogn.},
month = apr,
pages = {17–32},
numpages = {16},
keywords = {Latent variable support vector machine, Factor analysis, Dirichlet process mixture, Classification and rejection performance}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Multi-class classification, Imbalanced data, Ensemble learning, Classifier combination, Binary decomposition}
}

@inproceedings{10.5555/3504035.3504578,
author = {Zhang, Muhan and Cui, Zhicheng and Jiang, Shali and Chen, Yixin},
title = {Beyond link prediction: predicting hyperlinks in adjacency space},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {This paper addresses the hyperlink prediction problem in hypernetworks. Different from the traditional link prediction problem where only pairwise relations are considered as links, our task here is to predict the linkage of multiple nodes, i.e., hyperlink. Each hyperlink is a set of an arbitrary number of nodes which together form a multiway relationship. Hyperlink prediction is challenging - since the cardinality of a hyperlink is variable, existing classifiers based on a fixed number of input features become infeasible. Heuristic methods, such as the common neighbors and Katz index, do not work for hyperlink prediction, since they are restricted to pairwise similarities. In this paper, we formally define the hyperlink prediction problem, and propose a new algorithm called Coordinated Matrix Minimization (CMM), which alternately performs nonnegative matrix factorization and least square matching in the vertex adjacency space of the hyper-network, in order to infer a subset of candidate hyperlinks that are most suitable to fill the training hypernetwork. We evaluate CMM on two novel tasks: predicting recipes of Chinese food, and finding missing reactions of metabolic networks. Experimental results demonstrate the superior performance of our method over many seemingly promising baselines.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {543},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11042-017-4472-9,
author = {Ali, Tenvir and Jhandhir, Zeeshan and Ahmad, Awais and Khan, Murad and Khan, Arif Ali and Choi, Gyu Sang},
title = {Detecting fraudulent labeling of rice samples using computer vision and fuzzy knowledge},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {23},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-4472-9},
doi = {10.1007/s11042-017-4472-9},
abstract = {Pakistan's climate allows growing several types of crops, among them is rice. Basmati is one of the most harvested and most profitable varieties of rice because of its unique fragrance. Rice varieties are difficult to differentiate accurately by visual inspection. Therefore, dishonest dealers could easily mislabel or adulterate basmati rice with less valuable assortments that look similar. We need a way to guard the interests of our trade partners. Many different approaches have been proposed to detect adulteration or fraud labeling of rice, in particular, to detect mixtures of authentic basmati and non-basmati varieties. These techniques employ characteristics such as morphological parameters, physicochemical properties, DNA, protein, or metabolites and are expensive and time-consuming. In this paper, we propose a novel and inexpensive technique to detect fraudulent labeling. We use computer vision and a fuzzy classification database for detecting fault labels. For classification, we employ a neural network based approach, and for detecting fraudulent labels, we create a fuzzy classification knowledge database to label rice samples accurately. Our proposed approach is novel and achieves a precision of more than 90% (for 10 gram sample) in identifying fraudulent labels of rice. We conclude that our approach can help in identifying the rice varieties with a higher accuracy.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {24675–24704},
numpages = {30},
keywords = {Possibility theory, Neural network, Fuzzy knowledge, Computer vision, Classification}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@article{10.5555/3122009.3153015,
author = {Chakrabarti, Deepayan and Funiak, Stanislav and Chang, Jonathan and Macskassy, Sofus A.},
title = {Joint label inference in networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called Edge-Explain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EdgeExplain outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1941–1979},
numpages = {39},
keywords = {variational methods, social networks, label propagation, label inference, graphs}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.3233/JIFS-169299,
author = {Sehgal, Priti and Goel, Nidhi and Mishra, K.K.},
title = {Non-destructive low-cost approach for fuzzy classification of tomato images based on firmness prediction using regression},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169299},
doi = {10.3233/JIFS-169299},
abstract = {Non-destructive techniques such as hyperspectral imaging, backscattering imaging are the advanced techniques used for predicting mechanical properties of horticulture products. They show relatively good performance but at the expense of costly measuring setups. This application-oriented paper investigates the feasibility of employing simple digital color camera imaging for prediction and fuzzy classification of firmness of tomatoes. Images acquired using digital color camera are preprocessed and subject to texture analysis in order to extract the number of features. The proposed approach exploits four texture feature extraction algorithms: three are based on statistical techniques viz. first order statistics (FOS), gray level co-occurrence matrix (GLCM), gray level run length matrix (GLRLM), and one is based on transform-based technique viz. wavelet-transform. Out of all extracted features, redundant features are eliminated using various attribute selection methods. Subsequently, prediction models are built and analyzed using regression analysis. Sample space has been split into two sets; 80% training and 20% testing data having tomatoes with almost identical formation. Experimental results illustrates that RBF regression gave the lowest RMSE of 0.174 and highest prediction correlation coefficient of 0.929 for wavelet feature set. Grounded on the prediction model, fuzzy rule based classification (FRBC) is proposed to classify tomatoes into three firmness categories soft, medium, and hard. Accuracy statistics of the proposed FRBCS are compared with the state-of-the-art result and highest classification accuracy of 92.68% is achieved by proposed FRBCS. The results exhibit the possibility of using a&nbsp;digital color imaging system for firmness estimation and further for classification.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3641–3653},
numpages = {13},
keywords = {machinevision, RBF regression, fuzzy rule based classification system, Tomato firmness, Image texture analysis}
}

@inproceedings{10.1609/aaai.v33i01.33018279,
author = {Feng, Yutong and Feng, Yifan and You, Haoxuan and Zhao, Xibin and Gao, Yue},
title = {MeshNet: mesh neural network for 3D shape representation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33018279},
doi = {10.1609/aaai.v33i01.33018279},
abstract = {Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named MeshNet, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, MeshNet is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed MeshNet method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed MeshNet can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1015},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {total recall, high-recall retrieval, cost modeling, active learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@article{10.1016/j.neucom.2020.05.111,
author = {Lin, Gaojie and Zhao, Sanyuan and Shen, Jianbing},
title = {Video person re-identification with global statistic pooling and self-attention distillation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {453},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2020.05.111},
doi = {10.1016/j.neucom.2020.05.111},
journal = {Neurocomput.},
month = sep,
pages = {777–789},
numpages = {13},
keywords = {Video re-identification, Attention mechanism, Higher-order pooling, Person re-identification}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Video face recognition, Prototype learning, Metric learning, Image set classification}
}

@article{10.5555/3122009.3122042,
author = {Shi, Tianlin and Zhu, Jun},
title = {Online Bayesian passive-aggressive learning},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive-Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1084–1122},
numpages = {39}
}

@article{10.1007/s00521-014-1764-0,
author = {Wang, Zhiqiong and Qu, Qixun and Yu, Ge and Kang, Yan},
title = {Breast tumor detection in double views mammography based on extreme learning machine},
year = {2016},
issue_date = {January   2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-014-1764-0},
doi = {10.1007/s00521-014-1764-0},
abstract = {Mammography is one of the most important methods for breast tumor detection, while existing computer-aided diagnosis (CAD) technology based on single-view mammograms ignores the contrastive feature between medio-lateral oblique (MLO) and cranio-caudal (CC) views, and CAD technology based on double-view overlooks features of single views. But in clinical environment, radiologists not only read both CC view images and MLO view images individually, but also contrast these two types of views to diagnose each case. Therefore, to simulate diagnosis process of radiologists, in this paper, a fused feature model which blends features of single views with contrastive features of double views is proposed. The fused feature model is optimized by means of feature selection methods. Then, a CAD detection method based on extreme learning machine, a classifier with wonderful universal approximation capability, is proposed to improve the effectiveness of breast tumor detection by applying the optimum fused feature. The effectiveness of proposed method is verified by 222 pairs of mammograms from 222 women in Northeast China through the complete experiment.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {227–240},
numpages = {14},
keywords = {Mammograms, Image processing, Feature selection, Extreme learning machine (ELM), Computer-aided diagnosis (CAD)}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Variability, Program analysis, Maintenance, Change impact analysis}
}

@article{10.1016/j.eswa.2013.08.089,
author = {Farid, Dewan Md. and Zhang, Li and Rahman, Chowdhury Mofizur and Hossain, M. A. and Strachan, Rebecca},
title = {Hybrid decision tree and na\"{\i}ve Bayes classifiers for multi-class classification tasks},
year = {2014},
issue_date = {March, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.08.089},
doi = {10.1016/j.eswa.2013.08.089},
abstract = {In this paper, we introduce two independent hybrid mining algorithms to improve the classification accuracy rates of decision tree (DT) and naive Bayes (NB) classifiers for the classification of multi-class problems. Both DT and NB classifiers are useful, efficient and commonly used for solving classification problems in data mining. Since the presence of noisy contradictory instances in the training set may cause the generated decision tree suffers from overfitting and its accuracy may decrease, in our first proposed hybrid DT algorithm, we employ a naive Bayes (NB) classifier to remove the noisy troublesome instances from the training set before the DT induction. Moreover, it is extremely computationally expensive for a NB classifier to compute class conditional independence for a dataset with high dimensional attributes. Thus, in the second proposed hybrid NB classifier, we employ a DT induction to select a comparatively more important subset of attributes for the production of naive assumption of class conditional independence. We tested the performances of the two proposed hybrid algorithms against those of the existing DT and NB classifiers respectively using the classification accuracy, precision, sensitivity-specificity analysis, and 10-fold cross validation on 10 real benchmark datasets from UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed methods have produced impressive results in the classification of real life challenging multi-class problems. They are also able to automatically extract the most valuable training datasets and identify the most effective attributes for the description of instances from noisy complex training databases with large dimensions of attributes.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1937–1946},
numpages = {10},
keywords = {Na\"{\i}ve Bayes classifier, Hybrid, Decision tree, Data mining, Classification}
}

@inproceedings{10.1007/978-3-030-00308-1_3,
author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
title = {Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Network Training in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_3},
doi = {10.1007/978-3-030-00308-1_3},
abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {33–44},
numpages = {12},
keywords = {Static Head Pose, Robotics, Standard Platform League (SPL), RoboCup SPL, Deep Convolutional Neural Networks},
location = {Nagoya, Japan}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Sleep EEG, Mutual information, Data mining, Classification, Brain connectivity}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Support vector machine, Smart homes, Probability estimation, Pervasive healthcare, Distance minimization, Activity recognition}
}

@article{10.5555/2951132.2951432,
author = {Martinel, Niki and Piciarelli, Claudio and Micheloni, Christian},
title = {A supervised extreme learning committee for food recognition},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {148},
number = {C},
issn = {1077-3142},
abstract = {A food recognition system exploiting a supervised committee of classifiers is proposed.The system automatically selects the optimal features for the task.The structured fusion approach is designed to achieve an optimal ranking.Evaluations have been conducted on 4 publicly available benchmark datasets.Superior results than existing methods are achieved. Food recognition is an emerging topic in computer vision. The problem is being addressed especially in health-oriented systems where it is used as a support for food diary applications. The goal is to improve current food diaries, where the users have to manually insert their daily food intake, with an automatic recognition of the food type, quantity and consequent calories intake estimation. In addition to the classical recognition challenges, the food recognition problem is characterized by the absence of a rigid structure of the food and by large intra-class variations. To tackle such challenges, a food recognition system based on a committee classification is proposed. The aim is to provide a system capable of automatically choosing the optimal features for food recognition out of the existing plethora of available ones (e.g., color, texture, etc.). Following this idea, each committee member, i.e., an Extreme Learning Machine, is trained to specialize on a single feature type. Then, a Structural Support Vector Machine is exploited to produce the final ranking of possible matches by filtering out the irrelevant features and thus merging only the relevant ones. Experimental results show that the proposed system outperforms state-of-the-art works on four publicly available benchmark datasets.},
journal = {Comput. Vis. Image Underst.},
month = jul,
pages = {67–86},
numpages = {20},
keywords = {Structural SVM, Food recognition, Extreme Learning Machines}
}

@inproceedings{10.1145/3342999.3343003,
author = {Lyu, Peng-hui and Wang, Jun and Wei, Rui-yuan},
title = {Pavement Crack Image Detection based on Deep Learning},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343003},
doi = {10.1145/3342999.3343003},
abstract = {Crack is one of the most common road diseases. Once it appears, the quality of road engineering will be greatly reduced and even cause road collapse. If the cracks can be found in the early stage of timely maintenance, it will greatly save maintenance costs. However, the range of image cracks on the actual pavement is too wide, the image clarity is not enough, the composition is complex, and direct detection is very difficult. The traditional manual detection method takes too long time, has not enough precision, high risk of detection operation, and has a series of shortcomings. Therefore, according to the characteristics of pavement cracks, this paper adopts an automatic detection method based on deep learning. The method first preprocesses the crack image, and then inputs the preprocessed pavement image into the convolution neural network (CNN) model for detection. Experimental results show that this method is accurate and can better detect pavement cracks.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {6–10},
numpages = {5},
keywords = {image segmentation, deep learning, convolution neural network, Pavement crack},
location = {Xiamen, China},
series = {ICDLT '19}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Cyberspace, Representation learning, Multi-layer networks}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@article{10.1007/s10845-020-01533-w,
author = {Shi, Peizhi and Qi, Qunfen and Qin, Yuchu and Scott, Paul J. and Jiang, Xiangqian},
title = {A novel learning-based feature recognition method using multiple sectional view representation},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01533-w},
doi = {10.1007/s10845-020-01533-w},
abstract = {In computer-aided design (CAD) and process planning (CAPP), feature recognition is an essential task which identifies the feature type of a 3D model for computer-aided manufacturing (CAM). In general, traditional rule-based feature recognition methods are computationally expensive, and dependent on surface or feature types. In addition, it is quite challenging to design proper rules to recognise intersecting features. Recently, a learning-based method, named FeatureNet, has been proposed for both single and multi-feature recognition. This is a general purpose algorithm which is capable of dealing with any type of features and surfaces. However, thousands of annotated training samples for each feature are required for training to achieve a high single feature recognition accuracy, which makes this technique difficult to use in practice. In addition, experimental results suggest that multi-feature recognition part in this approach works very well on intersecting features with small overlapping areas, but may fail when recognising highly intersecting features. To address the above issues, a deep learning framework based on multiple sectional view (MSV) representation named MsvNet is proposed for feature recognition. In the MsvNet, MSVs of a 3D model are collected as the input of the deep network, and the information achieved from different views are combined via the neural network for recognition. In addition to MSV representation, some advanced learning strategies (e.g. transfer learning, data augmentation) are also employed to minimise the number of training samples and training time. For multi-feature recognition, a novel view-based feature segmentation and recognition algorithm is presented. Experimental results demonstrate that the proposed approach can achieve the state-of-the-art single feature performance on the FeatureNet dataset with only a very small number of training samples (e.g. 8–32 samples for each feature), and outperforms the state-of-the-art learning-based multi-feature recognition method in terms of recognition performances.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {1291–1309},
numpages = {19},
keywords = {Data augmentation, Transfer learning, Multiple sectional views, Deep learning, Feature recognition}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1109/TASLP.2018.2820429,
author = {Proenca, Jorge and Lopes, Carla and Tjalve, Michael and Stolcke, Andreas and Candeias, Sara and Perdigao, Fernando},
title = {Mispronunciation Detection in Children's Reading of Sentences},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2820429},
doi = {10.1109/TASLP.2018.2820429},
abstract = {This paper proposes an approach to automatically parse children's reading of sentences by detecting word pronunciations and extra content, and to classify words as correctly or incorrectly pronounced. This approach can be directly helpful for automatic assessment of reading level or for automatic reading tutors, where a correct reading must be identified. We propose a first segmentation stage to locate candidate word pronunciations based on allowing repetitions and false starts of a word's syllables. A decoding grammar based solely on syllables allows silence to appear during a word pronunciation. At a second stage, word candidates are classified as mispronounced or not. The feature that best classifies mispronunciations is found to be the log-likelihood ratio between a free phone loop and a word spotting model in the very close vicinity of the candidate segmentation. Additional features are combined in multifeature models to further improve classification, including: normalizations of the log-likelihood ratio, derivations from phone likelihoods, and Levenshtein distances between the correct pronunciation and recognized phonemes through two phoneme recognition approaches. Results show that most extra events were detected close to 2% word error rate achieved and that using automatic segmentation for mispronunciation classification approaches the performance of manual segmentation. Although the log-likelihood ratio from a spotting approach is already a good metric to classify word pronunciations, the combination of additional features provides a relative reduction of the miss rate of 18% from 34.03% to 27.79% using manual segmentation and from 35.58% to 29.35% using automatic segmentation, at constant 5% false alarm rate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1203–1215},
numpages = {13}
}

@inproceedings{10.1109/MSR.2019.00063,
author = {Le, Triet Huynh Minh and Sabir, Bushra and Babar, M. Ali},
title = {Automated software vulnerability assessment with concept drift},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00063},
doi = {10.1109/MSR.2019.00063},
abstract = {Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {371–382},
numpages = {12},
keywords = {software vulnerability, natural language processing, multi-class classification, mining software repositories, machine learning},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Vocal effort level, Robust speech recognition, Machine learning}
}

@inproceedings{10.5555/1939659.1939712,
author = {Wang, Dingyan and King, Irwin},
title = {An enhanced semi-supervised recommendation model based on green's function},
year = {2010},
isbn = {3642175368},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Recommendation, in the filed of machine learning, is known as a technique of identifying user preferences to new items with ratings from recommender systems. Recently, one novel recommendation model using Green's function treats recommendation as the process of label propagation. Although this model outperforms many standard recommendation methods, it suffers from information loss during graph construction because of data sparsity. In this paper, aiming at solving this problem and improving prediction accuracy, we propose an enhanced semi-supervised Green's function recommendation model. The main contributions are two-fold: 1) To reduce information loss, we propose a novel graph construction method with global and local consistent similarity; 2) We enhance the recommendation algorithm with the multiclass semi-supervised learning framework. Finally, experimental results on real world data demonstrate the effectiveness of our model.},
booktitle = {Proceedings of the 17th International Conference on Neural Information Processing: Theory and Algorithms - Volume Part I},
pages = {397–404},
numpages = {8},
keywords = {semi-supervised learning, recommender system, item graph, Green's function},
location = {Sydney, Australia},
series = {ICONIP'10}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {deep active learning, active learning, Deep learning}
}

@inproceedings{10.1145/2736277.2741128,
author = {Tsourakakis, Charalampos},
title = {Provably Fast Inference of Latent Features from Networks: with Applications to Learning Social Circles and Multilabel Classification},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741128},
doi = {10.1145/2736277.2741128},
abstract = {A well known phenomenon in social networks is homophily, the tendency of agents to connect with similar agents. A derivative of this phenomenon is the emergence of communities. Another phenomenon observed in numerous networks is the existence of certain agents that belong simultaneously to multiple communities. An understanding of these phenomena constitutes a central research topic of network science.In this work we focus on a fundamental theoretical question related to the above phenomena with various applications: given an undirected graph G, can we infer efficiently the latent vertex features which explain the observed network structure under the assumption of a generative model that exhibits homophily? We propose a probabilistic generative model with the property that the probability of an edge among two vertices is a non-decreasing function of the common features they possess. This property is true for many real-world networks and surprisingly is ignored by many popular overlapping community detection methods as it was shown recently by the empirical work of Yang and Leskovec [44]. Our main theoretical contribution is the first provably rapidly mixing Markov chain for inferring latent features. On the experimental side, we verify the efficiency of our method in terms of run times, where we observe that it significantly outperforms state-of-the-art methods. Our method is more than 2,400 times faster than a state-of-the-art machine learning method [37] and typically provides non-trivial speedups compared to BigClam [43]. Furthermore, we verify on real-data with ground-truth available that our method learns efficiently high quality labelings. We use our method to learn social circles from Twitter ego-networks and perform multilabel classification.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1111–1121},
numpages = {11},
keywords = {social circles, overlapping clustering, markov chains, machine learning, graph algorithms},
location = {Florence, Italy},
series = {WWW '15}
}

@article{10.1007/s10845-016-1240-z,
author = {Jiao, Roger J. and Zhou, Feng and Chu, Chih-Hsing},
title = {Decision theoretic modeling of affective and cognitive needs for product experience engineering: key issues and a conceptual framework},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {7},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1240-z},
doi = {10.1007/s10845-016-1240-z},
abstract = {User experience (UX) design plays a critical role in product experience engineering. To create a theoretical foundation of UX design, it is imperative to develop mathematical and computational models for elicitation, quantification, evaluation and reasoning of affective---cognitive needs that are inherent in the fulfillment of user experience. This paper explores the key research issues for understanding how human users' subjective experience and affective prediction impact their choice behavior under uncertainty. A conceptual framework is envisioned by extending prospect theory in the field of behavioral economics to the modeling of user experience choice behavior, in which inference of affective influence is enacted through the shape parameters of prospect value functions.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1755–1767},
numpages = {13},
keywords = {User modeling, User experience, Product design, Cognitive engineering, Affective design}
}

@inproceedings{10.3233/978-1-61499-672-9-107,
author = {Coppola, Claudio and Krajn\'{\i}k, Tom\'{a}\v{s} and Duckett, Tom and Bellotto, Nicola},
title = {Learning temporal context for activity recognition},
year = {2016},
isbn = {9781614996712},
publisher = {IOS Press},
address = {NLD},
url = {https://doi.org/10.3233/978-1-61499-672-9-107},
doi = {10.3233/978-1-61499-672-9-107},
abstract = {We investigate how incremental learning of long-term human activity patterns improves the accuracy of activity classification over time. Rather than trying to improve the classification methods themselves, we assume that they can take into account prior probabilities of activities occurring at a particular time. We use the classification results to build temporal models that can provide these priors to the classifiers. As our system gradually learns about typical patterns of human activities, the accuracy of activity classification improves, which results in even more accurate priors. Two datasets collected over several months containing hand-annotated activity in residential and office environments were chosen to evaluate the approach. Several types of temporal models were evaluated for each of these datasets. The results indicate that incremental learning of daily routines leads to a significant improvement in activity classification.},
booktitle = {Proceedings of the Twenty-Second European Conference on Artificial Intelligence},
pages = {107–115},
numpages = {9},
location = {The Hague, The Netherlands},
series = {ECAI'16}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@article{10.1016/j.neucom.2014.03.063,
author = {Niu, Biao and Cheng, Jian and Liu, Yang and Lu, Hanqing},
title = {Beyond semantic attributes: Auxiliary feature discovery for image classification},
year = {2014},
issue_date = {October, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {142},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.03.063},
doi = {10.1016/j.neucom.2014.03.063},
abstract = {Semantic attributes have been proved to be an effective representation approach for image classification in recent years. However, most of the existing semantic attributes are explicitly pre-defined and the attribute number is also very limited in practice. Therefore, we often encounter difficulties by using a fixed incomplete semantic attribute set for different classification tasks. One possible solution is to expand the semantic attribute representation with some non-semantic mid-level features. However, how to make the expanded features more effective and discriminative is still seldom exploited. In this paper, we propose a Sequential augmented Attributes learning (SAL) method to implement semantic attribute augmentation. In our SAL method, the non-semantic mid-level features are learned one by one under a sequential error-correcting scheme so that we can obtain more discriminating power with very compact expanded features. Extensive experiments are conducted on two public datasets and the results show that our approach achieves encouraging performance.},
journal = {Neurocomput.},
month = oct,
pages = {155–164},
numpages = {10},
keywords = {Sequential feature learning, Augmented feature, Attribute}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {semi-supervised learning, multi-modal learning, label propagation, Curriculum learning}
}

@inproceedings{10.1145/3242969.3242986,
author = {Mock, Philipp and Tibus, Maike and Ehlis, Ann-Christine and Baayen, Harald and Gerjets, Peter},
title = {Predicting ADHD Risk from Touch Interaction Data},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3242986},
doi = {10.1145/3242969.3242986},
abstract = {This paper presents a novel approach for automatic prediction of risk of ADHD in schoolchildren based on touch interaction data. We performed a study with 129 fourth-grade students solving math problems on a multiple-choice interface to obtain a large dataset of touch trajectories. Using Support Vector Machines, we analyzed the predictive power of such data for ADHD scales. For regression of overall ADHD scores, we achieve a mean squared error of 0.0962 on a four-point scale (R² = 0.5667). Classification accuracy for increased ADHD risk (upper vs. lower third of collected scores) is 91.1%.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {446–454},
numpages = {9},
keywords = {user modeling, multi-touch, machine-learning, adhd},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@article{10.1007/s10044-018-0713-4,
author = {Fathi, Abdolhossein and Mohamadi, Mahboobeh},
title = {Metric-learning-based high-discriminative local features extraction for iris recognition},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {4},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-018-0713-4},
doi = {10.1007/s10044-018-0713-4},
abstract = {Biometric-based authentication system is one of the main strategies to protect and control the access of users to important resources in any system and organization. Iris pattern is one of the best and most reliable biological features used in these systems. Extraction of high-discriminative local features can increase the recognition accuracy of iris-based biometric systems, especially when the number of users is high. Most of the existing methods utilize a combination of simple handcraft local feature models that deteriorate system performance when the number of users is increased. In this paper, after identification and segmentation of iris region, a new learning-based method is proposed to define and extract rotation- and illumination-invariant main local patterns associated with the iris texture. Afterwards, the metric-learning-based transform is employed to improve the discrimination of these patterns in recognition process. The proposed method was applied on more than 10,000 images from CASIA-V4, UBIRIS and ICE data sets. The identification accuracy of this method is 99.7, 98.13 and 99.26%, respectively, that is, higher than other methods in terms of both recognition accuracy and the number of used images.},
journal = {Pattern Anal. Appl.},
month = nov,
pages = {1427–1438},
numpages = {12},
keywords = {High-discriminative local features, Feature extraction, Segmentation, Authentication}
}

@article{10.1016/j.ins.2016.04.044,
title = {An investigation on the use of local multi-resolution patterns for image classification},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {361},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.04.044},
doi = {10.1016/j.ins.2016.04.044},
abstract = {This paper investigates the use of local multi-dimensional patterns for image classification.We present a rigorous and general mathematical full model for encoding multi-resolution, rotation-invariant local patterns.We evaluate the use of multi-resolution patterns for image classification both from an information- and performance-based standpoint. The subject of this study is the use of local multi-dimensional patterns for image classification. The contribution is both theoretical and experimental: on the one hand the paper introduces a complete and general mathematical model for encoding multi-resolution, rotation-invariant local patterns; on the other experimentally evaluates the use of multi-resolution patterns for image classification both from an information- and performance-based standpoint. The results indicate that the joint multi-resolution model proposed in the paper can actually convey an additional amount of information with respect to the marginal model; but also that the marginal model (i.e. concatenation of features computed at different resolutions) can be a good enough approximation for practical applications.},
journal = {Inf. Sci.},
month = sep,
pages = {1–13},
numpages = {13}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {upper confidence bounds, stochastic linear bandits, profile-based exploration}
}

@inproceedings{10.5555/3305890.3306045,
author = {Valera, Isabel and Ghahramani, Zoubin},
title = {Automatic discovery of the statistical types of variables in a dataset},
year = {2017},
publisher = {JMLR.org},
abstract = {A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3521–3529},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {wild unsupervised domain adaptation, source data weighting, noisy source data, co-teaching},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@article{10.1016/j.asoc.2015.09.002,
author = {Sridevi, S. and Nirmala, S.},
title = {ANFIS based decision support system for prenatal detection of Truncus Arteriosus congenital heart defect},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.09.002},
doi = {10.1016/j.asoc.2015.09.002},
abstract = {Graphical abstractDisplay Omitted Congenital Heart Disease or Defect (CHD) is one of the most crucial causes of neonatal mortality. According to the consensus reported by Cardiological society of India, CHD is responsible for around 10% of infant mortality in India. Clinical investigation of CHD is normally performed with ultrasound (US) imaging modality. It captures biological internal structures with improper boundary due to inherent speckle noise. The fetal heart particularly has thin wall chambers and hence this fact protrudes to be a main motivation to contrive a new Computer Aided Diagnostic Support System (CADSS) to diagnose prenatal CHD from US images. This proposed CADSS is the first framework implemented to diagnose the prenatal Truncus Arteriosus congenital heart defect (TACHD) from 2D US images. The system starts with pre-processing the clinical data-set utilizing Probabilistic Patch Based Maximum Likelihood Estimation (PPBMLE). Then the anatomical structures are highlighted from the pre-processed information, utilizing the Fuzzy Connectedness based image segmentation process. Then 32 diagnostic features are extracted by utilizing seven different feature extraction models. Amongst, a subset of potential features are selected by applying Fisher Discriminant Ratio (FDR) analysis. Finally, Adaptive Neuro Fuzzy Inference System (ANFIS) is built with the selected feature subset as classifier, to perceive and show clinical results of prenatal TACHD. The performance analysis of various classifiers is evaluated by using 10-fold cross validation process for the image data-set. Comparative results prove that the proposed classifier has the potential to produce the higher classification accuracy than existing classifiers.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {577–587},
numpages = {11},
keywords = {Truncus Arteriosus, Fuzzy Connectedness, Feature extraction, Congenital heart defects, Computer aided diagnostic support system, ANFIS}
}

@inproceedings{10.5555/1308171.1308193,
author = {Gruler, Alexander and Harhurin, Alexander and Hartmann, Judith},
title = {Development and Configuration of Service-based Product Lines},
year = {2007},
isbn = {0769528880},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced during the development of multi-functional system families. Addressing this trend we present an approach combining model-based development with product line techniques aiming at a consistent description of a software product family as well as supporting the configuration of its variants. We integrate the concept of variability in our framework [7] which only supported the representation of single software systems on subsequent abstraction levels so far. For the configuration of a concrete product we extend this framework by a feature-based model which allows to configure and derive single systems from a system family model. Furthermore, we explain how the complexity due to the possibly huge amount of configuration decisions can be handled by means of a staged configuration process.},
booktitle = {Proceedings of the 11th International Software Product Line Conference},
pages = {107–116},
numpages = {10},
series = {SPLC '07}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {machine learning, Reverberation, Human experiments, Feature extraction, Audio signal processing},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@inproceedings{10.1007/978-3-030-29765-7_27,
author = {Ruschel, Andrey and Gusm\~{a}o, Arthur Colombini and Polleti, Gustavo Padilha and Cozman, Fabio Gagliardi},
title = {Explaining Completions Produced by Embeddings of Knowledge Graphs},
year = {2019},
isbn = {978-3-030-29764-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29765-7_27},
doi = {10.1007/978-3-030-29765-7_27},
abstract = {Advanced question answering typically employs large-scale knowledge bases such as DBpedia or Freebase, and are often based on mappings from entities to real-valued vectors. These mappings, called embeddings, are accurate but very hard to explain to a human subject. Although interpretability has become a central concern in machine learning, the literature so far has focused on non-relational classifiers (such as deep neural networks); embeddings, however, require a whole range of different approaches. In this paper, we describe a combination of symbolic and quantitative processes that explain, using sequences of predicates, completions generated by embeddings.},
booktitle = {Symbolic and Quantitative Approaches to Reasoning with Uncertainty: 15th European Conference, ECSQARU 2019, Belgrade, Serbia, September 18-20, 2019, Proceedings},
pages = {324–335},
numpages = {12},
keywords = {Interpretability, Embedding, Explainable AI, Knowledge base, Knowledge graph},
location = {Belgrade, Serbia}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1504/IJISTA.2016.078353,
author = {Al-Nuzaili, Qais and Hashim, Siti Z. Mohd. and Saeed, Faisal and Khalil, Mohammed Sayim and Mohamad, Dzulkifli Bin},
title = {Enhanced structural perceptual feature extraction model for Arabic literal amount recognition},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1740-8865},
url = {https://doi.org/10.1504/IJISTA.2016.078353},
doi = {10.1504/IJISTA.2016.078353},
abstract = {One of the important applications for document recognition is the bank cheque processing, which is known as cheque literal amount. A few studies focused on Arabic bank cheque processing system compared to other systems, such as Latin and Chinese. The Arabic script has a number of characteristics that makes it unique among other scripts. It is known that humans are the best pattern recognisers. As such, the features detected while human reads the script can get better recognition rates. Therefore, proposing human reading inspired features which are called perceptual features can overcome the unique technical challenges in Arabic literal amount recognition. In this paper, the enhanced structural perceptual feature extraction model PFM has been proposed. Two main groups of features, which are the components and dots features and the loops and characters shapes features were combined to construct the PFM. This model was evaluated on standard Arabic Handwriting DataBase AHDB dataset. The PFM results outperformed the results reported in the previous studies.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {240–254},
numpages = {15},
keywords = {perceptual features, literal amount recognition, handwriting recognition, feature extraction, document recognition, bank cheques, bank cheque processing, Arabic script, Arabic handwriting}
}

@article{10.1145/3355612,
author = {Shamsolmoali, Pourya and Zareapoor, Masoumeh and Zhou, Huiyu and Yang, Jie},
title = {AMIL: Adversarial Multi-instance Learning for Human Pose Estimation},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3355612},
doi = {10.1145/3355612},
abstract = {Human pose estimation has an important impact on a wide range of applications, from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual Multiple-Instance Learning (MIL) models with identical architecture—one is used as the generator, and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates results that the discriminator is not able to distinguish from the real ones, then the model has successfully learned the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-the-art models. The code will be made available on https://github.com/pshams55/AMIL.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {23},
numpages = {23},
keywords = {neural networks, multiple-instance learning, adversarial network, Pose estimations}
}

@article{10.1007/s10916-019-1424-0,
author = {Zhai, Jiemin and Li, Huiqi},
title = {An Improved Full Convolutional Network Combined with Conditional Random Fields for Brain MR Image Segmentation Algorithm and its 3D Visualization Analysis},
year = {2019},
issue_date = {Sep 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {9},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1424-0},
doi = {10.1007/s10916-019-1424-0},
abstract = {Existing brain region segmentation algorithms based on deep convolutional neural networks (CNN) are inefficient for object boundary segmentation. In order to enhance the segmentation accuracy of brain tissue, this paper proposed an object region segmentation algorithm that combines pixel-level information and semantic information. Firstly, we extract semantic information by CNN with the attention module and get the coarse segmentation results through a specific pixel-level classifier. Then, we exploit conditional random fields to model the relationship between the underlying pixels so as to get local features. Finally, the semantic information and the local pixel-level information are respectively used as the unary potential and the binary potential of the Gibbs distribution, and the combination of both can obtain the fine region segmentation algorithm based on the fusion of pixel-level information and the semantic information. A large number of qualitative and quantitative test results show that our proposed algorithm has higher precision than the existing state-of-the-art deep feature models, which can better solve the problem of rough edge segmentation and produce good 3D visualization effect.},
journal = {J. Med. Syst.},
month = sep,
pages = {1–10},
numpages = {10},
keywords = {3D visualization, Brain tissue, Binary potential, Unary potential, Gibbs distribution, Coarse segmentation, Semantic information}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and V\"{o}lzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {time-series clustering, patient similarity, medical data mining, hepatic steatosis, feature selection, epidemiological studies, classification},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@inproceedings{10.1145/3292500.3330990,
author = {Chen, Yu-Chia and Bijral, Avleen S. and Ferres, Juan Lavista},
title = {On Dynamic Network Models and Application to Causal Impact},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330990},
doi = {10.1145/3292500.3330990},
abstract = {Dynamic extensions of Stochastic block model (SBM) are of importance in several fields that generate temporal interaction data. These models, besides producing compact and interpretable network representations, can be useful in applications such as link prediction or network forecasting. In this paper we present a conditional pseudo-likelihood based extension to dynamic SBM that can be efficiently estimated by optimizing a regularized objective. Our formulation leads to a highly scalable approach that can handle very large networks, even with millions of nodes. We also extend our formalism to causal impact for networks that allows us to quantify the impact of external events on a time dependent sequence of networks. We support our work with extensive results on both synthetic and real networks.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1194–1204},
numpages = {11},
keywords = {stochastic block model, dynamic networks, clustering, causal impact},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {Understandability, Separation of concerns, Maintainability, Controlled experiment, Aspect-oriented programming, AOP}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Stochastic Point Location (SPL), Stochastic Learning Weak Estimation (SLWE), Mutual probability flux, Last Transition-based Estimation Solution (LTES), Flux-based Estimation Solution (FES), Estimating environment effectiveness}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Support Vector Machine, Second-order cone programming, Linear programming SVM}
}

@article{10.1007/s10851-014-0500-9,
author = {Gonz\'{a}lez, Elena and Fern\'{a}ndez, Antonio and Bianconi, Francesco},
title = {General Framework for Rotation Invariant Texture Classification Through Co-occurrence of Patterns},
year = {2014},
issue_date = {November  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {3},
issn = {0924-9907},
url = {https://doi.org/10.1007/s10851-014-0500-9},
doi = {10.1007/s10851-014-0500-9},
abstract = {The use of co-occurrences of patterns in image analysis has been recently suggested as one of the possible strategies to improve on the bag-of-features model. The intrinsically high number of features of the method, however, is a potential limit to its widespread application. Its extension into rotation invariant versions also requires careful consideration. In this paper we present a general, rotation invariant framework for co-occurrences of patterns and investigate possible solutions to the dimensionality problem. Using local binary patterns as bag-of-features model, we experimentally evaluate the potential advantages that co-occurrences can provide in comparison with bag-of-features. The results show that co-occurrences remarkably improve classification accuracy in some datasets, but in others the gain is negligible, or even negative. We found that this surprising outcome has an interesting explanation in terms of the degree of association between pairs of patterns in an image, and, in particular, that the higher the degree of association, the lower the gain provided by co-occurrences in comparison with bag-of-features.},
journal = {J. Math. Imaging Vis.},
month = nov,
pages = {300–313},
numpages = {14}
}

@article{10.1016/j.compag.2012.08.008,
author = {Wen, Chenglu and Guyer, Daniel},
title = {Image-based orchard insect automated identification and classification method},
year = {2012},
issue_date = {November, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2012.08.008},
doi = {10.1016/j.compag.2012.08.008},
abstract = {Insect identification and classification is time-consuming work requiring expert knowledge for integrated pest management in orchards. An image-based automated insect identification and classification method is described in the paper. The complete method includes three models. An invariant local feature model was built for insect identification and classification using affine invariant local features; a global feature model was built for insect identification and classification using 54 global features; and a hierarchical combination model was proposed based on local feature and global feature models to combine advantages of the two models and increase performance. The three models were applied and tested for insect classification on eight insect species from pest colonies and orchards. The hierarchical combination model yielded better performance over global and local models. Moreover, to study the pose change of insects on traps and the hypothesis that an optimal time to acquire and image after landing exists, advanced analysis on time-dependent pose change of insects on traps is included in this study. The experimental results on field insect image classification with field-based images for training achieved the classification rate of 86.6% when testing with the combination model. This demonstrates the image-based insect identification and classification method could be a potential way for automated insect classification in integrated pest management.},
journal = {Comput. Electron. Agric.},
month = nov,
pages = {110–115},
numpages = {6},
keywords = {Local feature, Integrated pest management, Insect classification, Image processing, Global feature}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@article{10.5555/3122009.3122010,
author = {Ishiguro, Katsuhiko and Sato, Issei and Ueda, Naonori},
title = {Averaged collapsed variational bayes inference},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This paper presents the Averaged CVB (ACVB) inference and oers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1–29},
numpages = {29},
keywords = {nonparametric Bayes, collapsed variational Bayes inference, averaged CVB}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@article{10.1007/s00500-014-1364-z,
author = {Baladhandapani, Arunadevi and Nachimuthu, Deepa Subramaniam},
title = {Evolutionary learning of spiking neural networks towards quantification of 3D MRI brain tumor tissues},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1364-z},
doi = {10.1007/s00500-014-1364-z},
abstract = {This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.},
journal = {Soft Comput.},
month = jul,
pages = {1803–1816},
numpages = {14},
keywords = {Spiking neural networks, Multi-dimensional co-occurrence matrices, Izhikevich neurons, Genetic algorithm, 3D Magnetic resonance imaging}
}

@article{10.1016/j.patcog.2008.09.015,
author = {Liu, Manhua and Jiang, Xudong and Kot, Alex C.},
title = {A multi-prototype clustering algorithm},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.09.015},
doi = {10.1016/j.patcog.2008.09.015},
abstract = {Clustering is an important unsupervised learning technique widely used to discover the inherent structure of a given data set. Some existing clustering algorithms uses single prototype to represent each cluster, which may not adequately model the clusters of arbitrary shape and size and hence limit the clustering performance on complex data structure. This paper proposes a clustering algorithm to represent one cluster by multiple prototypes. The squared-error clustering is used to produce a number of prototypes to locate the regions of high density because of its low computational cost and yet good performance. A separation measure is proposed to evaluate how well two prototypes are separated. Multiple prototypes with small separations are grouped into a given number of clusters in the agglomerative method. New prototypes are iteratively added to improve the poor cluster separations. As a result, the proposed algorithm can discover the clusters of complex structure with robustness to initial settings. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the proposed clustering algorithm.},
journal = {Pattern Recogn.},
month = may,
pages = {689–698},
numpages = {10},
keywords = {Squared-error clustering, Separation measure, Data clustering, Cluster prototype}
}

@article{10.1016/j.future.2018.09.040,
author = {Nie, Binling and Sun, Shouqian},
title = {Knowledge graph embedding via reasoning over entities, relations, and text},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.040},
doi = {10.1016/j.future.2018.09.040},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {426–433},
numpages = {8},
keywords = {Knowledge inference, Text-enhanced, Knowledge graph embedding, 99-00, 00-01}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {vector space model, model comparison, clustering, Model-driven engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1007/s00500-015-1643-3,
author = {Diaz-Valenzuela, Irene and Loia, Vincenzo and Martin-Bautista, Maria J. and Senatore, Sabrina and Vila, M. Amparo},
title = {Automatic constraints generation for semisupervised clustering: experiences with documents classification},
year = {2016},
issue_date = {June      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1643-3},
doi = {10.1007/s00500-015-1643-3},
abstract = {In the last times, semi-supervised clustering has been an area that has received a lot of attention. It is distinguished from more traditional unsupervised approaches on the use of a small amount of supervision to "steer" clustering. Unfortunately in the real world, the supervision is not always available: data to process are often too large and so the cost (in terms of time and human resources) for user-provided information is not conceivable. To address this issue, this work presents an automatic generation of the supervision, by the analysis of the data structure itself. This analysis is performed using a partitional clustering algorithm that discovers relationships between pairs of instances that may be used as a semi-supervision in the clustering process. The methodology has been studied in the document clustering domain, an area where novel approaches for accurate documents classifications are strongly required. Experimental result shows the validity of this approach.},
journal = {Soft Comput.},
month = jun,
pages = {2329–2339},
numpages = {11}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Spline, Random forest, Multivariate regression, Linear, Data loss}
}

@inproceedings{10.1145/3338906.3342508,
author = {Radavelli, Marco},
title = {Using software testing to repair models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342508},
doi = {10.1145/3338906.3342508},
abstract = {Software testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1253–1255},
numpages = {3},
keywords = {timed automata, software testing, software product lines, search-based software engineering, mutation, model repair, CIT},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1155/2015/781207,
author = {Martinez-Leon, Juan-Antonio and Cano-Izquierdo, Jose-Manuel and Ibarrola, Julio},
title = {Feature selection applying statistical and neurofuzzy methods to EEG-Based BCI},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/781207},
doi = {10.1155/2015/781207},
abstract = {This paper presents an investigation aimed at drastically reducing the processing burden required by motor imagery brain-computer interface (BCI) systems based on electroencephalography (EEG). In this research, the focus has moved from the channel to the feature paradigm, and a 96% reduction of the number of features required in the process has been achieved maintaining and even improving the classification success rate. This way, it is possible to build cheaper, quicker, and more portable BCI systems. The data set used was provided within the framework of BCI Competition III, which allows it to compare the presented results with the classification accuracy achieved in the contest. Furthermore, a new three-step methodology has been developed which includes a feature discriminant character calculation stage; a score, order, and selection phase; and a final feature selection step. For the first stage, both statistics method and fuzzy criteria are used. The fuzzy criteria are based on the S-dFasArt classification algorithm which has shown excellent performance in previous papers undertaking the BCI multiclass motor imagery problem. The score, order, and selection stage is used to sort the features according to their discriminant nature. Finally, both order selection and Group Method Data Handling (GMDH) approaches are used to choose the most discriminant ones.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {54},
numpages = {1}
}

@article{10.1007/s10845-020-01554-5,
author = {Wang, Gang and Zhang, Feng and Cheng, Bayi and Fang, Fang},
title = {DAMER: a novel diagnosis aggregation method with evidential reasoning rule for bearing fault diagnosis},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01554-5},
doi = {10.1007/s10845-020-01554-5},
abstract = {Ensemble learning method has shown its superiority in bearing fault diagnosis based on the condition based monitoring. Nevertheless, features extracted from the monitoring signals of bearing systems often contain interrelated and redundant components, leading to poor performances of the base classifiers in the ensemble. Moreover, the current ensemble methods rely on voting strategies to aggregate the diagnostic predictions of these base classifiers without considering their reliabilities and weights simultaneously. To address the aforementioned issues, we propose a novel Diagnosis Aggregation Method with Evidential Reasoning rule, i.e., DAMER, for bearing fault diagnosis. In this method, a semi-random subspace approach using a structured sparsity learning model is developed to decrease the negative effect of interrelated and redundant features, and in the meanwhile to generate accurate and diverse base classifiers. Furthermore, an adaptive evidential reasoning rule (ER rule) incorporating with ensemble learning theory is utilized to aggregate the diagnostic predictions of the base classifiers by taking both their weights and reliabilities into account. To validate the proposed DAMER, an empirical study is conducted on Case Western Reserve University bearing vibration dataset, and the experimental results verify the effectiveness of the proposed DAMER as well as its superiority over commonly used ensemble methods. The performances of feature subsets from multiple domains and the aggregation capability of the adaptive ER rule were also investigated. Results illustrate that DAMER can be utilized as an effective method for bearing fault diagnosis.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {1–20},
numpages = {20},
keywords = {Evidential reasoning rule, Structured sparsity learning, Ensemble learning, Condition based monitoring, Bearing fault diagnosis}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1145/2971648.2971748,
author = {Early, Kirstin and Fienberg, Stephen E. and Mankoff, Jennifer},
title = {Test time feature ordering with FOCUS: interactive predictions with minimal user burden},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971748},
doi = {10.1145/2971648.2971748},
abstract = {Predictive algorithms are a critical part of the ubiquitous computing vision, enabling appropriate action on behalf of users. A common class of algorithms, which has seen uptake in ubiquitous computing, is supervised machine learning algorithms. Such algorithms are trained to make predictions based on a set of features (selected at training time). However, features needed at prediction time (such as mobile information that impacts battery life, or information collected from users via experience sampling) may be costly to collect. In addition, both cost and value of a feature may change dynamically based on real-world context (such as battery life or user location) and prediction context (what features are already known, and what their values are). We contribute a framework for dynamically trading off feature cost against prediction quality at prediction time. We demonstrate this work in the context of three prediction tasks: providing prospective tenants estimates for energy costs in potential homes, estimating momentary stress levels from both sensed and user-provided mobile data, and classifying images to facilitate opportunistic device interactions. Our results show that while our approach to cost-sensitive feature selection is up to 45% less costly than competing approaches, error rates are equivalent or better.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {992–1003},
numpages = {12},
keywords = {online data collection, interactive machine learning, cost-based dynamic question ordering},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@article{10.1016/j.neucom.2015.04.087,
author = {Liu, Weifeng and Li, Yang and Tao, Dapeng and Wang, Yanjiang},
title = {A general framework for co-training and its applications},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.087},
doi = {10.1016/j.neucom.2015.04.087},
abstract = {Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.},
journal = {Neurocomput.},
month = nov,
pages = {112–121},
numpages = {10},
keywords = {Social activity recognition, Semi-supervised learning, Multi-view, Human action recognition, Co-training}
}

@article{10.1007/s11227-018-2641-x,
author = {Adewole, Kayode Sakariyah and Han, Tao and Wu, Wanqing and Song, Houbing and Sangaiah, Arun Kumar},
title = {Twitter spam account detection based on clustering and classification methods},
year = {2020},
issue_date = {Jul 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {7},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2641-x},
doi = {10.1007/s11227-018-2641-x},
abstract = {Twitter social network has gained more popularity due to the increase in social activities of registered users. Twitter performs dual functions of online social network (OSN), acting as a microblogging OSN, and at the same time as a news update platform. Recently, the growth in Twitter social interactions has attracted the attention of cybercriminals. Spammers have used Twitter to spread malicious messages, post phishing links, flood the network with fake accounts, and engage in other malicious activities. The process of detecting the network of spammers who engage in these activities is an important step toward identifying individual spam account. Researchers have proposed a number of approaches to identify a group of spammers. However, each of these approaches addressed a specific category of spammer. This paper proposes a different approach to detect spammers on Twitter based on the similarities that exist among spam accounts. A number of features were introduced to improve the performance of the three classification algorithms selected in this study. The proposed approach applied principal component analysis and tuned K-means algorithm to cluster over 200,000 accounts, randomly selected from more than 2 million tweets to detect the clusters of spammers. Experimental results show that Random Forest achieved the highest accuracy of 96.30%. This result is followed by multilayer perceptron with 96.00% and support vector machine, which achieved 95.60%. The performance of the selected classifiers based on class imbalance also revealed that Random Forest achieved the highest accuracy, precision, recall, and F-measure.},
journal = {J. Supercomput.},
month = jul,
pages = {4802–4837},
numpages = {36},
keywords = {Classification, Clustering, Fake account, Spam detection, Online social network}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {target curriculum, multi-label classification, k-feature Set, Boolean betworks}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {stochastic proximal point, stochastic convex optimization, rates of convergence, nonasymptotic convergence analysis, intersection of convex constraints}
}

@article{10.1109/TCBB.2021.3107621,
author = {Zhou, Haohao and Wang, Hao and Tang, Jijun and Ding, Yijie and Guo, Fei},
title = {Identify ncRNA Subcellular Localization via Graph Regularized &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="guo-ieq1-3107621.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-Local Hyperplane Distance Nearest Neighbor Model on Multi-Kernel Learning},
year = {2021},
issue_date = {Nov.-Dec. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3107621},
doi = {10.1109/TCBB.2021.3107621},
abstract = {Non-coding RNAs (ncRNAs) are a type of RNAs which are not used to encode protein sequences. Emerging evidence shows that lots of ncRNAs may participate in many biological processes and must be widely involved in many types of cancers. Therefore, understanding their functionality is of great importance. Similar to proteins, various functions of ncRNAs relies on their subcellular localizations. Traditional high-throughput methods in wet-lab to identify subcellular localization is time-consuming and costly. In this paper, we propose a novel computational method based on multi-kernel learning to identify multi-label ncRNA subcellular localizations, via graph regularized &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="guo-ieq2-3107621.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-local hyperplane distance nearest neighbor algorithm. First, we construct six types of sequence-based feature descriptors and select important feature vectors. Then, we build a multi-kernel learning model with Hilbert-Schmidt independence criterion (HSIC) to obtain optimal weights for vairous features. Furthermore, we propose the graph regularized &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="guo-ieq3-3107621.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-local hyperplane distance nearest neighbor algorithm (GHKNN) as a binary classification model for detecting one kind of non-coding RNA subcellular localization. Finally, we apply One-vs-Rest strategy to decompose multi-label problem of non-coding RNA subcellular localizations. Our method achieves excellent performance on three ncRNA datasets and three human ncRNA datasets, and out-performs other outstanding machine learning methods. Comparing to existing method, our model also performs well especially on small datasets. We expect that this model will be useful for the prediction of subcellular localization and the study of important functional mechanisms of ncRNAs. Furthermore, we establish user-friendly web server (&lt;uri&gt;http://ncrna.lbci.net/&lt;/uri&gt;) with the implementation of our method, which can be easily used by most experimental scientists.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = aug,
pages = {3517–3529},
numpages = {13}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Maximum entropy classifier, Machine learning, Feature selection, Feature reduction, Biomedical named entity recognition}
}

@article{10.5555/1953048.2021031,
author = {Henao, Ricardo and Winther, Ole},
title = {Sparse Linear Identifiable Multivariate Modeling},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {863–905},
numpages = {43}
}

@article{10.1145/2983644,
author = {Squicciarini, Anna and Caragea, Cornelia and Balakavi, Rahul},
title = {Toward Automated Online Photo Privacy},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/2983644},
doi = {10.1145/2983644},
abstract = {Online photo sharing is an increasingly popular activity for Internet users. More and more users are now constantly sharing their images in various social media, from social networking sites to online communities, blogs, and content sharing sites. In this article, we present an extensive study exploring privacy and sharing needs of users’ uploaded images. We develop learning models to estimate adequate privacy settings for newly uploaded images, based on carefully selected image-specific features. Our study investigates both visual and textual features of images for privacy classification. We consider both basic image-specific features, commonly used for image processing, as well as more sophisticated and abstract visual features. Additionally, we include a visual representation of the sentiment evoked by images. To our knowledge, sentiment has never been used in the context of image classification for privacy purposes. We identify the smallest set of features, that by themselves or combined together with others, can perform well in properly predicting the degree of sensitivity of users’ images. We consider both the case of binary privacy settings (i.e., public, private), as well as the case of more complex privacy options, characterized by multiple sharing options. Our results show that with few carefully selected features, one may achieve high accuracy, especially when high-quality tags are available.},
journal = {ACM Trans. Web},
month = apr,
articleno = {2},
numpages = {29},
keywords = {privacy, machine learning, image analysis, Social networks}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, G\"{u}nther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Spectrum, Optical character recognition, Human activity recognition, Graph-associated matrices, Graph classification}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.cie.2021.107480,
author = {Cheung, W.L. and Piplani, R. and Alam, S. and Bernard-Peyre, L.},
title = {Dynamic capacity and variable runway configurations in airport slot allocation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107480},
doi = {10.1016/j.cie.2021.107480},
journal = {Comput. Ind. Eng.},
month = sep,
numpages = {12},
keywords = {Mixed mode operations, Demand capacity imbalance, Capacity, Airport demand management, Airport slot allocation}
}

@article{10.5555/2946645.3007031,
author = {Ho, Qirong and Yin, Junming and Xing, Eric P.},
title = {Latent space inference of internet-scale networks},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2756–2796},
numpages = {41},
keywords = {triangular modeling, stochastic variational inference, probabilistic network models, distributed computation, big data}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Feature representation, Local geometrical structure preservation, Dictionary learning, Unsupervised feature selection}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ePRO, eLab, eCRF, caBIG, WHO, UMLS, TDM, SmPC, SPL, SNOMEDCT, SMAA, SEND, SDTM, RIM, QRD, PhRMA, PRM, PMDA, PIM, OWL, ODM, OCRe, OBX, NIHUS, NDA, NCI, MedDRA, MeSH, MCDA, LAB, JAMA, ICTRP, ICMJE, ICD, HSDB, HL7, GUI, GCP, FDAAA, FDA, Evidence-based medicine, Evidence synthesis, EPAR, EMA, EHR, EDC, EBM, EAV, Decision analysis, Data model, DSS, DOI, DIS, DED, DB, Clinical trial, CTMS, CTIS, CRO, CRF, CPOE, CHMP, CDMS, CDISC, CDASH, BRIDG, ATC, ANSI, AMIA, ADaM, ADR, ADE}
}

@inproceedings{10.5555/3495724.3496445,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Curriculum learning by dynamic instance hardness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A good teacher can adjust a curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure. Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.neunet.2014.05.012,
author = {Kang, Hyohyeong and Choi, Seungjin},
title = {Bayesian common spatial patterns for multi-subject EEG classification},
year = {2014},
issue_date = {September, 2014},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {57},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2014.05.012},
doi = {10.1016/j.neunet.2014.05.012},
abstract = {Multi-subject electroencephalography (EEG) classification involves algorithm development for automatically categorizing brain waves measured from multiple subjects who undergo the same mental task. Common spatial patterns (CSP) or its probabilistic counterpart, PCSP, is a popular discriminative feature extraction method for EEG classification. Models in CSP or PCSP are trained on a subject-by-subject basis so that inter-subject information is neglected. In the case of multi-subject EEG classification, however, it is desirable to capture inter-subject relatedness in learning a model. In this paper we present a nonparametric Bayesian model for a multi-subject extension of PCSP where subject relatedness is captured by assuming that spatial patterns across subjects share a latent subspace. Spatial patterns and the shared latent subspace are jointly learned by variational inference. We use an infinite latent feature model to automatically infer the dimension of the shared latent subspace, placing Indian Buffet process (IBP) priors on our model. Numerical experiments on BCI competition III IVa and IV 2a dataset demonstrate the high performance of our method, compared to PCSP and existing Bayesian multi-task CSP models.},
journal = {Neural Netw.},
month = sep,
pages = {39–50},
numpages = {12},
keywords = {Nonparametric Bayesian methods, Indian Buffet processes, EEG classification, Common spatial patterns, Brain-computer interface}
}

@inproceedings{10.5555/3540261.3541582,
author = {Elesedy, Bryn},
title = {Provably strict generalisation benefit for invariance in kernel methods},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {It is a commonly held belief that enforcing invariance improves generalisation. Although this approach enjoys widespread popularity, it is only very recently that a rigorous theoretical demonstration of this benefit has been established. In this work we build on the function space perspective of Elesedy and Zaidi [8] to derive a strictly non-zero generalisation benefit of incorporating invariance in kernel ridge regression when the target is invariant to the action of a compact group. We study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In building towards this result, we find that the action of the group induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1321},
numpages = {11},
series = {NIPS '21}
}

@article{10.5555/1756006.1859910,
author = {Yoshida, Ryo and West, Mike},
title = {Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We describe a class of sparse latent factor models, called graphical factor models (GFMs), and relevant sparse learning algorithms for posterior mode estimation. Linear, Gaussian GFMs have sparse, orthogonal factor loadings matrices, that, in addition to sparsity of the implied covariance matrices, also induce conditional independence structures via zeros in the implied precision matrices. We describe the models and their use for robust estimation of sparse latent factor structure and data/signal reconstruction. We develop computational algorithms for model exploration and posterior mode search, addressing the hard combinatorial optimization involved in the search over a huge space of potential sparse configurations. A mean-field variational technique coupled with annealing is developed to successively generate "artificial" posterior distributions that, at the limiting temperature in the annealing schedule, define required posterior modes in the GFM parameter space. Several detailed empirical studies and comparisons to related approaches are discussed, including analyses of handwritten digit image and cancer gene expression data.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {1771–1798},
numpages = {28}
}

@article{10.1016/j.comnet.2013.08.031,
author = {Ouyang, Tu and Ray, Soumya and Allman, Mark and Rabinovich, Michael},
title = {A large-scale empirical analysis of email spam detection through network characteristics in a stand-alone enterprise},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {59},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2013.08.031},
doi = {10.1016/j.comnet.2013.08.031},
abstract = {Spam is a never-ending issue that constantly consumes resources to no useful end. In this paper, we envision spam filtering as a pipeline consisting of DNS blacklists, filters based on SYN packet features, filters based on traffic characteristics and filters based on message content. Each stage of the pipeline examines more information in the message but is more computationally expensive. A message is rejected as spam once any layer is sufficiently confident. We analyze this pipeline, focusing on the first three layers, from a single-enterprise perspective. To do this we use a large email dataset collected over two years. We devise a novel ground truth determination system to allow us to label this large dataset accurately. Using two machine learning algorithms, we study (i) how the different pipeline layers interact with each other and the value added by each layer, (ii) the utility of individual features in each layer, (iii) stability of the layers across time and network events and (iv) an operational use case investigating whether this architecture can be practically useful. We find that (i) the pipeline architecture is generally useful in terms of accuracy as well as in an operational setting, (ii) it generally ages gracefully across long time periods and (iii) in some cases, later layers can compensate for poor performance in the earlier layers. Among the caveats we find are that (i) the utility of network features is not as high in the single enterprise viewpoint as reported in other prior work, (ii) major network events can sharply affect the detection rate, and (iii) the operational (computational) benefit of the pipeline may depend on the efficiency of the final content filter.},
journal = {Comput. Netw.},
month = feb,
pages = {101–121},
numpages = {21},
keywords = {Spam, Network-level characteristics, Longitudinal analysis}
}

@article{10.5555/1756006.1859931,
author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
title = {Spectral Regularization Algorithms for Learning Large Incomplete Matrices},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 X 106 incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {2287–2322},
numpages = {36}
}

@inproceedings{10.1007/11908029_95,
author = {Ichihashi, Hidetomo and Honda, Katsuhiro and Notsu, Akira},
title = {Postsupervised hard c-means classifier},
year = {2006},
isbn = {3540476938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11908029_95},
doi = {10.1007/11908029_95},
abstract = {Miyamoto et al. derived a hard clustering algorithms by defuzzifying a generalized entropy-based fuzzy c-means in which covariance matrices are introduced as decision variables. We apply the hard c-means (HCM) clustering algorithms to a postsupervised classifier to improve resubstitution error rate by choosing best clustering results from local minima of an objective function. Due to the nature of the prototype based classifier, the error rates can easily be improved by increasing the number of clusters with the cost of computer memory and CPU speed. But, with the HCM classifier, the resubstitution error rate along with the data set compression ratio is improved on several benchmark data sets by using a small number of clusters for each class.},
booktitle = {Proceedings of the 5th International Conference on Rough Sets and Current Trends in Computing},
pages = {918–927},
numpages = {10},
location = {Kobe, Japan},
series = {RSCTC'06}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Supplier selection, Multiple attribute group decision making (MAGDM), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Interval type-2 fuzzy sets (IT2FSs)}
}

@article{10.1016/j.patcog.2011.12.019,
author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
title = {Clustering with proximity knowledge and relational knowledge},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.12.019},
doi = {10.1016/j.patcog.2011.12.019},
abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
journal = {Pattern Recogn.},
month = jul,
pages = {2633–2644},
numpages = {12},
keywords = {Software requirements, Relational clustering, Proximity, Knowledge representation, Fuzzy clustering}
}

@inproceedings{10.1007/978-3-642-34166-3_71,
author = {Hidaka, Akinori and Kurita, Takio},
title = {Sparse discriminant analysis based on the bayesian posterior probability obtained by L1 regression},
year = {2012},
isbn = {9783642341656},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34166-3_71},
doi = {10.1007/978-3-642-34166-3_71},
abstract = {Recently the kernel discriminant analysis (KDA) has been successfully applied in many applications. However, kernel functions are usually defined a priori and it is not known what the optimum kernel function for nonlinear discriminant analysis is. Otsu derived the optimum nonlinear discriminant analysis (ONDA) by assuming the underlying probabilities similar with the Bayesian decision theory. Kurita derived discriminant kernels function (DKF) as the optimum kernel functions in terms of the discriminant criterion by investigating the optimum discriminant mapping constructed by the ONDA. The derived kernel function is defined by using the Bayesian posterior probabilities. We can define a family of DKFs by changing the estimation method of the Bayesian posterior probabilities. In this paper, we propose a novel discriminant kernel function based on L1-regularized regression, called L1 DKF. L1 DKF is given by using the Bayesian posterior probabilities estimated by L1 regression. Since L1 regression yields a sparse representation for given samples, we can naturally introduce the sparseness into the discriminant kernel function. To introduce the sparseness into LDA, we use L1 DKF as the kernel function of LDA. In experiments, we show sparseness and classification performance of L1 DKF.},
booktitle = {Proceedings of the 2012 Joint IAPR International Conference on Structural, Syntactic, and Statistical Pattern Recognition},
pages = {648–656},
numpages = {9},
location = {Hiroshima, Japan},
series = {SSPR'12/SPR'12}
}

@article{10.1016/j.ins.2013.11.005,
author = {Wang, Mei and Xia, Xiaoling and Le, Jiajin and Zhou, Xiangdong},
title = {Effective automatic image annotation via integrated discriminative and generative models},
year = {2014},
issue_date = {March, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {262},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.11.005},
doi = {10.1016/j.ins.2013.11.005},
abstract = {In this paper, we present a novel image annotation method that leverages on the advantages of both generative and discriminative models. To label an image, we first identify a visual neighborhood in the training image set based on generative approach. Then, the neighborhood is refined by an optimal discriminative hyperplane tree classifier based on concept feature. The tree classifier is built according to a local topic hierarchy, which is adaptively constructed by exploiting the semantic contextual correlations of the corresponding visual neighborhood. Experiments conducted on the ECCV2002 and TRECVID 2005 benchmarks demonstrate the effectiveness and efficiency of the proposed method.},
journal = {Inf. Sci.},
month = mar,
pages = {159–171},
numpages = {13},
keywords = {Hierarchical classification, Generative model, Discriminative model, Discriminative hyperplane tree, Automatic image annotation}
}

@article{10.1145/3448253,
author = {\c{C}oban, \"{O}nder and undefinednan, Ali and \"{O}zel, Selma Ay\c{s}e},
title = {Facebook Tells Me Your Gender: An Exploratory Study of Gender Prediction for Turkish Facebook Users},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3448253},
doi = {10.1145/3448253},
abstract = {Online Social Networks (OSNs) are very popular platforms for social interaction. Data posted publicly over OSNs pose various threats against the individual privacy of OSN users. Adversaries can try to predict private attribute values, such as gender, as well as links/connections. Quantifying an adversary’s capacity in inferring the gender of an OSN user is an important first step towards privacy protection. Numerous studies have been made on the problem of predicting the gender of an author/user, especially in the context of the English language. Conversely, studies in this field are quite limited for the Turkish language and specifically in the domain of OSNs. Previous studies for gender prediction of Turkish OSN users have mostly been performed by using the content of tweets and Facebook comments. In this article, we propose using various features, not just user comments, for the gender prediction problem over the Facebook OSN. Unlike existing studies, we exploited features extracted from profile, wall content, and network structure, as well as wall interactions of the user. Therefore, our study differs from the existing work in the broadness of the features considered, machine learning and deep learning methods applied, and the size of the OSN dataset used in the experimental evaluation. Our results indicate that basic profile information provides better results; moreover, using this information together with wall interactions improves prediction quality. We measured the best accuracy value as 0.982, which was obtained by combining profile data and wall interactions of Turkish OSN users. In the wall interactions model, we introduced 34 different features that provide better results than the existing content-based studies for Turkish.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {66},
numpages = {38},
keywords = {text categorization, gender detection, attribute inference, online social networks, Facebook}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Personalized education, Deep learning, Self-paced learning, Knowledge tracing},
location = {Hanoi, Vietnam}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {policy distillation, deep reinforcement learning, autonomous driving, Transfer reinforcement learning}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Software Verification, Software Testing, Fault Suspiciousness, Fault Localization, Du-Pair, Definition-Use, Data Flow Coverage, Artificial Neural Network}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Root mean square error, Training time, Noise prediction, Oil-rig, Extreme learning machine, Multiple frequency dependent data}
}

@inproceedings{10.1145/3324884.3416573,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Identifying software performance changes across variants and versions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416573},
doi = {10.1145/3324884.3416573},
abstract = {We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {active learning, configurable software systems, machine learning, software evolution, software performance},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {Tree-structured class hierarchies, Hierarchical classification, DAG-structured class hierarchies}
}

@inproceedings{10.5555/1699571.1699617,
author = {Bollegala, Danushka and Matsuo, Yutaka and Ishizuka, Mitsuru},
title = {A relational model of semantic similarity between words using automatically extracted lexical pattern clusters from the web},
year = {2009},
isbn = {9781932432626},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Semantic similarity is a central concept that extends across numerous fields such as artificial intelligence, natural language processing, cognitive science and psychology. Accurate measurement of semantic similarity between words is essential for various tasks such as, document clustering, information retrieval, and synonym extraction. We propose a novel model of semantic similarity using the semantic relations that exist among words. Given two words, first, we represent the semantic relations that hold between those words using automatically extracted lexical pattern clusters. Next, the semantic similarity between the two words is computed using a Mahalanobis distance measure. We compare the proposed similarity measure against previously proposed semantic similarity measures on Miller-Charles benchmark dataset and WordSimilarity-353 collection. The proposed method outperforms all existing web-based semantic similarity measures, achieving a Pearson correlation coefficient of 0.867 on the Millet-Charles dataset.},
booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2},
pages = {803–812},
numpages = {10},
location = {Singapore},
series = {EMNLP '09}
}

@inproceedings{10.1145/3431379.3460640,
author = {Rajesh, Neeraj and Devarajan, Hariharan and Garcia, Jaime Cernuda and Bateman, Keith and Logan, Luke and Ye, Jie and Kougkas, Anthony and Sun, Xian-He},
title = {Apollo: An ML-assisted Real-Time Storage Resource Observer},
year = {2021},
isbn = {9781450382175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431379.3460640},
doi = {10.1145/3431379.3460640},
abstract = {Applications and middleware services, such as data placement engines, I/O scheduling, and prefetching engines, require low-latency access to telemetry data in order to make optimal decisions. However, typical monitoring services store their telemetry data in a database in order to allow applications to query them, resulting in significant latency penalties. This work presents Apollo: a low-latency monitoring service that aims to provide applications and middleware libraries with direct access to relational telemetry data. Monitoring the system can create interference and overhead, slowing down raw performance of the resources for the job. However, having a current view of the system can aid middleware services in making more optimal decisions which can ultimately improve the overall performance. Apollo has been designed from the ground up to provide low latency, using Publish-Subscriber Pub-Sub semantics, and low overhead, using adaptive intervals in order to change the length of time between polling the resource for telemetry data and machine learning in order to predict changes to the telemetry data between actual resource polling. This work also provides some high level abstractions called I/O curators, which can further aid middleware libraries and applications to make optimal decisions. Evaluations showcase that Apollo can achieve sub-millisecond latency for acquiring complex insights with a memory overhead of ~57 MB and CPU overhead being only 7% more than existing state-of-the-art systems.},
booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {147–159},
numpages = {13},
keywords = {storage utilization, storage monitoring, resource monitoring, real-time monitoring, low latency monitoring, hpc cluster monitoring},
location = {Virtual Event, Sweden},
series = {HPDC '21}
}

@inproceedings{10.5555/2969033.2969059,
author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced learning with diversity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{10.1016/j.patrec.2017.08.016,
author = {Veena, S.T. and Arivazhagan, S.},
title = {Quantitative steganalysis of spatial LSB based stego images using reduced instances and features},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {105},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2017.08.016},
doi = {10.1016/j.patrec.2017.08.016},
abstract = {A universal quantitative steganalyser for spatial LSB based algorithms.Reduced number of training instances and features.A rich combination of global and local features.Three level optimisation strategy for multi-model feature sets.Novel instance selection method D-AllCNN. Universal quantitative steganalysis suffers from the curse of dimensionality and requires large number of instances (training samples) to produce qualitative results. In this paper, a universal quantitative steganalyser for spatial LSB based algorithms using reduced number of instances and features is proposed. A rich combination of global and local features models are employed as core features. The AdaBoost ensemble regressor with regression trees as its base learners is used to estimate the change rate caused in the stego images. To obtain low dimensional features and training instances, a three level optimisation approach is proposed. The trilevel optimisation comprises of bilevel optimisation of feature selection and an intermediate level of novel instance selection method. The first level of optimisation is done for choosing the optimal concatenation of discriminant feature models from the core feature set by Greedy Randomised Adaptive Search Procedure (GRASP). The instances of the optimal concatenated feature model are selected by the proposed Discretized-All Condensed Nearest Neighbour (D-AllCNN) method. The optimally concatenated features of the reduced instances are then reduced dimensionally by Recursive Feature Elimination (RFE) feature selection process. This process yields an improved quantitative steganalyser working on reduced instances and features. Experimental results confirm that the proposed steganalyser is better than state-of-the-art quantitative steganalysers for both traditional non-adaptive (LSBR, LSBM, LSBMR, LSBR2, LSBRmod5) and content adaptive (HUGO, WOW and SW) spatial LSB based steganographic schemes.},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {39–49},
numpages = {11},
keywords = {Trilevel optimisation, Spatial LSB steganography, RFE, Quantitative steganalysis, Instance selection, GRASP, AdaBoost ensemble regressor}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {Multi-modal attention, Embodied AI, Vision-and-language navigation, 68T45, 68T40, 68T01}
}

@article{10.1186/s13673-020-00219-9,
author = {Cao, Danyang and Chen, Zhixin and Gao, Lei},
title = {An improved object detection algorithm based on multi-scaled and deformable convolutional neural networks},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1},
issn = {2192-1962},
url = {https://doi.org/10.1186/s13673-020-00219-9},
doi = {10.1186/s13673-020-00219-9},
abstract = {Object detection methods aim to identify all target objects in the target image and determine the categories and position information in order to achieve machine vision understanding. Numerous approaches have been proposed to solve this problem, mainly inspired by methods of computer vision and deep learning. However, existing approaches always perform poorly for the detection of small, dense objects, and even fail to detect objects with random geometric transformations. In this study, we compare and analyse mainstream object detection algorithms and propose a multi-scaled deformable convolutional object detection network to deal with the challenges faced by current methods. Our analysis demonstrates a strong performance on par, or even better, than state of the art methods. We use deep convolutional networks to obtain multi-scaled features, and add deformable convolutional structures to overcome geometric transformations. We then fuse the multi-scaled features by up sampling, in order to implement the final object recognition and region regress. Experiments prove that our suggested framework improves the accuracy of detecting small target objects with geometric deformation, showing significant improvements in the trade-off between accuracy and speed.},
journal = {Hum.-Centric Comput. Inf. Sci.},
month = apr,
numpages = {22},
keywords = {Computer vision, Deformable convolution, AI, Machine learning, Object detection}
}

@article{10.1007/s00521-018-3934-y,
author = {Lu, Guangquan and Gan, Jiangzhang and Yin, Jian and Luo, Zhiping and Li, Bo and Zhao, Xishun},
title = {Multi-task learning using a hybrid representation for text classification},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3934-y},
doi = {10.1007/s00521-018-3934-y},
abstract = {Text classification is an important task in machine learning. Specifically, deep neural network has been shown strong capability to improve performance in different fields, for example speech recognition, objects recognition and natural language processing. However, in most previous work, the extracted feature models do not achieve the relative text tasks well. To address this issue, we introduce a novel multi-task learning approach called a hybrid representation-learning network for text classification tasks. Our method consists of two network components: a bidirectional gated recurrent unit with attention network module and a convolutional neural network module. In particular, the attention module allows for the task learning private feature representation in local dependence from training texts and that the convolutional neural network module can learn the global representation on sharing. Experiments on 16 subsets of Amazon review data show that our method outperforms several baselines and also proves the effectiveness of joint learning multi-relative tasks.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6467–6480},
numpages = {14},
keywords = {Big data, CNN, LSTM, Feature representation, Multi-task learning, Deep learning, Text classification}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed stereo audio classification using a stereo-input mixed-to-panned level feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {speech/music discrimination, speech processing, music processing, music information retrieval, mel-frequency cepstral coefficients, classification algorithms, audio segmentation, audio processing, audio classification, Gaussian mixture model}
}

@article{10.1155/2021/1140611,
author = {Wang, Lin and Zhang, Haiyan and Yuan, Guoliang and Zhang, Yuanpeng},
title = {Big Data and Deep Learning-Based Video Classification Model for Sports},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/1140611},
doi = {10.1155/2021/1140611},
abstract = {Information technologies such as deep learning, big data, cloud computing, and the Internet of Things provide key technical tools to drive the rapid development of integrated manufacturing. In recent years, breakthroughs have been made in big data analysis using deep learning. The research on the sports video high-precision classification model in this paper, more specifically, is the automatic understanding of human movements in free gymnastics videos. This paper will combine knowledge related to big data-based computer vision and deep learning to achieve intelligent labeling and representation of specific human movements present in video sequences. This paper mainly implements an automatic narrative based on long- and short-term memory networks to achieve the classification of sports videos. In the classical video description model S2VT, long- and short-term memory networks are used to learn the mapping relationship between word sequences and video frame sequences. In this paper, we introduce an attention mechanism to highlight the importance of keyframes that determine freestyle gymnastic movements. In this paper, a dataset of freestyle gymnastics breakdown movements for professional events is built. Experiments are conducted on the data and the self-constructed dataset, and the planned sampling method is applied to eliminate the differences between the training decoder and the prediction decoder. The experimental results show that the improved method in this paper can improve the accuracy of sports video classification. The video classification model based on big data and deep learning is to provide users with a better user experience and improve the accuracy of video classification. Also, in the experiments of this paper, the effect of extracting features for the classification of different lifting sports models is compared, and the effect of feature extraction network on the automatic description of free gymnastic movements is analyzed.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {11}
}

@inproceedings{10.1145/3357384.3358001,
author = {Jenkins, Porter and Farag, Ahmad and Wang, Suhang and Li, Zhenhui},
title = {Unsupervised Representation Learning of Spatial Data via Multimodal Embedding},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358001},
doi = {10.1145/3357384.3358001},
abstract = {Increasing urbanization across the globe has coincided with greater access to urban data; this enables researchers and city administrators with better tools to understand urban dynamics, such as crime, traffic, and living standards. In this paper, we study the Learning an Embedding Space for Regions (LESR) problem, wherein we aim to produce vector representations of discrete regions. Recent studies have shown that embedding geospatial regions in a latent vector space can be useful in a variety of urban computing tasks. However, previous studies do not consider regions across multiple modalities in an end-to-end framework. We argue that doing so facilitates the learning of greater semantic relationships among regions. We propose a novel method, RegionEncoder, that jointly learns region representations from satellite image, point-of-interest, human mobility, and spatial graph data. We demonstrate that these region embeddings are useful as features in two regression tasks and across two distinct urban environments. Additionally, we perform an ablation study that evaluates each major architectural component. Finally, we qualitatively explore the learned embedding space, and show that semantic relationships are discovered across modalities},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1993–2002},
numpages = {10},
keywords = {spatial data, satellite imagery, representation learning, multimodal embeding},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1007/978-3-642-40270-8_4,
author = {Bari, A. T. and Reaz, Mst. Rokeya and Choi, Ho-Jin and Jeong, Byeong-Soo},
title = {DNA Encoding for Splice Site Prediction in Large DNA Sequence},
year = {2013},
isbn = {9783642402692},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40270-8_4},
doi = {10.1007/978-3-642-40270-8_4},
abstract = {Splice site prediction in the pre-mRNA is a very important task for understanding gene structure and its function. To predict splice sites, SVM support vector machine based classification technique is frequently used because of its classification accuracy. High classification accuracy of SVM largely depends on DNA encoding method for feature extraction of DNA sequences. However, existing encoding approaches do not reveal the characteristics of DNA sequence very well enough to provide as much information as DNA sequences have. In this paper, we propose new effective DNA encoding method which can give more information of DNA sequence. Our encoding method can provide density information of each nucleotide along with positional information and chemical property. Extensive performance study shows that our method can provide better performance than existing encoding methods based on several performance criteria such as classification accuracy, sensitivity, specificity and area under receiver operating characteristics curve ROC.},
booktitle = {Proceedings of the 18th International Conference on Database Systems for Advanced Applications - Volume 7827},
pages = {46–58},
numpages = {13},
keywords = {support vector machine, splice site, orthogonal encoding, nucleotide density, gene prediction, ROC, DNA sequence}
}

@inproceedings{10.5555/1776814.1776823,
author = {Lappas, Georgios},
title = {Estimating the size of neural networks from the number of available training data},
year = {2007},
isbn = {3540746897},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Estimating a priori the size of neural networks for achieving high classification accuracy is a hard problem. Existing studies provide theoretical upper bounds on the size of neural networks that are unrealistic to implement. This work provides a computational study for estimating the size of neural networks using as an estimation parameter the size of available training data. We will also show that the size of a neural network is problem dependent and that one only needs the number of available training data to determine the size of the required network for achieving high classification rate. We use for our experiments a threshold neural network that combines the perceptron algorithm with simulated annealing and we tested our results on datasets from the UCI Machine Learning Repository. Based on our experimental results, we propose a formula to estimate the number of perceptrons that have to be trained in order to achieve a high classification accuracy.},
booktitle = {Proceedings of the 17th International Conference on Artificial Neural Networks},
pages = {68–77},
numpages = {10},
location = {Porto, Portugal},
series = {ICANN'07}
}

@inproceedings{10.1145/2912845.2912861,
author = {Melo, Andr\'{e} and Paulheim, Heiko and V\"{o}lker, Johanna},
title = {Type Prediction in RDF Knowledge Bases Using Hierarchical Multilabel Classification},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912861},
doi = {10.1145/2912845.2912861},
abstract = {Large Semantic Web knowledge bases are often noisy, incorrect, and incomplete with respect to type information. Automatic type prediction can help reduce such incompleteness, and, as previous works show, statistical methods are well-suited for this kind of data. Since most Semantic Web knowledge bases come with an ontology defining a type hierarchy, in this paper, we rephrase the type prediction problem as a hierarchical multilabel classification problem. We propose SLCN, a modification of the local classifier per node approach, which performs feature selection, instance sampling, and class balancing for each local classifier. Our approach improves scalability, facilitating its application on large Semantic Web datasets with high-dimensional feature and label spaces. We compare the performance of our proposed method with a state-of-the-art type prediction approach and popular hierarchical multilabel classifiers, and report on experiments with large-scale RDF datasets.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {Type Prediction, Knowledge Base, Hierarchical Multilabel Classification},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Hierarchical relationship, Semantic graph, Visual navigation},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3341105.3374085,
author = {Franco, Annalisa and Magnani, Antonio and Maio, Dario},
title = {Template co-updating in multi-modal human activity recognition systems},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374085},
doi = {10.1145/3341105.3374085},
abstract = {Multi-modal systems are quite common in the context of human activity recognition since widely used RGB-D sensors give access to parallel data streams (RGB, depth, skeleton). This paper is aimed at defining a general framework for unsupervised template updating in multi-modal systems, where the different data sources can provide complementary information, increasing the effectiveness of the updating procedure and reducing at the same time the probability of incorrect template modifications.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2113–2116},
numpages = {4},
keywords = {template co-updating, kinect ® sensor, human activity recognition},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3331184.3331249,
author = {Chiang, Meng-Fen and Lim, Ee-Peng and Lee, Wang-Chien and Ashok, Xavier Jayaraj Siddarth and Prasetyo, Philips Kokoh},
title = {One-Class Order Embedding for Dependency Relation Prediction},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331249},
doi = {10.1145/3331184.3331249},
abstract = {Learning the dependency relations among entities and the hierarchy formed by these relations by mapping entities into some order embedding space can effectively enable several important applications, including knowledge base completion and prerequisite relations prediction. Nevertheless, it is very challenging to learn a good order embedding due to the existence of partial ordering and missing relations in the observed data. Moreover, most application scenarios do not provide non-trivial negative dependency relation instances. We therefore propose a framework that performs dependency relation prediction by exploring both rich semantic and hierarchical structure information in the data. In particular, we propose several negative sampling strategies based on graph-specific centrality properties, which supplement the positive dependency relations with appropriate negative samples to effectively learn order embeddings. This research not only addresses the needs of automatically recovering missing dependency relations, but also unravels dependencies among entities using several real-world datasets, such as course dependency hierarchy involving course prerequisite relations, job hierarchy in organizations, and paper citation hierarchy. Extensive experiments are conducted on both synthetic and real-world datasets to demonstrate the prediction accuracy as well as to gain insights using the learned order embedding.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {205–214},
numpages = {10},
keywords = {order embedding, one-class learning, dependency relation prediction},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1016/j.inffus.2017.05.003,
author = {Tommasel, Antonela and Godoy, Daniela},
title = {A Social-aware online short-text feature selection technique for social media},
year = {2018},
issue_date = {March 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2017.05.003},
doi = {10.1016/j.inffus.2017.05.003},
abstract = {Online feature selection is highly relevant for large-scale social applications.We present an online feature selection approach based on social and content information.The goal is to classify continuously generated short-texts in social networks.The approach analyses sets of socially linked posts to select their content features.Classification results outperformed state-of-the-art techniques. Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques.},
journal = {Inf. Fusion},
month = mar,
pages = {1–17},
numpages = {17},
keywords = {Online feature selection, Micro-blogging communities, Classification}
}

@inproceedings{10.1145/1553374.1553409,
author = {Doshi-Velez, Finale and Ghahramani, Zoubin},
title = {Accelerated sampling for the Indian Buffet Process},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553409},
doi = {10.1145/1553374.1553409},
abstract = {We often seek to identify co-occurring hidden features in a set of observations. The Indian Buffet Process (IBP) provides a non-parametric prior on the features present in each observation, but current inference techniques for the IBP often scale poorly. The collapsed Gibbs sampler for the IBP has a running time cubic in the number of observations, and the uncollapsed Gibbs sampler, while linear, is often slow to mix. We present a new linear-time collapsed Gibbs sampler for conjugate likelihood models and demonstrate its efficacy on large real-world datasets.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {273–280},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.1145/2911451.2914709,
author = {Roegiest, Adam and Cormack, Gordon V.},
title = {Impact of Review-Set Selection on Human Assessment for Text Classification},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914709},
doi = {10.1145/2911451.2914709},
abstract = {In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p&lt;0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {assessor error, ediscovery, electronic discovery, evaluation, recall, supervised learning, user study},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Decision-making, Cloud service selection, Cloud computing}
}

@inproceedings{10.1145/3357384.3358069,
author = {Shaposhnikov, Denis and Bezzubtseva, Anastasia and Gladkikh, Ekaterina and Drutsa, Alexey},
title = {Labelling for Venue Visit Detection by Matching Wi-Fi Hotspots with Businesses},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358069},
doi = {10.1145/3357384.3358069},
abstract = {User behaviour data is essential for modern companies, as it allows them to measure the impact of decisions they make and to gain new insights. A particular type of such data is user location trajectories, which can be clustered into Points of Interest, which, in turn, can be tied to certain venues (restaurants, schools, theaters, etc.). Machine learning is extensively utilized to detect and predict venue visits given the location data, but it requires a sufficient sample of labeled visits. Few Internet services provide a possibility to check-in for a user --- to send a signal that she is visiting a particular venue. However, for the majority of mobile applications it is unreasonable or far-fetched to introduce such a functionality for labeling purposes only. In this paper, we present a novel approach to label large quantities of location data as visits based on the following intuition: if a user is connected to a Wi-Fi hotspot of some venue, she is visiting the venue. Namely, we address the problem of matching Wi-Fi hotspots with venues by means of machine learning achieving 95% precision and 85% recall. The method has been deployed to production of one of the most popular global geo-based web services. We also release our dataset (that we utilize to develop the matching model) to facilitate research in this area.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2281–2284},
numpages = {4},
keywords = {wi-fi matching, venue visit detection, user location, geocoding, entity matching, data mining},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Feature selection, Feature ranking, Data mining, Classification}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Imbalance learning, Equidistant distribution loss, Person re-identification}
}

@article{10.1016/j.eswa.2021.115756,
author = {Banerjee, Debanshu and Chatterjee, Bitanu and Bhowal, Pratik and Bhattacharyya, Trinav and Malakar, Samir and Sarkar, Ram},
title = {A new wrapper feature selection method for language-invariant offline signature verification},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115756},
doi = {10.1016/j.eswa.2021.115756},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {20},
keywords = {Offline signature verification, Wrapper feature selection, Red Deer Algorithm, Biometric, Meta-heuristic optimization}
}

@inproceedings{10.1145/3167132.3167346,
author = {Mohamed, Sameh K. and Nov\'{a}\v{c}ek, V\'{\i}t and Vandenbussche, Pierre-Yves},
title = {Knowledge base completion using distinct subgraph paths},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167346},
doi = {10.1145/3167132.3167346},
abstract = {Graph feature models facilitate efficient and interpretable predictions of missing links in knowledge bases with network structure (i.e. knowledge graphs). However, existing graph feature models---e.g. Subgraph Feature Extractor (SFE) or its predecessor, Path Ranking Algorithm (PRA) and its variants---depend on a limited set of graph features, connecting paths. This type of features may be missing for many interesting potential links, though, and the existing techniques cannot provide any predictions at all then. In this paper, we address the limitations of existing works by introducing a new graph-based feature model - Distinct Subgraph Paths (DSP). Our model uses a richer set of graph features and therefore can predict new relevant facts that neither SFE, nor PRA or its variants can discover by principle. We use a standard benchmark data set to show that DSP model performs better than the state-of-the-art - SFE (ANYREL) and PRA - in terms of mean average precision (MAP), mean reciprocal rank (MRR) and Hits@5, 10, 20, with no extra computational cost incurred.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1992–1999},
numpages = {8},
keywords = {path ranking, knowledge graphs, knowledge base completion},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.5555/3305381.3305588,
author = {Lee, Juho and Heaukulani, Creighton and Ghahramani, Zoubin and James, Lancelot F. and Choi, Seungjin},
title = {Bayesian inference on random simple graphs with power law degree distributions},
year = {2017},
publisher = {JMLR.org},
abstract = {We present a model for random simple graphs with power law (i.e., heavy-tailed) degree distributions. To attain this behavior, the edge probabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic block-models, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2004–2013},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3382225.3382412,
author = {Xylogiannopoulos, Konstantinos F.},
title = {From data points to data curves: a new approach on big data curves clustering},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {In the new era of IoT, enormous real-values datasets are produced daily. Time series created by smart devices, financial data, weather analysis, medical applications, traffic control etc. become more and more important in human day life. Analyzing and clustering these time series or in general any kind of curve could be critical. In the current paper, a new methodology (BD2C) is presented, which applies text mining and pattern detection techniques in order to cluster curves according to their shape. Several experiments have been conducted on artificial and real datasets in order to present the accuracy, efficiency and rapid discovery of the best possible clustering that the proposed methodology can achieve.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {881–884},
numpages = {4},
keywords = {pattern detection, multivariate data analytics, curve clustering, LERP-RSA, BD2C, ARPaD},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1007/s11042-020-08921-7,
author = {Chen, Jianjun and Tian, Youliang and Ma, Wei and Mao, Zhengdong and Hu, Yue},
title = {Scale channel attention network for image segmentation},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {11},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-08921-7},
doi = {10.1007/s11042-020-08921-7},
abstract = {The object scale variation results in a negative effect on image segmentation performance. Spatial pyramid pooling module or the attention mechanism are two widely used components in deep neural networks to handle this problem. Applying the single component commonly achieves limited benefit. To push the limit, in this paper, we propose a scale channel attention network (SCA-Net), which enhances the fusion feature of multi-scale by using channel attention components. After the multiple-scale pooling step, the multi-scale spatial information distributes in different feature channels. Meanwhile, the channel attention block is employed to guide SCA-Net focus on the object-relevant scale channels. We further explore the channel attention block and find a simple yet effective structure to combine global average pooling and global maximum pooling, resulting in a robust global information encoder. The SCA-Net does not contain any time-consuming post-processing, which is an extra step after the neural network for the segmentation result optimization. The assessment results on PASCAL VOC 2012 and Cityscapes benchmarks achieve the test set performance of 75.5% and 77.0%.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {16473–16489},
numpages = {17},
keywords = {Multi-source and heterogeneous data, Spatial pyramid pooling, Attention mechanism, Convolutional neural network, Image segmentation}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.5555/3305890.3306089,
author = {Yen, Ian E.H. and Lee, Wei-Cheng and Chang, Sung-En and Suggala, Arun S. and Lin, Shou-De and Ravikumar, Pradeep},
title = {Latent feature lasso},
year = {2017},
publisher = {JMLR.org},
abstract = {The latent feature model (LFM), proposed in (Griffiths &amp; Ghahramani, 2005), but possibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of latent features. Thus, each instance has an associated latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on non-parametric LFMs, with priors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this complexity either still have computational complexity that is exponential, or sample complexity that is high-order polynomial w.r.t. the number of latent features. In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the data distribution.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3949–3957},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1007/978-3-030-49435-3_29,
author = {Reinhartz-Berger, Iris and Abbas, Sameh},
title = {A Variability-Driven Analysis Method for Automatic Extraction of Domain Behaviors},
year = {2020},
isbn = {978-3-030-49434-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49435-3_29},
doi = {10.1007/978-3-030-49435-3_29},
abstract = {Domain engineering focuses on modeling knowledge in an application domain for supporting systematic reuse in the context of complex and constantly evolving systems. Automatically supporting this task is challenging; most existing methods assume high similarity of variants which limits reuse of the generated domain artifacts, or provide very low-level features rather than actual domain features. As a result, these methods are limited in handling common scenarios such as similarly behaving systems developed by different teams, or merging existing products. To address this gap, we propose a method for extracting domain knowledge in the form of domain behaviors, building on a previously developed framework for behavior-based variability analysis among class operations. Machine learning techniques are applied for identifying clusters of operations that can potentially form domain behaviors. The approach is evaluated on a set of open-source video games, named apo-games.},
booktitle = {Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020, Grenoble, France, June 8–12, 2020, Proceedings},
pages = {467–481},
numpages = {15},
keywords = {Domain engineering, Systematic reuse, Variability analysis},
location = {Grenoble, France}
}

@inproceedings{10.1007/978-3-642-32695-0_35,
author = {Orimaye, Sylvester Olubolu and Alhashmi, Saadat M. and Siew, Eu-Gene},
title = {Buy it - don't buy it: sentiment classification on amazon reviews using sentence polarity shift},
year = {2012},
isbn = {9783642326943},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-32695-0_35},
doi = {10.1007/978-3-642-32695-0_35},
abstract = {In recent years, sentiment classification has been an appealing task for so many reasons. However, the subtle manner in which people write reviews has made achieving high accuracy more challenging. In this paper, we investigate the improvements on sentiment classification baselines using sentiment polarity shift in reviews. We focus on Amazon online reviews for different types of product. First, we use our newly-proposed Sentence Polarity Shift (SPS) algorithm on review documents, reducing the relative classification loss due to inconsistent sentiment polarities within reviews by an average of 16% over a supervised sentiment classifier. Second, we build up on a popular supervised sentiment classification baseline by adding different features which provide better improvement over the original baseline. The improvement shown by this technique suggests modeling sentiment classification systems based on polarity shift combined with sentence and document-level features.},
booktitle = {Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelligence},
pages = {386–399},
numpages = {14},
keywords = {sentiment classification, sentence polarity shift, reviews},
location = {Kuching, Malaysia},
series = {PRICAI'12}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Self-paced learning, Instance weighting, Noisy and outlier corruption, Deep learning, Recommendation}
}

@article{10.1016/j.jbi.2013.05.005,
author = {Zhu, Qian and Freimuth, Robert R. and Pathak, Jyotishman and Durski, Matthew J. and Chute, Christopher G.},
title = {Disambiguation of PharmGKB drug-disease relations with NDF-RT and SPL},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {46},
number = {4},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2013.05.005},
doi = {10.1016/j.jbi.2013.05.005},
abstract = {We disambiguate PharmGKB drug and disease associations by NDF-RT and SPL.Detailed clinical associations are clearly represented in PharmGKB.The work helps to understand drug and disease relations in details from PharmGKB.Reveals standardized drug information will accelerate clinical drug integration. PharmGKB is a leading resource of high quality pharmacogenomics data that provides information about how genetic variations modulate an individual's response to drugs. PharmGKB contains information about genetic variations, pharmacokinetic and pharmacodynamic pathways, and the effect of variations on drug-related phenotypes. These relationships are represented using very general terms, however, and the precise semantic relationships among drugs, and diseases are not often captured. In this paper we develop a protocol to detect and disambiguate general clinical associations between drugs and diseases using more precise annotation terms from other data sources. PharmGKB provides very detailed clinical associations between genetic variants and drug response, including genotype-specific drug dosing guidelines, and this procedure will armGKB. The availability of more detailed data will help investigators to conduct more precise queries, such as finding particular diseases caused or treated by a specific drug.We first mapped drugs extracted from PharmGKB drug-disease relationships to those in the National Drug File Reference Terminology (NDF-RT) and to Structured Product Labels (SPLs). Specifically, we retrieved drug and disease role relationships describing and defining concepts according to their relationships with other concepts from NDF-RT. We also used the NCBO (National Center for Biomedical Ontology) annotator to annotate disease terms from the free text extracted from five SPL sections (indication, contraindication, ADE, precaution, and warning). Finally, we used the detailed drug and disease relationship information from NDF-RT and the SPLs to annotate and disambiguate the more general PharmGKB drug and disease associations.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {690–696},
numpages = {7},
keywords = {SPL, Pharmacogenomics, PharmGKB, NDF-RT, Clinical associations}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {semi-supervised learning, multi-view feature, graph optimization, co-saliency detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1007/s00521-018-3477-2,
author = {Chen, Xin and Xue, Yun and Zhao, Hongya and Lu, Xin and Hu, Xiaohui and Ma, Zhihao},
title = {A novel feature extraction methodology for sentiment analysis of product reviews},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3477-2},
doi = {10.1007/s00521-018-3477-2},
abstract = {Feature extraction is one of the key steps for text sentiment analysis (SA), and the corresponding algorithms have important effect on the results. In the paper, a novel methodology is proposed to extract the feature for SA of product reviews. First, based on the diversified expression forms of product reviews, the generalized TF–IDF feature vectors are obtained by introducing the semantic similarity of synonyms. Then, in view of the different lengths of product reviews, the local patterns of the feature vectors are identified with OPSM biclustering algorithm. Finally, we improve PrefixSpan algorithm to detect the frequent and pseudo-consecutive phrases with high discriminative ability (namely FPCD phrases), which contain word-order information. Furthermore, some important factors, such as the separation and discriminative ability of words, are also employed to improve the discriminative ability of sentiment polarity. Based on the previous steps, the text feature vectors are extracted. A series of the experiment and comparison results indicate that the performance for SA on product review is greatly improved.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6625–6642},
numpages = {18},
keywords = {Frequent phrase feature, Bicluster, Word embedding, Sentiment analysis (SA), Feature extraction}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/S0933-3657(03)00035-6,
author = {Peterson, Carsten and Ringn\'{e}r, Markus},
title = {Analyzing tumor gene expression profiles},
year = {2003},
issue_date = {May, 2003},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {28},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/S0933-3657(03)00035-6},
doi = {10.1016/S0933-3657(03)00035-6},
abstract = {A brief introduction to high throughput technologies for measuring and analyzing gene expression is given. Various supervised and unsupervised data mining methods for analyzing the produced high-dimensional data are discussed. The main emphasis is on supervised machine learning methods for classification and prediction of tumor gene expression profiles. Furthermore, methods to rank the genes according to their importance for the classification are explored. The approaches are illustrated by exploratory studies using two examples of retrospective clinical data from routine tests; diagnostic prediction of small round blue cell tumors (SRBCT) of childhood and determining the estrogen receptor (ER) status of sporadic breast cancer. The classification performance is gauged using blind tests. These studies demonstrate the feasibility of machine learning-based molecular cancer classification.},
journal = {Artif. Intell. Med.},
month = may,
pages = {59–74},
numpages = {16},
keywords = {Microarray, Genes, Drug target identification, Diagnostic prediction, Bioinformatics, Artificial neural networks}
}

@article{10.1016/j.neunet.2019.11.009,
author = {Umer, Saiyed and Sardar, Alamgir and Dhara, Bibhas Chandra and Rout, Ranjeet Kumar and Pandey, Hari Mohan},
title = {Person identification using fusion of iris and periocular deep features},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {122},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.11.009},
doi = {10.1016/j.neunet.2019.11.009},
journal = {Neural Netw.},
month = feb,
pages = {407–419},
numpages = {13},
keywords = {Rank-level fusion, Deep learning, Data augmentation, Periocular recognition, Iris recognition, Person identification}
}

@inproceedings{10.5555/3042817.3043050,
author = {Reed, Colorado and Ghahramani, Zoubin},
title = {Scaling the Indian buffet process via submodular maximization},
year = {2013},
publisher = {JMLR.org},
abstract = {Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara &amp; Welling (2008)'s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3- approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1013–III–1021},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.1145/3474085.3475471,
author = {Huang, Zongmo and Ren, Yazhou and Pu, Xiaorong and He, Lifang},
title = {Non-Linear Fusion for Self-Paced Multi-View Clustering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475471},
doi = {10.1145/3474085.3475471},
abstract = {With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3211–3219},
numpages = {9},
keywords = {self-paced learning, non-linear fusion, multi-view clustering},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.5555/3540261.3541546,
author = {Ma, Chao and Ying, Lexing},
title = {On linear stability of SGD and input-smoothness of neural networks},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The multiplicative structure of parameters and input data in the first layer of neural networks is explored to build connection between the landscape of the loss function with respect to parameters and the landscape of the model function with respect to input data. By this connection, it is shown that flat minima regularize the gradient of the model function, which explains the good generalization performance of flat minima. Then, we go beyond the flatness and consider high-order moments of the gradient noise, and show that Stochastic Gradient Descent (SGD) tends to impose constraints on these moments by a linear stability analysis of SGD around global minima. Together with the multiplicative structure, we identify the Sobolev regularization effect of SGD, i.e. SGD regularizes the Sobolev seminorms of the model function with respect to the input data. Finally, bounds for generalization error and adversarial robustness are provided for solutions found by SGD under assumptions of the data distribution.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1285},
numpages = {13},
series = {NIPS '21}
}

@article{10.1007/s11257-010-9087-z,
author = {K\"{o}ck, Mirjam and Paramythis, Alexandros},
title = {Activity sequence modelling and dynamic clustering for personalized e-learning},
year = {2011},
issue_date = {April     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {1–2},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-010-9087-z},
doi = {10.1007/s11257-010-9087-z},
abstract = {Monitoring and interpreting sequential learner activities has the potential to improve adaptivity and personalization within educational environments. We present an approach based on the modeling of learners' problem solving activity sequences, and on the use of the models in targeted, and ultimately automated clustering, resulting in the discovery of new, semantically meaningful information about the learners. The approach is applicable at different levels: to detect pre-defined, well-established problem solving styles, to identify problem solving styles by analyzing learner behaviour along known learning dimensions, and to semi-automatically discover learning dimensions and concrete problem solving patterns. This article describes the approach itself, demonstrates the feasibility of applying it on real-world data, and discusses aspects of the approach that can be adjusted for different learning contexts. Finally, we address the incorporation of the proposed approach in the adaptation cycle, from data acquisition to adaptive system interventions in the interaction process.},
journal = {User Modeling and User-Adapted Interaction},
month = apr,
pages = {51–97},
numpages = {47},
keywords = {User modeling, Unsupervised learning, E-learning, Data mining, Clustering, Adaptivity}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Scale pyramid loss, Scale-aware, Attention mechanism, Deep learning, Crowd counting},
location = {Sanur, Bali, Indonesia}
}

@inbook{10.5555/3454287.3454551,
author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
title = {On lazy training in differentiable programming},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {264},
numpages = {11}
}

@article{10.1016/j.specom.2008.05.003,
author = {de Gispert, A. and Mari\~{n}o, J. B.},
title = {On the impact of morphology in English to Spanish statistical MT},
year = {2008},
issue_date = {November, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {11–12},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2008.05.003},
doi = {10.1016/j.specom.2008.05.003},
abstract = {This paper presents a thorough study of the impact of morphology derivation on N-gram-based Statistical Machine Translation (SMT) models from English into a morphology-rich language such as Spanish. For this purpose, we define a framework under the assumption that a certain degree of morphology-related information is not only being ignored by current statistical translation models, but also has a negative impact on their estimation due to the data sparseness it causes. Moreover, we describe how this information can be decoupled from the standard bilingual N-gram models and introduced separately by means of a well-defined and better informed feature-based classification task. Results are presented for the European Parliament Plenary Sessions (EPPS) English-&gt;Spanish task, showing oracle scores based on to what extent SMT models can benefit from simplifying Spanish morphological surface forms for each Part-Of-Speech category. We show that verb form morphological richness greatly weakens the standard statistical models, and we carry out a posterior morphology classification by defining a simple set of features and applying machine learning techniques. In addition to that, we propose a simple technique to deal with Spanish enclitic pronouns. Both techniques are empirically evaluated and final translation results show improvements over the baseline by just dealing with Spanish morphology. In principle, the study is also valid for translation from English into any other Romance language (Portuguese, Catalan, French, Galician, Italian, etc.). The proposed method can be applied to both monotonic and non-monotonic decoding scenarios, thus revealing the interaction between word-order decoding and the proposed morphology simplification techniques. Overall results achieve statistically significant improvement over baseline performance in this demanding task.},
journal = {Speech Commun.},
month = nov,
pages = {1034–1046},
numpages = {13},
keywords = {Statistical machine translation, N-gram based translation, Morphology generation, Machine learning}
}

@article{10.1016/j.specom.2015.01.003,
author = {Sadeghian, Amir and Dajani, Hilmi R. and Chan, Adrian D.C.},
title = {Classification of speech-evoked brainstem responses to English vowels},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.01.003},
doi = {10.1016/j.specom.2015.01.003},
abstract = {We investigated the automatic classification of speech-evoked brainstem responses.Responses to five vowels were classified based on onset and sustained features.Combined sustained features gave a classification accuracy of 83.33%.Classification accuracy with onset response features was better than chance.Vowel-specific information in responses may be useful for fitting hearing aids. This study investigated whether speech-evoked auditory brainstem responses (speech ABRs) can be automatically separated into distinct classes. With five English synthetic vowels, the speech ABRs were classified using linear discriminant analysis based on features contained in the transient onset response, the sustained envelope following response (EFR), and the sustained frequency following response (FFR). EFR contains components mainly at frequencies well below the first formant, while the FFR has more energy around the first formant. Accuracies of 83.33% were obtained for combined EFR and FFR features and 38.33% were obtained for transient response features. The EFR features performed relatively well with a classification accuracy of 70.83% despite the belief that vowel discrimination is primarily dependent on the formants. The FFR features obtained a lower accuracy of 59.58% possibly because the second formant is not well represented in all the responses. Moreover, the classification accuracy based on the transient features exceeded chance level which indicates that the initial response transients contain vowel specific information. The results of this study will be useful in a proposed application of speech ABR to objective hearing aid fitting, if the separation of the brain's responses to different vowels is found to be correlated with perceptual discrimination.},
journal = {Speech Commun.},
month = apr,
pages = {69–84},
numpages = {16},
keywords = {Speech-evoked auditory brainstem response, Frequency following response, Fitting hearing aids, Envelope following response, Classification of evoked responses, Auditory processing of speech}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Unsupervised feature selection, Structure preserving, Self-expression model}
}

@inproceedings{10.1007/11585978_6,
author = {Figueiredo, M\'{a}rio A. T.},
title = {Bayesian image segmentation using gaussian field priors},
year = {2005},
isbn = {3540302875},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11585978_6},
doi = {10.1007/11585978_6},
abstract = {The goal of segmentation is to partition an image into a finite set of regions, homogeneous in some (e.g., statistical) sense, thus being an intrinsically discrete problem. Bayesian approaches to segmentation use priors to impose spatial coherence; the discrete nature of segmentation demands priors defined on discrete-valued fields, thus leading to difficult combinatorial problems.This paper presents a formulation which allows using continuous priors, namely Gaussian fields, for image segmentation. Our approach completely avoids the combinatorial nature of standard Bayesian approaches to segmentation. Moreover, it's completely general, i.e., it can be used in supervised, unsupervised, or semi-supervised modes, with any probabilistic observation model (intensity, multispectral, or texture features).To use continuous priors for image segmentation, we adopt a formulation which is common in Bayesian machine learning: introduction of hidden fields to which the region labels are probabilistically related. Since these hidden fields are real-valued, we can adopt any type of spatial prior for continuous-valued fields, such as Gaussian priors. We show how, under this model, Bayesian MAP segmentation is carried out by a (generalized) EM algorithm. Experiments on synthetic and real data shows that the proposed approach performs very well at a low computational cost.},
booktitle = {Proceedings of the 5th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition},
pages = {74–89},
numpages = {16},
location = {St. Augustine, FL},
series = {EMMCVPR'05}
}

@article{10.1186/s13673-019-0177-6,
author = {Yin, Chunyong and Ding, Shilei and Wang, Jin},
title = {Mobile marketing recommendation method based on user location feedback},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {1},
issn = {2192-1962},
url = {https://doi.org/10.1186/s13673-019-0177-6},
doi = {10.1186/s13673-019-0177-6},
abstract = {Location-based mobile marketing recommendation has become one of the hot spots in e-commerce. The current mobile marketing recommendation system only treats location information as a recommended attribute, which weakens the role of users and shopping location information in the recommendation. This paper focuses on location feedback data of user and proposes a location-based mobile marketing recommendation model by convolutional neural network (LBCNN). First, the users' location-based behaviors are divided into different time windows. For each window, the extractor achieves users' timing preference characteristics from different dimensions. Next, we use the convolutional model in the convolutional neural network model to train a classifier. The experimental results show that the model proposed in this paper is better than the traditional recommendation models in the terms of accuracy rate and recall rate, both of which increase nearly 10%.},
journal = {Hum.-Centric Comput. Inf. Sci.},
month = dec,
articleno = {177},
numpages = {17},
keywords = {Sequential behavior, Mobile marketing, Location feedback, Convolutional neural network}
}

@article{10.1007/s10664-015-9394-4,
author = {Chatterji, Debarshi and Carver, Jeffrey C. and Kraft, Nicholas A.},
title = {Code clones and developer behavior: results of two surveys of the clone research community},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9394-4},
doi = {10.1007/s10664-015-9394-4},
abstract = {The literature presents conflicting claims regarding the effects of clones on software maintainability. For a community to progress, it is important to identify and address those areas of disagreement. Many claims, such as those related to developer behavior, either lack human-based empirical validation or are contradicted by other studies. This paper describes the results of two surveys to evaluate the level of agreement among clone researchers regarding claims that have not yet been validated through human-based empirical study. The surveys covered three key clone-related research topics: general information, developer behavior, and evolution. Survey 1 focused on high-level information about all three topics, whereas Survey 2 focused specifically on developer behavior. Approximately 20 clone researchers responded to each survey. The survey responses showed a lack of agreement on some major clone-related topics. First, the respondents disagree about the definitions of clone types, with some indicating the need for a taxonomy based upon developer intent. Second, the respondents were uncertain whether the ratio of cloned to non-cloned code affected system quality. Finally, the respondents disagree about the usefulness of various detection, analysis, evolution, and visualization tools for clone management tasks such as tracking and refactoring of clones. The overall results indicate the need for more focused, human-based empirical research regarding the effects of clones during maintenance. The paper proposes a strategy for future research regarding developer behavior and code clones in order to bridge the gap between clone research and the application of that research in clone maintenance.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1476–1508},
numpages = {33},
keywords = {Software maintenance, Developer behavior, Community survey, Code clones, Clone management, Clone evolution}
}

@article{10.1007/s10772-020-09750-7,
author = {Hadj Ali, Ikbel and Mnasri, Zied and Lachiri, Zied},
title = {DNN-based grapheme-to-phoneme conversion for Arabic text-to-speech synthesis},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-020-09750-7},
doi = {10.1007/s10772-020-09750-7},
abstract = {Arabic text-to-speech synthesis from non-diacritized text is still a big challenge, because of unique Arabic language rules and characteristics. Indeed, the diacritic and gemination signs, which are special characters representing respectively short vowels and consonant doubling, have a major effect on accurate pronunciation of Arabic. However these signs are often not mentioned in written texts, since most of Arab readers are used to guess them from the context. To tackle this issue, this paper presents a grapheme-to-phoneme conversion system for Arabic, which constitutes the text processing module of a deep neural networks (DNN)-based Arabic TTS systems. In the case of Arabic text, this step starts with predicting the diacritic and gemination signs. In this work, this step was fully realized based on DNN. Finally, the grapheme-to-phoneme conversion of the diacritized text was achieved using the Buckwalter code. In comparison to state-of-the-art approaches, the proposed system gives a higher accuracy rate either for all phonemes or for each class, and high precision, recall and F1 score for each class of diacritic signs.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {569–584},
numpages = {16},
keywords = {Gemination, Diacritic signs, Grapheme-to-phoneme conversion, Deep neural networks (DNN), Arabic text-to-speech synthesis}
}

@article{10.1016/j.comnet.2006.08.003,
author = {Weiss, M. and Esfandiari, B. and Luo, Y.},
title = {Towards a classification of web service feature interactions},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {51},
number = {2},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2006.08.003},
doi = {10.1016/j.comnet.2006.08.003},
abstract = {The rapid introduction of new web services into a dynamic business environment can lead to undesirable interactions that negatively affect service quality and user satisfaction. In previous work, we have demonstrated how such interactions between web services can be modeled as feature interactions. In this paper, we outline a classification of web service feature interactions. The goals of this classification are to understand the scope of the feature interaction problem in the web services domain, and to propose a benchmark against which to assess the coverage of solutions to this problem. As there is no standard set of web services that one could use as examples, we illustrate the interactions using a fictitious e-commerce scenario.},
journal = {Comput. Netw.},
month = feb,
pages = {359–381},
numpages = {23},
keywords = {Web services, Feature interaction, Classification}
}

@article{10.1016/j.cag.2009.06.006,
author = {Nam, Julia EunJu and Maurer, Mauricio and Mueller, Klaus},
title = {Knowledge Assisted Visualization: A high-dimensional feature clustering approach to support knowledge-assisted visualization},
year = {2009},
issue_date = {October, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {33},
number = {5},
issn = {0097-8493},
url = {https://doi.org/10.1016/j.cag.2009.06.006},
doi = {10.1016/j.cag.2009.06.006},
abstract = {The ever-growing arsenal of methods and parameters available for data visualization can be daunting to the casual user and even to domain experts. Furthermore, comprehensive expertise is often not available in a centralized venue, but distributed over sub-communities. As a means to overcome this inherent problem, efforts have begun to store visualization expertise directly with the visualization method and possibly the dataset, to then be utilized for user guidance in the data visualization, suggesting to the user both the visualization method and its best parameters for the data and task at hand. While this is certainly an immensely useful and promising development, one requirement remains - the matching of a newly acquired dataset with the appropriate segment of the library storing the expert knowledge. This requires one to detect and recognize the dataset's category at some level of granularity and then use this information as a library index. We describe a possible framework for accomplishing the first stage of this process, namely the data categorization, using data classification via a rich set of feature vectors sufficiently sensitive to detect critical variations. We demonstrate the utility of our framework by ways of a set of medical and computational datasets and visualize the resulting categorization as a layout in 2D.},
journal = {Comput. Graph.},
month = oct,
pages = {607–615},
numpages = {9},
keywords = {Feature descriptors, Data indexing, Classification, Categorization}
}

@article{10.1007/s11042-017-5172-1,
author = {Ma, Xueqi and Tao, Dapeng and Liu, Weifeng},
title = {Effective human action recognition by combining manifold regularization and pairwise constraints},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5172-1},
doi = {10.1007/s11042-017-5172-1},
abstract = {The ever-growing popularity of mobile networks and electronics has prompted intensive research on multimedia data (e.g. text, image, video, audio, etc.) management. This leads to the researches of semi-supervised learning that can incorporate a small number of labeled and a large number of unlabeled data by exploiting the local structure of data distribution. Manifold regularization and pairwise constraints are representative semi-supervised learning methods. In this paper, we introduce a novel local structure preserving approach by considering both manifold regularization and pairwise constraints. Specifically, we construct a new graph Laplacian that takes advantage of pairwise constraints compared with the traditional Laplacian. The proposed graph Laplacian can better preserve the local geometry of data distribution and achieve the effective recognition. Upon this, we build the graph regularized classifiers including support vector machines and kernel least squares as special cases for action recognition. Experimental results on a multimodal human action database (CAS-YNU-MHAD) show that our proposed algorithms outperform the general algorithms.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13313–13329},
numpages = {17},
keywords = {Pairwise constraints, Manifold regularization, Local structure preserving, Action recognition}
}

@article{10.5555/2541581.2541582,
author = {Sandberg, Kristian and Bahrami, Bahador and Kanai, Ryota and Barnes, Gareth Robert and Overgaard, Morten and Rees, Geraint},
title = {Early visual responses predict conscious face perception within and between subjects during binocular rivalry},
year = {2013},
issue_date = {June 2013},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {6},
issn = {0898-929X},
abstract = {Previous studies indicate that conscious face perception may be related to neural activity in a large time window around 170-800 msec after stimulus presentation, yet in the majority of these studies changes in conscious experience are confounded with changes in physical stimulation. Using multivariate classification on MEG data recorded when participants reported changes in conscious perception evoked by binocular rivalry between a face and a grating, we showed that only MEG signals in the 120-320 msec time range, peaking at the M170 around 180 msec and the P2m at around 260 msec, reliably predicted conscious experience. Conscious perception could not only be decoded significantly better than chance from the sensors that showed the largest average difference, as previous studies suggest, but also from patterns of activity across groups of occipital sensors that individually were unable to predict perception better than chance. In addition, source space analyses showed that sources in the early and late visual system predicted conscious perception more accurately than frontal and parietal sites, although conscious perception could also be decoded there. Finally, the patterns of neural activity associated with conscious face perception generalized from one participant to another around the times of maximum prediction accuracy. Our work thus demonstrates that the neural correlates of particular conscious contents here, faces are highly consistent in time and space within individuals and that these correlates are shared to some extent between individuals.},
journal = {J. Cognitive Neuroscience},
month = jun,
pages = {969–985},
numpages = {17}
}

@article{10.1016/j.procs.2015.02.001,
author = {Patri, Ashutosh and Patnaik, Yugesh},
title = {Random Forest and Stochastic Gradient Tree Boosting Based Approach for the Prediction of Airfoil Self-noise},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.001},
doi = {10.1016/j.procs.2015.02.001},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {109–121},
numpages = {13},
keywords = {NACA0012., regression, CART, Stochastic Gradient Boosting, Random Forest, prediction, airfoil self-noise}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {Software Reuse, Software Evolution Theory, Programming Languages, OSS Prediction, Co-Evolution, Automation Support, ARIMA Modelling}
}

@article{10.1016/j.camwa.2013.06.027,
author = {Borges, Helyane Bronoski and Silla, Carlos N. and Nievola, J\'{u}lio Cesar},
title = {An evaluation of global-model hierarchical classification algorithms for hierarchical classification problems with single path of labels},
year = {2013},
issue_date = {December, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {10},
issn = {0898-1221},
url = {https://doi.org/10.1016/j.camwa.2013.06.027},
doi = {10.1016/j.camwa.2013.06.027},
abstract = {Several classification tasks in different application domains can be seen as hierarchical classification problems. In order to deal with hierarchical classification problems, the use of existing flat classification approaches is not appropriate. For these reason, there has been a growing number of studies focusing on the development of novel algorithms able to induce classification models for hierarchical classification problems. In this paper we study the performance of a novel algorithm called Hierarchical Classification using a Competitive Neural Network (HC-CNN) and compare its performance against the Global-Model Naive Bayes (GMNB) on eight protein function prediction datasets. Interestingly enough, the comparison of two global-model hierarchical classification algorithms for single path of labels hierarchical classification problems has never been done before.},
journal = {Comput. Math. Appl.},
month = dec,
pages = {1991–2002},
numpages = {12},
keywords = {Hierarchical classification, Global approach}
}

@article{10.1016/j.jbi.2011.11.002,
author = {Forestier, Germain and Lalys, Florent and Riffaud, Laurent and Trelhu, Brivael and Jannin, Pierre},
title = {Classification of surgical processes using dynamic time warping},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {45},
number = {2},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.11.002},
doi = {10.1016/j.jbi.2011.11.002},
abstract = {In the creation of new computer-assisted intervention systems, Surgical Process Models (SPMs) are an emerging concept used for analyzing and assessing surgical interventions. SPMs represent Surgical Processes (SPs) which are formalized as symbolic structured descriptions of surgical interventions using a pre-defined level of granularity and a dedicated terminology. In this context, one major challenge is the creation of new metrics for the comparison and the evaluation of SPs. Thus, correlations between these metrics and pre-operative data are used to classify surgeries and highlight specific information on the surgery itself and on the surgeon, such as his/her level of expertise. In this paper, we explore the automatic classification of a set of SPs based on the Dynamic Time Warping (DTW) algorithm. DTW is used to compute a similarity measure between two SPs that focuses on the different types of activities performed during surgery and their sequencing, by minimizing time differences. Indeed, it turns out to be a complementary approach to the classical methods that only focus on differences in the time and the number of activities. Experiments were carried out on 24 lumbar disk herniation surgeries to discriminate the surgeons level of expertise according to a prior classification of SPs. Supervised and unsupervised classification experiments have shown that this approach was able to automatically identify groups of surgeons according to their level of expertise (senior and junior), and opens many perspectives for the creation of new metrics for comparing and evaluating surgeries.},
journal = {J. of Biomedical Informatics},
month = apr,
pages = {255–264},
numpages = {10},
keywords = {Surgical process models, Surgery evaluation, Dynamic time warping, Clustering, Classification}
}

@article{10.1007/s10772-016-9367-z,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with Parkinson's diseases using PCA and NPCA},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-016-9367-z},
doi = {10.1007/s10772-016-9367-z},
abstract = {In this study, we wanted to discriminate between two groups of people. The database used in this study contains 20 patients with Parkinson's disease and 20 healthy people. Three types of sustained vowels (/a/, /o/ and /u/) were recorded from each participant and then the analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used linear and nonlinear feature extraction techniques, principal component analysis (PCA), and nonlinear PCA. These techniques reduce the number of parameters and choose the most effective acoustic features used for classification. Support vector machine with its different kernel was used for classification. We obtained an accuracy up to 87.50 % for discrimination between PD patients and healthy people.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {743–754},
numpages = {12},
keywords = {SVM, Parkinson's disease, PCA, NPCA, Feature selection}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Sound recognition, RoboCup SPL, Crowd noise interpretation},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1109/ICIP.2017.8296599,
author = {Jing, Longlong and Ye, Yuancheng and Yang, Xiaodong and Tian, Yingli},
title = {3D convolutional neural network with multi-model framework for action recognition},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICIP.2017.8296599},
doi = {10.1109/ICIP.2017.8296599},
abstract = {In this paper, we propose an efficient and effective action recognition framework by combining multiple feature models from dynamic image, optical flow and raw frame, with 3D convolutional neural network (CNN). Dynamic image preserves the long-term temporal information, while optical flow captures short-term temporal information, and raw frame represents the appearance information. Experiments demonstrate that dynamic image provides complementary information to raw frame feature and optical flow feature. Furthermore, with the approximate rank pooling, the computation of dynamic images is about 360 times faster than optical flow, and the dynamic image requires far less memory than optical flow and raw frame.},
booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
pages = {1837–1841},
numpages = {5},
location = {Beijing, China}
}

@article{10.1007/s00138-013-0514-0,
author = {Burghouts, G. J. and Schutte, K. and Bouma, H. and Hollander, R. J.},
title = {Selection of negative samples and two-stage combination of multiple features for action detection in thousands of videos},
year = {2014},
issue_date = {January   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {1},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-013-0514-0},
doi = {10.1007/s00138-013-0514-0},
abstract = {In this paper, a system is presented that can detect 48 human actions in realistic videos, ranging from simple actions such as `walk' to complex actions such as `exchange'. We propose a method that gives a major contribution in performance. The reason for this major improvement is related to a different approach on three themes: sample selection, two-stage classification, and the combination of multiple features. First, we show that the sampling can be improved by smart selection of the negatives. Second, we show that exploiting all 48 actions' posteriors by two-stage classification greatly improves its detection. Third, we show how low-level motion and high-level object features should be combined. These three yield a performance improvement of a factor 2.37 for human action detection in the visint.org test set of 1,294 realistic videos. In addition, we demonstrate that selective sampling and the two-stage setup improve on standard bag-of-feature methods on the UT-interaction dataset, and our method outperforms state-of-the-art for the IXMAS dataset.},
journal = {Mach. Vision Appl.},
month = jan,
pages = {85–98},
numpages = {14},
keywords = {Tracking of humans, Support vector machines, Spatiotemporal features, Sparse representation, STIP, Random forest, Pose estimation, Person detection, Interactions between people, Human action detection, Event recognition}
}

@article{10.4018/IJCAC.2021040106,
author = {Moh, Melody and Yen, Steven and Moh, Teng-Sheng},
title = {Detecting Compromised Social Network Accounts Using Deep Learning for Behavior and Text Analyses},
year = {2021},
issue_date = {Apr 2021},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {2},
issn = {2156-1834},
url = {https://doi.org/10.4018/IJCAC.2021040106},
doi = {10.4018/IJCAC.2021040106},
abstract = {Social networks allow people to connect to one another. Over time, these accounts become an essential part of one's online identity. The account stores various personal data and contains one's network of acquaintances. Attackers seek to compromise user accounts for various malicious purposes, such as distributing spam, phishing, and much more. Timely detection of compromises becomes crucial for protecting users and social networks. This article proposes a novel system for detecting compromises of a social network account by considering both post behavior and textual content. A deep multi-layer perceptron-based autoencoder is leveraged to consolidate diverse features and extract underlying relationships. Experiments show that the proposed system outperforms previous techniques that considered only behavioral information. The authors believe that this work is well-timed, significant especially in the world that has been largely locked down by the COVID-19 pandemic and thus depends much more on reliable social networks to stay connected.},
journal = {Int. J. Cloud Appl. Comput.},
month = apr,
pages = {1–13},
numpages = {13},
keywords = {Anomaly Score Algorithm, Autoencoder (AE), Behavioral Information, Mean-Squared-Error (MSE), Multi-Layer Perceptron (MLP), Natural Language Processing (NLP), Online Attacks, Textural Information}
}

@inproceedings{10.1007/978-3-319-29339-4_24,
author = {Leottau, David L. and Ruiz-del-Solar, Javier and MacAlpine, Patrick and Stone, Peter},
title = {A Study of Layered Learning Strategies Applied to Individual Behaviors in Robot Soccer},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_24},
doi = {10.1007/978-3-319-29339-4_24},
abstract = {Hierarchical task decomposition strategies allow robots and agents in general to address complex decision-making tasks. Layered learning is a hierarchical machine learning paradigm where a complex behavior is learned from a series of incrementally trained sub-tasks. This paper describes how layered learning can be applied to design individual behaviors in the context of soccer robotics. Three different layered learning strategies are implemented and analyzed using a ball-dribbling behavior as a case study. Performance indices for evaluating dribbling speed and ball-control are defined and measured. Experimental results validate the usefulness of the implemented layered learning strategies showing a trade-off between performance and learning speed.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {290–302},
numpages = {13},
keywords = {Reinforcement learning, Layered learning, Machine learning, Soccer robotics, Biped robot, NAO, Behavior, Dribbling, Fuzzy logic},
location = {Hefei, China}
}

@article{10.1007/s11042-018-5626-0,
author = {Lei, Qing and Zhang, Hongbo and Xin, Minghai and Cai, Yiqiao},
title = {A hierarchical representation for human action recognition in realistic scenes},
year = {2018},
issue_date = {May       2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {9},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-5626-0},
doi = {10.1007/s11042-018-5626-0},
abstract = {BoF statistic-based local space-time features action representation is very popular for human action recognition due to its simplicity. However, the problem of large quantization error and weak semantic representation decrease traditional BoF model's discriminant ability when applied to human action recognition in realistic scenes. To deal with the problems, we investigate the generalization ability of BoF framework for action representation as well as more effective feature encoding about high-level semantics. Towards this end, we present two-layer hierarchical codebook learning framework for human action classification in realistic scenes. In the first-layer action modelling, superpixel GMM model is developed to filter out noise features in STIP extraction resulted from cluttered background, and class-specific learning strategy is employed on the refined STIP feature space to construct compact and descriptive in-class action codebooks. In the second-layer of action representation, LDA-Km learning algorithm is proposed for feature dimensionality reduction and for acquiring more discriminative inter-class action codebook for classification. We take advantage of hierarchical framework's representational power and the efficiency of BoF model to boost recognition performance in realistic scenes. In experiments, the performance of our proposed method is evaluated on four benchmark datasets: KTH, YouTube (UCF11), UCF Sports and Hollywood2. Experimental results show that the proposed approach achieves improved recognition accuracy than the baseline method. Comparisons with state-of-the-art works demonstrates the competitive ability both in recognition performance and time complexity.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {11403–11423},
numpages = {21},
keywords = {Video processing, Realistic scenes, Feature selection, Action recognition, Action modelling}
}

@inproceedings{10.5555/3042094.3042398,
author = {Liotta, Giacomo and Chaudhuri, Atanu},
title = {Minimizing recall risk by collaborative digitized information sharing between OEM and suppliers: a simulation based investigation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Many Original Equipment Manufacturers (OEMs) and their suppliers face recall and warranty risks due to complex supply chains and products. OEMs and suppliers can hardly take appropriate actions for mitigating these quality risks due to lack of product history data and understanding of their probability. In this work, the product consists of two components delivered by two Tier II suppliers. Probabilities of OEM's acceptance, rework and rejection of the assembled product by a Tier I supplier and probabilities of acceptance, warranty and recall are calculated combining Bayesian Belief Network and simulation of a digitized supply chain. Results show that sharing of incoming quality information between an OEM and Tier I supplier and decision models to estimate warranty and recall probabilities can help in assessing quality improvement benefits to minimize recall risks. Suitable quality improvement contracts between an OEM and Tier I supplier can be designed using embedded product quality data.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2454–2465},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inbook{10.5555/3454287.3454621,
author = {Hwang, Gunpil and Kim, Seohyeon and Bae, Hyeon-Min},
title = {Bat-G net: bat-inspired high-resolution 3D image reconstruction using ultrasonic echoes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {334},
numpages = {12}
}

@article{10.1155/2021/4430886,
author = {Sheng, Meihong and Tang, Weixia and Tang, Jiahuan and Zhang, Ming and Gong, Shenchu and Xing, Wei and Lai, Khin wee},
title = {Feasibility of Using Improved Convolutional Neural Network to Classify BI-RADS 4 Breast Lesions: Compare Deep Learning Features of the Lesion Itself and the Minimum Bounding Cube of Lesion},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/4430886},
doi = {10.1155/2021/4430886},
abstract = {To determine the feasibility of using a deep learning (DL) approach to identify benign and malignant BI-RADS 4 lesions with preoperative breast DCE-MRI images and compare two 3D segmentation methods. The patients admitted from January 2014 to October 2020 were retrospectively analyzed. Breast MRI examination was performed before surgical resection or biopsy, and the masses were classified as BI-RADS 4. The first postcontrast images of DCE-MRI T1WI sequence were selected. There were two 3D segmentation methods for the lesions, one was manual segmentation along the edge of the lesion slice by slice, and the other was the minimum bounding cube of the lesion. Then, DL feature extraction was carried out; the pixel values of the image data are normalized to 0-1 range. The model was established based on the blueprint of the classic residual network ResNet50, retaining its residual module and improved 2D convolution module to 3D. At the same time, an attention mechanism was added to transform the attention mechanism module, which only fit the 2D image convolution module, into a 3D-Convolutional Block Attention Module (CBAM) to adapt to 3D-MRI. After the last CBAM, the algorithm stretches the output high-dimensional features into a one-dimensional vector and connects 2 fully connected slices, before finally setting two output results (P1, P2), which, respectively, represent the probability of benign and malignant lesions. Accuracy, sensitivity, specificity, negative predictive value, positive predictive value, the recall rate and area under the ROC curve (AUC) were used as evaluation indicators. A total of 203 patients were enrolled, with 207 mass lesions including 101 benign lesions and 106 malignant lesions. The data set was divided into the training set (n=145), the validation set (n=22), and the test set (n=40) at the ratio of 7 : 1 : 2; fivefold cross-validation was performed. The mean AUC based on the minimum bounding cube of lesion and the 3D-ROI of lesion itself were 0.827 and 0.799, the accuracy was 78.54% and 74.63%, the sensitivity was 78.85% and 83.65%, the specificity was 78.22% and 65.35%, the NPV was 78.85% and 71.31%, the PPV was 78.22% and 79.52%, the recall rate was 78.85% and 83.65%, respectively. There was no statistical difference in AUC based on the lesion itself model and the minimum bounding cube model (Z=0.771, p=0.4408). The minimum bounding cube based on the edge of the lesion showed higher accuracy, specificity, and lower recall rate in identifying benign and malignant lesions. Based on the lesion 3D-ROI segmentation using a minimum bounding cube can more effectively reflect the information of the lesion itself and the surrounding tissues. Its DL model performs better than the lesion itself. Using the DL approach with a 3D attention mechanism based on ResNet50 to identify benign and malignant BI-RADS 4 lesions was feasible.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {9}
}

@inproceedings{10.1109/WI-IAT.2011.16,
author = {Wang, Zhimin and Jia, Yuxiang and He, Li},
title = {The Syntactic Features and Identification Analysis of "Planting" Verb Metaphors},
year = {2011},
isbn = {9780769545134},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2011.16},
doi = {10.1109/WI-IAT.2011.16},
abstract = {This paper explores the rules of the construction and the syntactic features of the "planting" verbal metaphors, with the detailed analysis on syntax, semantics and constitute of the verbal metaphorical expression. Furthermore, we extract the key features to distinguish the verbal metaphors based on the differences between the verbal metaphorical expressions and the literal meanings, and apply the machine learning method to the identification of the verbal metaphor. The experiments show that verbal metaphor has its own unique features on syntax. It would greatly improve the performance of the machine learning models with the addition of these features. Lastly, this method could be extended to other kinds of verbal metaphor.},
booktitle = {Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 03},
pages = {185–188},
numpages = {4},
keywords = {syntactic features, metaphorical mapping, metaphor recognition, machine learning},
series = {WI-IAT '11}
}

@article{10.1177/0165551516639801,
author = {Chen, Kun and Ji, Xiaowen and Wang, Huaiqing},
title = {A search index-enhanced feature model for news recommendation},
year = {2017},
issue_date = {6 2017},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {43},
number = {3},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551516639801},
doi = {10.1177/0165551516639801},
abstract = {General news recommendations are important but have received limited attention because of the difficulties of measuring public interest. In public search engines, the objects of search terms reflect the issues that interest or concern search engine users. Because of the popularity of search engines, search indexes have become a new measure for describing public interest trends. With the help of a public search index provided by search engines, we construct a news topic search feature and a news object search feature. These features measure the public attention on key elements of the news. In the experiment, we compare various feature models with machine learning algorithms with respect to financial news recommendations. The results demonstrate that the topic search features perform best compared with other feature models. This research contributes to both the feature generation and news recommendation domains.},
journal = {J. Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {topic model, news recommendation, feature generation, Classification}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@inproceedings{10.5555/3540261.3541399,
author = {Zhang, Yaoyu and Zhang, Zhongwang and Luo, Tao and Xu, Zhi-Qin John},
title = {Embedding principle of loss landscape of deep neural networks},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding the structure of loss landscape of deep neural networks (DNNs) is obviously important. In this work, we prove an embedding principle that the loss landscape of a DNN "contains" all the critical points of all the narrower DNNs. More precisely, we propose a critical embedding such that any critical point, e.g., local or global minima, of a narrower DNN can be embedded to a critical point/affine subspace of the target DNN with higher degeneracy and preserving the DNN output function. Note that, given any training data, differentiable loss function and differentiable activation function, this embedding structure of critical points holds. This general structure of DNNs is starkly different from other nonconvex problems such as protein-folding. Empirically, we find that a wide DNN is often attracted by highly-degenerate critical points that are embedded from narrow DNNs. The embedding principle provides a new perspective to study the general easy optimization of wide DNNs and unravels a potential implicit low-complexity regularization during the training. Overall, our work provides a skeleton for the study of loss landscape of DNNs and its implication, by which a more exact and comprehensive understanding can be anticipated in the near future.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1138},
numpages = {12},
series = {NIPS '21}
}

@article{10.1155/2021/9956244,
author = {Li, Lei and Zhu, Yuquan and Cai, Tao and Niu, Dejiao and Shi, Huaji and Zou, Tingting and Huang, Chenxi},
title = {A Temporal Pool Learning Algorithm Based on Location Awareness},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9956244},
doi = {10.1155/2021/9956244},
abstract = {Hierarchical Temporal Memory is a new type of artificial neural network model, which imitates the structure and information processing flow of the human brain. Hierarchical Temporal Memory has strong adaptability and fast learning ability and becomes a hot spot in current research. Hierarchical Temporal Memory obtains and saves the temporal characteristics of input sequences by the temporal pool learning algorithm. However, the current algorithm has some problems such as low learning efficiency and poor learning effect when learning time series data. In this paper, a temporal pool learning algorithm based on location awareness is proposed. The cell selection rules based on location awareness and the dendritic updating rules based on adjacent inputs are designed to improve the learning efficiency and effect of the algorithm. Through the algorithm prototype, three different datasets are used to test and analyze the algorithm performance. The experimental results verify that the algorithm can quickly obtain the complete characteristics of the input sequence. No matter whether there are similar segments in the sequence, the proposed algorithm has higher prediction recall and precision than the existing algorithms.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@inproceedings{10.1145/3459930.3469518,
author = {Bose, Priyankar and Sleeman, William C. and Syed, Khajamoinuddin and Hagan, Michael and Palta, Jatinder and Kapoor, Rishabh and Ghosh, Preetam},
title = {Deep neural network models to automate incident triage in the radiation oncology incident learning system},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469518},
doi = {10.1145/3459930.3469518},
abstract = {Radiotherapy treatment for cancer patients involves a complex workflow involving radiation physicists, therapists, dosimetrists, physicians and nurses. Multiple hand-offs between these care team members often lead to errors varying in severity levels. Such errors are logged in incident reports stored in the Radiation Oncology Incident Learning System. Here, we present an automated incident triage and severity determination pipeline that can predict high and low severity incidents. Incident reports are collected from the US Veterans Health Affairs (VHA) and Virginia Commonwealth University (VCU) radiation oncology centers. Natural language processing (NLP) and deep learning (DL) methods, like CNN and BiLSTM, are used to predict severity using the 'Incident Description' information. Other features like 'Incident Type', 'Action taken by reporter' and 'Incident discovered at' are used to infer the best performing model. Random oversampling and minority class oversampling are employed to address large class imbalance ratios in the data.We observed that CNN performs best on both VHA data (0.83 F1-score) and the combined VCU+VHA data (0.83 F1-score) while CNN with minority sampling performs better on VCU data (0.60 F1-score) using the 'Incident Description' feature. Different feature combinations suggest that the two feature model using 'Incident Description' and 'Action taken by reporter' performs better with CNN on both the VHA (0.84 F1-score) and combined VCU+VHA data (0.81 F1-score). Multiple features were considered for the first time where the two feature model using CNNs emerge as the best suited for automating the radiotherapy incident triage and prioritization process.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {51},
numpages = {10},
keywords = {automated triage, class imbalance, deep learning, incident learning system, natural language processing, random oversampling},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1007/978-3-030-98682-7_17,
author = {Hasselbring, Arne and Baude, Andreas},
title = {Soccer Field Boundary Detection Using Convolutional Neural Networks},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_17},
doi = {10.1007/978-3-030-98682-7_17},
abstract = {Detecting the field boundary is often one of the first steps in the vision pipeline of soccer robots. Conventional methods make use of a (possibly adaptive) green classifier, selection of boundary points and possibly model fitting. We present an approach to predict the coordinates of the field boundary column-wise in the image using a convolutional neural network. This is combined with a method to let the network predict the uncertainty of its output, which allows to fit a line model in which columns are weighted according to the network’s confidence. Experiments show that the resulting models are accurate enough in different lighting conditions as well as real-time capable. Code and data are available online (, ).},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {202–213},
numpages = {12},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3302506.3310407,
author = {Shih, Oliver and Rowe, Anthony},
title = {Can a phone hear the shape of a room?},
year = {2019},
isbn = {9781450362849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302506.3310407},
doi = {10.1145/3302506.3310407},
abstract = {Understanding the location of acoustically reflective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room's acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single fixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of reflective surfaces like walls and then estimate properties like surface absorption.Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known fixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room's estimated geometry and absorption overlaid on the actual sensed space with augmented reality.},
booktitle = {Proceedings of the 18th International Conference on Information Processing in Sensor Networks},
pages = {277–288},
numpages = {12},
keywords = {active acoustic sensing, room reconstruction and mapping},
location = {Montreal, Quebec, Canada},
series = {IPSN '19}
}

@inproceedings{10.1145/1553374.1553474,
author = {Paisley, John and Carin, Lawrence},
title = {Nonparametric factor analysis with beta process priors},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553474},
doi = {10.1145/1553374.1553474},
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {777–784},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.1007/978-3-030-87199-4_50,
author = {Sedlar, Sara and Alimi, Abib and Papadopoulo, Th\'{e}odore and Deriche, Rachid and Deslauriers-Gauthier, Samuel},
title = {A Spherical Convolutional Neural Network for White Matter Structure Imaging via dMRI},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_50},
doi = {10.1007/978-3-030-87199-4_50},
abstract = {Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive and in-vivo imaging modality for probing brain white matter structure. Convolutional neural networks (CNNs) have been shown to be a powerful tool for many computer vision problems where the signals are acquired on a regular grid and where translational invariance is important. However, as we are considering dMRI signals that are acquired on a sphere, rotational invariance, rather than translational, is desired. In this work, we propose a spherical CNN model with fully spectral domain convolutional and non-linear layers. It provides rotational invariance and is adapted to the real nature of dMRI signals and uniform random distribution of sampling points. The proposed model is positively evaluated on the problem of estimation of neurite orientation dispersion and density imaging (NODDI) parameters on the data from Human Connectome Project (HCP).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {529–539},
numpages = {11},
keywords = {White matter micro-structures, Diffusion MRI, Spherical CNN},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3292500.3330861,
author = {Zhao, Jiejie and Du, Bowen and Sun, Leilei and Zhuang, Fuzhen and Lv, Weifeng and Xiong, Hui},
title = {Multiple Relational Attention Network for Multi-task Learning},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330861},
doi = {10.1145/3292500.3330861},
abstract = {Multi-task learning is a successful machine learning framework which improves the performance of prediction models by leveraging knowledge among tasks, e.g., the relationships between different tasks. Most of existing multi-task learning methods focus on guiding learning process by predefined task relationships. In fact, these methods have not fully exploited the associated relationships during the learning process. On the one hand, replacing predefined task relationships by adaptively learned ones may result in higher prediction accuracy as it can avoid the risk of misguiding caused by improperly predefined relationships. On the other hand, apart from the task relationships, feature-task dependence and feature-feature interactions could also be employed to guide the learning process. Along this line, we propose aMultiple Relational Attention Network (MRAN) framework for multi-task learning, in which three types of relationships are considered. Correspondingly, MRAN consists of three attention-based relationship learning modules: 1) a task-task relationship learning module which captures the relationships among tasks automatically and controls the positive and negative knowledge transfer adaptively; 2) a feature-feature interaction learning module that handles the complicated interactions among features; 3) a task-feature dependence learning module, which can associate the related features with target tasks separately. To evaluate the effectiveness of the proposed MARN, experiments are conducted on two public datasets and a real-world dataset crawled from a review hosting site. Experimental results demonstrate the superiority of our method over both classical and the state-of-the-art multi-task learning methods.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1123–1131},
numpages = {9},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1007/978-3-030-75765-6_49,
author = {Thanthriwatta, Thilina and Rosenblum, David S.},
title = {Instance Selection for Online Updating in Dynamic Recommender Environments},
year = {2021},
isbn = {978-3-030-75764-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75765-6_49},
doi = {10.1007/978-3-030-75765-6_49},
abstract = {Online recommender systems continuously learn from user interactions that occur in a streaming manner. A fundamental challenge of online recommendation is to select important instances (i.e., user interactions) for model updates to achieve higher prediction accuracy while omitting noisy instances. In this paper, we study&nbsp;(1) how to select the best instances and&nbsp;(2) how to effectively utilize the selected instances in dynamic recommender environments. We present two instance selection strategies based on Self-Paced Learning and rating profiles. We integrate them with Factorization Machines to perform online updates. Moreover, we study the impact of contextual information in online updating. We conducted experiments on a real-world check-in dataset, which contains temporal contextual features. Empirical results demonstrate that ox ur instance selection strategies effectively balance the trade-off between prediction accuracy and efficiency.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II},
pages = {612–624},
numpages = {13},
keywords = {Online recommender systems, Context-aware recommender systems, Instance selection}
}

@article{10.1016/j.eswa.2014.04.046,
author = {Chin, Kwai-Sang and Fu, Chao},
title = {Integrated evidential reasoning approach in the presence of cardinal and ordinal preferences and its applications in software selection},
year = {2014},
issue_date = {November, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {15},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.04.046},
doi = {10.1016/j.eswa.2014.04.046},
abstract = {A combination of cardinal and ordinal preferences in multiple-attribute decision making (MADM) demonstrates more reliability and flexibility compared with sole cardinal or ordinal preferences derived from a decision maker. This situation occurs particularly when the knowledge and experience of the decision maker, as well as the data regarding specific alternatives on certain attributes, are insufficient or incomplete. This paper proposes an integrated evidential reasoning (IER) approach to analyze uncertain MADM problems in the presence of cardinal and ordinal preferences. The decision maker provides complete or incomplete cardinal and ordinal preferences of each alternative on each attribute. Ordinal preferences are expressed as unknown distributed assessment vectors and integrated with cardinal preferences to form aggregated preferences of alternatives. Three optimization models considering cardinal and ordinal preferences are constructed to determine the minimum and maximum minimal satisfaction of alternatives, simultaneous maximum minimal satisfaction of alternatives, and simultaneous minimum minimal satisfaction of alternatives. The minimax regret rule, the maximax rule, and the maximin rule are employed respectively in the three models to generate three kinds of value functions of alternatives, which are aggregated to find solutions. The attribute weights in the three models can be precise or imprecise (i.e., characterized by six types of constraints). The IER approach is used to select the optimum software for product lifecycle management of a famous Chinese automobile manufacturing enterprise.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6718–6727},
numpages = {10},
keywords = {Multiple-attribute decision making, Integrated decision, Evidential reasoning approach, Decision analysis, Cardinal and ordinal preferences}
}

@inproceedings{10.1145/1858996.1859010,
author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {Variability modeling in the real: a perspective from the operating systems domain},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859010},
doi = {10.1145/1858996.1859010},
abstract = {Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {variability modeling, product line architectures, feature models, empirical software engineering, configuration},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2961111.2962600,
author = {Al-Subaihin, A. A. and Sarro, F. and Black, S. and Capra, L. and Harman, M. and Jia, Y. and Zhang, Y.},
title = {Clustering Mobile Apps Based on Mined Textual Features},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962600},
doi = {10.1145/2961111.2962600},
abstract = {Context: Categorising software systems according to their functionality yields many benefits to both users and developers. Goal: In order to uncover the latent clustering of mobile apps in app stores, we propose a novel technique that measures app similarity based on claimed behaviour. Method: Features are extracted using information retrieval augmented with ontological analysis and used as attributes to characterise apps. These attributes are then used to cluster the apps using agglomerative hierarchical clustering. We empirically evaluate our approach on 17,877 apps mined from the BlackBerry and Google app stores in 2014. Results: The results show that our approach dramatically improves the existing categorisation quality for both Blackberry (from 0.02 to 0.41 on average) and Google (from 0.03 to 0.21 on average) stores. We also find a strong Spearman rank correlation (ρ= 0.96 for Google and ρ= 0.99 for BlackBerry) between the number of apps and the ideal granularity within each category, indicating that ideal granularity increases with category size, as expected. Conclusions: Current categorisation in the app stores studied do not exhibit a good classification quality in terms of the claimed feature space. However, a better quality can be achieved using a good feature extraction technique and a traditional clustering method.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {38},
numpages = {10},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1007/978-3-030-67832-6_30,
author = {Wang, Fei and Ding, Youdong and Liang, Huan and Wen, Jing},
title = {Discriminative and Selective Pseudo-Labeling for Domain Adaptation},
year = {2021},
isbn = {978-3-030-67831-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67832-6_30},
doi = {10.1007/978-3-030-67832-6_30},
abstract = {Unsupervised domain adaptation aims to transfer the knowledge of source domain to a related but not labeled target domain. Due to the lack of label information of target domain, most existing methods train a weak classifier and directly apply to pseudo-labeling which may downgrade adaptation performance. To address this problem, in this paper, we propose a novel discriminative and selective pseudo-labeling (DSPL) method for domain adaptation. Specifically, we first match the marginal distributions of two domains and increase inter-class distance simultaneously. Then a feature transformation method is proposed to learn a low-dimensional transfer subspace which is discriminative enough. Finally, after data has formed good clusters, we introduce a structured prediction based selective pseudo-labeling strategy which is able to sufficiently exploit target data structure. We conduct extensive experiments on three popular visual datasets, demonstrating the good domian adaptation performance of our method.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I},
pages = {365–377},
numpages = {13},
keywords = {Discriminative learned subspace, Pseudo-labeling, Unsupervised domain adaptation},
location = {Prague, Czech Republic}
}

@inproceedings{10.1007/978-3-030-29888-3_17,
author = {Li, Jing and Wong, Yongkang and Sim, Terence},
title = {Learning Controllable Face Generator from Disjoint Datasets},
year = {2019},
isbn = {978-3-030-29887-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29888-3_17},
doi = {10.1007/978-3-030-29888-3_17},
abstract = {Recently, GANs have become popular for synthesizing photorealistic facial images with desired facial attributes. However, crucial to the success of such networks is the availability of large-scale datasets that are fully-attributed, i.e.,&nbsp;datasets in which the Cartesian product of all attribute values is present, as otherwise the learning becomes skewed. Such fully-attributed datasets are impractically expensive to collect. Many existing datasets are only partially-attributed, and do not have any subjects in common. It thus becomes important to be able to jointly learn from such datasets. In this paper, we propose a GAN-based facial image generator that can be trained on partially-attributed disjoint datasets. The key idea is to use a smaller, fully-attributed dataset to bridge the learning. Our generator (i) provides independent control of multiple attributes, and (ii) renders photorealistic facial images with target attributes.},
booktitle = {Computer Analysis of Images and Patterns: 18th International Conference, CAIP 2019, Salerno, Italy, September 3–5, 2019, Proceedings, Part I},
pages = {209–223},
numpages = {15},
keywords = {Disjoint-learning, Disentanglement, Face generator},
location = {Salerno, Italy}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {65D17, 65D05, 41A10, 41A05, Image super-resolution, Up and down-sampling projection block, Multi-scale attention residual block}
}

@article{10.1016/j.eswa.2017.07.021,
author = {Zhou, Feng and Jiao, Jianxin Roger and Yang, Xi Jessie and Lei, Baiying},
title = {Augmenting feature model through customer preference mining by hybrid sentiment analysis},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {89},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.07.021},
doi = {10.1016/j.eswa.2017.07.021},
abstract = {We use sentiment analysis of online product reviewers to extract customer preference information.The proposed sentiment analysis method is a hybrid combination of various affective lexicons.We adopt the commented features from product users to enhance the basic feature.We incorporate the customer preference information as attribute into the model.We demonstrate the feasibility and potential of the proposed method via an application case. A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {306–317},
numpages = {12},
keywords = {Sentiment analysis, Product line planning, Feature model, Customer preference mining}
}

@article{10.1145/2533670.2533673,
author = {Wetzler, Philipp and Bethard, Steven and Leary, Heather and Butcher, Kirsten and Bahreini, Soheil Danesh and Zhao, Jin and Martin, James H. and Sumner, Tamara},
title = {Characterizing and Predicting the Multifaceted Nature of Quality in Educational Web Resources},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/2533670.2533673},
doi = {10.1145/2533670.2533673},
abstract = {Efficient learning from Web resources can depend on accurately assessing the quality of each resource. We present a methodology for developing computational models of quality that can assist users in assessing Web resources. The methodology consists of four steps: 1) a meta-analysis of previous studies to decompose quality into high-level dimensions and low-level indicators, 2) an expert study to identify the key low-level indicators of quality in the target domain, 3) human annotation to provide a collection of example resources where the presence or absence of quality indicators has been tagged, and 4) training of a machine learning model to predict quality indicators based on content and link features of Web resources. We find that quality is a multifaceted construct, with different aspects that may be important to different users at different times. We show that machine learning models can predict this multifaceted nature of quality, both in the context of aiding curators as they evaluate resources submitted to digital libraries, and in the context of aiding teachers as they develop online educational resources. Finally, we demonstrate how computational models of quality can be provided as a service, and embedded into applications such as Web search.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = oct,
articleno = {15},
numpages = {25},
keywords = {meta-analysis, machine learning, interactive digital library interface, expert annotation, Open education resources}
}

@inproceedings{10.1145/3474085.3475403,
author = {Deng, Xiangwen and Zhu, Junlin and Yang, Shangming},
title = {SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475403},
doi = {10.1145/3474085.3475403},
abstract = {Emotion recognition based on EEG (electroencephalography) has been widely used in human-computer interaction, distance education and health care. However, the conventional methods ignore the adjacent and symmetrical characteristics of EEG signals, which also contain salient information related to emotion. In this paper, a spatial folding ensemble network (SFE-Net) is presented for EEG feature extraction and emotion recognition. Firstly, for the undetected area between EEG electrodes, an improved Bicubic-EEG interpolation algorithm is developed for EEG channels information completion, which allows us to extract a wider range of adjacent space features. Then, motivated by the spatial symmetric mechanism of human brain, we fold the input EEG channels data with five different symmetrical strategies, which enable the proposed network to extract the information of space features of EEG signals more effectively. Finally, a 3DCNN-based spatial, temporal extraction, and a multi-voting strategy of ensemble learning are integrated to model a new neural network. With this network, the spatial features of different symmetric folding signals can be extracted simultaneously, which greatly improves the robustness and accuracy of emotion recognition. The experimental results on DEAP and SEED datasets show that the proposed algorithm has comparable performance in terms of recognition accuracy.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2391–2400},
numpages = {10},
keywords = {interpolation, folding, emotion recognition, EEG},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.5555/2888619.2888848,
author = {Rabe, Markus and Dross, Felix},
title = {A reinforcement learning approach for a decision support system for logistics networks},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {This paper presents the architecture and working principles of a Decision Support System (DSS) for logistics networks. The system relies on a data-driven discrete-event simulation model. A brief introduction to Reinforcement Learning (RL) and an explanation of the adoption of RL to the concepts of the DSS is given. An illustration of the realization is presented using a specific aspect of a logistics network. The logistics network is described in a data model which is represented by database tables. The tables are used to dynamically instantiate the simulation model. The authors describe how SQL queries can be used to model actions of an RL agent. A Data Warehouse can be used to measure Key Performance Indicators on the simulation output data of the simulation model, which can be used as a reward criterion for the RL agent. The paper presents a basis for the ongoing development of an RL agent.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {2020–2032},
numpages = {13},
location = {Huntington Beach, California},
series = {WSC '15}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {99-00, 00-01, Deep neural network, Weighted cross-entropy loss, Triplet loss, Deep feature representation, Software defect prediction}
}

@article{10.1145/3363818,
author = {Horne, Benjamin D. and N\o{}rregaard, Jeppe and Adali, Sibel},
title = {Robust Fake News Detection Over Time and Attack},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3363818},
doi = {10.1145/3363818},
abstract = {In this study, we examine the impact of time on state-of-the-art news veracity classifiers. We show that, as time progresses, classification performance for both unreliable and hyper-partisan news classification slowly degrade. While this degradation does happen, it happens slower than expected, illustrating that hand-crafted, content-based features, such as style of writing, are fairly robust to changes in the news cycle. We show that this small degradation can be mitigated using online learning. Last, we examine the impact of adversarial content manipulation by malicious news producers. Specifically, we test three types of attack based on changes in the input space and data availability. We show that static models are susceptible to content manipulation attacks, but online models can recover from such attacks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {7},
numpages = {23},
keywords = {robust machine learning, misleading news, misinformation, fake news detection, disinformation, concept drift, biased news, adversarial machine learning, Fake news}
}

@article{10.1007/s00034-021-01674-0,
author = {Naiemi, Fatemeh and Ghods, Vahid and Khalesi, Hassan},
title = {MOSTL: An Accurate Multi-Oriented Scene Text Localization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01674-0},
doi = {10.1007/s00034-021-01674-0},
abstract = {Automatic text localization in natural environments is the main element of many applications including self-driving cars, identifying vehicles, and providing scene information to visually impaired people. However, text in the natural and irregular scene has different degrees in orientations, shapes, and colors that make it difficult to detect. In this paper, an accurate multi-oriented scene text localization (MOSTL) is presented to obtain high efficiency of detecting text-based on convolutional neural networks. In the proposed method, an improved ReLU layer (i.ReLU) and an improved inception layer (i.inception) were introduced. Firstly, the proposed structure is used to extract low-level visual features. Then, an extra layer has been used to improve the feature extraction. The i.ReLU and i.inception layers have improved valuable information in text detection. The i.ReLU layers cause to extract some low-level features appropriately. The i.inception layers (specially 3 \texttimes{} 3 convolutions) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer (without inception layers). The output of i.ReLU layers and i.inception layers was fed to an extra layer, which enables MOSTL to detect multi-oriented even curved and vertical texts. We conducted text detection experiments on well-known databases including ICDAR 2019, ICDAR 2017, ICDAR 2015, ICDAR 2003, and MSRA-TD500. MOSTL results yielded performance improvement remarkably.},
journal = {Circuits Syst. Signal Process.},
month = sep,
pages = {4452–4473},
numpages = {22},
keywords = {Curved text, Improved ReLU layer, Improved inception layer, Convolutional neural network, Multi-oriented, Object detection, Scene text localization}
}

@inproceedings{10.1007/978-3-642-28509-7_11,
author = {Felfernig, Alexander and Zehentner, Christoph and Ninaus, Gerald and Grabner, Harald and Maalej, Walid and Pagano, Dennis and Weninger, Leopold and Reinfrank, Florian},
title = {Group decision support for requirements negotiation},
year = {2011},
isbn = {9783642285080},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28509-7_11},
doi = {10.1007/978-3-642-28509-7_11},
abstract = {Requirements engineering is one of the most critical phases in software development. Requirements verbalize decision alternatives that are negotiated by stakeholders. In this paper we present the results of an empirical analysis of the effects of applying group recommendation technologies to requirements negotiation. This analysis has been conducted within the scope of software development projects at our university where development teams were supported with group recommendation technologies when deciding which requirements should be implemented. A major result of the study is that group recommendation technologies can improve the perceived usability (in certain cases) and the perceived quality of decision support. Furthermore, it is not recommended to disclose preferences of individual group members at the beginning of a decision process --- this could lead to an insufficient exchange of decision-relevant information.},
booktitle = {Proceedings of the 19th International Conference on Advances in User Modeling},
pages = {105–116},
numpages = {12},
keywords = {requirements engineering, group recommender systems},
location = {Girona, Spain},
series = {UMAP'11}
}

@inproceedings{10.5555/3305890.3306102,
author = {Zhao, He and Du, Lan and Buntine, Wray},
title = {Leveraging node attributes for incomplete relational data},
year = {2017},
publisher = {JMLR.org},
abstract = {Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the state-of-the-art link prediction results, especially with highly incomplete relational data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4072–4081},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.jbi.2011.01.011,
author = {Caicedo, Juan C. and Gonz\'{a}lez, Fabio A. and Romero, Eduardo},
title = {Content-based histopathology image retrieval using a kernel-based semantic annotation framework},
year = {2011},
issue_date = {August, 2011},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {44},
number = {4},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.01.011},
doi = {10.1016/j.jbi.2011.01.011},
abstract = {Large amounts of histology images are captured and archived in pathology departments due to the ever expanding use of digital microscopy. The ability to manage and access these collections of digital images is regarded as a key component of next generation medical imaging systems. This paper addresses the problem of retrieving histopathology images from a large collection using an example image as query. The proposed approach automatically annotates the images in the collection, as well as the query images, with high-level semantic concepts. This semantic representation delivers an improved retrieval performance providing more meaningful results. We model the problem of automatic image annotation using kernel methods, resulting in a unified framework that includes: (1) multiple features for image representation, (2) a feature integration and selection mechanism (3) and an automatic semantic image annotation strategy. An extensive experimental evaluation demonstrated the effectiveness of the proposed framework to build meaningful image representations for learning and useful semantic annotations for image retrieval.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {519–528},
numpages = {10},
keywords = {Kernels, Kernel alignment, Image retrieval, Histopathology, Histology, Biomedical images, Auto-annotation}
}

@inbook{10.5555/3454287.3454826,
author = {Zhang, Jiong and Yu, Hsiang-Fu and Dhillon, Inderjit S.},
title = {AutoAssist: a framework to accelerate training of deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Neural Networks (DNNs) have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as the Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss model such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a non-blocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {539},
numpages = {11}
}

@article{10.1016/j.patrec.2010.02.001,
author = {Balkema, Wietse and van der Heijden, Ferdi},
title = {Music playlist generation by assimilating GMMs into SOMs},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {11},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2010.02.001},
doi = {10.1016/j.patrec.2010.02.001},
abstract = {A method for music playlist generation, using assimilated Gaussian mixture models (GMMs) in self organizing maps (SOMs) is presented. Traditionally, the neurons in a SOM are represented by vectors, but in this paper we propose to use GMMs instead. To this end, we introduce a method to adapt a GMM such that its distance to a second GMM decreases at a controllable rate. Self organization is demonstrated using a small music database and a music classification task.},
journal = {Pattern Recogn. Lett.},
month = aug,
pages = {1396–1402},
numpages = {7},
keywords = {Self organization, Music playlists, Genre classification, Gaussian mixtures, Earth mover's distance}
}

@article{10.1109/TPAMI.2020.2972281,
author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
title = {Vision-Language Navigation Policy Learning and Adaptation},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {43},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2020.2972281},
doi = {10.1109/TPAMI.2020.2972281},
abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {4205–4216},
numpages = {12}
}

@inproceedings{10.1145/3177148.3180085,
author = {Surendranath, Ajay and Jayagopi, Dinesh Babu},
title = {Curriculum Learning for Depth Estimation with Deep Convolutional Neural Networks},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180085},
doi = {10.1145/3177148.3180085},
abstract = {Curriculum learning is a machine learning technique adapted from the way humans acquire knowledge and skills, initially mastering simple tasks and progressing to more complex tasks. The work explores curriculum training by creating multiple levels of dataset with increasing complexity on which the trainings are performed. The experiments demonstrated that there is an average of 12% improvement test loss when compared to a non-curriculum approach. The experiment also demonstrates the advantage of creating synthetic dataset and how it aids in the overall improvement of accuracy. An improvement of 26% is attained on the test error loss when curriculum trained model was compared to training on a limited real world dataset. The work also goes onto propose a novel learning approach, the Self Paced Learning approach with Error-Diversity (SPL-ED) An overall reduction of 32% in the test loss is observed when compared to the non-curriculum training limited to real-world dataset.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {95–100},
numpages = {6},
keywords = {Depth Estimation, Curriculum Learning},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@inproceedings{10.5555/2984093.2984119,
author = {Caron, Fran\c{c}ois and Doucet, Arnaud},
title = {Bayesian nonparametric models on decomposable graphs},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. These models are attractive because they ensure exchangeability (over samples). We propose here extensions of these models where the dependency between samples is given by a known decomposable graph. These models have appealing properties and can be easily learned using Monte Carlo techniques.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems},
pages = {225–233},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@article{10.1016/j.dss.2010.12.009,
author = {Ribeiro, Rita A. and Moreira, Ana M. and van den Broek, Pim and Pimentel, Afonso},
title = {Hybrid assessment method for software engineering decisions},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2010.12.009},
doi = {10.1016/j.dss.2010.12.009},
abstract = {During software development, many decisions need to be made to guarantee the satisfaction of the stakeholders' requirements and goals. The full satisfaction of all of these requirements and goals may not be possible, requiring decisions over conflicting human interests as well as technological alternatives, with an impact on the quality and cost of the final solution. This work aims at assessing the suitability of multi-criteria decision making (MCDM) methods to support software engineers' decisions. To fulfil this aim, a HAM (Hybrid Assessment Method) is proposed, which gives its user the ability to perceive the influence different decisions may have on the final result. HAM is a simple and efficient method that combines one single pairwise comparison decision matrix (to determine the weights of criteria) with one classical weighted decision matrix (to prioritize the alternatives). To avoid consistency problems regarding the scale and the prioritization method, HAM uses a geometric scale for assessing the criteria and the geometric mean for determining the alternative ratings.},
journal = {Decis. Support Syst.},
month = apr,
pages = {208–219},
numpages = {12},
keywords = {Software engineering, Non-functional software requirements, Multi-criteria decision making, Aggregation operators}
}

@inproceedings{10.1007/978-3-642-37247-6_43,
author = {de Pi\~{n}erez Reyes, Ra\'{u}l Ernesto Guti\'{e}rrez and Frias, Juan Francisco D\'{\i}az},
title = {Building a discourse parser for informal mathematical discourse in the context of a controlled natural language},
year = {2013},
isbn = {9783642372469},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37247-6_43},
doi = {10.1007/978-3-642-37247-6_43},
abstract = {The lack of specific data sets makes difficult the discourse parsing for Informal Mathematical Discourse (IMD). In this paper, we propose a data driven approach to identify arguments and connectives in an IMD structure within the context of Controlled Natural Language (CNL). Our approach follows a low-level discourse parsing under Peen Discourse TreeBank (PDTB) guidelines. Three classifiers have been trained: one that identifies the Arg2, other that locates the relative position of Arg1 and a third that identifies the (Arg1 and Arg2) arguments of each connective. These classifiers are instances of Support Vector Machines (SVMs), fed from an own Mathematical TreeBank. Finally, our approach defines an End-to-End discourse parser into IMD, whose results will be used to classify of informal deductive proofs via the low level discourse in IMD processing.},
booktitle = {Proceedings of the 14th International Conference on Computational Linguistics and Intelligent Text Processing - Volume Part I},
pages = {533–544},
numpages = {12},
keywords = {support vector machines, informal discourse mathematical, discourse parser, controlled natural language, connectives, arguments},
location = {Samos, Greece},
series = {CICLing'13}
}

@article{10.1007/s10846-018-0839-z,
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
title = {An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {95},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-018-0839-z},
doi = {10.1007/s10846-018-0839-z},
abstract = {The main goal of this article is to present COACH (COrrective Advice Communicated by Humans), a new learning framework that allows non-expert humans to advise an agent while it interacts with the environment in continuous action problems. The human feedback is given in the action domain as binary corrective signals (increase/decrease the current action magnitude), and COACH is able to adjust the amount of correction that a given action receives adaptively, taking state-dependent past feedback into consideration. COACH also manages the credit assignment problem that normally arises when actions in continuous time receive delayed corrections. The proposed framework is characterized and validated extensively using four well-known learning problems. The experimental analysis includes comparisons with other interactive learning frameworks, with classical reinforcement learning approaches, and with human teleoperators trying to solve the same learning problems by themselves. In all the reported experiments COACH outperforms the other methods in terms of learning speed and final performance. It is of interest to add that COACH has been applied successfully for addressing a complex real-world learning problem: the dribbling of the ball by humanoid soccer players.},
journal = {J. Intell. Robotics Syst.},
month = jul,
pages = {77–97},
numpages = {21},
keywords = {Learning from demonstration, Interactive machine learning, Human teachers, Human feedback, Decision making systems}
}

@article{10.1016/j.neucom.2019.05.009,
author = {Song, Shaoyue and Yu, Hongkai and Miao, Zhenjiang and Guo, Dazhou and Ke, Wei and Ma, Cong and Wang, Song},
title = {An easy-to-hard learning strategy for within-image co-saliency detection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.009},
doi = {10.1016/j.neucom.2019.05.009},
journal = {Neurocomput.},
month = sep,
pages = {166–176},
numpages = {11},
keywords = {Multiple instance learning, Easy-to-hard learning, Within-image co-saliency}
}

@article{10.1016/j.patcog.2017.10.005,
author = {Zhou, Sanping and Wang, Jinjun and Meng, Deyu and Xin, Xiaomeng and Li, Yubing and Gong, Yihong and Zheng, Nanning},
title = {Deep self-paced learning for person re-identification},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.005},
doi = {10.1016/j.patcog.2017.10.005},
abstract = {We propose a novel deep self-paced learning algorithm to supervise the learning of deep neural network, in which a soft polynomial regularizer term is proposed to gradually involve the faithful samples into training process in a self-paced manner.We optimize the gradient back-propagation of relative distance metric by introducing a symmetric regularizer term, which can convert the back-propagation from the asymmetric mode to a symmetric one.We build an effective part-based deep neural network, in which features of different body parts are first discriminately learned in the convolutional layers and then fused in the fully connected layers. Person re-identification(Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning(DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning(SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.},
journal = {Pattern Recogn.},
month = apr,
pages = {739–751},
numpages = {13},
keywords = {Self-paced learning, Person re-identification, Metric learning, Convolutional neural network}
}

@inproceedings{10.1145/2492517.2492632,
author = {Alowibdi, Jalal S. and Buy, Ugo A. and Yu, Philip},
title = {Language independent gender classification on Twitter},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492632},
doi = {10.1145/2492517.2492632},
abstract = {Online Social Networks (OSNs) generate a huge volume of user-originated texts. Gender classification can serve multiple purposes. For example, commercial organizations can use gender classification for advertising. Law enforcement may use gender classification as part of legal investigations. Others may use gender information for social reasons. Here we explore language independent gender classification. Our approach predicts gender using five color-based features extracted from Twitter profiles (e.g., the background color in a user's profile page). Most other methods for gender prediction are typically language dependent. Those methods use high-dimensional spaces consisting of unique words extracted from such text fields as postings, user names, and profile descriptions. Our approach is independent of the user's language, efficient, and scalable, while attaining a good level of accuracy. We prove the validity of our approach by examining different classifiers over a large dataset of Twitter profiles.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {739–743},
numpages = {5},
keywords = {social network, language independent, color-based features, application for social network},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@article{10.1007/s11042-019-7251-y,
author = {Mei, Jianhan and Wu, Ziming and Chen, Xiang and Qiao, Yu and Ding, Henghui and Jiang, Xudong},
title = {DeepDeblur: text image recovery from blur to sharp},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7251-y},
doi = {10.1007/s11042-019-7251-y},
abstract = {Digital images could be degraded by a variety of blur during the image acquisition (i.e. relative motion of cameras, electronic noise, capturing defocus, and so on). Blurring images can be computationally modeled as the result of a convolution process with the corresponding blur kernel and thus, image deblurring can be regarded as a deconvolution operation. In this paper, we explore to deblur images by approximating blind deconvolutions using a deep neural network. Different deep neural network structures are investigated to evaluate their deblurring capabilities, which contributes to the optimal design of a network architecture. It is found that shallow and narrow networks are not capable of handling complex motion blur. We thus, present a deep network with 20 layers to cope with text image blur. In addition, a novel network structure with Sequential Highway Connections (SHC) is leveraged to gain superior convergence. The experiment results demonstrate the state-of-the-art performance of the proposed framework with the higher visual quality of the delurred images.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18869–18885},
numpages = {17},
keywords = {Text Deblurring, Short connection, Convolutional Neural Network (CNN), Blind deconvolution}
}

@inproceedings{10.5555/3540261.3540920,
author = {Richards, Dominic and Kuzborskij, Ilja},
title = {Stability &amp; generalisation of gradient descent for shallow neural networks without the neural tangent kernel},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We revisit on-average algorithmic stability of Gradient Descent (GD) for training overparameterised shallow neural networks and prove new generalisation and excess risk bounds without the Neural Tangent Kernel (NTK) or Polyak-\L{}ojasiewicz (PL) assumptions. In particular, we show oracle type bounds which reveal that the generalisation and excess risk of GD is controlled by an interpolating network with the shortest GD path from initialisation (in a sense, an interpolating network with the smallest relative norm). While this was known for kernelised interpolants, our proof applies directly to networks trained by GD without intermediate kernelisation. At the same time, by relaxing oracle inequalities developed here we recover existing NTK-based risk bounds in a straightforward way, which demonstrates that our analysis is tighter. Finally, unlike most of the NTK-based analyses we focus on regression with label noise and show that GD with early stopping is consistent.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {659},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/2976456.2976462,
author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
title = {Multi-task feature learning},
year = {2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the latter step we learn commonacross-tasks representations and in the former step we learn task-specific functions using these representations. We report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently. Our algorithm can also be used, as a special case, to simply select – not learn – a few common features across the tasks.},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {41–48},
numpages = {8},
location = {Canada},
series = {NIPS'06}
}

@inproceedings{10.1007/978-3-030-69532-3_4,
author = {Huang, Bowen and Zhou, Jinjia and Yan, Xiao and Jing, Ming’e and Wan, Rentao and Fan, Yibo},
title = {CS-MCNet: A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_4},
doi = {10.1007/978-3-030-69532-3_4},
abstract = {In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames (as shown in Fig.&nbsp;1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22&nbsp;dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up&nbsp;to three orders of magnitude faster than traditional iterative methods.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {54–67},
numpages = {14},
location = {Kyoto, Japan}
}

@inproceedings{10.5555/645882.672394,
author = {Simon, Daniel and Eisenbarth, Thomas},
title = {Evolutionary Introduction of Software Product Lines},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines have proved to be a successful and efficient means for managing the development of software in industry. The significant benefits over traditional software architectures have the potential to convince software companies to adopt the product line approach for their existing products. In that case, the question arises how to best convert the existing products into a software product line. For several reasons, an evolutionary approach is desirable. But so far, there is little guidance on the evolutionary introduction of software product lines.In this paper, we propose a lightweight iterative process supporting the incremental introduction of product line concepts for existing software products. Starting with the analysis of the legacy code, we assess what parts of the software can be restructured for product line needs at reasonable costs. For the analysis of the products, we use feature analysis, a reengineering technique tailored to the specific needs of the initiation of software product lines.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {272–282},
numpages = {11},
series = {SPLC 2}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Reuse, Requirements, Product line, Feature model, Configuration}
}

@inproceedings{10.1007/978-3-030-69532-3_29,
author = {Priisalu, Maria and Paduraru, Ciprian and Pirinen, Aleksis and Sminchisescu, Cristian},
title = {Semantic Synthesis of Pedestrian Locomotion},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_29},
doi = {10.1007/978-3-030-69532-3_29},
abstract = {We present a model for generating 3d articulated pedestrian locomotion in urban scenarios, with synthesis capabilities informed by the 3d scene semantics and geometry. We reformulate pedestrian trajectory forecasting as a structured reinforcement learning (RL) problem. This allows us to naturally combine prior knowledge on collision avoidance, 3d human motion capture and the motion of pedestrians as observed e.g. in Cityscapes, Waymo or simulation environments like Carla. Our proposed RL-based model allows pedestrians to accelerate and slow down to avoid imminent danger (e.g. cars), while obeying human dynamics learnt from in-lab motion capture datasets. Specifically, we propose a hierarchical model consisting of a semantic trajectory policy network that provides a distribution over possible movements, and a human locomotion network that generates 3d human poses in each step. The RL-formulation allows the model to learn even from states that are seldom exhibited in the dataset, utilizing all of the available prior and scene information. Extensive evaluations using both real and simulated data illustrate that the proposed model is on par with recent models such as S-GAN, ST-GAT and S-STGCNN in pedestrian forecasting, while outperforming these in collision avoidance. We also show that our model can be used to plan goal reaching trajectories in urban scenes with dynamic actors.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {470–487},
numpages = {18},
location = {Kyoto, Japan}
}

@article{10.1016/j.eswa.2009.09.003,
author = {Beg, Azam and Chandana Prasad, P. W.},
title = {Prediction of area and length complexity measures for binary decision diagrams},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.09.003},
doi = {10.1016/j.eswa.2009.09.003},
abstract = {Measuring the complexity of functions that represent digital circuits in non-uniform computation models is an important area of computer science theory. This paper presents a comprehensive set of machine learnt models for predicting the complexity properties of circuits represented by binary decision diagrams. The models are created using Monte Carlo data for a wide range of circuit inputs and number of minterms. The models predict number of nodes as representations of circuit size/area and path lengths: average path length, longest path length, and shortest path length. The models have been validated using an arbitrarily-chosen subset of ISCAS-85 and MCNC-91 benchmark circuits. The models yield reasonably low RMS errors for predictions, so they can be used to estimate complexity metrics of circuits without having to synthesize them.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2864–2873},
numpages = {10},
keywords = {Path length complexity, Neural network modeling, Machine learning, Complexity prediction, Circuit complexity, Binary decision diagrams, Area complexity}
}

@inproceedings{10.5555/3305381.3305544,
author = {Hu, Changwei and Rai, Piyush and Carin, Lawrence},
title = {Deep generative models for relational data with side information},
year = {2017},
publisher = {JMLR.org},
abstract = {We present a probabilistic framework for overlapping community discovery and link prediction for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of latent features/communities for each node, providing superior link prediction performance on more complex networks and better interpretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs sampling. Moreover, inference cost scales in the number of edges which is attractive for massive but sparse networks. Our framework is also easily extendable to model weighted networks with count-valued edges. We compare with various state-of-the-art methods and report results, both quantitative and qualitative, on several benchmark data sets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1578–1586},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3540261.3540688,
author = {Conwell, Colin and Mayo, David and Buice, Michael A. and Katz, Boris and Alvarez, George A. and Barbu, Andrei},
title = {Neural regression, representational similarity, model zoology &amp; neural Taskonomy at scale in rodent visual cortex},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {How well do deep neural networks fare as models of mouse visual cortex? A majority of research to date suggests results far more mixed than those produced in the modeling of primate visual cortex. Here, we perform a large-scale benchmarking of dozens of deep neural network models in mouse visual cortex with both representational similarity analysis and neural regression. Using the Allen Brain Observatory's 2-photon calcium-imaging dataset of activity in over 6,000 reliable rodent visual cortical neurons recorded in response to natural scenes, we replicate previous findings and resolve previous discrepancies, ultimately demonstrating that modern neural networks can in fact be used to explain activity in the mouse visual cortex to a more reasonable degree than previously suggested. Using our benchmark as an atlas, we offer preliminary answers to overarching questions about levels of analysis (e.g. do models that better predict the representations of individual neurons also predict representational similarity across neural populations?); questions about the properties of models that best predict the visual system overall (e.g. is convolution or category-supervision necessary to better predict neural activity?); and questions about the mapping between biological and artificial representations (e.g. does the information processing hierarchy in deep nets match the anatomical hierarchy of mouse visual cortex?). Along the way, we catalogue a number of models (including vision transformers, MLP-Mixers, normalization free networks, Taskonomy encoders and self-supervised models) outside the traditional circuit of convolutional object recognition. Taken together, our results provide a reference point for future ventures in the deep neural network modeling of mouse visual cortex, hinting at novel combinations of mapping method, architecture, and task to more fully characterize the computational motifs of visual representation in a species so central to neuroscience, but with a perceptual physiology and ecology markedly different from the ones we study in primates.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {427},
numpages = {18},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-27544-0_8,
author = {Szemenyei, Marton and Estivill-Castro, Vladimir},
title = {Real-Time Scene Understanding Using Deep Neural Networks for RoboCup SPL},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_8},
doi = {10.1007/978-3-030-27544-0_8},
abstract = {Convolutional neural networks (CNNs) are the state-of-the-art method for most computer vision tasks. But, the deployment of CNNs on mobile or embedded platforms is challenging because of CNNs’ excessive computational requirements. We present an end-to-end neural network solution to scene understanding for robot soccer. We compose two key neural networks: one to perform semantic segmentation on an image, and another to propagate class labels between consecutive frames. We trained our networks on synthetic datasets and fine-tuned them on a set consisting of real images from a Nao robot. Furthermore, we investigate and evaluate several practical methods for increasing the efficiency and performance of our networks. Finally, we present RoboDNN, a C++ neural network library designed for fast inference on the Nao robots.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {96–108},
numpages = {13},
keywords = {Neural networks, Semantic segmentation, Deep learning, Computer vision},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {UBM weight posterior probability supervectors, Sparse representation, Score level fusion, SVM, Prosodic features, Polynomial expansion, Pitch, Maximum likelihood linear regression, Harmonic structure, Gender recognition, GMM, Formant, Age recognition}
}

@article{10.1007/s11042-016-4043-5,
author = {Qu, Tao and Zhang, Quanyuan and Sun, Shilei},
title = {Vehicle detection from high-resolution aerial images using spatial pyramid pooling-based deep convolutional neural networks},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-016-4043-5},
doi = {10.1007/s11042-016-4043-5},
abstract = {In recent years, vehicle detection from aerial images obtained using unmanned aerial vehicles (UAVs) has become a research focus in image processing as remote sensing platforms on UAVs are rapidly popularised. This study proposes a detection algorithm using a deep convolutional neural network (DCNN) based on multi-scale spatial pyramid pooling (SPP). By using multi-scale SPP models to sample characteristic patterns with different sizes, feature vectors with a fixed length are generated. This avoids the stretching- or cropping-induced deformation of input images of different sizes, thus improving the detection effect. In addition, an imaging pre-processing algorithm based on maximum normed gradient (NG) with multiple thresholds is proposed. By using this algorithm, this research restores the edges of objects disturbed by clutter in the environment. Meanwhile, the raised candidate object extraction algorithm based on the maximum binarized NG entails fewer computations as it generates fewer candidate windows. Experimental results indicate that the multi-scale SPP based DCNN can better adapt to input images of different sizes to learn of the multi-scale characteristics of objects, thus further improving the detection effect.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {21651–21663},
numpages = {13},
keywords = {Vehicle detection, Unmanned aerial vehicle, Multi-scale spatial pyramid, Deep convolutional neural network}
}

@inproceedings{10.5555/3042573.3042640,
author = {Wang, Yingjian and Carin, Lawrence},
title = {L\'{e}vy measure decompositions for the beta and gamma processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop new representations for the L\'{e}vy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on L\'{e}vy processes, as these are of increasing importance in machine learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {499–506},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Stability, Requirements engineering, Product lines, Maintainability, Crosscutting, Concern metrics}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@article{10.1007/s11263-019-01278-x,
author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
title = {Representation Learning on Unit Ball with 3D Roto-translational Equivariance},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01278-x},
doi = {10.1007/s11263-019-01278-x},
abstract = {Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges. In this work, we propose a novel ‘volumetric convolution’ operation that can effectively model and convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1612–1634},
numpages = {23},
keywords = {Deep learning, Zernike polynomials, Volumetric convolution, 3D moments, Convolution neural networks}
}

@article{10.1016/j.compbiomed.2016.01.002,
author = {Bokov, Plamen and Mahut, Bruno and Flaud, Patrice and Delclaux, Christophe},
title = {Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population},
year = {2016},
issue_date = {March 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {70},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.01.002},
doi = {10.1016/j.compbiomed.2016.01.002},
abstract = {BackgroundRespiratory diseases in children are a common reason for physician visits. A diagnostic difficulty arises when parents hear wheezing that is no longer present during the medical consultation. Thus, an outpatient objective tool for recognition of wheezing is of clinical value. MethodWe developed a wheezing recognition algorithm from recorded respiratory sounds with a Smartphone placed near the mouth. A total of 186 recordings were obtained in a pediatric emergency department, mostly in toddlers (mean age 20 months). After exclusion of recordings with artefacts and those with a single clinical operator auscultation, 95 recordings with the agreement of two operators on auscultation diagnosis (27 with wheezing and 68 without) were subjected to a two phase algorithm (signal analysis and pattern classifier using machine learning algorithms) to classify records. ResultsThe best performance (71.4% sensitivity and 88.9% specificity) was observed with a Support Vector Machine-based algorithm. We further tested the algorithm over a set of 39 recordings having a single operator and found a fair agreement (kappa=0.28, CI95% 0.12, 0.45) between the algorithm and the operator. ConclusionsThe main advantage of such an algorithm is its use in contact-free sound recording, thus valuable in the pediatric population. We recorded by Smartphone respiratory sounds at the mouth in pediatric population.Two clinical operators validated the presence or absence of wheezing in 97 toddlers.We used Short-Time Fourier Transform and SVM classifier for wheeze recognition.71.4% Sensitivity and 88.9% Specificity were observed for wheeze detection.An independent test found a fair agreement with a clinical operator.},
journal = {Comput. Biol. Med.},
month = mar,
pages = {40–50},
numpages = {11},
keywords = {Support vector machine, ROC analysis, Childhood asthma, Bronchiolitis, Automated wheezing detection}
}

@inproceedings{10.5555/3305381.3305479,
author = {Dawson, Colin Reimer and Huang, Chaofan and Morrison, Clayton T.},
title = {An infinite Hidden Markov Model with similarity-biased transitions},
year = {2017},
publisher = {JMLR.org},
abstract = {We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between "nearby" states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a "harmonic parsing" task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {942–950},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@inproceedings{10.5555/2002472.2002655,
author = {Burfoot, Clinton and Bird, Steven and Baldwin, Timothy},
title = {Collective classification of congressional floor-debate transcripts},
year = {2011},
isbn = {9781932432879},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper explores approaches to sentiment classification of U. S. Congressional floor-debate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {1506–1515},
numpages = {10},
location = {Portland, Oregon},
series = {HLT '11}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Pi\~{n}ero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Edge computing, FIWARE, Urban sound classification, Acoustic sensor networks}
}

@inproceedings{10.1145/1891903.1891912,
author = {Schauerte, Boris and Fink, Gernot A.},
title = {Focusing computational visual attention in multi-modal human-robot interaction},
year = {2010},
isbn = {9781450304146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1891903.1891912},
doi = {10.1145/1891903.1891912},
abstract = {Identifying verbally and non-verbally referred-to objects is an important aspect of human-robot interaction. Most importantly, it is essential to achieve a joint focus of attention and, thus, a natural interaction behavior. In this contribution, we introduce a saliency-based model that reflects how multi-modal referring acts influence the visual search, i.e. the task to find a specific object in a scene. Therefore, we combine positional information obtained from pointing gestures with contextual knowledge about the visual appearance of the referred-to object obtained from language. The available information is then integrated into a biologically-motivated saliency model that forms the basis for visual search. We prove the feasibility of the proposed approach by presenting the results of an experimental evaluation.},
booktitle = {International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction},
articleno = {6},
numpages = {8},
keywords = {visual search, shared attention, saliency, pointing, objects, multi-modal, language, joint attention, human-robot interaction, gestures, deictic interaction, color, attention},
location = {Beijing, China},
series = {ICMI-MLMI '10}
}

@inproceedings{10.5555/3540261.3540835,
author = {Yao, Huaxiu and Wang, Yu and Wei, Ying and Zhao, Peilin and Mahdavi, Mehrdad and Lian, Defu and Finn, Chelsea},
title = {Meta-learning with an adaptive task scheduler},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-58565-5_43,
author = {Cao, Bingyi and Araujo, Andr\'{e} and Sim, Jack},
title = {Unifying Deep Local and Global Features for Image Search},
year = {2020},
isbn = {978-3-030-58564-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58565-5_43},
doi = {10.1007/978-3-030-58565-5_43},
abstract = {Image retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads – requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the Revisited Oxford and Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX},
pages = {726–743},
numpages = {18},
keywords = {Unified model, Image retrieval, Deep features},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/2997189.2997277,
author = {Goodman, Dan F. M. and Brette, Romain},
title = {Learning to localise sounds with spiking neural networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
keywords = {auditory perception &amp; modeling (primary), computational neural models, neuroscience, supervised learning (secondary)},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@article{10.1016/j.image.2019.04.017,
author = {Wu, Hehe and Wang, Anhong and Liang, Jie and Li, Suyue and Li, Peihao},
title = {DCSN-Cast: Deep compressed sensing network for wireless video multicast},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.017},
doi = {10.1016/j.image.2019.04.017},
journal = {Image Commun.},
month = aug,
pages = {56–67},
numpages = {12},
keywords = {Deep residual network, Compressed sensing, Fully connected network, DCSFCN-cast, DCSRN-cast, DCSN-cast}
}

@inproceedings{10.1145/3474085.3475335,
author = {Zhang, Ji and Song, Jingkuan and Yao, Yazhou and Gao, Lianli},
title = {Curriculum-Based Meta-learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475335},
doi = {10.1145/3474085.3475335},
abstract = {Meta-learning offers an effective solution to learn new concepts with scarce supervision through an episodic training scheme: a series of target-like tasks sampled from base classes are sequentially fed into a meta-learner to extract common knowledge across tasks, which can facilitate the quick acquisition of task-specific knowledge of the target task with few samples. Despite its noticeable improvements, the episodic training strategy samples tasks randomly and uniformly, without considering their hardness and quality, which may not progressively improve the meta-leaner's generalization ability. In this paper, we present a Curriculum-Based Meta-learning (CubMeta) method to train the meta-learner using tasks from easy to hard. Specifically, the framework of CubMeta is in a progressive way, and in each step, we design a module named BrotherNet to establish harder tasks and an effective learning scheme for obtaining an ensemble of stronger meta-learners. In this way, the meta-learner's generalization ability can be progressively improved, and better performance can be obtained even with fewer training tasks. We evaluate our method for few-shot classification on two benchmarks - mini-ImageNet and tiered-ImageNet, where it achieves consistent performance improvements on various meta-learning paradigms.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {meta-learning, few-shot learning, curriculum learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1007/s11042-020-10381-y,
author = {Wang, Hei-Chia and Syu, Sheng-Wei and Wongchaisuwat, Papis},
title = {A method of music autotagging based on audio and lyrics},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10381-y},
doi = {10.1007/s11042-020-10381-y},
abstract = {With the development of the Internet and technology, online music platforms and music streaming services are flourishing. Information overload due to an abundance of digital music has become a common problem for many users. Social tags that are helpful for music recommendations have been discussed. However, label sparsity and a cold start problem, commonly observed with social tags, limit the effectiveness in supporting the recommendation system. A music autotagging system then becomes an alternative solution for supplementing a shortage of tags. Most prior studies on automatic labeling used only audio data for their analysis. However, some studies have suggested that lyrics enhance the music classification system to obtain more information and improve the overall accuracy. In addition to lyrics, audio data are also an important resource for finding music features. In summary, this paper proposes a music autotagging system that relies on both audio and lyrics to solve the above problems. Due to the development of deep learning algorithms in recent years, many scholars have effectively used neural networks to extract audio and textual features. Some of them also considered a structure of lyrics to extract features that consequentially improves the classification task. For lyric feature extraction, this study employs two types of deep learning models: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The feature extraction architecture is mainly motivated and characterized by the lyric architecture. In addition, a multitask learning method is adopted to learn correlations between tags. The experiments support that a multitask learning classifier that combines audio and lyric information has a better performance than a single-task learning classification method using only audio data than previous studies.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {15511–15539},
numpages = {29},
keywords = {Multitag classification, Multitask learning, Deep learning, Music autotagging}
}

@article{10.1145/3352020.3352032,
author = {L\`{e}cuyer, Mathias and Spahn, Riley and Vodrahalli, Kiran and Geambasu, Roxana and Hsu, Daniel},
title = {Privacy Accounting and Quality Control in the Sage Differentially Private ML Platform},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3352020.3352032},
doi = {10.1145/3352020.3352032},
abstract = {We present Sage, the first ML platform that enforces a global differential privacy (DP) guarantee across all models produced from a sensitive data stream. Sage extends the Tensorflow-Extended ML platform with novel mechanisms and DP theory to address operational challenges that arise from incorporating DP into ML training processes. First, to avoid the typical problem with DP systems of "running out of privacy budget" after a pre-established number of training processes, we develop block composition. It is a new DP composition theory that leverages the time-bounded structure of training processes to keep training models endlessly on a sensitive data stream while enforcing event-level DP on the stream. Second, to control the quality of ML models produced by Sage, we develop a novel iterative training process that trains a model on increasing amounts of data from a stream until, with high probability, the model meets developer-configured quality criteria.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jul,
pages = {75–84},
numpages = {10}
}

@inproceedings{10.1007/978-3-030-52237-7_46,
author = {Wang, Yeyu and Kai, Shimin and Baker, Ryan Shaun},
title = {Early Detection of Wheel-Spinning in ASSISTments},
year = {2020},
isbn = {978-3-030-52236-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52237-7_46},
doi = {10.1007/978-3-030-52237-7_46},
abstract = {Persistence is a crucial trait for learners. However, a common issue in mastery learning is that persistence is not always productive, a construct termed wheel-spinning. In this paper, we extend on prior work to develop wheel-spinning detectors in the ASSISTments learning system that distinguish between non-persistence, productive persistence and wheel-spinning. To understand how quickly we can detect each state, we use data from different numbers of practice opportunities and compare model performance across student-problem set pairs. We identify that a model constructed using data from the first nine practice opportunities outperforms models using less practice data. However, it is possible to differentiate students who will eventually wheel-spin from learners who will persist productively using data from only the first three opportunities. Wheel-spinning can be differentiated from non-persistence from the first five opportunities, and non-persistence can be differentiated from productive persistence from the first seven opportunities. These results show that early differentiation between wheel-spinning and productive persistence is feasible. These detectors relied upon hint requests, the correctness of prior opportunities, and the amount of practice and time on the skill. Identifying predictive features offer insights into the impact of in-system behaviors on wheel-spinning and guide the system design.},
booktitle = {Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I},
pages = {574–585},
numpages = {12},
keywords = {Intelligent tutoring system, Early detection, Decision tree, Persistence, Wheel-spinning},
location = {Ifrane, Morocco}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {Probabilistic logic machine learning, Rule-based constraint solving, Argumentation, Autonomic architecture, AI certification},
location = {Turku, Finland}
}

@article{10.1007/s10994-016-5570-z,
author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
title = {A topological insight into restricted Boltzmann machines},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-016-5570-z},
doi = {10.1007/s10994-016-5570-z},
abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
journal = {Mach. Learn.},
month = sep,
pages = {243–270},
numpages = {28},
keywords = {Sparse restricted Boltzmann machines, Small-world networks, Scale-free networks, Deep learning, Complex networks}
}

@inproceedings{10.5555/3305890.3305965,
author = {Palla, Konstantina and Knowles, David and Ghahramani, Zoubin},
title = {A birth-death process for feature allocation},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birth-death feature allocation process (BDFP). The BDFP models the evolution of the feature allocation of a set of N objects across a covari-ate (e.g. time) by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP). We show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference. The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2751–2759},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.jss.2021.111062,
author = {Luu, Quang-Hung and Lau, Man F. and Ng, Sebastian P.H. and Chen, Tsong Yueh},
title = {Testing multiple linear regression systems with metamorphic testing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111062},
doi = {10.1016/j.jss.2021.111062},
journal = {J. Syst. Softw.},
month = dec,
numpages = {21},
keywords = {Metamorphic relation, Metamorphic testing, Multiple linear regression}
}

@article{10.1007/s00521-019-04435-y,
author = {Kolokas, Nikolaos and Drosou, Anastasios and Tzovaras, Dimitrios},
title = {Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04435-y},
doi = {10.1007/s00521-019-04435-y},
abstract = {This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4259–4274},
numpages = {16},
keywords = {Text mining, Natural language processing, Sequence modeling, Deep machine learning}
}

@inproceedings{10.1145/2063576.2063780,
author = {Wang, Wei and Besan\c{c}on, Romaric and Ferret, Olivier and Grau, Brigitte},
title = {Filtering and clustering relations for unsupervised information extraction in open domain},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2063780},
doi = {10.1145/2063576.2063780},
abstract = {Information Extraction has recently been extended to new areas by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing a priori unknown relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for the filtering of such candidate relations based on heuristics and machine learning models. More precisely, we show that the best model for achieving this task is a Conditional Random Field model according to evaluations performed on a manually annotated corpus of about one thousand relations. We also tackle the problem of identifying semantically similar relations by clustering large sets of them. Such clustering is achieved by combining a classical clustering algorithm and a method for the efficient identification of highly similar relation pairs. Finally, we evaluate the impact of our filtering of relations on this semantic clustering with both internal measures and external measures. Results show that the filtering procedure doubles the recall of the clustering while keeping the same precision.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {1405–1414},
numpages = {10},
keywords = {unsupervised information extraction, machine learning, filtering, clustering},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@inproceedings{10.5555/3540261.3542543,
author = {Zhu, Zhihui and Ding, Tianyu and Zhou, Jinxin and Li, Xiao and You, Chong and Sulam, Jeremias and Qu, Qing},
title = {A geometric analysis of neural collapse with unconstrained features},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide the first global optimization landscape analysis of Neural Collapse— an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported in [1], this phenomenon implies that (i) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and (ii) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified unconstrained feature model, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. Our analysis of the simplified model not only explains what kind of features are learned in the last layer, but also shows why they can be efficiently optimized, matching the empirical observations in practical deep network architectures. These findings provide important practical implications. As an example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over 20% on ResNet18 without sacrificing the generalization performance.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2282},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.eswa.2017.07.051,
author = {Kardas, Karani and Cicekli, Nihan Kesim},
title = {SVAS},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {89},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.07.051},
doi = {10.1016/j.eswa.2017.07.051},
abstract = {A Surveillance Video Analysis System (SVAS) is proposed.An Interval-Based Spatio-Temporal Model (IBSTM) is introduced.Automatic learning, detection and inference of complex video events are provided.Hybrid machine learning methods are used and extended via Threshold Models.The empirical evaluation shows that SVAS succeeds in complex event recognition. This paper introduces a Surveillance Video Analysis System, called SVAS, for surveillance domain, in which the semantic rules and the definition of event models can be learned or defined by the user for automatic detection and inference of complex video events. In the scope of SVAS, an event model method named Interval-Based Spatio-Temporal Model (IBSTM) is proposed. SVAS can learn action models and event models without any predefined threshold values and generates understandable and manageable IBSTM event models. Hybrid machine learning methods are proposed and used. A set of feature models named Threshold Model, which reflects the spatio-temporal motion analysis of an event, is kept as the first model. As the second model, Bag of Actions (BoA) model is used in order to reduce the search space in the detection phase. Markov Logic Network (MLN) model, which provides understandable and manageable logic predicates for users, is kept as the third model. SVAS has high performance event detection capability due to its interval-based hierarchical manner. It determines related candidate intervals for each main model of IBSTM and uses the related main model when needed rather than using all models as a whole. The main contribution of this study is to fill the semantic gap between humans and video computer systems such that, on the one hand it decreases human intervention through its learning capabilities, but on the other hand it also enables human intervention when necessary through its manageable event model method. The study achieves all of them in the most efficient way through its machine learning methods. The proposed system is applied to different event datasets from CAVIAR, BEHAVE and our synthetic datasets. The experimental results show that our approach improves the event recognition performance and precision as compared to the current state-of-the-art approaches.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {343–361},
numpages = {19},
keywords = {Video surveillance, Markov logic networks, Event model learning, Event inference, Event detection}
}

@article{10.1016/j.cmpb.2009.03.001,
author = {Moca, Vasile V. and Scheller, Bertram and Mure\c{s}an, Raul C. and Daunderer, Michael and Pipa, Gordon},
title = {EEG under anesthesia-Feature extraction with TESPAR},
year = {2009},
issue_date = {September, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {95},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2009.03.001},
doi = {10.1016/j.cmpb.2009.03.001},
abstract = {We investigated the problem of automatic depth of anesthesia (DOA) estimation from electroencephalogram (EEG) recordings. We employed Time Encoded Signal Processing And Recognition (TESPAR), a time-domain signal processing technique, in combination with multi-layer perceptrons to identify DOA levels. The presented system learns to discriminate between five DOA classes assessed by human experts whose judgements were based on EEG mid-latency auditory evoked potentials (MLAEPs) and clinical observations. We found that our system closely mimicked the behavior of the human expert, thus proving the utility of the method. Further analyses on the features extracted by our technique indicated that information related to DOA is mostly distributed across frequency bands and that the presence of high frequencies (&gt;80Hz), which reflect mostly muscle activity, is beneficial for DOA detection.},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {191–202},
numpages = {12},
keywords = {TESPAR, MLP, MLAEP, EEG, Depth of anesthesia}
}

@article{10.1007/s11265-021-01676-w,
author = {Ting, Yu-Ching and Lo, Fang-Wen and Tsai, Pei-Yun},
title = {Implementation for Fetal ECG Detection from Multi-channel Abdominal Recordings with 2D Convolutional Neural Network},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {9},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01676-w},
doi = {10.1007/s11265-021-01676-w},
abstract = {A convolutional neural network (CNN)-based approach for fetal ECG detection from the abdominal ECG recording is proposed. The flow contains a pre-processing phase and a classification phase. In the pre-processing phase, short-time Fourier transform is applied to obtain the spectrogram, which is sent to 2D CNN for classification. The classified results from multiple channels are then fused and high detection accuracy up to 95.2% is achieved and the CNN-based approach outperforms the conventional algorithm. The hardware of this fetal ECG detector composed of the spectrogram processor and 2D CNN classifier is then implemented on the FPGA platform. Because the two dimensions of the spectrogram and the kernel are asymmetric, a pre-fetch mechanism is designed to eliminate the long latency resulted from data buffering for large-size convolution. From the implementation results, it takes 20258 clock cycles for inference and almost 50% computation cycles are reduced. The power consumption is 12.33mW at 324KHz and 1V for real-time operations. The implementation demonstrates the feasibility of real-time applications in wearable devices.},
journal = {J. Signal Process. Syst.},
month = sep,
pages = {1101–1113},
numpages = {13},
keywords = {Short-time Fourier transform, Convolutional neural network, Fetal electrocardiogram}
}

@inproceedings{10.5555/3045390.3045557,
author = {Shibagaki, Atsushi and Karasuyama, Masayuki and Hatano, Kohei and Takeuchi, Ichiro},
title = {Simultaneous safe screening of features and samples in doubly sparse modeling},
year = {2016},
publisher = {JMLR.org},
abstract = {The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of nonactive features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and vice-versa. We first theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1577–1586},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inbook{10.5555/3454287.3455259,
author = {Cao, Yuan and Gu, Quanquan},
title = {Generalization bounds of stochastic gradient descent for wide and deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of \~{O}(n-1/2) that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {972},
numpages = {11}
}

@inproceedings{10.5555/3540261.3542303,
author = {Hahn, Meera and Chaplot, Devendra and Tulsiani, Shubham and Mukadam, Mustafa and Rehg, James M. and Gupta, Abhinav},
title = {No RL, no simulation: learning to navigate without navigating},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2042},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/2887007.2887061,
author = {You, Quanzeng and Luo, Jiebo and Jin, Hailin and Yang, Jianchao},
title = {Robust image sentiment analysis using progressively trained and domain transferred deep networks},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {381–388},
numpages = {8},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1145/3097983.3097996,
author = {Zhang, Muhan and Chen, Yixin},
title = {Weisfeiler-Lehman Neural Machine for Link Prediction},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3097996},
doi = {10.1145/3097983.3097996},
abstract = {In this paper, we propose a next-generation link prediction method, Weisfeiler-Lehman Neural Machine (WLNM), which learns topological features in the form of graph patterns that promote the formation of links. WLNM has unmatched advantages including higher performance than state-of-the-art methods and universal applicability over various kinds of networks. WLNM extracts an enclosing subgraph of each target link and encodes the subgraph as an adjacency matrix. The key novelty of the encoding comes from a fast hashing-based Weisfeiler-Lehman (WL) algorithm that labels the vertices according to their structural roles in the subgraph while preserving the subgraph's intrinsic directionality. After that, a neural network is trained on these adjacency matrices to learn a predictive model. Compared with traditional link prediction methods, WLNM does not assume a particular link formation mechanism (such as common neighbors), but learns this mechanism from the graph itself. We conduct comprehensive experiments to show that WLNM not only outperforms a great number of state-of-the-art link prediction methods, but also consistently performs well across networks with different characteristics.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {575–583},
numpages = {9},
keywords = {neural network, link prediction, graph labeling, color refinement},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{10.1016/j.jvcir.2021.103325,
author = {Qin, Chuan and Zhang, Weiming and Dong, Xiaoyi and Zha, Hongyue and Yu, Nenghai},
title = {Adversarial steganography based on sparse cover enhancement},
year = {2021},
issue_date = {Oct 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {80},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2021.103325},
doi = {10.1016/j.jvcir.2021.103325},
journal = {J. Vis. Comun. Image Represent.},
month = oct,
numpages = {9},
keywords = {Deep neural network, Adversarial example, Steganography}
}

@article{10.1504/IJCSE.2017.082882,
title = {Hierarchical regression test case selection using slicing},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2017.082882},
doi = {10.1504/IJCSE.2017.082882},
abstract = {In this paper, we propose a novel regression test case selection approach by decomposing an object-oriented OO program into packages, classes, methods and statements that are affected by some modification made to the program. This decomposition is based on the proposed hierarchical slicing of an OO program. By mapping these decompositions to the existing test suite, we select a new reduced regression test suite and add some new test cases, if necessary, to retest the modified program. We apply hierarchical slicing on a suitable intermediate graph proposed for representing an OO program. This intermediate graph representation corresponds to all the possible dependences among the different parts of an OO program. We improve the scalability of the intermediate graph to a considerable extent by identifying and removing the redundant edges from the graph and thus detect the affected program parts in less time. The average reduction in time achieved for all the ten programs under experimentation is approximately 28.1%. The test cases that cover these affected parts of the program are then selected for regression testing. The average reduction in the number of test cases selected for regression testing of experimental programs is approximately 56.3%.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {179–197},
numpages = {19}
}

@article{10.1007/s11219-018-9439-1,
author = {K\i{}ra\c{c}, M. Furkan and Aktemur, Bar\i{}\c{s} and S\"{o}zer, Hasan and Gebizli, Ceren \c{S}ahin},
title = {Automatically learning usage behavior and generating event sequences for black-box testing of reactive systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9439-1},
doi = {10.1007/s11219-018-9439-1},
abstract = {We propose a novel technique based on recurrent artificial neural networks to generate test cases for black-box testing of reactive systems. We combine functional testing inputs that are automatically generated from a model together with manually-applied test cases for robustness testing. We use this combination to train a long short-term memory (LSTM) network. As a result, the network learns an implicit representation of the usage behavior that is liable to failures. We use this network to generate new event sequences as test cases. We applied our approach in the context of an industrial case study for the black-box testing of a digital TV system. LSTM-generated test cases were able to reveal several faults, including critical ones, that were not detected with existing automated or manual testing activities. Our approach is complementary to model-based and exploratory testing, and the combined approach outperforms random testing in terms of both fault coverage and execution time.},
journal = {Software Quality Journal},
month = jun,
pages = {861–883},
numpages = {23},
keywords = {Test case generation, Recurrent neural networks, Long short-term memory networks, Learning usage behavior, Black-box testing}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Nearest neighbor, Instance selection, Feature selection, Evolutionary algorithms, Cooperative coevolution}
}

@inproceedings{10.1145/2658761.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient testing of software product lines via centralization (short paper)},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658768},
doi = {10.1145/2658761.2658768},
abstract = {Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {49–52},
numpages = {4},
keywords = {random testing, automatic test generation, Software Product Lines},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1109/TASLP.2020.3015035,
author = {Saxon, Michael and Tripathi, Ayush and Jiao, Yishan and Liss, Julie M. and Berisha, Visar},
title = {Robust Estimation of Hypernasality in Dysarthria With Acoustic Model Likelihood Features},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3015035},
doi = {10.1109/TASLP.2020.3015035},
abstract = {Hypernasality is a common characteristic symptom across many motor-speech disorders. For voiced sounds, hypernasality introduces an additional resonance in the lower frequencies and, for unvoiced sounds, there is reduced articulatory precision due to air escaping through the nasal cavity. However, the acoustic manifestation of these symptoms is highly variable, making hypernasality estimation very challenging, both for human specialists and automated systems. Previous work in this area relies on either engineered features based on statistical signal processing or machine learning models trained on clinical ratings. Engineered features often fail to capture the complex acoustic patterns associated with hypernasality, whereas metrics based on machine learning are prone to overfitting to the small disease-specific speech datasets on which they are trained. Here we propose a new set of acoustic features that capture these complementary dimensions. The features are based on two acoustic models trained on a large corpus of healthy speech. The first acoustic model aims to measure nasal resonance from voiced sounds, whereas the second acoustic model aims to measure articulatory imprecision from unvoiced sounds. To demonstrate that the features derived from these acoustic models are specific to hypernasal speech, we evaluate them across different dysarthria corpora. Our results show that the features generalize even when training on hypernasal speech from one disease and evaluating on hypernasal speech from another disease (e.g., training on Parkinson's disease, evaluation on Huntington's disease), and when training on neurologically disordered speech but evaluating on cleft palate speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2511–2522},
numpages = {12}
}

@inbook{10.5555/2167748.2167761,
author = {Wojna, Arkadiusz},
title = {Analogy-based reasoning in classifier construction},
year = {2005},
isbn = {3540298304},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Analogy-based reasoning methods in machine learning make it possible to reason about properties of objects on the basis of similarities between objects. A specific similarity based method is the k nearest neighbors (k-nn) classification algorithm. In the k-nn algorithm, a decision about a new object x is inferred on the basis of a fixed number k of the objects most similar to x in a given set of examples. The primary contribution of the dissertation is the introduction of two new classification models based on the k-nn algorithm.The first model is a hybrid combination of the k-nn algorithm with rule induction. The proposed combination uses minimal consistent rules defined by local reducts of a set of examples. To make this combination possible the model of minimal consistent rules is generalized to a metric-dependent form. An effective polynomial algorithm implementing the classification model based on minimal consistent rules has been proposed by Bazan. We modify this algorithm in such a way that after addition of the modified algorithm to the k-nn algorithm the increase of the computation time is inconsiderable. For some tested classification problems the combined model was significantly more accurate than the classical k-nn classification algorithm.For many real-life problems it is impossible to induce relevant global mathematical models from available sets of examples. The second model proposed in the dissertation is a method for dealing with such sets based on locally induced metrics. This method adapts the notion of similarity to the properties of a given test object. It makes it possible to select the correct decision in specific fragments of the space of objects. The method with local metrics improved significantly the classification accuracy of methods with global models in the hardest tested problems.The important issues of quality and efficiency of the k-nn based methods are a similarity measure and the performance time in searching for the most similar objects in a given set of examples, respectively. In this dissertation both issues are studied in detail and some significant improvements are proposed for the similarity measures and for the search methods found in the literature.},
booktitle = {Transactions on Rough Sets IV},
pages = {277–374},
numpages = {98}
}

@article{10.1109/TCBB.2017.2708701,
author = {Choudhury, Olivia and Chakrabarty, Ankush and Emrich, Scott J.},
title = {Highly Accurate and Efficient Data-Driven Methods for Genotype Imputation},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2708701},
doi = {10.1109/TCBB.2017.2708701},
abstract = {High-throughput sequencing techniques have generated massive quantities of genotype data. Haplotype phasing has proven to be a useful and effective method for analyzing these data. However, the quality of phasing is undermined due to missing information. Imputation provides an effective means of improving the underlying genotype information. For model organisms, imputation can rely on an available reference genotype panel and a physical or genetic map. For non-model organisms, which often do not have a genotype panel, it is important to design an imputation technique that does not rely on reference data. Here, we present Accurate Data-Driven Imputation Technique ADDIT, which is composed of two data-driven algorithms capable of handling data generated from model and non-model organisms. The non-model variant of ADDIT referred to as ADDIT-NM employs statistical inference methods to impute missing genotypes, whereas the model variant referred to as ADDIT-M leverages a supervised learning-based approach for imputation. We demonstrate that both variants of ADDIT are more accurate, faster, and require less memory than leading state-of-the-art imputation tools using model human and non-model maize, apple, and grape genotype data. Software Availability: The source code of ADDIT and test data sets are available at https://github.com/NDBL/ADDIT.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1107–1116},
numpages = {10}
}

@inbook{10.5555/3454287.3454823,
author = {Wang, Siqi and Zeng, Yijie and Liu, Xinwang and Zhu, En and Yin, Jianping and Xu, Chuanfu and Kloft, Marius},
title = {Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E3 Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E3 Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unla-belled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E3 Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {536},
numpages = {14}
}

@article{10.1016/j.patcog.2008.05.019,
author = {Farhangfar, Alireza and Kurgan, Lukasz and Dy, Jennifer},
title = {Impact of imputation of missing values on classification error for discrete data},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {12},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.05.019},
doi = {10.1016/j.patcog.2008.05.019},
abstract = {Numerous industrial and research databases include missing values. It is not uncommon to encounter databases that have up to a half of the entries missing, making it very difficult to mine them using data analysis methods that can work only with complete data. A common way of dealing with this problem is to impute (fill-in) the missing values. This paper evaluates how the choice of different imputation methods affects the performance of classifiers that are subsequently used with the imputed data. The experiments here focus on discrete data. This paper studies the effect of missing data imputation using five single imputation methods (a mean method, a Hot deck method, a Nai@?ve-Bayes method, and the latter two methods with a recently proposed imputation framework) and one multiple imputation method (a polytomous regression based method) on classification accuracy for six popular classifiers (RIPPER, C4.5, K-nearest-neighbor, support vector machine with polynomial and RBF kernels, and Nai@?ve-Bayes) on 15 datasets. This experimental study shows that imputation with the tested methods on average improves classification accuracy when compared to classification without imputation. Although the results show that there is no universally best imputation method, Nai@?ve-Bayes imputation is shown to give the best results for the RIPPER classifier for datasets with high amount (i.e., 40% and 50%) of missing data, polytomous regression imputation is shown to be the best for support vector machine classifier with polynomial kernel, and the application of the imputation framework is shown to be superior for the support vector machine with RBF kernel and K-nearest-neighbor. The analysis of the quality of the imputation with respect to varying amounts of missing data (i.e., between 5% and 50%) shows that all imputation methods, except for the mean imputation, improve classification error for data with more than 10% of missing data. Finally, some classifiers such as C4.5 and Nai@?ve-Bayes were found to be missing data resistant, i.e., they can produce accurate classification in the presence of missing data, while other classifiers such as K-nearest-neighbor, SVMs and RIPPER benefit from the imputation.},
journal = {Pattern Recogn.},
month = dec,
pages = {3692–3705},
numpages = {14},
keywords = {Single imputation, Multiple imputations, Missing values, Imputation of missing values, Classification}
}

@article{10.1016/j.ijar.2021.07.015,
author = {Bodewes, Tjebbe and Scutari, Marco},
title = {Learning Bayesian networks from incomplete data with the node-average likelihood},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.07.015},
doi = {10.1016/j.ijar.2021.07.015},
journal = {Int. J. Approx. Reasoning},
month = nov,
pages = {145–160},
numpages = {16},
keywords = {Incomplete data, Score-based structure learning, Bayesian networks}
}

@inproceedings{10.1109/SMC.2018.00108,
author = {Nakai, Tomoya and Koide-Majima, Naoko and Nishimoto, Shinji},
title = {Encoding and Decoding of Music-Genre Representations in the Human Brain},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2018.00108},
doi = {10.1109/SMC.2018.00108},
abstract = {Music-genre recognition (MGR) has been a central issue in understanding human preferences of music. Previous studies have used various acoustic features to achieve MGR, though it has been largely unknown how music genres and related features are represented in the brain. Here, we measured brain activity while subjects passively listened to naturalistic music of various genres. A voxel-wise encoding model showed different activation patterns for each music genre in the bilateral superior temporal gyrus. We further performed music-genre classification using both a feature-based approach and a brain activity-based approach. Both approaches provided above-chance classification accuracy. Among four feature models, a biologically plausible spectro-temporal modulation transfer function (MTF) model showed the highest performance. These results provide a new insight into biologically plausible models of music genre.},
booktitle = {2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {584–589},
numpages = {6},
location = {Miyazaki, Japan}
}

@inproceedings{10.1145/1143844.1143890,
author = {G\"{o}r\"{u}r, Dilan and J\"{a}kel, Frank and Rasmussen, Carl Edward},
title = {A choice model with infinitely many latent features},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143890},
doi = {10.1145/1143844.1143890},
abstract = {Elimination by aspects (EBA) is a probabilistic choice model describing how humans decide between several options. The options from which the choice is made are characterized by binary features and associated weights. For instance, when choosing which mobile phone to buy the features to consider may be: long lasting battery, color screen, etc. Existing methods for inferring the parameters of the model assume pre-specified features. However, the features that lead to the observed choices are not always known. Here, we present a non-parametric Bayesian model to infer the features of the options and the corresponding weights from choice data. We use the Indian buffet process (IBP) as a prior over the features. Inference using Markov chain Monte Carlo (MCMC) in conjugate IBP models has been previously described. The main contribution of this paper is an MCMC algorithm for the EBA model that can also be used in inference for other non-conjugate IBP models---this may broaden the use of IBP priors considerably.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {361–368},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@article{10.1155/2021/9917246,
author = {Owusu, Ebenezer and Kumi, Jacqueline Asor and Appati, Justice Kwame and Ejbali, Ridha},
title = {On Facial Expression Recognition Benchmarks},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-9724},
url = {https://doi.org/10.1155/2021/9917246},
doi = {10.1155/2021/9917246},
abstract = {Facial expression is an important form of nonverbal communication, as it is noted that 55% of what humans communicate is expressed in facial expressions. There are several applications of facial expressions in diverse fields including medicine, security, gaming, and even business enterprises. Thus, currently, automatic facial expression recognition is a hotbed research area that attracts lots of grants and therefore the need to understand the trends very well. This study, as a result, aims to review selected published works in the domain of study and conduct valuable analysis to determine the most common and useful algorithms employed in the study. We selected published works from 2010 to 2021 and extracted, analyzed, and summarized the findings based on the most used techniques in feature extraction, feature selection, validation, databases, and classification. The result of the study indicates strongly that local binary pattern (LBP), principal component analysis (PCA), saturated vector machine (SVM), CK+, and 10-fold cross-validation are the most widely used feature extraction, feature selection, classifier, database, and validation method used, respectively. Therefore, in line with our findings, this study provides recommendations for research specifically for new researchers with little or no background as to which methods they can employ and strive to improve.},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
numpages = {20}
}

@inproceedings{10.5555/3045390.3045598,
author = {Michaeli, Tomer and Wang, Weiran and Livescu, Karen},
title = {Nonparametric canonical correlation analysis},
year = {2016},
publisher = {JMLR.org},
abstract = {Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1967–1976},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Systematic review, Software evolvability, Software architecture, Evolvability analysis, Architecture evolution, Architecture analysis}
}

@inproceedings{10.1007/978-3-642-36620-8_15,
author = {Schneider, Matthias and Hirsch, Sven and Sz\'{e}kely, G\'{a}bor and Weber, Bruno and Menze, Bjoern H.},
title = {Oblique random forests for 3-d vessel detection using steerable filters and orthogonal subspace filtering},
year = {2012},
isbn = {9783642366192},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36620-8_15},
doi = {10.1007/978-3-642-36620-8_15},
abstract = {We propose a machine learning-based framework using oblique random forests for 3-D vessel segmentation. Two different kinds of features are compared. One is based on orthogonal subspace filtering where we learn 3-D eigenspace filters from local image patches that return task optimal feature responses. The other uses a specific set of steerable filters that show, qualitatively, similarities to the learned eigenspace filters, but also allow for explicit parametrization of scale and orientation that we formally generalize to the 3-D spatial context. In this way, steerable filters allow to efficiently compute oriented features along arbitrary directions in 3-D. The segmentation performance is evaluated on four 3-D imaging datasets of the murine visual cortex at a spatial resolution of 0.7μm. Our experiments show that the learning-based approach is able to significantly improve the segmentation compared to conventional Hessian-based methods. Features computed based on steerable filters prove to be superior to eigenfilter-based features for the considered datasets. We further demonstrate that random forests using oblique split directions outperform decision tree ensembles with univariate orthogonal splits.},
booktitle = {Proceedings of the Second International Conference on Medical Computer Vision: Recognition Techniques and Applications in Medical Imaging},
pages = {142–154},
numpages = {13},
keywords = {vessel segmentation, steerable filters, orthogonal subspace filtering, oblique random forest},
location = {Nice, France},
series = {MCV'12}
}

@article{10.1016/j.artint.2016.05.003,
author = {Agerri, Rodrigo and Rigau, German},
title = {Robust multilingual Named Entity Recognition with shallow semi-supervised features},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {238},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2016.05.003},
doi = {10.1016/j.artint.2016.05.003},
abstract = {We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results.},
journal = {Artif. Intell.},
month = sep,
pages = {63–82},
numpages = {20},
keywords = {Semi-supervised learning, Natural Language Processing, Named Entity Recognition, Information Extraction, Clustering}
}

@inproceedings{10.1007/978-3-030-98682-7_6,
author = {Bestmann, Marc and Engelke, Timon and Fiedler, Niklas and G\"{u}ldenstein, Jasper and Gutsche, Jan and Hagge, Jonas and Vahl, Florian},
title = {TORSO-21 Dataset: Typical Objects in&nbsp;RoboCup Soccer 2021},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_6},
doi = {10.1007/978-3-030-98682-7_6},
abstract = {We present a dataset specifically designed to be used as a benchmark to compare vision systems in the RoboCup Humanoid Soccer domain. The dataset is composed of a collection of images taken in various real-world locations as well as a collection of simulated images. It enables comparing vision approaches with a meaningful and expressive metric. The contributions of this paper consist of providing a comprehensive and annotated dataset, an overview of the recent approaches to vision in RoboCup, methods to generate vision training data in a simulated environment, and an approach to increase the variety of a dataset by automatically selecting a diverse set of images from a larger pool. Additionally, we provide a baseline of YOLOv4 and YOLOv4-tiny on this dataset.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {65–77},
numpages = {13},
keywords = {Deep learning, Vision dataset, Computer vision},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.5555/3305890.3305963,
author = {Pad, Pedram and Salehi, Farnood and Celis, Elisa and Thiran, Patrick and Unser, Michael},
title = {Dictionary learning based on sparse distribution tomography},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily α-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓp-norms that constitutes the foundation of our approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2731–2740},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1007/978-3-642-40261-6_54,
author = {Rauber, Thomas W. and Varej\~{a}o, Fl\'{a}vio M.},
title = {Motor Pump Fault Diagnosis with Feature Selection and Levenberg-Marquardt Trained Feedforward Neural Network},
year = {2013},
isbn = {9783642402609},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40261-6_54},
doi = {10.1007/978-3-642-40261-6_54},
abstract = {We present a system for automatic model-free fault detection based on a feature set from vibrational patterns. The complexity of the feature model is reduced by feature selection. We use a wrapper approach for the selection criteria, incorporating the training of an artificial neural network into the selection process. For fast convergence we train with the Levenberg-Marquardt algorithm. Experiments are presented for eight different fault classes.},
booktitle = {Proceedings, Part I, of the 15th International Conference on Computer Analysis of Images and Patterns - Volume 8047},
pages = {449–456},
numpages = {8},
keywords = {feedforward neural network, feature selection, Levenberg-Marquardt, Fault diagnosis},
location = {York, UK},
series = {CAIP 2013}
}

@article{10.1016/j.dsp.2021.103106,
author = {Zhang, Hai and Xie, Qiangqiang and Lu, Bei and Gai, Shan},
title = {Dual attention residual group networks for single image deraining},
year = {2021},
issue_date = {Sep 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103106},
doi = {10.1016/j.dsp.2021.103106},
journal = {Digit. Signal Process.},
month = sep,
numpages = {11},
keywords = {Single image rain removal, Channel attention, Residual groups, Spatial attention}
}

@inproceedings{10.1007/978-3-030-58545-7_8,
author = {Wang, Hu and Wu, Qi and Shen, Chunhua},
title = {Soft Expert Reward Learning for Vision-and-Language Navigation},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_8},
doi = {10.1007/978-3-030-58545-7_8},
abstract = {Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {126–141},
numpages = {16},
keywords = {Soft expert distillation, Self perceiving reward, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.5555/1599081.1599162,
author = {Nivre, Joakim and Boguslavsky, Igor M. and Iomdin, Leonid L.},
title = {Parsing the SynTagRus treebank of Russian},
year = {2008},
isbn = {9781905593446},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present the first results on parsing the SynTagRus treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%. A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features. We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available.},
booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1},
pages = {641–648},
numpages = {8},
location = {Manchester, United Kingdom},
series = {COLING '08}
}

@phdthesis{10.5555/AAI28930589,
author = {Rukat, Tammo},
title = {Logical Factorisation Machines: Probabilistic Boolean Factor Models for Binary Data},
year = {2018},
publisher = {University of Oxford (United Kingdom)},
abstract = {Logical Factorisation Machines (LFMs) are a class of latent feature models, that aim to decompose binary matrices, tensors or higher-arity relations into an approximate logical product of low rank, binary matrices. These products are defined through the use of logical operators instead of arithmetic operations. The resulting factor matrices contain interpretable patterns that lend themselves to the discovery of hidden causal structure. We frame LFMs as probabilistic generative models and derive sampling based posterior inference. Despite full uncertainty quantification, the inference procedure scales to Billions of data points which is possible through exploitation of the logical structure in the factor conditionals. OrMachines are a particularly interesting subset of LFMs, where a single latent cause is sufficient to explain an outcome. They represent a probabilistic approach to the well-studied problems of Boolean Matrix Factorisation and Boolean Tensor Factorisation. The proposed model and inference procedure yield decompositions of higher accuracy than the existing techniques throughout a wide range of conditions. Real-world examples include single-cell genomics, cancer genomics, relational and spatio-temporal data. We propose several extensions, including hierarchies of OrMachines for data integration and a Bayesian nonparametric approach to infer the latent dimensionality. LFMs with less established logics, their relationships and interpretation are considered. While the latent structure in many of these models is inherently difficult to recover, we demonstrate that certain LFMs can provide complementary representations and reveal new findings. A fast and flexible implementation is introduced and publicly available on GitHub.},
note = {AAI28930589}
}

@article{10.1007/s11042-018-6409-3,
author = {Gen\c{c}, An\i{}l and Ekenel, Haz\i{}m Kemal},
title = {Cross-dataset person re-identification using deep convolutional neural networks: effects of context and domain adaptation},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6409-3},
doi = {10.1007/s11042-018-6409-3},
abstract = {Over the past years, the impact of surveillance systems on public safety increases dramatically. One significant challenge in this domain is person re-identification, which aims to detect whether a person has already been captured by another camera in the surveillance network or not. Most of the work that has been conducted on person re-identification problem uses a single dataset, in which the training and test data are coming from the same source. However, as we have shown in this work, there is a strong bias among the person re-identification datasets, therefore, a method that has been trained and optimized on a specific person re-identification dataset may not generalize well and perform successfully on the other datasets. This is a problem for many real-world applications, since it is not feasible to collect and annotate sufficient amount of data from the target application to train or fine-tune a deep convolutional neural network model. Taking this issue into account, in this work, we have focused on cross-dataset person re-identification problem and first explored and analyzed in detail the use of the state-of-the-art deep convolutional neural network architectures, namely AlexNet, VGGNet, GoogLeNet, ResNet, and DenseNet that have been developed for generic image classification task. These deep CNN models have been adapted to the person re-identification domain by fine-tuning them for each human body part separately, as well as on the entire body, with the two relatively large person re-identification datasets: CUHK03 and Market-1501. Then, the performance of each adapted model has been evaluated on two different publicly available datasets: VIPeR and PRID2011. We have shown that, even just a domain adaptation leads comparable results to the state-of-the-art cross-dataset approaches. Another point that we have addressed in this paper is context adaptation. It has been known that person re-identification approaches implicitly utilizes background as context information. Therefore, to have a consistent background across different camera views, we have employed the cycle-consistent generative adversarial network. We have shown that this further improves the performance.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {5843–5861},
numpages = {19},
keywords = {Person re-identification, Domain adaptation, CycleGAN, Cross-dataset, Convolutional neural networks, Context adaptation}
}

@article{10.1016/j.cad.2008.05.004,
author = {Yang, Dong and Dong, Ming and Miao, Rui},
title = {Development of a product configuration system with an ontology-based approach},
year = {2008},
issue_date = {August, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {40},
number = {8},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2008.05.004},
doi = {10.1016/j.cad.2008.05.004},
abstract = {Product configuration is a crucial means to implement the mass customization paradigm by assembling a set of customizable components to satisfy both customers' needs and technical constraints. With the aim of enabling efficient and effective development of product configuration systems by reusing configuration knowledge, an ontology-based approach to modeling product configuration knowledge is presented in this paper. The ontology-based product configuration models are hierarchically organized. At the lower level, a configuration meta-model is defined. Based on this meta-model, domain-specific configuration knowledge can be derived by reusing or inheriting the classes or relations in the meta-model. Configuration models are formalized using OWL (Ontology Web Language), an ontology representation language developed by W3C. As a result, configuration models have well-defined semantics due to the logic semantics of OWL, making it possible to automatically detect inconsistencies of configuration knowledge bases. Furthermore, configuration constraints are represented in SWRL, a rule language based on OWL. Finally, actual configuration processes are carried out using JESS, a rule engine for the Java platform, by mapping OWL-based configuration facts and SWRL-based configuration constraints into JESS facts and JESS rules, respectively. The proposed methodology is illustrated with an example for configuring the ranger drilling machine.},
journal = {Comput. Aided Des.},
month = aug,
pages = {863–878},
numpages = {16},
keywords = {Semantic web, Product configuration, Ontology, OWL}
}

@inproceedings{10.5555/2907132.2907135,
author = {Viappiani, Paolo and Boutilier, Craig},
title = {Optimal set recommendations based on regret},
year = {2009},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Current conversational recommender systems do not offer guarantees on the quality of their recommendations, either because they do not maintain a model of a user's utility function, or do so in an ad hoc fashion. In this paper, we propose an approach to recommender systems that incorporates explicit utility models into the recommendation process in a decision-theoretically sound fashion. The system maintains explicit constraints on the user's utility based on the semantics of the preferences revealed by the user's actions. In particular, we propose and investigate a new decision criterion, setwise maximum regret, for constructing optimal recommendation sets. This new criterion extends the mathematical notion of maximum regret used in decision theory and preference elicitation to sets. We develop computational procedures for computing setwise max regret. We also show that the criterion suggests choice sets for queries that are myopically optimal: that is, it refines knowledge of a user's utility function in a way that reduces max regret more quickly than any other choice set. Thus setwise max regret acts both as guarantee on the quality of our recommendations and as a driver for further utility elicitation.Our simulation results suggest that this utility-theoretically sound approach to user modeling allows much more effective navigation of a product space than traditional approaches based on, for example, heuristic utility models and product similarity measures.},
booktitle = {Proceedings of the 7th International Conference on Intelligent Techniques for Web Personalization &amp; Recommender Systems - Volume 528},
pages = {20–31},
numpages = {12},
location = {Pasadena, California},
series = {ITWP'09}
}

@phdthesis{10.5555/AAI28713793,
author = {Palaparthi, Anil Kumar Reddy and A., Weiss, Jeffrey and D., Rabbitt, Richard and II, Dorval, Alan D., and M, Barkmeier-Kraemer, Julie},
advisor = {R, Titze, Ingo},
title = {Computational Motor Learning and Control of the Vocal Source for Voice Production},
year = {2021},
isbn = {9798780644439},
publisher = {The University of Utah},
abstract = {Voice production is a motor skill and requires the coordinated function of many brain regions, namely the brainstem, cerebellum, basal ganglia, diencephalon, and cerebral hemispheres. The vocal system can be subdivided into three major components: lungs, larynx, and vocal tract. Lung pressure drives the airflow in the trachea towards the larynx. The airflow causes the vocal folds in the larynx (vocal source) to oscillate under certain prephonatory conditions, generating audible pulses of airflow into the vocal tract. The vocal tract filters these pulses and radiates the sound into the air. The intrinsic laryngeal muscles play a significant role in voice production. They position the glottis, the space between the vocal folds in several pre-phonatory positions that facilitate vocal fold vibration. The resulting glottal flow is the vocal source. This project aims to develop a control system that controls the vocal source based on four acoustic and four somatosensory features. Nonlinear control theory and artificial neural networks were used to develop the controllers. A voice simulator with a biomechanical model of the vocal system, LeTalker, was used to model the voice production mechanism. In Aim 1, interrelationships between the intrinsic laryngeal muscles and lung pressure in producing various acoustic and somatosensory features during phonation were obtained. In Aim 2, feedforward and feedback controllers based on acoustic and somatosensory features to control the vocal source were developed. In Aim 3, the controllers' sensitivity and performance were assessed using perturbation analysis. The results demonstrated that the control system was able to generate the lung pressure and muscle activations such that the four acoustic and four somatosensory targets were reached with high accuracy. It was observed that for most of the test cases, the control system produces lung pressure and muscle activations that result in phonation that is within ±15 Hz of the targeted fo and ±2 dB of the targeted SPL. The three studies conducted in this dissertation can be a stepping stone to simulate and study various motor disorders related to voice.},
note = {AAI28713793}
}

@inproceedings{10.5555/3104322.3104379,
author = {Hoffman, Matthew D. and Blei, David M. and Cook, Perry R.},
title = {Bayesian nonparametric matrix factorization for recorded music},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recent research in machine learning has focused on breaking audio spectrograms into separate sources of sound using latent variable decompositions. These methods require that the number of sources be specified in advance, which is not always possible. To address this problem, we develop Gamma Process Nonnegative Matrix Factorization (GaP-NMF), a Bayesian nonparametric approach to decomposing spectrograms. The assumptions behind GaP-NMF are based on research in signal processing regarding the expected distributions of spectrogram data, and GaP-NMF automatically discovers the number of latent sources. We derive a mean-field variational inference algorithm and evaluate GaP-NMF on both synthetic data and recorded music.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {439–446},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{10.23919/ICCAS50221.2020.9268247,
author = {Yoo, Hwiyeon and Kim, Nuri and Park, Jeongho and Oh, Songhwai},
title = {Path-Following Navigation Network Using Sparse Visual Memory},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.23919/ICCAS50221.2020.9268247},
doi = {10.23919/ICCAS50221.2020.9268247},
abstract = {Following a demonstration path without observing exact location of an agent is a challenging navigation problem. Especially, considering the probabilistic transition function of the agent makes the problem hard to solve with an exact action decision, so learning-based approaches have been used to solve this task. For example, a previous method by Kumar and Gupta et al., robust path following network (RPF), is a neural-network-based method using visual memories of the demonstration. Although the RPF shows good performances on the path-following task, it does not consider the efficiency of the visual memory since it requires the entire visual memory of the demonstration. In this paper, we propose a path-following network using sparse memory of the demonstration path that can deal with various sparsity of the visual memory. For each time step, the proposed network makes soft attention on the sparse memory to control the agent. We test the proposed model on the Habitat simulator using MatterPort3D dataset with various sparsity of memory. The experimental results show that the proposed method achieves 81.9% of success rate and 73.7% of SPL on a model with 0.8 memory sparsity, and also the results of the models with other memory sparsity achieve reasonable performances compare to the baseline methods.},
booktitle = {2020 20th International Conference on Control, Automation and Systems (ICCAS)},
pages = {883–886},
numpages = {4},
location = {Busan, Korea (South)}
}

@article{10.4018/IJEIS.2019040104,
author = {Sbai, Hanae and El Faquih, Loubna and Fredj, Mounia},
title = {A Novel Tool for Configurable Process Evolution and Service Derivation},
year = {2019},
issue_date = {Apr 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-1115},
url = {https://doi.org/10.4018/IJEIS.2019040104},
doi = {10.4018/IJEIS.2019040104},
abstract = {In recent years, variability management in business processes is considered a key of reuse. Research works in this field focused mainly on variability modeling and resolution; whereas, evolution has been somehow neglected. In fact, new business requirements may occur, and business processes must evolve in order to meet the new needs. Furthermore, the evolution at business layer represented by configurable processes impact the IT layer represented by services. In this case, it is necessary to synchronize the changes between these two layers. In other words, the alignment of configurable processes and configurable services must occur to maintain an integrated view of an organization. This can be reached by the concept of service-based configurable processes. The study of existing tools in this domain shows the lack of solutions integrating both the evolution management, and the change propagation with respect to the variability. This article aims to represent the CPMEv, a novel tool for evolution management of service-based configurable processes.},
journal = {Int. J. Enterp. Inf. Syst.},
month = apr,
pages = {58–75},
numpages = {18},
keywords = {Variability, Evolution, Configurable Services, Change Propagation, Alignment}
}

@article{10.1016/j.patcog.2007.06.007,
author = {Li, Yun and Wu, Zhong-Fu},
title = {Fuzzy feature selection based on min-max learning rule and extension matrix},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2007.06.007},
doi = {10.1016/j.patcog.2007.06.007},
abstract = {In many systems, such as fuzzy neural network, we often adopt the language labels (such as large, medium, small, etc.) to split the original feature into several fuzzy features. In order to reduce the computation complexity of the system after the fuzzification of features, the optimal fuzzy feature subset should be selected. In this paper, we propose a new heuristic algorithm, where the criterion is based on min-max learning rule and fuzzy extension matrix is designed as the search strategy. The algorithm is proved in theory and has shown its high performance over several real-world benchmark data sets.},
journal = {Pattern Recogn.},
month = jan,
pages = {217–226},
numpages = {10},
keywords = {Min-max rule, Fuzzy set theory, Feature selection, Extension matrix}
}

@inproceedings{10.1609/aaai.v33i01.33014951,
author = {Shu, Yang and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
title = {Transferable curriculum for weakly-supervised domain adaptation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014951},
doi = {10.1609/aaai.v33i01.33014951},
abstract = {Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that "source-domain engineering" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weakly-supervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {608},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.5555/3044805.3044849,
author = {Shi, Tianlin and Zhu, Jun},
title = {Online Bayesian passive-aggressive learning},
year = {2014},
publisher = {JMLR.org},
abstract = {Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {I–378–I–386},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1145/3336191.3371775,
author = {Meng, Yu and Karimzadehgan, Maryam and Zhuang, Honglei and Metzler, Donald},
title = {Separate and Attend in Personal Email Search},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371775},
doi = {10.1145/3336191.3371775},
abstract = {In personal email search, user queries often impose different requirements on different aspects of the retrieved emails. For example, the query "my recent flight to the US" requires emails to be ranked based on both textual contents and recency of the email documents, while other queries such as "medical history" do not impose any constraints on the recency of the email. Recent deep learning-to-rank models for personal email search often directly concatenate dense numerical features (e.g., document age) with embedded sparse features (e.g., n-gram embeddings). In this paper, we first show with a set of experiments on synthetic datasets that direct concatenation of dense and sparse features does not lead to the optimal search performance of deep neural ranking models. To effectively incorporate both sparse and dense email features into personal email search ranking, we propose a novel neural model, SepAttn. SepAttn first builds two separate neural models to learn from sparse and dense features respectively, and then applies an attention mechanism at the prediction level to derive the final prediction from these two models. We conduct a comprehensive set of experiments on a large-scale email search dataset, and demonstrate that our SepAttn model consistently improves the search quality over the baseline models.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {429–437},
numpages = {9},
keywords = {neural attention model, learning-to-rank, email search},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1016/j.neucom.2017.07.037,
author = {Li, Honggui and Trocan, Maria},
title = {Deep neural network based single pixel prediction for unified video coding},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {272},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.07.037},
doi = {10.1016/j.neucom.2017.07.037},
abstract = {Classical video prediction methods exploit directly and shallowly the intra-frame, inter-frame and multi-view similarities within the video sequences; the proposed video prediction methods indirectly and intensively transform the frame correlations into nonlinear mappings by using a general deep neural network (DNN) with single output node. Traditional DNN based video prediction algorithms wholly and coarsely forecast the next frame, but the proposed video prediction algorithms severally and precisely anticipate single pixel of future frame in order to achieve high prediction accuracy and low computation cost. First of all, general DNN based prediction algorithms for intra-frame coding, inter-frame coding and multi-view coding are presented respectively. Then, general DNN based prediction algorithm for unified video coding is raised, which relies on the preceding three prediction algorithms. It is evaluated by simulation experiments that the proposed methods hold better performance than state of the art High Efficiency Video Coding (HEVC) in peak signal to noise ratio (PSNR) and bit per pixel (BPP) in the situation of low bitrate transmission. It is also verified by experimental results that the proposed general DNN architecture possesses higher prediction accuracy and lower computation load than those of conventional DNN architectures. It is further testified by experimental results that the proposed methods are very suitable for multi-view videos with small correlations and big disparities.},
journal = {Neurocomput.},
month = jan,
pages = {558–570},
numpages = {13},
keywords = {Video prediction, Unified video coding, Multi-view coding, Intra-frame coding, Inter-frame coding, Deep neural network}
}

@inproceedings{10.5555/1596409.1596426,
author = {Ren, Han and Ji, Donghong and Wan, Jing and Zhang, Mingyao},
title = {Parsing syntactic and semantic dependencies for multiple languages with a pipeline approach},
year = {2009},
isbn = {9781932432299},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes a pipelined approach for CoNLL-09 shared task on joint learning of syntactic and semantic dependencies. In the system, we handle syntactic dependency parsing with a transition-based approach and utilize MaltParser as the base model. For SRL, we utilize a Maximum Entropy model to identify predicate senses and classify arguments. Experimental results show that the average performance of our system for all languages achieves 67.81% of macro F1 Score, 78.01% of syntactic accuracy, 56.69% of semantic labeled F1, 71.66% of macro precision and 64.66% of micro recall.},
booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task},
pages = {97–102},
numpages = {6},
location = {Boulder, Colorado},
series = {CoNLL '09}
}

@inproceedings{10.1111/cgf.14116,
author = {Ghorbani, S. and Wloka, C. and Etemad, A. and Brubaker, M. A. and Troje, N. F.},
title = {Probabilistic character motion synthesis using a hierarchical deep latent variable model},
year = {2020},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.14116},
doi = {10.1111/cgf.14116},
abstract = {We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
articleno = {21},
numpages = {15},
location = {Virtual Event, Canada},
series = {SCA '20}
}

@article{10.1016/j.sigpro.2020.107466,
author = {Zhu, Qi and Xu, Xiangyu and Yuan, Ning and Zhang, Zheng and Guan, Donghai and Huang, Sheng-Jun and Zhang, Daoqiang},
title = {Latent correlation embedded discriminative multi-modal data fusion},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2020.107466},
doi = {10.1016/j.sigpro.2020.107466},
journal = {Signal Process.},
month = jun,
numpages = {11},
keywords = {Sparse representation, Self-paced learning, Classification, Multi-modal data fusion}
}

@inproceedings{10.1007/978-3-030-98682-7_11,
author = {Blumenkamp, Jan and Baude, Andreas and Laue, Tim},
title = {Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_11},
doi = {10.1007/978-3-030-98682-7_11},
abstract = {Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {127–139},
numpages = {13},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.engappai.2021.104473,
author = {Liu, Ze-yu and Liu, Jian-wei and Zuo, Xin and Hu, Ming-fei},
title = {Multi-scale iterative refinement network for RGB-D salient object detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104473},
doi = {10.1016/j.engappai.2021.104473},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {16},
keywords = {Multi-scale refinement, RGB-D image, Salient object detection}
}

@inproceedings{10.1007/978-3-030-27544-0_11,
author = {A\c{s}\i{}k, Okan and G\"{o}rer, Binnur and Ak\i{}n, H. Levent},
title = {End-to-End Deep Imitation Learning: Robot Soccer Case Study},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_11},
doi = {10.1007/978-3-030-27544-0_11},
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {137–149},
numpages = {13},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.5555/3104482.3104571,
author = {Rai, Piyush and III, Hal Daum\'{e}},
title = {Beam search based MAP estimates for the indian buffet process},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Nonparametric latent feature models offer a flexible way to discover the latent features underlying the data, without having to a priori specify their number. The Indian Buffet Process (IBP) is a popular example of such a model. Inference in IBP based models, however, remains a challenge. Sampling techniques such as MCMC can be computationally expensive and can take a long time to converge to the stationary distribution. Variational techniques, although faster than sampling, can be difficult to design, and can still remain slow on large data. In many problems, however, we only seek a maximum a posteriori (MAP) estimate of the latent feature assignment matrix. For such cases, we show that techniques such as beam search can give fast, approximate MAP estimates in the IBP based models. If samples from the posterior are desired, these MAP estimates can also serve as sensible initializers for MCMC based algorithms. Experimental results on a variety of datasets suggest that our algorithms can be a computationally viable alternative to Gibbs sampling, the particle filter, and variational inference based approaches for the IBP, and also perform better than other heuristics such as greedy search.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {705–712},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{10.1007/978-3-030-27272-2_26,
author = {Neher, Helmut and Arlette, John and Wong, Alexander},
title = {Discovery Radiomics for Detection of Severely Atypical Melanocytic Lesions (SAML) from Skin Imaging via Deep Residual Group Convolutional Radiomic Sequencer},
year = {2019},
isbn = {978-3-030-27271-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27272-2_26},
doi = {10.1007/978-3-030-27272-2_26},
abstract = {The incidence of severely atypical melanocytic lesions (SAML) has been increasing year after year. Early detection of SAML by skin surveillance followed by biopsy and treatment may improve survival and reduce the burden on health care systems. Discovery radiomics can be used to analyze a variety of quantitative features present in pigmented lesions that determine which lesions demonstrate enough atypical changes to pursue medical attention. This study utilizes a novel deep residual group convolutional radiomic sequencer to assess SAML. The discovery radiomic sequencer was evaluated against over 18,000 dermoscopic images of different atypical nevi to achieve a sensitivity of 90% and specificity of 83%. Furthermore, the radiomic sequences produced using the novel deep residual group convolutional radiomic sequencer are visualized and analyzed via t-SNE analysis.},
booktitle = {Image Analysis and Recognition: 16th International Conference, ICIAR 2019, Waterloo, ON, Canada, August 27–29, 2019, Proceedings, Part II},
pages = {307–315},
numpages = {9},
keywords = {Deep residual group convolutional radiomic sequencers, Melanoma, Radiomics},
location = {Waterloo, ON, Canada}
}

@inproceedings{10.1007/978-3-030-32248-9_54,
author = {Han, Shuo and Carass, Aaron and Prince, Jerry L.},
title = {Hierarchical Parcellation of the Cerebellum},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_54},
doi = {10.1007/978-3-030-32248-9_54},
abstract = {Parcellation of the cerebellum in an MR image has been used to study regional associations with both motion and cognitive functions. Despite the fact that the division of the cerebellum is defined hierarchically—i.e., the cerebellum can be divided into lobes and the lobes can be further divided into lobules—previous automatic methods to parcellate the cerebellum do not utilize this information. In this work, we propose a method based on convolutional neural networks&nbsp;(CNNs) to explicitly incorporate the hierarchical organization of the cerebellum. The network is constructed in a tree structure with each node representing a cerebellar region and having child nodes that further subdivide the region into finer substructures. Thus, our CNN is aware of the hierarchical organization of the cerebellum. Furthermore, by selecting tree nodes to represent the hierarchical properties of a given training sample, our network can be trained with heterogeneous training data that are labeled to different hierarchical depths. The proposed method was compared with a state-of-the-art cerebellum parcellation network. Our approach shows promising results as a first parcellation method to take the cerebellar hierarchical organization into consideration.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {484–491},
numpages = {8},
location = {Shenzhen, China}
}

@inproceedings{10.5555/2997046.2997184,
author = {Zhu, Jun and Li, Li-Jia and Li, Fei-Fei and Xing, Eric P.},
title = {Large margin learning of upstream scene understanding models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an unbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2586–2594},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@article{10.1007/s10489-021-02324-3,
author = {\c{S}ahin, Canan Batur and Dinler, \"{O}zlem Batur and Abualigah, Laith},
title = {Prediction of software vulnerability based deep symbiotic genetic algorithms: Phenotyping of dominant-features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02324-3},
doi = {10.1007/s10489-021-02324-3},
abstract = {The detection of software vulnerabilities is considered a vital problem in the software security area for a long time. Nowadays, it is challenging to manage software security due to its increased complexity and diversity. So, vulnerability detection applications play a significant part in software development and maintenance. The ability of the forecasting techniques in vulnerability detection is still weak. Thus, one of the efficient defining features methods that have been used to determine the software vulnerabilities is the metaheuristic optimization methods. This paper proposes a novel software vulnerability prediction model based on using a deep learning method and SYMbiotic Genetic algorithm. We are first to apply Diploid Genetic algorithms with deep learning networks on software vulnerability prediction to the best of our knowledge. In this proposed method, a deep SYMbiotic-based genetic algorithm model (DNN-SYMbiotic GAs) is used by learning the phenotyping of dominant-features for software vulnerability prediction problems. The proposed method aimed at increasing the detection abilities of vulnerability patterns with vulnerable components in the software. Comprehensive experiments are conducted on several benchmark datasets; these datasets are taken from Drupal, Moodle, and PHPMyAdmin projects. The obtained results revealed that the proposed method (DNN-SYMbiotic GAs) enhanced vulnerability prediction, which reflects improving software quality prediction.},
journal = {Applied Intelligence},
month = nov,
pages = {8271–8287},
numpages = {17},
keywords = {Dominance mechanism, Symbiotic learning, Genetic algorithms, Software vulnerability, Deep learning}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1821–1830},
numpages = {10}
}

@article{10.1016/j.neunet.2006.01.018,
author = {Raudys, \v{S}arnas},
title = {Trainable fusion rules. I. Large sample size case},
year = {2006},
issue_date = {December, 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {19},
number = {10},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2006.01.018},
doi = {10.1016/j.neunet.2006.01.018},
abstract = {A wide selection of standard statistical pattern classification algorithms can be applied as trainable fusion rules while designing neural network ensembles. A focus of the present two-part paper is finite sample effects: the complexity of base classifiers and fusion rules; the type of outputs provided by experts to the fusion rule; non-linearity of the fusion rule; degradation of experts and the fusion rule due to the lack of information in the design set; the adaptation of base classifiers to training set size, etc. In the first part of this paper, we consider arguments for utilizing continuous outputs of base classifiers versus categorical outputs and conclude: if one succeeds in having a small number of expert networks working perfectly in different parts of the input feature space, then crisp outputs may be preferable over continuous outputs. Afterwards, we oppose fixed fusion rules versus trainable ones and demonstrate situations where weighted average fusion can outperform simple average fusion. We present a review of statistical classification rules, paying special attention to these linear and non-linear rules, which are employed rarely but, according to our opinion, could be useful in neural network ensembles. We consider ideal and sample-based oracle decision rules and illustrate characteristic features of diverse fusion rules by considering an artificial two-dimensional (2D) example where the base classifiers perform well in different regions of input feature space.},
journal = {Neural Netw.},
month = dec,
pages = {1506–1516},
numpages = {11},
keywords = {Neural network ensembles, Multiple classifiers systems, Learning set, Generalization error, Fusion, Complexity, Classifier combination}
}

@article{10.1016/j.neucom.2019.04.017,
author = {Lin, Jiatai and Liu, Zhi and Chen, C.L. Philip and Zhang, Yun},
title = {A wavelet broad learning adaptive filter for forecasting and cancelling the physiological tremor in teleoperation},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {356},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.017},
doi = {10.1016/j.neucom.2019.04.017},
journal = {Neurocomput.},
month = sep,
pages = {170–183},
numpages = {14},
keywords = {Incremental learning, Self-paced wavelet auto-encoder(SPWAE), Wavelet broad learning adaptive filter(WBLAF)}
}

@article{10.1155/2018/2678976,
author = {Wang, Yuanlong and Li, Ru and Zhang, Hu and Tan, Hongyan and Chai, Qinghua and Hao, Tianyong},
title = {Using Sentence-Level Neural Network Models for Multiple-Choice Reading Comprehension Tasks},
year = {2018},
issue_date = {2018},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2018},
issn = {1530-8669},
url = {https://doi.org/10.1155/2018/2678976},
doi = {10.1155/2018/2678976},
abstract = {Comprehending unstructured text is a challenging task for machines because it involves understanding texts and answering questions. In this paper, we study the multiple-choice task for reading comprehension based on MC Test datasets and Chinese reading comprehension datasets, among which Chinese reading comprehension datasets which are built by ourselves. Observing the above-mentioned training sets, we find that “sentence comprehension” is more important than “word comprehension” in multiple-choice task, and therefore we propose sentence-level neural network models. Our model firstly uses LSTM network and a composition model to learn compositional vector representation for sentences and then trains a sentence-level attention model for obtaining the sentence-level attention between the sentence embedding in documents and the optional sentences embedding by dot product. Finally, a consensus attention is gained by merging individual attention with the merging function. Experimental results show that our model outperforms various state-of-the-art baselines significantly for both the multiple-choice reading comprehension datasets.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {8}
}

@article{10.1007/s10009-014-0341-2,
author = {Filho, Jo\~{a}o Bosco and Barais, Olivier and Acher, Mathieu and Le Noir, J\'{e}r\^{o}me and Legay, Axel and Baudry, Benoit},
title = {Generating counterexamples of model-based software product lines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0341-2},
doi = {10.1007/s10009-014-0341-2},
abstract = {In a model-based software product line (MSPL), the variability of the domain is characterized in a variability model and the core artifacts are base models conforming to a modeling language (also called metamodel). A realization model connects the features of the variability model to the base model elements, triggering operations over these elements based on a configuration. The design space of an MSPL is extremely complex to manage for the engineer, since the number of variants may be exponential and the derived product models have to be conforming to numerous well-formedness and business rules. In this paper, the objective is to provide a way to generate MSPLs, called counterexamples (also called antipatterns), that can produce invalid product models despite a valid configuration in the variability model. We describe the foundations and motivate the usefulness of counterexamples (e.g., inference of guidelines or domain-specific rules to avoid earlier the specification of incorrect mappings; testing oracles for increasing the robustness of derivation engines given a modeling language). We provide a generic process, based on the common variability language (CVL) to randomly search the space of MSPLs for a specific modeling language. We develop LineGen a tool on top of CVL and modeling technologies to support the methodology and the process. LineGen targets different scenarios and is flexible to work either with just a domain metamodel as input or also with pre-defined variability models and base models. We validate the effectiveness of this process for three formalisms at different scales (up to 247 metaclasses and 684 rules). We also apply the approach in the context of a real industrial scenario involving a large-scale metamodel.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {585–600},
numpages = {16},
keywords = {Software product lines, Model-based engineering, Counterexamples}
}

@inproceedings{10.1145/3136755.3143012,
author = {Ouyang, Xi and Kawaai, Shigenori and Goh, Ester Gue Hua and Shen, Shengmei and Ding, Wan and Ming, Huaiping and Huang, Dong-Yan},
title = {Audio-visual emotion recognition using deep transfer learning and multiple temporal models},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3143012},
doi = {10.1145/3136755.3143012},
abstract = {This paper presents the techniques used in our contribution to Emotion Recognition in the Wild 2017 video based sub-challenge. The purpose of the sub-challenge is to classify the six basic emotions (angry, sad, happy, surprise, fear and disgust) and neutral. Our proposed solution utilizes three state-of-the-arts techniques to overcome the challenges for the wild emotion recognition. Deep network transfer learning is used for feature extraction. Spatial-temporal model fusion is to make full use of the complementary of different networks. Semi-auto reinforcement learning is for the optimization of fusion strategy based on dynamic outside feedbacks given by challenge organizers. The overall accuracy of the proposed approach on the challenge test dataset is 57.2%, which is better than the challenge baseline of 40.47% .},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {577–582},
numpages = {6},
keywords = {Transfer Learning, Reinforcement Learning, Model Fusion, Long Short Term Memory network, Emotion Recognition, Convolutional Neural Network, 3D convolutional Network},
location = {Glasgow, UK},
series = {ICMI '17}
}

@article{10.1016/j.patcog.2017.04.024,
author = {Ren, Jianfeng and Jiang, Xudong},
title = {Regularized 2-D complex-log spectral analysis and subspace reliability analysis of micro-Doppler signature for UAV detection},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.04.024},
doi = {10.1016/j.patcog.2017.04.024},
abstract = {The proposed 2-D regularized complex-log-Fourier transform better represents mDS.The proposed subspace reliability analysis better removes unreliable dimensions.The proposed approach demonstrates superior performance for UAV detection. Unmanned aerial vehicle (UAV) has become an important radar target recently because of its wide applications and potential security threats. Traditionally, visual features such as spectrogram were often extracted for human operators to identify the micro-Doppler signature (mDS) of UAVs, i.e. sinusoidal modulation. In this paper, the authors aim to design a system for machine automatic classification of UAVs from other targets, particularly from birds as both UAVs and birds are small and slow-moving radar targets. Most existing mDS representations such as spectrogram, cepstrogram and cadence velocity diagram discard the phase spectrum, and only make use of the magnitude spectrum. Whats more, people often take the logarithm of the spectrum to enlarge the weak mDS but without sufficient care, as noise may be enlarged at the same time. The authors thus propose a regularized 2-D complex-log-Fourier transform to address these problems. Furthermore, the authors propose an object-oriented dimension-reduction technique: subspace reliability analysis, which directly removes the unreliable feature dimensions of two class-conditional covariance matrices in two separate subspaces. On the benchmark dataset, the proposed approach demonstrates better performance than the state-of-the-art approaches. More specifically, the proposed approach significantly reduces the equal error rate of the second best approach, cadence velocity diagram, from 6.68% to 3.27%.},
journal = {Pattern Recogn.},
month = sep,
pages = {225–237},
numpages = {13},
keywords = {UAV detection, Subspace reliability analysis, Radar, Micro-Doppler signature, 2-D regularized complex-log-Fourier transform}
}

@inproceedings{10.1145/3349341.3349407,
author = {Ma, Yaolong and Zhang, Yingzhong and Luo, Xiaofang},
title = {Automatic Recognition of Machining Features Based on Point Cloud Data Using Convolution Neural Networks},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349407},
doi = {10.1145/3349341.3349407},
abstract = {Automatic recognition of machining features is the key technology to realize the integration of CAD/CAM. With the development of intelligent manufacturing technology, it has important significance to automatically recognize machining features from parts represented by real-time point clouds. At the same time, it puts new challenges for feature recognition technology. In this paper, based on the 3D point cloud data of part models and PointNet architecture, an approach and data structure for automatic recognition of machining features using convolution neural networks (CNN) is proposed. A sample library for learning 3D point cloud data is constructed by CAD model transformation and feature sampling. The presented CNN recognition system can recognize twenty-four kinds of machined features by sample training and recognition experiments. The recognition accuracy rate is more than 95%. This approach makes full use of the invariance of transformation of the point cloud model, and reduces the unnecessary data that is used by conventional 3D voxel network models. The presented method is simple and efficient, and has good robustness to the point cloud disturbance and noise.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {229–235},
numpages = {7},
keywords = {Machining feature, Feature recognition, Convolution neural network, 3D point cloud},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.5555/1558109.1558283,
author = {Nunes, Ingrid and Kulesza, Uir\'{a} and Nunes, Camila and Lucena, Carlos J. P.},
title = {A domain engineering process for developing multi-agent systems product lines},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent Systems Product Lines (MAS-PLs) have emerged to integrate two promising trends of software engineering: agent-oriented software engineering and software product lines. In this paper, we propose a domain engineering process to develop MAS-PLs, built on top of agent-oriented and software product line approaches.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1339–1340},
numpages = {2},
keywords = {software product lines, process, multi-agent systems, domain engineering, agent-oriented software engineering},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@article{10.1016/j.cl.2018.04.004,
author = {Zhao, Tian and Huang, Xiaobing},
title = {Design and implementation of DeepDSL: A DSL for deep learning},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.04.004},
doi = {10.1016/j.cl.2018.04.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {39–70},
numpages = {32}
}

@article{10.1016/j.patcog.2019.106972,
author = {Dong, Ganggang and Liu, Hongwei and Kuang, Gangyao and Chanussot, Jocelyn},
title = {Target recognition in SAR images via sparse representation in the frequency domain},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.106972},
doi = {10.1016/j.patcog.2019.106972},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Target recognition, Transformed domain, Sparse representation}
}

@article{10.1016/j.ins.2019.12.046,
author = {Chen, Dongzi and Yang, Qinli and Liu, Jiaming and Zeng, Zhu},
title = {Selective prototype-based learning on concept-drifting data streams},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {516},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.046},
doi = {10.1016/j.ins.2019.12.046},
journal = {Inf. Sci.},
month = apr,
pages = {20–32},
numpages = {13},
keywords = {99-00, 00-01, Prototype, Classification, Concept drift, Data stream}
}

@article{10.1016/j.neucom.2021.01.040,
author = {Mo, Linzhang and Wei, Jielong and Huang, Qingbao and Cai, Yi and Liu, Qingguang and Zhang, Xingmao and Li, Qing},
title = {Incorporating sentimental trend into gated mechanism based transformer network for story ending generation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {453},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.01.040},
doi = {10.1016/j.neucom.2021.01.040},
journal = {Neurocomput.},
month = sep,
pages = {453–464},
numpages = {12},
keywords = {Gated mechanism, Transformer, Sentiment trend, Story ending generation}
}

@inproceedings{10.1007/978-3-030-30645-8_7,
author = {Bucci, Silvia and D’Innocente, Antonio and Tommasi, Tatiana},
title = {Tackling Partial Domain Adaptation with Self-supervision},
year = {2019},
isbn = {978-3-030-30644-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30645-8_7},
doi = {10.1007/978-3-030-30645-8_7},
abstract = {Domain adaptation approaches have shown promising results in reducing the marginal distribution difference among visual domains. They allow to train reliable models that work over datasets of different nature (photos, paintings etc.), but they still struggle when the domains do not share an identical label space. In the partial domain adaptation setting, where the target covers only a subset of the source classes, it is challenging to reduce the domain gap without incurring in negative transfer. Many solutions just keep the standard domain adaptation techniques by adding heuristic sample weighting strategies. In this work we show how the self-supervisory signal obtained from the spatial co-location of patches can be used to define a side task that supports adaptation regardless of the exact label sharing condition across domains. We build over a recent work that introduced a jigsaw puzzle task for domain generalization: we describe how to reformulate this approach for partial domain adaptation and we show how it boosts existing adaptive solutions when combined with them. The obtained experimental results on three datasets supports the effectiveness of our approach.},
booktitle = {Image Analysis and Processing – ICIAP 2019: 20th International Conference, Trento, Italy, September 9–13, 2019, Proceedings, Part II},
pages = {70–81},
numpages = {12},
keywords = {Multi-task learning, Self-supervision, Domain adaptation},
location = {Trento, Italy}
}

@article{10.1109/TSE.2013.37,
author = {Esfahani, Naeem and Elkhodary, Ahmed and Malek, Sam},
title = {A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems},
year = {2013},
issue_date = {November 2013},
publisher = {IEEE Press},
volume = {39},
number = {11},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2013.37},
doi = {10.1109/TSE.2013.37},
abstract = {Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.},
journal = {IEEE Trans. Softw. Eng.},
month = nov,
pages = {1467–1493},
numpages = {27},
keywords = {machine learning, feature-orientation, autonomic computing, Self-adaptive software}
}

@inproceedings{10.1145/2911451.2911510,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Engineering Quality and Reliability in Technology-Assisted Review},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911510},
doi = {10.1145/2911451.2911510},
abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {75–84},
numpages = {10},
keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1007/978-3-030-58571-6_2,
author = {Du, Heming and Yu, Xin and Zheng, Liang},
title = {Learning Object Relation Graph and Tentative Policy for Visual Navigation},
year = {2020},
isbn = {978-3-030-58570-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58571-6_2},
doi = {10.1007/978-3-030-58571-6_2},
abstract = {Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII},
pages = {19–34},
numpages = {16},
keywords = {Visual navigation, Tentative policy learning, Imitation learning, Graph},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-031-02444-3_32,
author = {Pi, Zhixiong and Gao, Changxin and Sang, Nong},
title = {Siamese Tracking with&nbsp;Bilinear Features},
year = {2021},
isbn = {978-3-031-02443-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-02444-3_32},
doi = {10.1007/978-3-031-02444-3_32},
abstract = {Bilinear features arise in fine-grained visual recognition. They are advantageous to encode detailed representations and attributes to differentiate visually similar objects. The apparent similarity is challenging in visual tracking where background distractors interfere siamese trackers to localize the target object. Especially when distractors and the target belong to the same object category. To increase the discrimination between similar appearance objects, we propose an efficient bilinear encoding method for siamese tracking. The proposed method consists of a self-bilinear encoder and an cross-bilinear encoder. The bilinear features generated via the self-bilinear encoder and the cross-bilinear encoder represent target variations itself and target distractor difference, respectively. To this end, the proposed bilinear encoders advance siamese trackers to capture target appearance variations while differentiating the target and background distractors. Experiments on the benchmark datasets show the effectiveness of bilinear features. Our tracker performs favorably against state-of-the-art approaches.},
booktitle = {Pattern Recognition: 6th Asian Conference, ACPR 2021, Jeju Island, South Korea, November 9–12, 2021, Revised Selected Papers, Part II},
pages = {421–435},
numpages = {15},
keywords = {Visual tracking, Bilinear feature, Siamese network},
location = {Jeju Island, Korea (Republic of)}
}

@article{10.5555/1460332.1460337,
author = {Vityaev, E. E. and Lapardin, K. A. and Khomicheva, I. V. and Proskura, A. L.},
title = {Transcription factor binding site recognition by regularity matrices based on the natural classification method},
year = {2008},
issue_date = {December 2008},
publisher = {IOS Press},
address = {NLD},
volume = {12},
number = {5},
issn = {1088-467X},
abstract = {A principally new approach to the classifications of nucleotide sequences based on the "natural" classification concept is proposed. As a result of "natural" classification of the nucleotide sequences, we obtain regularity matrices, where nucleotides are interconnected by regularities. Method, algorithm and software system DNANatClass for performing the "natural" classification have been developed. Experimental results comparing weight matrices with regularity matrices are presented. In this experiment, site recognition by regularity matrices appears to be more accurate than by weight matrixes.},
journal = {Intell. Data Anal.},
month = nov,
pages = {495–512},
numpages = {18}
}

@inproceedings{10.1145/3458817.3480856,
author = {Md, Vasimuddin and Misra, Sanchit and Ma, Guixiang and Mohanty, Ramanarayan and Georganas, Evangelos and Heinecke, Alexander and Kalamkar, Dhiraj and Ahmed, Nesreen K. and Avancha, Sasikanth},
title = {DistGNN: scalable distributed training for large-scale graph neural networks},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3480856},
doi = {10.1145/3458817.3480856},
abstract = {Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7\texttimes{} speed-up using a single CPU socket and up to 97\texttimes{} speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {76},
numpages = {14},
keywords = {graph partition, graph neural networks, distributed algorithm, deep learning, deep graph library},
location = {St. Louis, Missouri},
series = {SC '21}
}

@inproceedings{10.1007/978-3-030-26142-9_9,
author = {Wang, Yunyun and Zhao, Dan and Li, Yun and Chen, Kejia and Xue, Hui},
title = {The Most Related Knowledge First: A Progressive Domain Adaptation Method},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_9},
doi = {10.1007/978-3-030-26142-9_9},
abstract = {In domain adaptation, how to select and transfer related knowledge is critical for learning. Inspired by the fact that human usually transfer from the more related experience to the less related one, in this paper, we propose a novel progressive domain adaptation (PDA) model, which attempts to transfer source knowledge by considering the transfer order based on relevance. Specifically, PDA transfers source instances iteratively from the most related ones to the least related ones, until all related source instances have been adopted. It is an iterative learning process, source instances adopted in each iteration are determined by a gradually annealed weight such that the later iteration will introduce more source instances. Further, a reverse classification performance is used to set the termination of iteration. Experiments on real datasets demonstrate the competiveness of PDA compared with the state-of-arts.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {90–102},
numpages = {13},
keywords = {Reverse classification, Iteration, Progressive transfer, Domain adaptation},
location = {Macau, China}
}

@inproceedings{10.5555/3540261.3542304,
author = {Zhang, Jiangning and Xu, Chao and Li, Jian and Chen, Wenzhou and Wang, Yabiao and Tai, Ying and Chen, Shuo and Wang, Chengjie and Huang, Feiyue and Liu, Yong},
title = {Analogous to evolutionary algorithm: designing a unified sequence model},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2043},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-58539-6_16,
author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
title = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_16},
doi = {10.1007/978-3-030-58539-6_16},
abstract = {Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {259–274},
numpages = {16},
keywords = {Embodied AI, Transfer learning, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-030-27272-2_20,
author = {Adegun, Adekanmi and Viriri, Serestina},
title = {Deep Learning Model for Skin Lesion Segmentation: Fully Convolutional Network},
year = {2019},
isbn = {978-3-030-27271-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27272-2_20},
doi = {10.1007/978-3-030-27272-2_20},
abstract = {Segmentation of skin lesions is a crucial task in detecting and diagnosing melanoma cancer. Incidence of melanoma skin cancer which is the most deadly form of skin cancer has been on steady increase. Early detection of the melanoma cancer is necessary to improve the survival rate of the patients. Segmentation is an important task in analysing skin lesion images. Skin lesion segmentation has come with some challenges such as low contrast and fine grained nature of skin lesions. This has necessitated the need for automated analysis and segmentation of skin lesions using state-of-the-arts techniques. In this paper, a deep learning model has been adapted for the segmentation of skin lesions. This work demonstrates the segmentation of skin lesions using fully convolutional networks (FCNs) that train skin lesion images from end-to-end using only the images pixels and disease ground truth labels as inputs. The fully convolutional network adapted is based on U-Net architecture. The model is enhanced by employing multi-stage segmentation approach with batch normalisation and data augmentation. Performance metrics such as dice coefficient, accuracy, sensitivity and specificity were used for evaluating the performance of the model. Experimental results show that the proposed model achieved better performance compared with the other state-of-the arts methods for skin lesion image segmentation with a dice coefficient of  and sensitivity of .},
booktitle = {Image Analysis and Recognition: 16th International Conference, ICIAR 2019, Waterloo, ON, Canada, August 27–29, 2019, Proceedings, Part II},
pages = {232–242},
numpages = {11},
keywords = {U-Net, FCNs, Segmentation, Deep learning, Skin lesions, Melanoma},
location = {Waterloo, ON, Canada}
}

@inproceedings{10.1007/978-3-030-78270-2_74,
author = {Yun, Yue and Dai, Huan and Cao, Ruoqi and Zhang, Yupei and Shang, Xuequn},
title = {Self-paced Graph Memory Network for Student GPA Prediction and Abnormal Student Detection},
year = {2021},
isbn = {978-3-030-78269-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78270-2_74},
doi = {10.1007/978-3-030-78270-2_74},
abstract = {Student learning performance prediction (SLPP) is a crucial step in high school education. However, traditional methods fail to consider abnormal students. In this study, we organized every student’s learning data as a graph to use the schema of graph memory networks (GMNs). To distinguish the students and make GMNs learn robustly, we proposed to train GMNs in an “easy-to-hard” process, leading to self-paced graph memory network (SPGMN). SPGMN chooses the low-difficult samples as a batch to tune the model parameters in each training iteration. This approach not only improves the robustness but also rearranges the student sample from normal to abnormal. The experiment results show that SPGMN achieves a higher prediction accuracy and more robustness in comparison with traditional methods. The resulted student sequence reveals the abnormal student has a different pattern in course selection to normal students.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II},
pages = {417–421},
numpages = {5},
keywords = {Abnormal student detection, Graph memory networks, Self-paced learning, Student learning performance prediction},
location = {Utrecht, The Netherlands}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1016/j.compeleceng.2019.07.012,
author = {Asif, Muhammad Rizwan and Qi, Chun and Wang, Tiexiang and Fareed, Muhammad Sadiq and Raza, Syed Ali},
title = {License plate detection for multi-national vehicles: An illumination invariant approach in multi-lane environment},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2019.07.012},
doi = {10.1016/j.compeleceng.2019.07.012},
journal = {Comput. Electr. Eng.},
month = sep,
pages = {132–147},
numpages = {16},
keywords = {Vehicle rear lights, Vehicle identification, Corona effect, Multi-national vehicles, License plate detection, Intelligent traffic system}
}

@inproceedings{10.1109/COMPSAC.2012.95,
author = {Sabouri, Hamideh and Jaghoori, Mohammad Mahdi and Boer, Frank de and Khosravi, Ramtin},
title = {Scheduling and Analysis of Real-Time Software Families},
year = {2012},
isbn = {9780769547367},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2012.95},
doi = {10.1109/COMPSAC.2012.95},
abstract = {A software product line describes explicitly the commonalities of and differences between different products in a family of (software) systems. A formalization of these commonalities and differences amounts to reduced development, analysis and maintenance costs in the practice of software engineering. An important feature common to next-generation real-time software systems is the need of application-level control over scheduling for optimized utilization of resources provided by for example many-core and cloud infrastructures. In this paper, we introduce a formal model of real-time software product lines which supports variability in scheduling policies and rigorous and efficient techniques for modular schedulability analysis.},
booktitle = {Proceedings of the 2012 IEEE 36th Annual Computer Software and Applications Conference},
pages = {680–689},
numpages = {10},
keywords = {Software Product Lines, Real-Time, Formal Methods, Automata Theory, Application-level Scheduling},
series = {COMPSAC '12}
}

@article{10.1016/j.robot.2009.03.006,
author = {Cherubini, A. and Giannone, F. and Iocchi, L. and Lombardo, M. and Oriolo, G.},
title = {Policy gradient learning for a humanoid soccer robot},
year = {2009},
issue_date = {July, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {8},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2009.03.006},
doi = {10.1016/j.robot.2009.03.006},
abstract = {In humanoid robotic soccer, many factors, both at low-level (e.g., vision and motion control) and at high-level (e.g., behaviors and game strategies), determine the quality of the robot performance. In particular, the speed of individual robots, the precision of the trajectory, and the stability of the walking gaits, have a high impact on the success of a team. Consequently, humanoid soccer robots require fine tuning, especially for the basic behaviors. In recent years, machine learning techniques have been used to find optimal parameter sets for various humanoid robot behaviors. However, a drawback of learning techniques is time consumption: a practical learning method for robotic applications must be effective with a small amount of data. In this article, we compare two learning methods for humanoid walking gaits based on the Policy Gradient algorithm. We demonstrate that an extension of the classic Policy Gradient algorithm that takes into account parameter relevance allows for better solutions when only a few experiments are available. The results of our experimental work show the effectiveness of the policy gradient learning method, as well as its higher convergence rate, when the relevance of parameters is taken into account during learning.},
journal = {Robot. Auton. Syst.},
month = jul,
pages = {808–818},
numpages = {11},
keywords = {Motion control, Machine learning, Humanoid robotics}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Software engineering, Evolution, Automation, Automated production systems}
}

@article{10.1016/j.compag.2006.01.004,
author = {Pydipati, R. and Burks, T. F. and Lee, W. S.},
title = {Identification of citrus disease using color texture features and discriminant analysis},
year = {2006},
issue_date = {June, 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {1–2},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2006.01.004},
doi = {10.1016/j.compag.2006.01.004},
abstract = {The citrus industry is an important constituent of Florida's overall agricultural economy. Proper disease control measures must be undertaken in citrus groves to minimize losses. Technological strategies using machine vision and artificial intelligence are being investigated to achieve intelligent farming, including early detection of diseases in groves, selective fungicide application, etc. This research used the color co-occurrence method (CCM) to determine whether texture based hue, saturation, and intensity (HSI) color features in conjunction with statistical classification algorithms could be used to identify diseased and normal citrus leaves under laboratory conditions. Normal and diseased citrus leaf samples with greasy spot, melanose, and scab were evaluated. The leaf sample discriminant analysis using CCM textural features achieved classification accuracies of over 95% for all classes when using hue and saturation texture features. Data models that relied on intensity features suffered a reduction in classification accuracy when categorizing leaf fronts, due to the darker pigmentation of the leaf fronts. This reduction was not experienced on the leaf backs where the lighter pigmentation clearly revealed the disease discoloration. Although, high accuracies were achieved when using an unreduced dataset consisting of all HSI texture features, the overall best performer was determined to be a reduced data model that relied on hue and saturation features. This model was selected due to reduced computational load and the elimination of intensity features, which are not robust in the presence of ambient light variation.},
journal = {Comput. Electron. Agric.},
month = jun,
pages = {49–59},
numpages = {11},
keywords = {Texture features, Machine vision, Disease detection, Discriminant classifier, Citrus}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Stretch regularization, Label consistency constraint, Dictionary learning, Person re-identification}
}

@inproceedings{10.1109/CIG.2018.8490439,
author = {Gaina, Raluca D. and Lucas, Simon M. and Perez-Liebana, Diego},
title = {General Win Prediction from Agent Experience},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CIG.2018.8490439},
doi = {10.1109/CIG.2018.8490439},
abstract = {The question of whether the correct algorithm is used for the problem at hand usually comes at the end of execution, when the algorithm’s ability to solve the problem (or not) can be verified. But what if this question could be answered in advance, with enough notice to make changes in the approach in order for it to be more successful? This paper proposes a general agent performance prediction system, tested in real time within the context of the General Video Game AI framework. It is solely based on agent features, therefore removing potential human bias produced by game-based features observed in known games. Three different models can be queried while playing the game to determine whether the agent will win or lose, based on the current game state: early, mid and late game feature models. The models are trained on 80 games in the framework and tested on 20 new games, for 14 variations of 3 different methods. Results are positive, indicating that there is great scope for predicting the outcome of any given game.},
booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
pages = {1–8},
numpages = {8},
location = {Maastricht, Netherlands}
}

@article{10.1016/j.jvcir.2019.04.009,
author = {Ye, Yingsheng and Zhang, Xingming and Lin, Yubei and Wang, Haoxiang},
title = {Facial expression recognition via region-based convolutional fusion network},
year = {2019},
issue_date = {Jul 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {62},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2019.04.009},
doi = {10.1016/j.jvcir.2019.04.009},
journal = {J. Vis. Comun. Image Represent.},
month = jul,
pages = {1–11},
numpages = {11},
keywords = {Convolution neural network, Emotion recognition, Facial expression recognition}
}

@article{10.1007/s00779-011-0468-z,
author = {Broek, Egon L. and Sluis, Frans and Dijkstra, Ton},
title = {Cross-validation of bimodal health-related stress assessment},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-011-0468-z},
doi = {10.1007/s00779-011-0468-z},
abstract = {This study explores the feasibility of objective and ubiquitous stress assessment. 25 post-traumatic stress disorder patients participated in a controlled storytelling (ST) study and an ecologically valid reliving (RL) study. The two studies were meant to represent an early and a late therapy session, and each consisted of a "happy" and a "stress triggering" part. Two instruments were chosen to assess the stress level of the patients at various point in time during therapy: (i) speech, used as an objective and ubiquitous stress indicator and (ii) the subjective unit of distress (SUD), a clinically validated Likert scale. In total, 13 statistical parameters were derived from each of five speech features: amplitude, zero-crossings, power, high-frequency power, and pitch. To model the emotional state of the patients, 28 parameters were selected from this set by means of a linear regression model and, subsequently, compressed into 11 principal components. The SUD and speech model were cross-validated, using 3 machine learning algorithms. Between 90% (2 SUD levels) and 39% (10 SUD levels) correct classification was achieved. The two sessions could be discriminated in 89% (for ST) and 77% (for RL) of the cases. This report fills a gap between laboratory and clinical studies, and its results emphasize the usefulness of Computer Aided Diagnostics (CAD) for mental health care.},
journal = {Personal Ubiquitous Comput.},
month = feb,
pages = {215–227},
numpages = {13},
keywords = {Validity, Stress, Speech, Post-traumatic stress disorder (PTSD), Machine learning, Computer aided diagnostics (CAD)}
}

@inproceedings{10.1145/3350546.3352496,
author = {Rafailidis, Dimitrios},
title = {Bayesian Deep Learning with Trust and Distrust in Recommendation Systems},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352496},
doi = {10.1145/3350546.3352496},
abstract = {Exploiting the selections of social friends and foes can efficiently face the data scarcity of user preferences and the cold-start problem. In this paper, we present a Social Deep Pairwise Learning model, namely SDPL. According to the Bayesian Pairwise Ranking criterion, we design a loss function with multiple ranking criteria based on the selections of users, and those in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinearity in user preferences and the social information of trust and distrust relationships by designing a deep learning architecture. In each backpropagation step, we perform social negative sampling to meet the multiple ranking criteria of our loss function. Our experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature, demonstrate the effectiveness of the proposed approach, outperforming other state-of-the art methods. In addition, we show that our deep learning strategy plays an important role in capturing the nonlinear associations between user preferences and the social information of trust and distrust relationships, and demonstrate that our social negative sampling strategy is a key factor in SDPL.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
keywords = {social relationships, deep learning, collaborative filtering, Pairwise learning},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@article{10.1016/j.imavis.2019.10.003,
author = {Son Ly, Thai and Do, Nhu-Tai and Kim, Soo-Hyung and Yang, Hyung-Jeong and Lee, Guee-Sang},
title = {A novel 2D and 3D multimodal approach for in-the-wild facial expression recognition},
year = {2019},
issue_date = {Dec 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {92},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2019.10.003},
doi = {10.1016/j.imavis.2019.10.003},
journal = {Image Vision Comput.},
month = dec,
numpages = {12},
keywords = {2D + 3D FER, In-the-wild FER, Facial expression recognition}
}

@inproceedings{10.1007/978-3-030-72610-2_5,
author = {Badryzlova, Yulia and Nikiforova, Anastasia and Lyashevskaya, Olga},
title = {Do Topics Make a Metaphor? Topic Modeling for Metaphor Identification and Analysis in Russian},
year = {2020},
isbn = {978-3-030-72609-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72610-2_5},
doi = {10.1007/978-3-030-72610-2_5},
abstract = {The paper examines the efficiency of topic models as features for computational identification and conceptual analysis of linguistic metaphor on Russian data. We train topic models using three algorithms (LDA and ARTM – sparse and dense) and evaluate their quality. We compute topic vectors for sentences of a metaphor-annotated Russian corpus and train several classifiers to identify metaphor with these vectors. We compare the performance of the topic modeling classifiers with other state-of-the-art features (lexical, morphosyntactic, semantic coherence, and concreteness-abstractness) and their different combinations to see how topics contribute to metaphor identification. We show that some of the topics are more frequent in metaphoric contexts while others are more characteristic of non-metaphoric sentences, thus constituting topic predictors of metaphoricity, and discuss whether these predictors align with the conceptual mappings attested in literature. We also compare the topical heterogeneity of metaphoric and non-metaphoric contexts in order to test the hypothesis that metaphoric discourse should display greater topical variability due to the presence of Source and Target domains.},
booktitle = {Analysis of Images, Social Networks and Texts: 9th International Conference, AIST 2020, Skolkovo, Moscow, Russia, October 15–16, 2020, Revised Selected Papers},
pages = {69–81},
numpages = {13},
keywords = {Topical heterogeneity, Topical profiles, Topical predictors of metaphoricity, ARTM, LDA, Topic modelling, Metaphor identification},
location = {Moscow, Russia}
}

@inproceedings{10.5555/3044805.3044990,
author = {Chakrabarti, Deepayan and Funiak, Stanislav and Chang, Jonathan and Macskassy, Sofus A.},
title = {Joint inference of multiple label types in large networks},
year = {2014},
publisher = {JMLR.org},
abstract = {We tackle the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as home-town, current city, and employers, for users connected by a social network. Standard label propagation fails to consider the properties of the label types and the interactions between them. Our proposed method, called EDGEEXPLAIN, explicitly models these, while still enabling scalable inference under a distributed message-passing architecture. On a billion-node subset of the Facebook social network, EDGEEXPLAIN significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–874–II–882},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1007/978-3-030-89370-5_18,
author = {Tian, Yuze and Zhong, Xian and Liu, Wenxuan and Jia, Xuemei and Zhao, Shilei and Ye, Mang},
title = {Random Walk Erasing with Attention Calibration for Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_18},
doi = {10.1007/978-3-030-89370-5_18},
abstract = {Action recognition in videos has attracted growing research interests because of the explosive surveillance data in social security applications. In this process, due to the distraction and deviation of the network caused by occlusions, human action features usually suffer different degrees of performance degradation. Considering the occlusion scene in the wild, we find that the occluded objects usually move unpredictably but continuously. Thus, we propose a random walk erasing with attention calibration (RWEAC) for action recognition. Specifically, we introduce the random walk erasing (RWE) module to simulate the unknown occluded real conditions in frame sequence, expanding the diversity of data samples. In the case of erasing (or occlusion), the attention area is sparse. We leverage the attention calibration (AC) module to force the attention to stay stable in other regions of interest. In short, our novel RWEAC network enhances the ability to learn comprehensive features in a complex environment and make the feature representation robust. Experiments are conducted on the challenging video action recognition UCF101 and HMDB51 datasets. The extensive comparison results and ablation studies demonstrate the effectiveness and strength of the proposed method.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {236–251},
numpages = {16},
keywords = {Siamese network, Attention calibration, Data augmentation, Random walk erasing, Action recognition},
location = {Hanoi, Vietnam}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@inproceedings{10.5555/1596324.1596361,
author = {Morante, Roser and Daelemans, Walter and Van Asch, Vincent},
title = {A combined memory-based semantic role labeler of English},
year = {2008},
isbn = {9781905593484},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. Syntactic dependencies are processed with the Malt-Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers. The system achieves 78.43 labeled macro F1 for the complete problem, 86.07 labeled attachment score for syntactic dependencies, and 70.51 labeled F1 for semantic dependencies.},
booktitle = {Proceedings of the Twelfth Conference on Computational Natural Language Learning},
pages = {208–212},
numpages = {5},
location = {Manchester, United Kingdom},
series = {CoNLL '08}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@inproceedings{10.1145/3460418.3479281,
author = {Adhikari, Aakriti and Sur, Sanjib},
title = {MilliPose: Facilitating Full Body Silhouette Imaging from Millimeter-Wave Device},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479281},
doi = {10.1145/3460418.3479281},
abstract = {This paper proposes MilliPose, a system that facilitates full human body silhouette imaging and 3D pose estimation from millimeter-wave (mmWave) devices. Unlike existing vision-based motion capture systems, MilliPose is not privacy-invasive and is capable of working under obstructions, poor visibility, and low light conditions. MilliPose leverages machine-learning models based on conditional Generative Adversarial Networks and Recurrent Neural Network to solve the challenges of poor resolution, specularity, and variable reflectivity with existing mmWave imaging systems. Our preliminary results show the efficacy of MilliPose in accurately predicting body joint locations under natural human movement.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {1–3},
numpages = {3},
keywords = {Recurrent Neural Network, Millimeter-Wave, Conditional Generative Adversarial Networks},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@article{10.1016/j.dsp.2018.12.007,
author = {Yue, Guanghui and Hou, Chunping and Yan, Weiqing and Choi, Lark Kwon and Zhou, Tianwei and Hou, Yonghong},
title = {Blind quality assessment for screen content images via convolutional neural network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2018.12.007},
doi = {10.1016/j.dsp.2018.12.007},
journal = {Digit. Signal Process.},
month = aug,
pages = {21–30},
numpages = {10},
keywords = {Convolutional neural network (CNN), Image quality assessment (IQA), Blind/no reference (NR), Screen content image (SCI)}
}

@article{10.5555/3135535.3135553,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with neurological diseases using PCA and NPCA},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
abstract = {In this study, we wanted to discriminate between 30 patients who suffer from Parkinson's disease (PD) and 20 patients with other neurological diseases (ND). All participants were asked to pronounce sustained vowel /a/ hold as long as possible at comfortable level. The analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used principal component analysis (PCA) and nonlinear PCA (NPCA). These techniques reduce the number of parameters and select the most effective ones to be used for classification. Support vector machine and k-nearest neighbor with different kernels was used for classification. We obtained accuracy up to 88% for discrimination between PD patients ND patients using KNN with k equal to three and five.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {673–683},
numpages = {11},
keywords = {SVM, PCA, Neurological diseases, NPCA, KNN, Feature selection}
}

@article{10.3233/JIFS-169892,
author = {Kang, Yan and Li, Hao and Lu, Chenyang and Pu, Bin and Hsieh, Wen-Hsiang},
title = {A transfer learning algorithm for automatic requirement model generation},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169892},
doi = {10.3233/JIFS-169892},
abstract = {In this paper, we present a novel method for data-mining large informal product descriptions rather than extracting requirement features from proprietary project repositories. Our algorithm hybridizes deep-learning algorithms such as word2vec and recurrent neural network (RNN) with classical techniques to improve the performance of text analysis. Given the inaccuracy and incompleteness of the software requirement descriptions on websites, the instance-transfer learning method is utilized to construct a robust classifier and predict domain feature knowledge based on domain knowledge similar to the target domain. The bagging clustering algorithm is utilized with multiple clustering algorithms to help select transfer instances. [Author to confirm changes.]The RNN-based algorithm is utilized as a useful alternative to predict missing features by studying the requirement descriptions of a related software system, while word2vec is utilized to extract sensible feature keywords for the specific software domain. [Author to confirm changes.]Our RNN model for every subclass is based on the clustering result, and we construct subclass classifiers to recommend requirement keywords. Requirement features recommended by our algorithm potentially increase opportunities for requirement classification, promote software requirement quality, and deliver more reliable software products. We explain the details of implementation and perform experimental work on real requirement descriptions to establish its worth.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1183–1191},
numpages = {9},
keywords = {software requirement, feature model, transfer learning, RNN, Word2vec}
}

@inproceedings{10.1145/3387940.3392089,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {WES: Agent-based User Interaction Simulation on Real Infrastructure},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392089},
doi = {10.1145/3387940.3392089},
abstract = {We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {276–284},
numpages = {9},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2339530.2339674,
author = {Woznica, Adam and Nguyen, Phong and Kalousis, Alexandros},
title = {Model mining for robust feature selection},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339674},
doi = {10.1145/2339530.2339674},
abstract = {A common problem with most of the feature selection methods is that they often produce feature sets--models--that are not stable with respect to slight variations in the training data. Different authors tried to improve the feature selection stability using ensemble methods which aggregate different feature sets into a single model. However, the existing ensemble feature selection methods suffer from two main shortcomings: (i) the aggregation treats the features independently and does not account for their interactions, and (ii) a single feature set is returned, nevertheless, in various applications there might be more than one feature sets, potentially redundant, with similar information content. In this work we address these two limitations. We present a general framework in which we mine over different feature models produced from a given dataset in order to extract patterns over the models. We use these patterns to derive more complex feature model aggregation strategies that account for feature interactions, and identify core and distinct feature models. We conduct an extensive experimental evaluation of the proposed framework where we demonstrate its effectiveness over a number of high-dimensional problems from the fields of biology and text-mining.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {913–921},
numpages = {9},
keywords = {stability, model mining, high-dimensional data, feature selection, classification},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/3350546.3352511,
author = {Yu, Zheng and Pietrasik, Marcin and Reformat, Marek},
title = {Deep Dynamic Mixed Membership Stochastic Blockmodel},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352511},
doi = {10.1145/3350546.3352511},
abstract = {Latent community models are successful at statistically modeling network data by assigning network entities to communities and modelling entity relations as the relations of their communities. In this paper, we describe the limitation of these models in inferring relations between two communities when the entity relations between these communities are unobserved. We propose a solution to this problem by factorizing the community relations matrix into two community feature matrices, thereby adding a dependency between community relations. We introduce the deep dynamic mixed membership stochastic blockmodel based network (DDBN) to demonstrate the feasibility of such an approach. Our model marries the mixed membership stochastic blockmodel (MMSB) with deep neural networks for rich feature extraction and introduces a temporal dependency in latent features using a long short-term memory unit for dynamic network modeling. We evaluate our model on the link prediction task in static and dynamic networks and find that our model achieves comparable results with state-of-the-art methods.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {141–148},
numpages = {8},
keywords = {Matrix Factorization, Dynamic, Deep Learning},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.5555/1596324.1596369,
author = {Samuelsson, Yvonne and T\"{a}ckstr\"{o}m, Oscar and Velupillai, Sumithra and Eklund, Johan and Fi\v{s}el, Mark and Saers, Markus},
title = {Mixing and blending syntactic and semantic dependencies},
year = {2008},
isbn = {9781905593484},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Our system for the CoNLL 2008 shared task uses a set of individual parsers, a set of stand-alone semantic role labellers, and a joint system for parsing and semantic role labelling, all blended together. The system achieved a macro averaged labelled F1-score of 79.79 (WSJ 80.92, Brown 70.49) for the overall task. The labelled attachment score for syntactic dependencies was 86.63 (WSJ 87.36, Brown 80.77) and the labelled F1-score for semantic dependencies was 72.94 (WSJ 74.47, Brown 60.18).},
booktitle = {Proceedings of the Twelfth Conference on Computational Natural Language Learning},
pages = {248–252},
numpages = {5},
location = {Manchester, United Kingdom},
series = {CoNLL '08}
}

@article{10.1016/j.neucom.2018.10.086,
author = {Heinrich, Stefan and Springst\"{u}be, Peer and Kn\"{o}ppler, Tobias and Kerzel, Matthias and Wermter, Stefan},
title = {Continuous convolutional object tracking in developmental robot scenarios},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {342},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.10.086},
doi = {10.1016/j.neucom.2018.10.086},
journal = {Neurocomput.},
month = may,
pages = {137–144},
numpages = {8},
keywords = {Developmental robotics, Convolutional neural networks, Object tracking}
}

@article{10.1145/3337798,
author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337798},
doi = {10.1145/3337798},
abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {43},
numpages = {25},
keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification}
}

@inproceedings{10.5555/3495724.3496081,
author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
title = {Object goal navigation using goal-oriented semantic exploration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, 'Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {357},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1109/IROS51168.2021.9636743,
author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv},
title = {Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636743},
doi = {10.1109/IROS51168.2021.9636743},
abstract = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1562–1569},
numpages = {8},
location = {Prague, Czech Republic}
}

@article{10.1007/s10044-015-0489-8,
author = {Oliver, Javier and Albiol, Alberto and Albiol, Antonio and Mossi, Jos\'{e} M.},
title = {Using latent features for short-term person re-identification with RGB-D cameras},
year = {2016},
issue_date = {May       2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-015-0489-8},
doi = {10.1007/s10044-015-0489-8},
abstract = {This paper presents a system for people re-identification in uncontrolled scenarios using RGB-depth cameras. Compared to conventional RGB cameras, the use of depth information greatly simplifies the tasks of segmentation and tracking. In a previous work, we proposed a similar architecture where people were characterized using color-based descriptors that we named bodyprints. In this work, we propose the use of latent feature models to extract more relevant information from the bodyprint descriptors by reducing their dimensionality. Latent features can also cope with missing data in case of occlusions. Different probabilistic latent feature models, such as probabilistic principal component analysis and factor analysis, are compared in the paper. The main difference between the models is how the observation noise is handled in each case. Re-identification experiments have been conducted in a real store where people behaved naturally. The results show that the use of the latent features significantly improves the re-identification rates compared to state-of-the-art works.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {549–561},
numpages = {13},
keywords = {Surveillance, Re-identification, Probabilistic PCA, Person detection, Missing data, Kinect, Factor analysis, Bodyprint, Appearance matching}
}

@inbook{10.5555/3454287.3455442,
author = {Bietti, Alberto and Mairal, Julien},
title = {On the inductive bias of neural tangent kernels},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {State-of-the-art neural networks are heavily over-parameterized, making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain over-parameterized regime, the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization, called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space (RKHS). In particular, we study smoothness, approximation, and stability properties of functions with finite norm, including stability to image deformations in the case of convolutional networks, and compare to other known kernels for similar architectures.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1155},
numpages = {12}
}

@article{10.1504/ijsnet.2021.112886,
author = {Kang, Yan and Yang, Bing and Zhang, Lan and Chen, Tie and Li, Hao},
title = {ST-IFC: efficient spatial-temporal inception fully connected network for citywide crowd flow prediction},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {35},
number = {1},
issn = {1748-1279},
url = {https://doi.org/10.1504/ijsnet.2021.112886},
doi = {10.1504/ijsnet.2021.112886},
abstract = {Traffic flow prediction is important to urban management for the development of smart cities as well as further contribution to public safety. This paper analysed the spatial and temporal characteristics of large amounts of traffic data in depth and proposed an efficient spatial-temporal inception fully connected (ST-IFC) network for citywide traffic prediction. An inception fully connected (IFC) unit was proposed to capture the spatial dependence and multi-scale characteristics of the traffic dataset. In addition, a multi-level feature fusion strategy is proposed to effectively combine the flow features of low-level surface and high-level abstract to avoid feature loss. The strategy greatly enhances the utilisation of computing resources while improving the prediction results significantly. The simulations were carried out using the trajectory data of Beijing taxis and New York City bicycles. The experimental results show the state-of-the-art performance of our model in addition to high computational efficiency.},
journal = {Int. J. Sen. Netw.},
month = jan,
pages = {23–31},
numpages = {8},
keywords = {big data, inception network, fully connected network, computational efficiency, multi-scale characteristics, multi-level feature fusion, traffic prediction, spatial-temporal data, urban computing, deep learning}
}

@inproceedings{10.1145/2984751.2985721,
author = {Yokota, Tomohiro and Hashida, Tomoko},
title = {Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985721},
doi = {10.1145/2984751.2985721},
abstract = {In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–115},
numpages = {3},
keywords = {on-body touch, machine learning, hand gestures, combined input, acoustic sensing},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/3297001.3297023,
author = {Mani, Senthil and Sankaran, Anush and Aralikatte, Rahul},
title = {DeepTriage: Exploring the Effectiveness of Deep Learning for Bug Triaging},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297023},
doi = {10.1145/3297001.3297023},
abstract = {For a given software bug report, identifying an appropriate developer who could potentially fix the bug is the primary task of bug triaging. Automatic bug triaging is formulated as a classification problem, which takes the bug title and description as the input, and maps it to one of the available developers. A major challenge in doing this is that the bug description usually contains a combination of unstructured text, code snippets, and stack traces making the input data highly noisy. The existing bag-of-words (BOW) models do not consider the semantic information in the unstructured text.In this research, we propose a novel bug report representation using a deep bidirectional recurrent neural network with attention (DBRNN-A) that learns the syntactic and semantic features from long word sequences in an unsupervised manner. Using attention enables the model to remember and attend to important parts of text in a bug report. For training the model, we use unfixed bug reports (which constitute about 70% of bugs in a typical open source bug tracking system) which were ignored in previous studies.Another major contribution of this work is the release of a public benchmark dataset of bug reports from three open source bug tracking systems: Google Chromium, Mozilla Core, and Mozilla Firefox. The dataset consists of 383,104 bug reports from Google Chromium, 314,388 bug reports from Mozilla Core, and 162,307 bug reports from Mozilla Firefox. When compared to other systems, we observe that DBRNN-A provides a higher rank-10 average accuracy.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {171–179},
numpages = {9},
keywords = {Bug Triaging, Bidirectional LSTM, Attention LSTM},
location = {Kolkata, India},
series = {CODS-COMAD '19}
}

@inproceedings{10.1145/3416028.3416042,
author = {Pi, Sainan and An, Xin and Xu, Shuo and Li, Jinghong},
title = {A Comparative Study on Three Multi-Label Classification Tools},
year = {2020},
isbn = {9781450375467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416028.3416042},
doi = {10.1145/3416028.3416042},
abstract = {Many science, technology and innovation (STI) resources are attached with several different labels, such as IPC and CPC for patents, and PACS (Physics and Astronomy Classification Scheme) numbers for scientific publications. This problem is well known as the multi-label classification. Though there are a number of approaches and open-source tools for this task in the literature that work well on benchmark datasets, real-world is more complex in terms of both the number and hierarchy of labels. This work aims to compare comprehensively the performance of three state-of-the-art tools, Dependency LDA, Scikit-Multilearn and Neural Classifier on Scigraph of academic resource data. It is found that Neural Classifier works better on an unbalanced distribution dataset with more complex hierarchical structure and a larger number of label scale in terms of Micro F1, Micro F1 and Hamming Loss than the other two tools. On the basis of our comparisons, several directions are suggested in the near future.},
booktitle = {Proceedings of the 3rd International Conference on Information Management and Management Science},
pages = {8–12},
numpages = {5},
keywords = {Scikit-Multilearn, Neural Classifier, Multi-label classification, Dependency LDA},
location = {London, United Kingdom},
series = {IMMS '20}
}

@inproceedings{10.1145/2516821.2516850,
author = {Porter, Joseph and Szab\'{o}, Csan\'{a}d},
title = {Partition configuration for real-time systems with dependencies},
year = {2013},
isbn = {9781450320580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2516821.2516850},
doi = {10.1145/2516821.2516850},
abstract = {We present an analytical framework for formulating partition configuration problems in real-time systems with dependencies, particularly applicable to modeling systems with multiple criticality or security levels. Partition configuration constraints for real-time tasks include affinity and conflict. We also discuss the application of the framework to arbitrary partition schedulers, harmonic partition execution, and round robin partition execution (which is particularly problematic). Our interest is in minimizing end-to-end latency, though the computational complexity of the problem prevents us from finding optimal results. We conclude with some open problems.},
booktitle = {Proceedings of the 21st International Conference on Real-Time Networks and Systems},
pages = {87–96},
numpages = {10},
keywords = {security, safety, partitioned systems},
location = {Sophia Antipolis, France},
series = {RTNS '13}
}

@article{10.3233/JIFS-210505,
author = {Li, Bin and He, Qiyu and Liu, Xiaopeng and Jiang, Yajun and Hu, Zhigang},
title = {A joint structure of multi-distance based metric learning for person re-identification},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210505},
doi = {10.3233/JIFS-210505},
abstract = {Person re-identification problem is a valuable computer vision task, which aims at matching pedestrian images of different cameras in a non-overlapping surveillance network. Existing metric learning based methods address this problem by learning a robust distance function. These methods learn a mapping subspace by forcing the distance of the positive pair much smaller than the negative pair by a strict constraint. The metric model is over-fitting to the training dataset. Due to drastic appearance variations, the handcrafted features are weak of representation for person re-identification. To address these problems, we propose a joint distance measure based approach, which learns a Mahalanobis distance and a Euclidean distance with a novel feature jointly. The novel feature is represented with a dictionary representation based method which considers pedestrian images of different camera views with the same dictionary. The joint distance combine the Mahalanobis distance based on metric learning method with the Euclidean distance based on the novel feature to measure the similarity between matching pairs. Extensive experiments are conducted on the publicly available bench marking datasets VIPeR and CUHK01. The identification results show that the proposed method reaches a good performance than the comparison methods.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6629–6639},
numpages = {11},
keywords = {Mahalanobis distance, dictionary representation, multi-distance, metric learning, Person re-identification}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {Software architecture evaluation, CBAM, ANP, AHP}
}

@inproceedings{10.5555/3104482.3104557,
author = {Brouard, C\'{e}line and d'Alch\'{e}-Buc, Florence and Szafranski, Marie},
title = {Semi-supervised penalized output kernel regression for link prediction},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Link prediction is addressed as an output kernel learning task through semi-supervised Output Kernel Regression. Working in the framework of RKHS theory with vector-valued functions, we establish a new representer theorem devoted to semi-supervised least square regression. We then apply it to get a new model (POKR: Penalized Output Kernel Regression) and show its relevance using numerical experiments on artificial networks and two real applications using a very low percentage of labeled data in a transductive setting.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {593–600},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@article{10.1007/s11634-016-0266-6,
author = {Hayashi, Kenichi},
title = {Asymptotic comparison of semi-supervised and supervised linear discriminant functions for heteroscedastic normal populations},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-016-0266-6},
doi = {10.1007/s11634-016-0266-6},
abstract = {It has been reported that using unlabeled data together with labeled data to construct a discriminant function works successfully in practice. However, theoretical studies have implied that unlabeled data can sometimes adversely affect the performance of discriminant functions. Therefore, it is important to know what situations call for the use of unlabeled data. In this paper, asymptotic relative efficiency is presented as the measure for comparing analyses with and without unlabeled data under the heteroscedastic normality assumption. The linear discriminant function maximizing the area under the receiver operating characteristic curve is considered. Asymptotic relative efficiency is evaluated to investigate when and how unlabeled data contribute to improving discriminant performance under several conditions. The results show that asymptotic relative efficiency depends mainly on the heteroscedasticity of the covariance matrices and the stochastic structure of observing the labels of the cases.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {315–339},
numpages = {25},
keywords = {Semi-supervised learning, Receiver operating characteristic curve, Missing data, Linear discriminant function, Labeling mechanism, Area under the ROC curve, 68T10, 62H30, 62G20}
}

@inproceedings{10.1007/978-3-030-30793-6_20,
author = {Kristiadi, Agustinus and Khan, Mohammad Asif and Lukovnikov, Denis and Lehmann, Jens and Fischer, Asja},
title = {Incorporating Literals into Knowledge Graph Embeddings},
year = {2019},
isbn = {978-3-030-30792-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30793-6_20},
doi = {10.1007/978-3-030-30793-6_20},
abstract = {Knowledge graphs are composed of different elements: entity nodes, relation edges, and literal nodes. Each literal node contains an entity’s attribute value (e.g.&nbsp;the height of an entity of type person) and thereby encodes information which in general cannot be represented by relations between entities alone. However, most of the existing embedding- or latent-feature-based methods for knowledge graph analysis only consider entity nodes and relation edges, and thus do not take the information provided by literals into account. In this paper, we extend existing latent feature methods for link prediction by a simple portable module for incorporating literals, which we name LiteralE. Unlike in concurrent methods where literals are incorporated by adding a literal-dependent term to the output of the scoring function and thus only indirectly affect the entity embeddings, LiteralE directly enriches these embeddings with information from literals via a learnable parametrized function. This function can be easily integrated into the scoring function of existing methods and learned along with the entity embeddings in an end-to-end manner. In an extensive empirical study over three datasets, we evaluate LiteralE-extended versions of various state-of-the-art latent feature methods for link prediction and demonstrate that LiteralE presents an effective way to improve their performance. For these experiments, we augmented standard datasets with their literals, which we publicly provide as testbeds for further research. Moreover, we show that LiteralE leads to an qualitative improvement of the embeddings and that it can be easily extended to handle literals from different modalities.},
booktitle = {The Semantic Web – ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceedings, Part I},
pages = {347–363},
numpages = {17},
keywords = {Relational learning, Knowledge graph},
location = {Auckland, New Zealand}
}

@inproceedings{10.1007/11492542_47,
author = {Zhang, Xian and Zhu, Xiaoyan},
title = {Extended bi-gram features in text categorization},
year = {2005},
isbn = {3540261540},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11492542_47},
doi = {10.1007/11492542_47},
abstract = {Usually, in traditional text categorization systems based on Vector Space Model, there is no context information in a feature vector, which limited the performance of the system. To make use of more information, it is natural to select bi-gram feature in addition to unigram feature. However, the longer the feature is, the more important the feature selection algorithm is to get good balance in feature space This paper proposed two feature extraction methods which can get better feature balance for document categorization. Experiments show that our extended bi-gram feature improved system performance greatly.},
booktitle = {Proceedings of the Second Iberian Conference on Pattern Recognition and Image Analysis - Volume Part II},
pages = {379–386},
numpages = {8},
location = {Estoril, Portugal},
series = {IbPRIA'05}
}

@article{10.5555/1718098.1718104,
author = {El-Raouf, Amal Abd},
title = {Hierarchical clustering of distributed object-oriented software systems: a generic solution for software-hardware mismatch problem},
year = {2009},
issue_date = {November 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {8},
number = {11},
issn = {1109-2750},
abstract = {During the software lifecycle, the software structure is subject to many changes in order to fulfill the customer's requirements. In Distributed Object Oriented systems, software engineers face many challenges to solve the software-hardware mismatch problem in which the software structure does not match the customer's underlying hardware. A major design problem of Object Oriented software systems is the efficient distribution of software classes among the different nodes in the system while maintaining two features: low-coupling and high software quality. In this paper, we present a new methodology for efficiently restructuring Distributed Object Oriented software systems to improve the overall system performance and to solve the softwarehardware mismatch problem. Our method has two main phases. In the first phase, we use the hierarchical clustering method to restructure the target software application. As a result, all the possible clustering solutions that could be applied to the target software application are generated. In the second phase, we decide on the best-fit clustering solution according to the customer hardware organization.},
journal = {W. Trans. on Comp.},
month = nov,
pages = {1780–1789},
numpages = {10},
keywords = {software restructuring, performance analysis, object oriented software, low coupling, hierarchical clustering, distributed systems}
}

@inproceedings{10.1145/3462244.3479900,
author = {Senaratne, Hashini and Kuhlmann, Levin and Ellis, Kirsten and Melvin, Glenn and Oviatt, Sharon},
title = {A Multimodal Dataset and Evaluation for Feature Estimators of Temporal Phases of Anxiety},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479900},
doi = {10.1145/3462244.3479900},
abstract = {Vicious cycles of anxiety responses underlie the onset of increasingly prevalent and highly impairing anxiety disorders and also contribute to their maintenance. Our goal is to evaluate whether different anxiety responses are evident in temporal patterns of physiological and behavioral features. Consequently, we established a rich multimodal-multisensor dataset of cardiac, electrodermal, movement, posture, and speech measures from 95 young adults during two anxiety experiments that induce social anxiety and bug-phobic anxiety. A subset of this dataset is publicly available at “Anxiety Phases Dataset” Figshare repository. We adopted a generalized mixed model approach and found that 10 out of 14 feature trajectories modeled for high- and low-anxiety groups differ significantly at 0.001 level in magnitude, creating at least two temporal phases in both groups. Further differences in magnitude, duration and the number of phases were observed for responses of confrontation, safety behaviors, escape, and avoidance in the high-anxiety group. Our findings contribute to the long-term aim of designing multimodal systems that have great potential to reduce the impacts of anxiety disorders and improve therapy.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {52–61},
numpages = {10},
keywords = {Temporal phases, Objective detection, Multimodal-multisensor dataset, Generalized additive mixed models, Anxiety},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@article{10.1145/3478093,
author = {Zhang, Qian and Wang, Dong and Zhao, Run and Yu, Yinggang and Shen, Junjie},
title = {Sensing to Hear: Speech Enhancement for Mobile Devices Using Acoustic Signals},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478093},
doi = {10.1145/3478093},
abstract = {Voice interactions and voice messages on mobile phones are rapidly growing in popularity. However, the user experience of these services is still worse than desired in noisy environments, especially in multi-talker scenarios, where the phone can only provide low-quality voice recordings. Speech enhancement using only audio as the input remains a grand challenge in these scenarios. In this paper, we handle this with the help of the emerging acoustic sensing technology. The key insight is that the inaudible acoustic signals emitted by speakers of phones can capture the subtle lip movements when people speak. Instead of enabling lip reading for the classification of limited voice commands, we further unlock the potential of acoustic sensing and leverage the captured lip information to improve the voice recording quality. We propose WaveVoice, a joint audio-sensory deep learning method for end-to-end speech enhancement on mobile phones. The model of WaveVoice is structured as an encoder-decoder network, in which audio and acoustic sensing data are processed through two individual CNN branches, respectively, and then fused into a joint network to generate enhanced speech. In addition, to improve the performance on new users, a self-supervised learning methodology is developed to adapt the model to extract speaker-specific features. We construct a dataset to train and evaluate WaveVoice. We also perform online tests under various noisy conditions to show the applicability of our system in real-world scenarios. Experimental results show that WaveVoice can effectively reconstruct the target clean speech from the noisy audio signals, and yield notably superior performance compared with the audio-only encoder-decoder model and the state-of-the-art speech enhancement methods. Given its promising performance, we believe that WaveVoice has made a substantial contribution to the advancement of mobile voice input.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {137},
numpages = {30},
keywords = {mobile speech enhancement, lip movement, deep learning, acoustic sensing}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {T-wise sampling, Family model, Model learning, Software product lines}
}

@inproceedings{10.1109/ASE.2015.58,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Configuration-aware change impact analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.58},
doi = {10.1109/ASE.2015.58},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {program analysis, maintenance, configuration, change impact analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/3540261.3541724,
author = {Weihs, Luca and Jain, Unnat and Liu, Iou-Jen and Salvador, Jordi and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander},
title = {Bridging the imitation gap by adaptive insubordination},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, imitation learning is preferred over pure reinforcement learning whenever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized during imitation learning, resulting in an "imitation gap" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR). ADVISOR dynamically weights imitation and reward-based reinforcement learning losses during training, enabling on-the-fly switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we show that on-the-fly switching with ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1463},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-91825-5_3,
author = {Das, Susmoy and Sharma, Arpit},
title = {State Space Minimization Preserving Embeddings for Continuous-Time Markov Chains},
year = {2021},
isbn = {978-3-030-91824-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91825-5_3},
doi = {10.1007/978-3-030-91825-5_3},
abstract = {This paper defines embeddings which allow one to construct an action labeled continuous-time Markov chain (ACTMC) from a state labeled continuous-time Markov chain (SCTMC) and vice versa. We prove that these embeddings preserve strong forward bisimulation and strong backward bisimulation. We define weak backward bisimulation for ACTMCs and SCTMCs, and also prove that our embeddings preserve both weak forward and weak backward bisimulation. Next, we define the invertibility criteria and the inverse of these embeddings. Finally, we prove that an ACTMC can be minimized by minimizing its embedded model, i.e. SCTMC and taking the inverse of the embedding. Similarly, we prove that an SCTMC can be minimized by minimizing its embedded model, i.e. ACTMC and taking the inverse of the embedding.},
booktitle = {Performance Engineering and Stochastic Modeling: 17th European Workshop, EPEW 2021, and 26th International Conference, ASMTA 2021, Virtual Event, December 9–10 and December 13–14, 2021, Proceedings},
pages = {44–61},
numpages = {18},
keywords = {Embeddings, Stochastic systems, Bisimulation equivalence, Behavioral equivalence, Markov chain},
location = {Milan, Italy}
}

@article{10.1016/j.neucom.2017.01.093,
author = {Tareef, Afaf and Song, Yang and Huang, Heng and Wang, Yue and Feng, Dagan and Chen, Mei and Cai, Weidong},
title = {Optimizing the cervix cytological examination based on deep learning and dynamic shape modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {248},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.01.093},
doi = {10.1016/j.neucom.2017.01.093},
abstract = {The task of segmenting nuclei and cytoplasm in Papanicolau smear images is one of the most challenging tasks in automated cervix cytological analysis owing to the high degree of overlapping, the multiform shape of the cells and their complex structures resulting from inconsistent staining, poor contrast, and the presence of inflammatory cells. This article presents a robust variational segmentation framework based on superpixelwise convolutional neutral network and a learned shape prior enabling an accurate analysis of overlapping cervical mass. The cellular components of Pap image are first classified by automatic feature learning and classification model. Then, a learning shape prior model is employed to delineate the actual contour of each individual cytoplasm inside the overlapping mass. The shape prior is dynamically modeled during the segmentation process as a weighted linear combination of shape templates from an over-complete shape dictionary under sparsity constraints. We provide quantitative and qualitative assessment of the proposed method using two databases of 153 cervical cytology images, with 870 cells in total, synthesised by accumulating real isolated cervical cells to generate overlapping cellular masses with a varying number of cells and degree of overlap. The experimental results have demonstrated that our methodology can successfully segment nuclei and cytoplasm from highly overlapping mass. Our segmentation is also competitive when compared to the state-of-the-art methods.},
journal = {Neurocomput.},
month = jul,
pages = {28–40},
numpages = {13},
keywords = {Sparse approximation, Overlapping cell segmentation, Level set evolution, Feature learning, Convolutional neural network}
}

@article{10.1016/j.neucom.2016.09.070,
author = {Tareef, Afaf and Song, Yang and Cai, Weidong and Huang, Heng and Chang, Hang and Wang, Yue and Fulham, Michael and Feng, Dagan and Chen, Mei},
title = {Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {221},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.070},
doi = {10.1016/j.neucom.2016.09.070},
abstract = {Automated segmentation of cells from cervical smears poses great challenge to biomedical image analysis because of the noisy and complex background, poor cytoplasmic contrast and the presence of fuzzy and overlapping cells. In this paper, we propose an automated segmentation method for the nucleus and cytoplasm in a cluster of cervical cells based on distinctive local features and guided sparse shape deformation. Our proposed approach is performed in two stages: segmentation of nuclei and cellular clusters, and segmentation of overlapping cytoplasm. In the first stage, a set of local discriminative shape and appearance cues of image superpixels is incorporated and classified by the Support Vector Machine (SVM) to segment the image into nuclei, cellular clusters, and background. In the second stage, a robust shape deformation framework is proposed, based on Sparse Coding (SC) theory and guided by representative shape features, to construct the cytoplasmic shape of each overlapping cell. Then, the obtained shape is refined by the Distance Regularized Level Set Evolution (DRLSE) model. We evaluated our approach using the ISBI 2014 challenge dataset, which has 135 synthetic cell images for a total of 810 cells. Our results show that our approach outperformed existing approaches in segmenting overlapping cells and obtaining accurate nuclear boundaries. HighlightsA fully automated segmentation method is proposed for overlapping cervical cells.Our approach is based on superpixel-based features and guided shape deformation.Our shape initialization procedure is able to work with the different cell types.The practicality of our approach in segmenting highly overlapping cells is proved.Our approach outperformed existing approaches in nuclei and cytoplasm segmentation.},
journal = {Neurocomput.},
month = jan,
pages = {94–107},
numpages = {14},
keywords = {Sparse coding, Shape deformation, Overlapping cervical smear cells, Feature extraction, Distance regularized level set}
}

@article{10.1145/3432195,
author = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
title = {DeepRange: Acoustic Ranging via Deep Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432195},
doi = {10.1145/3432195},
abstract = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {143},
numpages = {23},
keywords = {Ranging, Motion Tracking, Convolutional Neural Network, Acoustic Sensing}
}

@article{10.1155/2020/7917021,
author = {Zhang, Cheng and He, Dan and Zhang, Qingchen},
title = {A Deep Multiscale Fusion Method via Low-Rank Sparse Decomposition for Object Saliency Detection Based on Urban Data in Optical Remote Sensing Images},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/7917021},
doi = {10.1155/2020/7917021},
abstract = {The urban data provides a wealth of information that can support the life and work for people. In this work, we research the object saliency detection in optical remote sensing images, which is conducive to the interpretation of urban scenes. Saliency detection selects the regions with important information in the remote sensing images, which severely imitates the human visual system. It plays a powerful role in other image processing. It has successfully made great achievements in change detection, object tracking, temperature reversal, and other tasks. The traditional method has some disadvantages such as poor robustness and high computational complexity. Therefore, this paper proposes a deep multiscale fusion method via low-rank sparse decomposition for object saliency detection in optical remote sensing images. First, we execute multiscale segmentation for remote sensing images. Then, we calculate the saliency value, and the proposal region is generated. The superpixel blocks of the remaining proposal regions of the segmentation map are input into the convolutional neural network. By extracting the depth feature, the saliency value is calculated and the proposal regions are updated. The feature transformation matrix is obtained based on the gradient descent method, and the high-level semantic prior knowledge is obtained by using the convolutional neural network. The process is iterated continuously to obtain the saliency map at each scale. The low-rank sparse decomposition of the transformed matrix is carried out by robust principal component analysis. Finally, the weight cellular automata method is utilized to fuse the multiscale saliency graphs and the saliency map calculated according to the sparse noise obtained by decomposition. Meanwhile, the object priors knowledge can filter most of the background information, reduce unnecessary depth feature extraction, and meaningfully improve the saliency detection rate. The experiment results show that the proposed method can effectively improve the detection effect compared to other deep learning methods.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-98682-7_5,
author = {Fernandes, Roberto and Rodrigues, Walber M. and Barros, Edna},
title = {Dataset and&nbsp;Benchmarking of&nbsp;Real-Time Embedded Object Detection for&nbsp;RoboCup SSL},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_5},
doi = {10.1007/978-3-030-98682-7_5},
abstract = {When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline is used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS), while running on an SSL robot.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {53–64},
numpages = {12},
keywords = {Object detection, Deep learning, Benchmark, Dataset},
location = {Sydney, NSW, Australia}
}

@article{10.1023/A:1008158907779,
author = {Shanahan, J. and Thomas, B. and Mirmehdi, M. and Martin, T. and Campbell, N. and Baldwin, J.},
title = {A Soft Computing Approach to Road Classification},
year = {2000},
issue_date = {December 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0921-0296},
url = {https://doi.org/10.1023/A:1008158907779},
doi = {10.1023/A:1008158907779},
abstract = {Current learning approaches to computer vision have mainly focussed on low-level image processing and object recognition, while tending to ignore high-level processing such as understanding. Here we propose an approach to object recognition that facilitates the transition from recognition to understanding. The proposed approach embraces the synergistic spirit of soft computing, exploiting the global search powers of genetic programming to determine fuzzy probabilistic models. It begins by segmenting the images into regions using standard image processing approaches, which are subsequently classified using a discovered fuzzy Cartesian granule feature classifier. Understanding is made possible through the transparent and succinct nature of the discovered models. The recognition of roads in images is taken as an illustrative problem in the vision domain. The discovered fuzzy models while providing high levels of accuracy (97%), also provide understanding of the problem domain through the transparency of the learnt models. The learning step in the proposed approach is compared with other techniques such as decision trees, na\"{\i}ve Bayes and neural networks using a variety of performance criteria such as accuracy, understandability and efficiency.},
journal = {J. Intell. Robotics Syst.},
month = dec,
pages = {349–387},
numpages = {39}
}

@inproceedings{10.1007/978-3-030-98682-7_9,
author = {Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Improving Sample Efficiency in&nbsp;Behavior Learning by&nbsp;Using Sub-optimal Planners for&nbsp;Robots},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_9},
doi = {10.1007/978-3-030-98682-7_9},
abstract = {The design and implementation of behaviors for robots operating in dynamic and complex environments are becoming mandatory in nowadays applications. Reinforcement learning is consistently showing remarkable results in learning effective action policies and in achieving super-human performance in various tasks – without exploiting prior knowledge. However, in robotics, the use of purely learning-based techniques is still subject to strong limitations. Foremost, sample efficiency. Such techniques, in fact, are known to require large training datasets, and long training sessions, in order to develop effective action policies. Hence in this paper, to alleviate such constraint, and to allow learning in such robotic scenarios, we introduce SErP (Sample Efficient robot Policies), an iterative algorithm to improve the sample-efficiency of learning algorithms. SErP exploits a sub-optimal planner (here implemented with a monitor-replanning algorithm) to lead the exploration of the learning agent through its initial iterations. Intuitively, SErP exploits the planner as an expert in order to enable focused exploration and to avoid portions of the search space that are not effective to solve the task of the robot. Finally, to confirm our insights and to show the improvements that SErP carries with, we report the results obtained in two different robotic scenarios: (1) a cartpole scenario and (2) a soccer-robots scenario within the RoboCup@Soccer SPL environment.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {103–114},
numpages = {12},
keywords = {Decision-making, Reinforcement learning, Automated planning},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3318464.3380599,
author = {Akrami, Farahnaz and Saeef, Mohammed Samiul and Zhang, Qingheng and Hu, Wei and Li, Chengkai},
title = {Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380599},
doi = {10.1145/3318464.3380599},
abstract = {In the active research area of employing embedding models for knowledge graph completion, particularly for the task of link prediction, most prior studies used two benchmark datasets FB15k and WN18 in evaluating such models. Most triples in these and other datasets in such studies belong to reverse and duplicate relations which exhibit high data redundancy due to semantic duplication, correlation or data incompleteness. This is a case of excessive data leakage---a model is trained using features that otherwise would not be available when the model needs to be applied for real prediction. There are also Cartesian product relations for which every triple formed by the Cartesian product of applicable subjects and objects is a true fact. Link prediction on the aforementioned relations is easy and can be achieved with even better accuracy using straightforward rules instead of sophisticated embedding models. A more fundamental defect of these models is that the link prediction scenario, given such data, is non-existent in the real-world. This paper is the first systematic study with the main objective of assessing the true effectiveness of embedding models when the unrealistic triples are removed. Our experiment results show these models are much less accurate than what we used to perceive. Their poor accuracy renders link prediction a task without truly effective automated solution. Hence, we call for re-investigation of possible effective approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1995–2010},
numpages = {16},
keywords = {link prediction, knowledge graph completion, embedding models},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inbook{10.5555/3454287.3454321,
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
title = {Chasing ghosts: instruction following as bayesian state tracking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking – learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {34},
numpages = {11}
}

@article{10.1016/j.sigpro.2019.01.018,
author = {Ahmed, Talal and Bajwa, Waheed U.},
title = {ExSIS: Extended sure independence screening for ultrahigh-dimensional linear models},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.01.018},
doi = {10.1016/j.sigpro.2019.01.018},
journal = {Signal Process.},
month = jun,
pages = {33–48},
numpages = {16},
keywords = {Sparse signal processing, High-dimensional statistics, Linear models, Sure screening, Variable screening, Variable selection}
}

@inproceedings{10.1007/11550518_46,
author = {Leitner, Raimund and Bischof, Horst},
title = {Recognition of 3D objects by learning from correspondences in a sequence of unlabeled training images},
year = {2005},
isbn = {3540287035},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11550518_46},
doi = {10.1007/11550518_46},
abstract = {This paper proposes an approach for the unsupervised learning of object models from local image feature correspondences. The object models are learned from an unlabeled sequence of training images showing one object after the other. The obtained object models enable the recognition of these objects in cluttered scenes, under occlusion, in-plane rotation and scale change. Maximally stable extremal regions are used as local image features and two different types of descriptors characterising the appearance and shape of the regions allow a robust matching. Experiments with real objects show the recognition performance of the presented approach under various conditions.},
booktitle = {Proceedings of the 27th DAGM Conference on Pattern Recognition},
pages = {369–376},
numpages = {8},
location = {Vienna, Austria},
series = {PR'05}
}

@inproceedings{10.1007/978-3-642-34713-9_30,
author = {Chang, Kai-Min and Murphy, Brian and Just, Marcel},
title = {A latent feature analysis of the neural representation of conceptual knowledge},
year = {2011},
isbn = {9783642347122},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34713-9_30},
doi = {10.1007/978-3-642-34713-9_30},
abstract = {Bayesian probabilistic analysis offers a new approach to characterize semantic representations by inferring the most likely feature structure directly from the patterns of brain activity. In this study, infinite latent feature models [1] are used to recover the semantic features that give rise to the brain activation vectors when people think about properties associated with 60 concrete concepts. The semantic features recovered by ILFM are consistent with the human ratings of the shelter, manipulation, and eating factors that were recovered by a previous factor analysis. Furthermore, different areas of the brain encode different perceptual and conceptual features. This neurally-inspired semantic representation is consistent with some existing conjectures regarding the role of different brain areas in processing different semantic and perceptual properties.},
booktitle = {Proceedings of the 1st International Conference on Machine Learning and Interpretation in Neuroimaging},
pages = {234–241},
numpages = {8},
location = {Sierra Nevada, Spain},
series = {MLINI'11}
}

@article{10.1016/j.csda.2006.09.005,
author = {Cai, D. Michael and Gokhale, Maya and Theiler, James},
title = {Comparison of feature selection and classification algorithms in identifying malicious executables},
year = {2007},
issue_date = {March, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {6},
issn = {0167-9473},
url = {https://doi.org/10.1016/j.csda.2006.09.005},
doi = {10.1016/j.csda.2006.09.005},
abstract = {Malicious executables, often spread as email attachments, impose serious security threats to computer systems and associated networks. We investigated the use of byte sequence frequencies as a way to automatically distinguish malicious from benign executables without actually executing them. In a series of experiments, we compared classification accuracies over seven feature selection methods, four classification algorithms, and variable byte sequence lengths. We found that single-byte patterns provided surprisingly reliable features to separate malicious executables from benign. Between classifiers and feature selection methods, the overall performance of the models depended more on the choice of classifier than the method of feature selection. Support vector machine (SVM) classifiers were found to be superior in terms of prediction accuracy, training time, and aversion to overfitting.},
journal = {Comput. Stat. Data Anal.},
month = mar,
pages = {3156–3172},
numpages = {17},
keywords = {Support vector machine, Na\"{\i}ve Bayes classifier, Malicious executable file, Feature selection, Email filtering}
}

@inproceedings{10.5555/3044805.3044969,
author = {Cuturi, Marco and Doucet, Arnaud},
title = {Fast computation of wasserstein barycenters},
year = {2014},
publisher = {JMLR.org},
abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–685–II–693},
location = {Beijing, China},
series = {ICML'14}
}

@article{10.1007/s00530-006-0032-2,
author = {Mandel, Michael I. and Poliner, Graham E. and Ellis, Daniel P.},
title = {Support vector machine active learning for music retrieval},
year = {2006},
issue_date = {August    2006},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {1},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-006-0032-2},
doi = {10.1007/s00530-006-0032-2},
abstract = {Searching and organizing growing digital music collections requires a computational model of music similarity. This paper describes a system for performing flexible music similarity queries using SVM active learning. We evaluated the success of our system by classifying 1210 pop songs according to mood and style (from an online music guide) and by the performing artist. In comparing a number of representations for songs, we found the statistics of mel-frequency cepstral coefficients to perform best in precision-at-20 comparisons. We also show that by choosing training examples intelligently, active learning requires half as many labeled examples to achieve the same accuracy as a standard scheme.},
journal = {Multimedia Syst.},
month = aug,
pages = {3–13},
numpages = {11},
keywords = {Support vector machines, Music classification, Active learning}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00022,
author = {Azanza, Maider and Irastorza, Arantza and Medeiros, Raul and D\'{\i}az, Oscar},
title = {Onboarding in software product lines: concept maps as welcome guides},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00022},
doi = {10.1109/ICSE-SEET52601.2021.00022},
abstract = {With a volatile labour and technological market, onboarding is becoming increasingly important. The process of incorporating a new developer, a.k.a. the newcomer, into a software development team is reckoned to be lengthy, frustrating and expensive. Newcomers face personal, interpersonal, process and technical barriers during their incorporation, which, in turn, affects the overall productivity of the whole team. This problem exacerbates for Software Product Lines (SPLs), where their size and variability combine to make onboarding even more challenging, even more so for developers that are transferred from the Application Engineering team into the Domain Engineering team, who will be our target newcomers. This work presents concept maps on the role of sensemaking scaffolds to help to introduce these newcomers into the SPL domain. Concept maps, used as knowledge visualisation tools, have been proven to be helpful for meaningful learning. Our main insight is to capture concepts of the SPL domain and their interrelationships in a concept map, and then, present them incrementally, helping newcomers grasp the SPL and aiding them in exploring it in a guided manner while avoiding information overload. This work's contributions are four-fold. First, concept maps are proposed as a representation to introduce newcomers into the SPL domain. Second, concept maps are presented as the means for a guided exploration of the SPL core assets. Third, a feature-driven concept map construction process is introduced. Last, the usefulness of concept maps as guides for SPL onboarding is tested through a formative evaluation.Link to the online demo: https://rebrand.ly/wacline-cmap},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {122–133},
numpages = {12},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.5555/646473.692986,
author = {Slezak, Dominik and Wroblewski, Jakub},
title = {Approximate Bayesian Network Classifiers},
year = {2002},
isbn = {354044274X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Bayesian network (BN) is a directed acyclic graph encoding probabilistic independence statements between variables. BN with decision attribute as a root can be applied to classification of new cases, by synthesis of conditional probabilities propagated along the edges. We consider approximate BNs, which almost keep entropy of a decision table. They have usually less edges than classical BNs. They enable to model and extend the well-known Naive Bayes approach. Experiments show that classifiers based on approximate BNs can be very efficient.},
booktitle = {Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing},
pages = {365–372},
numpages = {8},
series = {TSCTC '02}
}

@inproceedings{10.5555/3045118.3045263,
author = {Hayashi, Kohei and Maeda, Shin-Ichi and Fujimaki, Ryohei},
title = {Rebuilding factorized information criterion: asymptotically accurate marginal likelihood},
year = {2015},
publisher = {JMLR.org},
abstract = {Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs. Second, we investigate the model selection mechanism of the generalized FIC, which we provide a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables. Third, we uncover a few previously-unknown relationships between FIC and the variational free energy. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1358–1366},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1016/j.specom.2019.10.006,
author = {Michelsanti, Daniel and Tan, Zheng-Hua and Sigurdsson, Sigurdur and Jensen, Jesper},
title = {Deep-learning-based audio-visual speech enhancement in presence of Lombard effect},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.006},
doi = {10.1016/j.specom.2019.10.006},
journal = {Speech Commun.},
month = dec,
pages = {38–50},
numpages = {13},
keywords = {Speech intelligibility, Speech quality, Deep learning, Audio-visual speech enhancement, Lombard effect}
}

@inproceedings{10.5555/3045118.3045193,
author = {Huggins, Jonathan H. and Narasimhan, Karthik and Saeedi, Ardavan and Mansinghka, Vikash K.},
title = {JUMP-means: small-variance asymptotics for Markov jump processes},
year = {2015},
publisher = {JMLR.org},
abstract = {Markov jump processes (MJPs) are used to model a wide range of phenomena from disease progression to RNA path folding. However, maximum likelihood estimation of parametric models leads to degenerate trajectories and inferential performance is poor in nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call JUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {693–701},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.5555/3045118.3045279,
author = {Han, Qiuyi and Xu, Kevin S. and Airoldi, Edoardo M.},
title = {Consistent estimation of dynamic and multi-layer block models},
year = {2015},
publisher = {JMLR.org},
abstract = {Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multigraph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1511–1520},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1109/TASLP.2020.3019646,
author = {Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Ranjan, Rishabh and Jones, Douglas L.},
title = {Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3019646},
doi = {10.1109/TASLP.2020.3019646},
abstract = {Many signal processing-based methods for sound source direction-of-arrival estimation produce a spatial pseudo-spectrum of which the local maxima strongly indicate the source directions. Due to different levels of noise, reverberation and different number of overlapping sources, the spatial pseudo-spectra are noisy even after smoothing. In addition, the number of sources is often unknown. As a result, selecting the peaks from these spectra is susceptible to error. Convolutional neural network has been successfully applied to many image processing problems in general and direction-of-arrival estimation in particular. In addition, deep learning-based methods for direction-of-arrival estimation show good generalization to different environments. We propose to use a 2D convolutional neural network with multi-task learning to robustly estimate the number of sources and the directions-of-arrival from short-time spatial pseudo-spectra, which have useful directional information from audio input signals. This approach reduces the tendency of the neural network to learn unwanted association between sound classes and directional information, and helps the network generalize to unseen sound classes. The simulation and experimental results show that the proposed methods outperform other directional-of-arrival estimation methods in different levels of noise and reverberation, and different number of sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2626–2637},
numpages = {12}
}

@article{10.1155/2015/635840,
author = {Krishnamurti, Sridhar},
title = {Application of neural network modeling to identify auditory processing disorders in school-age children},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-7594},
url = {https://doi.org/10.1155/2015/635840},
doi = {10.1155/2015/635840},
abstract = {P300 Auditory Event-Related Potentials (P3AERPs) were recorded in nine school-age children with auditory processing disorders and nine age- and gender-matched controls in response to tone burst stimuli presented at varying rates (1/second or 3/second) under varying levels of competing noise (0 dB, 40 dB, or 60 dB SPL). Neural network modeling results indicated that speed of information processing and task-related demands significantly influenced P3AERP latency in children with auditory processing disorders. Competing noise and rapid stimulus rates influenced P3AERP amplitude in both groups.},
journal = {Adv. Artif. Neu. Sys.},
month = jan,
articleno = {5},
numpages = {1}
}

@inproceedings{10.1145/3177404.3177440,
author = {Rao, Jianghao and Zhang, Jianlin},
title = {Cut and Paste: Generate Artificial Labels for Object Detection},
year = {2017},
isbn = {9781450353830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177404.3177440},
doi = {10.1145/3177404.3177440},
abstract = {In the domain of object detection, region proposal, feature extraction, recognition and the localization are the main three tasks. The end-to-end detection models by integrating the three parts together to simplify the structure of network and accelerate the process of training and detection. While the issues of illumination change, object deformation and scale change undermine the performance of detection methods largely. To promote the object detection accuracy rate and boost the detection speed simultaneously, we propose a new method of data augmentation. Different from the traditional methods, our method can increase the training data largely and be free from overfitting to some extent. With the new method, the abstraction ability of models improves a lot, the model has better performance to multiscale objects detection, and also has a stronger distinguishing ability in complex background.},
booktitle = {Proceedings of the International Conference on Video and Image Processing},
pages = {29–33},
numpages = {5},
keywords = {data augmentation, artificial labels, Object detection},
location = {Singapore, Singapore},
series = {ICVIP '17}
}

@article{10.1007/s11036-017-0944-4,
author = {Syed, Khajamoinuddin and Abdelzaher, Ahmed and Mayo, Michael and Ghosh, Preetam},
title = {Similar Feed-forward Loop Crosstalk Patterns may Impact Robust Information Transport Across E. coli and S. Cerevisiae Transcriptional Networks},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {1383-469X},
url = {https://doi.org/10.1007/s11036-017-0944-4},
doi = {10.1007/s11036-017-0944-4},
abstract = {Evolved biological network topologies may resist perturbances to allow for more robust information transport across larger networks in which their network motifs may play a complex role. Although the abundance of individual motifs correlate with some metrics of biological robustness, the extent to which redundant regulatory interactions affect motif connectivity and how this connectivity affects robustness is unknown. To address this problem, we applied machine learning based regression modeling to evaluate how feed-forward loops interlinked by crosstalk altered information transport across a network in terms of packets successfully routed over networks of noisy channels via NS-2 simulation. The sample networks were extracted from the complete transcriptional regulatory networks of two well-studied bacteria: E.coli and Yeast. We developed 233 topological features for the E.coli subnetworks and 842 topological features for the Yeast subnetworks which distinctly account for the opportunities in which two feed-forward loops may exhibit crosstalk. Random forest regression modeling was used to infer significant features from this modest configuration space. The coefficient of determination was used as a primary performance metric to rank features within our regression models. Although only a handful of features were highly ranked, we observed that, in particular, feed-forward loop crosstalk patterns correlated substantially with an improved chance for successful information transmission. Additionally, both E.coli and Yeast subnetworks demonstrate very similar FFL crosstalk patterns that were considered significant in their contribution to information transport robustness in these two organisms. This finding may potentially allude to common design principles in transcriptomic networks from different organisms.},
journal = {Mob. Netw. Appl.},
month = oct,
pages = {1970–1982},
numpages = {13},
keywords = {Edge-connected motif, Crosstalk, Complex networks, Transcriptional networks, Motif connectivity}
}

@inproceedings{10.5555/3237383.3237966,
author = {Nascimento, Nathalia},
title = {A Self-Configurable IoT Agent System based on Environmental Variability},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This thesis develops a self-configurable system to design agents for Internet of Things (IoT) applications. The proposed approach goes beyond existing methods by supporting handling variability in IoT agents according to environmental changes. As part of the research, we have designed a software framework, prototyped several IoT applications, and conducted simulation and machine learning experiments. We find that (1) IoT agents vary according to the physical, software behavior and analysis architecture; (2) the configuration of the set of agents can be adjusted and reconfigured through feedback evaluative machine learning; and (3) reconfiguring a set of agents dynamically in accordance with environmental variants leads to better performance.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1761–1763},
numpages = {3},
keywords = {self-configurable system, neuroevolution, internet of things, human-in-the-loop, embodied agents},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1007/978-3-030-32248-9_56,
author = {Parvathaneni, Prasanna and Bao, Shunxing and Nath, Vishwesh and Woodward, Neil D. and Claassen, Daniel O. and Cascio, Carissa J. and Zald, David H. and Huo, Yuankai and Landman, Bennett A. and Lyu, Ilwoo},
title = {Cortical Surface Parcellation Using Spherical Convolutional Neural Networks},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_56},
doi = {10.1007/978-3-030-32248-9_56},
abstract = {We present cortical surface parcellation using spherical deep convolutional neural networks. Traditional multi-atlas cortical surface parcellation requires inter-subject surface registration using geometric features with slow processing speed on a single subject (2–3&nbsp;h). Moreover, even optimal surface registration does not necessarily produce optimal cortical parcellation as parcel boundaries are not fully matched to the geometric features. In this context, a choice of training features is important for accurate cortical parcellation. To utilize the networks efficiently, we propose cortical parcellation-specific input data from an irregular and complicated structure of cortical surfaces. To this end, we align ground-truth cortical parcel boundaries and use their resulting deformation fields to generate new pairs of deformed geometric features and parcellation maps. To extend the capability of the networks, we then smoothly morph cortical geometric features and parcellation maps using the intermediate deformation fields. We validate our method on 427 adult brains for 49 labels. The experimental results show that our method outperforms traditional multi-atlas and naive spherical U-Net approaches, while achieving full cortical parcellation in less than a minute.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {501–509},
numpages = {9},
keywords = {Cortical surface parcellation, Spherical deformation, Spherical U-Net, Surface registration},
location = {Shenzhen, China}
}

@article{10.1145/3395046,
author = {Zhou, Xinyi and Zafarani, Reza},
title = {A Survey of Fake News: Fundamental Theories, Detection Methods, and Opportunities},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3395046},
doi = {10.1145/3395046},
abstract = {The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: the false knowledge it carries, its writing style, its propagation patterns, and the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. It is our hope that this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but, more importantly, explainable.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {109},
numpages = {40},
keywords = {social media, news verification, misinformation, knowledge graph, information credibility, fact-checking, disinformation, deception detection, Fake news}
}

@article{10.1007/s10489-020-01912-z,
author = {Guo, Xiaoding and Zhang, Hongli and Ye, Lin and Li, Shang},
title = {TenLa: an approach based on controllable tensor decomposition and optimized lasso regression for judgement prediction of legal cases},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01912-z},
doi = {10.1007/s10489-020-01912-z},
abstract = {With the development of big data and artificial intelligence technology, the computer-assisted judgment of legal cases has become an inevitable trend in the intersection of computer science and law. Judgment prediction methods of legal cases mainly consist of two parts: (1) modeling of legal cases and (2) construction of judgment prediction algorithms. Previous methods for the judgment prediction of legal cases are mainly based on feature models and classification algorithms. Traditional feature models require extensive expert knowledge and manual annotation. They are highly dependent on vocabulary and grammatical information in databases, which are not conducive to the improvement of accuracy and universality of subsequent prediction algorithms. In addition, prediction results obtained by classification algorithms are coarse in granularity and low in accuracy. In general, judgments in similar legal cases are similar. This article proposes a new method for the judgment prediction of legal cases, namely, TenLa, which is based on a controllable algorithm of tensor decomposition and an optimized Lasso regression model. TenLa takes similarities between legal cases as an important indicator of judgment prediction and is mainly divided into three parts: (1) ModTen; we propose a modeling method for legal cases, namely, ModTen, which represents legal cases as three-dimensional tensors. (2) ConTen; we propose a new tensor decomposition algorithm, namely, ConTen, which decomposes tensors obtained by ModTen into core tensors through the intermediary tensor. Core tensors greatly reduce the dimensions of original tensors. (3) OLass; we propose an optimized Lasso regression algorithm, namely, OLass. Core tensors obtained by ConTen are used to train OLass. Specifically, we propose an optimization algorithm for OLass with respect to the intermediary tensor in ConTen; thus, the core tensors obtained by ConTen carry tensor elements and tensor structure information that is most conducive to the improvement of the accuracy of OLass. Experiments show that TenLa has higher accuracy than traditional judgment prediction algorithms.},
journal = {Applied Intelligence},
month = apr,
pages = {2233–2252},
numpages = {20},
keywords = {Judgment prediction, Lasso regression, Tensor decomposition}
}

@article{10.1016/j.neunet.2021.05.003,
author = {Zhou, Zikun and Fan, Nana and Yang, Kai and Wang, Hongpeng and He, Zhenyu},
title = {Adaptive ensemble perception tracking},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.05.003},
doi = {10.1016/j.neunet.2021.05.003},
journal = {Neural Netw.},
month = oct,
pages = {316–328},
numpages = {13},
keywords = {Siamese network, Receptive field adaption, Ensemble prediction, Visual tracking}
}

@inproceedings{10.1007/978-3-319-35122-3_8,
author = {Peng, Zhenlian and Wang, Jian and He, Keqing and Li, Hongtao},
title = {An Approach for Prioritizing Software Features Based on Node Centrality in Probability Network},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_8},
doi = {10.1007/978-3-319-35122-3_8},
abstract = {Due to the increasing complexity of software products as well as the restriction of the development budget and time, requirements prioritization, i.e., selecting more crucial requirements to be designed and developed firstly, has become increasingly important in the software development lifetime. Considering the fact that a feature in a feature model can be viewed as a set of closely related requirements, feature prioritization will contribute to requirements prioritization to a large extent. Therefore, how to measure the priority of features within a feature model becomes an important issue in requirements analysis. In this paper, a software feature prioritization approach is proposed, which utilizes the dependencies between features to build a feature probability network and measures feature prioritization through the nodes centrality in the network. Experiments conducted on real world feature models show that the proposed approach can accurately prioritize features in feature models.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {106–121},
numpages = {16},
keywords = {Feature probability network, Feature prioritization, Feature model, Centrality},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@article{10.1007/s00371-019-01707-5,
author = {Zhi, Ruicong and Liu, Mengyi and Zhang, Dezheng},
title = {A comprehensive survey on automatic facial action unit analysis},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {5},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-019-01707-5},
doi = {10.1007/s00371-019-01707-5},
abstract = {Facial Action Coding System is the most influential sign judgment method for facial behavior, and it is a comprehensive and anatomical system which could encode various facial movements by the combination of basic AUs (Action Units). AUs define certain facial configurations caused by contraction of one or more facial muscles, and they are independent of interpretation of emotions. However, automatic facial action unit recognition remains challenging due to several open questions such as rigid and non-rigid facial motions, multiple AUs detection, intensity estimation and naturalistic context application. This paper introduces recent advances in automatic facial action unit recognition, focusing on the fundamental components of face detection and registration, facial action representation, feature selection and classification. The comprehensive analysis of facial representations is presented according to the facial data properties (image and video, 2D and 3D) and characteristics of facial features (predesign and learning, appearance and geometry, hybrid and fusion). Facial action unit recognition involves AUs occurrence detection, AUs temporal segment detection and AUs intensity estimation. We discussed the role of each component, main techniques with their characteristics, challenges and potential directions of facial action unit analysis.},
journal = {Vis. Comput.},
month = may,
pages = {1067–1093},
numpages = {27},
keywords = {Survey, 3D, Video, Facial representation, Action unit, Facial Action Coding System}
}

@article{10.1109/TASLP.2016.2536478,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {A deep ensemble learning method for monaural speech separation},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536478},
doi = {10.1109/TASLP.2016.2536478},
abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–977},
numpages = {11},
keywords = {multicontext networks, monaural speech separation, masking-based separation, mapping-based separation, ensemble learning, deep neural networks}
}

@inproceedings{10.1145/2245276.2231938,
author = {Sardinha, Alberto and Yu, Yijun and Niu, Nan and Rashid, Awais},
title = {EA-tracer: identifying traceability links between code aspects and early aspects},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231938},
doi = {10.1145/2245276.2231938},
abstract = {Early aspects are crosscutting concerns that are identified and addressed at the requirements and architecture level, while code aspects are crosscutting concerns that manifest at the code level. Currently, there are many approaches to address the identification and modularization of these cross-cutting concerns at each level, but very few techniques try to analyze the relationship between early aspects and code aspects. This paper presents a tool for automating the process of identifying traceability links between requirements-level aspects and code aspects, which is a first step towards an in-depth analysis. We also present an empirical evaluation of the tool with a real-life Web-based information system and a software product line for handling data on mobile devices. The results show that we can identify traceability links between early aspects and code aspects with a high accuracy.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1035–1042},
numpages = {8},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.5555/2370969.2370974,
author = {Wojna, Arkadiusz},
title = {Center-Based Indexing in Vector and Metric Spaces},
year = {2003},
issue_date = {August 2003},
publisher = {IOS Press},
address = {NLD},
volume = {56},
number = {3},
issn = {0169-2968},
abstract = {The paper addresses the problem of indexing data for k nearest neighbors (k-nn) search. Given a collection of data objects and a similarity measure the searching goal is to find quickly the k most similar objects to a given query object. We present a top-down indexing method that employs a widely used scheme of indexing algorithms. It starts with the whole set of objects at the root of an indexing tree and iteratively splits data at each level of indexing hierarchy. In the paper two different data models are considered. In the first, objects are represented by vectors from a multi-dimensional vector space. The second, more general, is based on an assumption that objects satisfy only the axioms of a metric space. We propose an iterative k-means algorithm for tree node splitting in case of a vector space and an iterative k-approximate-centers algorithm in case when only a metric space is provided. The experiments show that the iterative k-means splitting procedure accelerates significantly k-nn searching over the one-step procedure used in other indexing structures such as GNAT, SS-tree and M-tree and that the relevant representation of a tree node is an important issue for the performance of the search process. We also combine different search pruning criteria used in BST, GHT nad GNAT structures into one and show that such a combination outperforms significantly each single pruning criterion. The experiments are performed for benchmark data sets of the size up to several hundreds of thousands of objects. The indexing tree with the k-means splitting procedure and the combined search criteria is particularly effective for the largest tested data sets for which this tree accelerates searching up to several thousands times.},
journal = {Fundam. Inf.},
month = aug,
pages = {285–310},
numpages = {26}
}

@inproceedings{10.1007/978-3-030-29551-6_44,
author = {Lin, Jiping and Zhou, Yu and Kang, Junhao},
title = {Low-Sampling Imagery Data Recovery by Deep Learning Inference and Iterative Approach},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_44},
doi = {10.1007/978-3-030-29551-6_44},
abstract = {Block-based compressed sensing (CS) recovery aims to reconstruct the high quality image from only a small number of observations in a block-wise manner. However, when the sampling rate is very low and the existence of additive noise, there are usually some block artifacts and detail blurs which degrades the reconstructed quality. In this paper, we propose an efficient method which takes both the advantages of deep learning (DL) framework and iterative approaches. First, a deep multi-layer perceptron (DMLP) is constructed to obtain the initial reconstructed image. Then, an efficient iterative approach is applied to keep the consistence and smoothness between the adjacent blocks. The proposed method demonstrates its efficacy on benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {488–493},
numpages = {6},
keywords = {Iterative approach, Deep learning, Compressed sensing},
location = {Athens, Greece}
}

@article{10.4018/jsse.2010102004,
author = {Nhlabatsi, Armstrong and Nuseibeh, Bashar and Yu, Yijun},
title = {Security Requirements Engineering for Evolving Software Systems: A Survey},
year = {2010},
issue_date = {January 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010102004},
doi = {10.4018/jsse.2010102004},
abstract = {Long-lived software systems often undergo evolution over an extended period. Evolution of these systems is inevitable as they need to continue to satisfy changing business needs, new regulations and standards, and introduction of novel technologies. Such evolution may involve changes that add, remove, or modify features; or that migrate the system from one operating platform to another. These changes may result in requirements that were satisfied in a previous release of a system not being satisfied in subsequent versions. When evolutionary changes violate security requirements, a system may be left vulnerable to attacks. In this article we review current approaches to security requirements engineering and conclude that they lack explicit support for managing the effects of software evolution. We then suggest that a cross fertilization of the areas of software evolution and security engineering would address the problem of maintaining compliance to security requirements of software systems as they evolve.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {54–73},
numpages = {20},
keywords = {Software Evolution, Security Requirements Engineering, Entailment Relation}
}

@inproceedings{10.1145/1134285.1134336,
author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
title = {Who should fix this bug?},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134336},
doi = {10.1145/1134285.1134336},
abstract = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {361–370},
numpages = {10},
keywords = {problem tracking, machine learning, issue tracking, bug triage, bug report assignment},
location = {Shanghai, China},
series = {ICSE '06}
}

@inproceedings{10.5555/3540261.3540707,
author = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
title = {History aware multimodal transformer for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {446},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/1754288.1754295,
author = {Indukuri, Kishore Varma and Krishna, P. Radha},
title = {Mining e-contract documents to classify clauses},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754295},
doi = {10.1145/1754288.1754295},
abstract = {E-contracts begin as legal documents and end up as processes that help organizations abide by legal rules while fulfilling contract terms. As contracts are complex, their deployment is predominantly established and fulfilled with significant human involvement. One of the key difficulties with any kind of contract processing is the legal ambiguity, which makes it difficult to address any violation of the contract terms. Thus, there is a need to track clauses for the contract activities under execution and violation of clauses. This necessitates deriving clause patterns from e-contract documents and map to their respective activities for further monitoring and fulfillment of e-contracts during their enactment. In this paper, we present a classification approach to extract clause patterns from e-contract documents. This is a challenging task as activities and clauses are mostly derived from both legal and business process driven contract knowledge.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {7},
numpages = {5},
keywords = {text analytics, e-contracts, data mining},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/3289600.3290961,
author = {Kashinskaya, Yana and Samosvat, Egor and Artikov, Akmal},
title = {Spring-Electrical Models For Link Prediction},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290961},
doi = {10.1145/3289600.3290961},
abstract = {We propose a link prediction algorithm that is based on spring-electrical models. The idea to study these models came from the fact that spring-electrical models have been successfully used for networks visualization. A good network visualization usually implies that nodes similar in terms of network topology, e.g., connected and/or belonging to one cluster, tend to be visualized close to each other. Therefore, we assumed that the Euclidean distance between nodes in the obtained network layout correlates with a probability of a link between them. We evaluate the proposed method against several popular baselines and demonstrate its flexibility by applying it to undirected, directed and bipartite networks.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {708–716},
numpages = {9},
keywords = {spring-electrical models, link prediction, graph embeddings},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@article{10.1016/j.specom.2019.01.002,
author = {Najnin, Shamima and Banerjee, Bonny},
title = {Speech recognition using cepstral articulatory features},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {107},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.01.002},
doi = {10.1016/j.specom.2019.01.002},
journal = {Speech Commun.},
month = feb,
pages = {26–37},
numpages = {12},
keywords = {Deep neural network, General regression neural network, Inversion mapping, Cepstral articulatory feature, Acoustic feature, Phoneme recognition}
}

@article{10.1016/j.neucom.2017.02.098,
author = {Zubizarreta, Asier and Larrea, Mikel and Irigoyen, Eloy and Cabanes, Itziar and Portillo, Eva},
title = {Real time direct kinematic problem computation of the 3PRS robot using neural networks},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {271},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.098},
doi = {10.1016/j.neucom.2017.02.098},
abstract = {The reliable calculation of the Direct Kinematic Problem (DKP) is one of the main challenges for the implementation of Real-Time (RT) controllers in Parallel Robots. The DKP estimates the pose of the end effector of the robot in terms of the sensors placed on the actuators. However, this calculation requires the use of time-consuming numerical iterative procedures.Artificial Neural Networks have been proposed to implement the complex DKP equation mapping due to their universal approximator property. However, the proposals in this area do not consider the Real Time implementation of the ANN based solution, and no approximation error vs computational time analysis is carried out.In this work, a methodology that uses Artificial Neural Networks (ANNs) to approximate the DKP is proposed. Based on the 3PRS parallel robot, a comprehensive study is carried out in which several network configurations are proposed to approximate the DKP. Moreover, to demonstrate the effectiveness of the approach, the proposed networks are evaluated considering not only their approximation capabilities, but also their Real Time performance in comparison with the traditional iterative procedures used in robotics.},
journal = {Neurocomput.},
month = jan,
pages = {104–114},
numpages = {11},
keywords = {Parallel robots, Kinematic problem, Artificial neural network}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Design diversity, IoT, Internet of things, Software architectures for dynamic environments, Runtime architecture evaluation, Run-time architecture evaluation}
}

@article{10.1016/j.knosys.2016.08.027,
author = {Sang, Yongsheng and Lv, Jiancheng and Qu, Hong and Yi, Zhang},
title = {Shortest path computation using pulse-coupled neural networks with restricted autowave},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.08.027},
doi = {10.1016/j.knosys.2016.08.027},
abstract = {Finding shortest paths is an important problem in transportation and communication networks. This paper develops a Pulse-Coupled Neural Network (PCNN) model to efficiently compute a single-pair shortest path. Unlike most of the existing PCNN models, the proposed model is endowed with a special mechanism, called on-forward/off-backward; if a neuron fires, its neighboring neurons in a certain forward region will be excited, whereas the neurons in a backward region will be inhibited. As a result, the model can produce a restricted autowave that propagates at different speeds corresponding to different directions, which is different from the completely nondeterministic PCNN models. Compared with some traditional methods, the proposed PCNN model significantly reduces the computational cost of searching for the shortest path. Experimental results further confirmed the efficiency and effectiveness of the proposed model.},
journal = {Know.-Based Syst.},
month = dec,
pages = {1–11},
numpages = {11},
keywords = {Shortest path, Restricted autowave, Pulse-coupled neural networks, On-forward/off-backward}
}

@inproceedings{10.1145/3286978.3287017,
author = {Comito, Carmela},
title = {Mining Pattern Similarity for Mobility Prediction in Location-based Social Networks},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287017},
doi = {10.1145/3286978.3287017},
abstract = {The widespread use of location-based social networks is making such social media one of the major sources of information about people activities and costumes within urban context, allowing to capture and enhance the comprehension of people behaviour, including human mobility regularities. In that sense, the present work describes a novel approach to predict human mobility by using Twitter data. The approach predict the future location of an individual based on her recent mobility history (like individuals typical mobility routines) and on global mobility in the considered geographic area (e.g., mobility routines of all the Twitter users). The prediction approach is based on a novel trajectory pattern similarity measure that allows to identify the more suitable historic patterns to exploit for the prediction of the user next location. If none of the patterns satisfies the similarity threshold, a set of spatio-temporal features characterizing locations and movements among them are combined in a supervised learning approach based on decision trees. The experimental evaluation, performed on a real-world dataset of tweets posted in London, shows the effectiveness and efficiency of the approach in predicting the user's next places, achieving a remarkable accuracy and precision.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {284–291},
numpages = {8},
keywords = {Twitter, Trajectory patterns, Next-place prediction},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.5555/2999611.2999641,
author = {Fidaner, I\c{s}\i{}k Bar\i{}\c{s} and Cemgil, Ali Taylan},
title = {Summary statistics for partitionings and feature allocations},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {261–269},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.5555/2025896.2025909,
author = {Przyby\l{}ek, Adam},
title = {Systems evolution and software reuse in object-oriented programming and aspect-oriented programming},
year = {2011},
isbn = {9783642219511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Every new programming technique makes claims that software engineers want to hear. Such is the case with aspect-oriented programming (AOP). This paper describes a quasi-controlled experiment which compares the evolution of two functionally equivalent programs, developed in two different paradigms. The aim of the study is to explore the claims that software developed with aspect-oriented languages is easier to maintain and reuse than this developed with object-oriented languages. We have found no evidence to support these claims.},
booktitle = {Proceedings of the 49th International Conference on Objects, Models, Components, Patterns},
pages = {163–178},
numpages = {16},
keywords = {separation of concerns, reusability, maintainability, AOP},
location = {Zurich, Switzerland},
series = {TOOLS'11}
}

@inproceedings{10.5555/3042817.3042922,
author = {Das, Mrinal Kanti and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Gopinath, K.},
title = {Subtle topic models and discovering subtly manifested software concerns automatically},
year = {2013},
publisher = {JMLR.org},
abstract = {In a recent pioneering approach LDA was used to discover cross cutting concerns (CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed in (Williamson et al., 2010) for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models (STM). STM uses a generalized stick breaking process (GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {II–253–II–261},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1145/2180921.2180923,
author = {Anwikar, Vallabh and Naik, Ravindra and Contractor, Adnan and Makkapati, Hemanth},
title = {Domain-driven technique for functionality identification in source code},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180923},
doi = {10.1145/2180921.2180923},
abstract = {While migrating existing software systems to Software Product Lines, finding out the functionalities in the software is critical. For maintenance activities like deleting or changing existing features, or adding new similar features, identifying and extracting functionalities from the software is significant. This paper describes a technique for creating mapping between the source code and functionalities implemented by it while exploiting the domain knowledge. The technique is based on the notion of function variables that are used by developers for expressing functionality in the source code. By tracking the known values of the function variables and evaluating the conditions that use them, the mapping is identified. Our technique makes use of static data ow analysis and partial evaluation, and is designed with automation perspective. After applying to few samples representing real-life code structure and programming practices, the technique identified precise mapping of the detailed program elements to functions},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–8},
numpages = {8},
keywords = {partial evaluation, functionality identification, function variables}
}

@article{10.1007/s10462-020-09907-5,
author = {Uma Maheswari, S. and Shahina, A. and Nayeemulla Khan, A.},
title = {Understanding Lombard speech: a review of compensation techniques towards improving speech based recognition systems},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09907-5},
doi = {10.1007/s10462-020-09907-5},
abstract = {Building voice-based Artificial Intelligence (AI) systems that can efficiently interact with humans through speech has become plausible today due to rapid strides in efficient data-driven AI techniques. Such a human–machine voice interaction in real world would often involve a noisy ambience, where humans tend to speak with additional vocal effort than in a quiet ambience, to mitigate the noise-induced suppression of vocal self-feedback. This noise induced change in the vocal effort is called Lombard speech. In order to build intelligent conversational devices that can operate in a noisy ambience, it is imperative to study the characteristics and processing of Lombard speech. Though the progress of research on Lombard speech started several decades ago, it needs to be explored further in the current scenario which is seeing an explosion of voice-driven applications. The system designed to work with normal speech spoken in a quiet ambience fails to provide the same performance in changing environmental contexts. Different contexts lead to different styles of Lombard speech and hence there arises a need for efficient ways of handling variations in speaking styles in noise. The Lombard speech is also more intelligible than normal speech of a speaker. Applications like public announcement systems with speech output interface should talk with varying degrees of vocal effort to enhance naturalness in a way that humans adapt to speak in noise, in real time. This review article is an attempt to summarize the progress of work on the possible ways of processing Lombard speech to build smart and robust human–machine interactive systems with speech input–output interface, irrespective of operating environmental contexts, for different application needs. This article is a comprehensive review of the studies on Lombard speech, highlighting the key differences observed in acoustic and perceptual analysis of Lombard speech and detailing the Lombard effect compensation methods towards improving the robustness of speech based recognition systems.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {2495–2523},
numpages = {29},
keywords = {Lombard speech synthesis, Lombard effect compensation, Automatic recognition systems, Perceptual analysis, Acoustic analysis, Lombard speech}
}

@article{10.1007/s00530-005-0180-9,
author = {Zhang, Ruofei and Zhang, Zhongfei Mark},
title = {FAST: Toward more effective and efficient image retrieval},
year = {2005},
issue_date = {October   2005},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {6},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-005-0180-9},
doi = {10.1007/s00530-005-0180-9},
abstract = {This paper focuses on developing a Fast And Semantics-Tailored (FAST) image retrieval methodology. Specifically, the contributions of FAST methodology to the CBIR literature include: (1) development of a new indexing method based on fuzzy logic to incorporate color, texture, and shape information into a region-based approach to improving the retrieval effectiveness and robustness; (2) development of a new hierarchical indexing structure and the corresponding hierarchical, elimination-based A* retrieval (HEAR) algorithm to significantly improve the retrieval efficiency without sacrificing the retrieval effectiveness; it is shown that HEAR is guaranteed to deliver a logarithm search in the average case; (3) employment of user relevance feedback to tailor the effective retrieval to each user's individualized query preference through the novel indexing tree pruning (ITP) and adaptive region weight updating (ARWU) algorithms. Theoretical analysis and experimental evaluations show that FAST methodology holds great promise in delivering fast and semantics-tailored image retrieval in CBIR.},
journal = {Multimedia Syst.},
month = oct,
pages = {529–543},
numpages = {15},
keywords = {Relevance feedback, Region-based features, Indexing tree pruning, Hierarchical indexing structure, Content-based image retrieval}
}

@inproceedings{10.1109/ICASSP.2016.7472112,
author = {May, Avner and Collins, Michael and Hsu, Daniel and Kingsbury, Brian},
title = {Compact kernel models for acoustic modeling via random feature selection},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2016.7472112},
doi = {10.1109/ICASSP.2016.7472112},
abstract = {A simple but effective method is proposed for learning compact random feature models that approximate non-linear kernel methods, in the context of acoustic modeling. The method is able to explore a large number of non-linear features while maintaining a compact model via feature selection more efficiently than existing approaches. For certain kernels, this random feature selection may be regarded as a means of non-linear feature selection at the level of the raw input features, which motivates additional methods for computational improvements. An empirical evaluation demonstrates the effectiveness of the proposed method relative to the natural baseline method for kernel approximation.2},
booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {2424–2428},
numpages = {5},
location = {Shanghai, China}
}

@article{10.1007/s11280-019-00766-x,
author = {Hu, Rongyao and Zhu, Xiaofeng and Zhu, Yonghua and Gan, Jiangzhang},
title = {Robust SVM with adaptive graph learning},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-019-00766-x},
doi = {10.1007/s11280-019-00766-x},
abstract = {Support Vector Machine (SVM) has been widely applied in real application due to its efficient performance in the classification task so that a large number of SVM methods have been proposed. In this paper, we present a novel SVM method by taking the dynamic graph learning and the self-paced learning into account. To do this, we propose utilizing self-paced learning to assign important samples with large weights, learning a transformation matrix for conducting feature selection to remove redundant features, and learning a graph matrix from the low-dimensional data of original data to preserve the data structure. As a consequence, both the important samples and the useful features are used to select support vectors in the SVM framework. Experimental analysis on four synthetic and sixteen benchmark data sets demonstrated that our method outperformed state-of-the-art methods in terms of both binary classification and multi-class classification tasks.},
journal = {World Wide Web},
month = may,
pages = {1945–1968},
numpages = {24},
keywords = {SVM, Graph learning, Feature selection, Self-paced learning}
}

@article{10.1155/2020/8826568,
author = {Asare, Sarpong Kwadwo and You, Fei and Nartey, Obed Tettey and Rakhshan, Vahid},
title = {A Semisupervised Learning Scheme with Self-Paced Learning for Classifying Breast Cancer Histopathological Images},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-5265},
url = {https://doi.org/10.1155/2020/8826568},
doi = {10.1155/2020/8826568},
abstract = {The unavailability of large amounts of well-labeled data poses a significant challenge in many medical imaging tasks. Even in the likelihood of having access to sufficient data, the process of accurately labeling the data is an arduous and time-consuming one, requiring expertise skills. Again, the issue of unbalanced data further compounds the abovementioned problems and presents a considerable challenge for many machine learning algorithms. In lieu of this, the ability to develop algorithms that can exploit large amounts of unlabeled data together with a small amount of labeled data, while demonstrating robustness to data imbalance, can offer promising prospects in building highly efficient classifiers. This work proposes a semisupervised learning method that integrates self-training and self-paced learning to generate and select pseudolabeled samples for classifying breast cancer histopathological images. A novel pseudolabel generation and selection algorithm is introduced in the learning scheme to generate and select highly confident pseudolabeled samples from both well-represented classes to less-represented classes. Such a learning approach improves the performance by jointly learning a model and optimizing the generation of pseudolabels on unlabeled-target data to augment the training data and retraining the model with the generated labels. A class balancing framework that normalizes the class-wise confidence scores is also proposed to prevent the model from ignoring samples from less represented classes (hard-to-learn samples), hence effectively handling the issue of data imbalance. Extensive experimental evaluation of the proposed method on the BreakHis dataset demonstrates the effectiveness of the proposed method.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {16}
}

@inproceedings{10.5555/3294996.3295179,
author = {G\"{u}\c{c}l\"{u}t\"{u}rk, Ya\u{g}mur and G\"{u}\c{c}l\"{u}, Umut and Seeliger, Katja and Bosch, Sander and van Lier, Rob and van Gerven, Marcel},
title = {Reconstructing perceived faces from brain activations with deep adversarial neural decoding},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4249–4260},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.1007/978-3-030-58580-8_28,
author = {Beeching, Edward and Dibangoye, Jilles and Simonin, Olivier and Wolf, Christian},
title = {Learning to Plan with Uncertain Topological Maps},
year = {2020},
isbn = {978-3-030-58579-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58580-8_28},
doi = {10.1007/978-3-030-58580-8_28},
abstract = {We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in&nbsp;topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III},
pages = {473–490},
numpages = {18},
keywords = {Visual navigation, Topological maps, Graph neural networks},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.ipm.2019.102097,
author = {Zhao, Ziyuan and Zhu, Huiying and Xue, Zehao and Liu, Zhao and Tian, Jing and Chua, Matthew Chin Heng and Liu, Maofu},
title = {An image-text consistency driven multimodal sentiment analysis approach for social media},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {56},
number = {6},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2019.102097},
doi = {10.1016/j.ipm.2019.102097},
journal = {Inf. Process. Manage.},
month = nov,
numpages = {9},
keywords = {Social media, Visual sentiment, Textual sentiment, Multimodal sentiment analysis}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Adaptive neuro-fuzzy inference system, Particle swarm algorithm, Genetic algorithm, Regression test suite optimization}
}

@article{10.1016/j.engappai.2007.09.004,
author = {Blomqvist, Eva and \"{O}hgren, Annika},
title = {Constructing an enterprise ontology for an automotive supplier},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {21},
number = {3},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2007.09.004},
doi = {10.1016/j.engappai.2007.09.004},
abstract = {This paper presents experiences and conclusions from ontology engineering applied in the automotive suppliers domain. The work focuses on construction of enterprise ontologies to support structuring of enterprise information and knowledge management. Two methods for ontology construction, developed by previous research activities, were used in parallel when developing an ontology for a company in automotive supplier industries. One method is automatic and the other method is a manual approach. A conclusion was that the developed ontologies complemented each other well and therefore the decision was made to merge them for use in the project. The resulting ontology will now be used in several pilot applications.},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
pages = {386–397},
numpages = {12},
keywords = {Ontology merging, Ontology evaluation, Ontology engineering, Knowledge engineering, Automobile industry}
}

@inproceedings{10.1007/978-3-030-58539-6_2,
author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
title = {SoundSpaces: Audio-Visual Navigation in&nbsp;3D&nbsp;Environments},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_2},
doi = {10.1007/978-3-030-58539-6_2},
abstract = {Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {17–36},
numpages = {20},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10489-021-02238-0,
author = {Xiao, Xinshuang and Xu, Yitian},
title = {Multi-target regression via self-parameterized Lasso and refactored target space},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02238-0},
doi = {10.1007/s10489-021-02238-0},
abstract = {Multi-target regression (MTR) aims at simultaneously predicting multiple continuous target variables based on the same set of input variables. It has been used to solve some challenging problems. A self-parameterized Lasso for MTR is proposed in this paper, which is applied to refactored target space via linear combinations of existing targets. Our approach can simultaneously model intrinsic inter-target correlations and input-target correlations, which makes full use of the information contained in the data. Meanwhile, this information helps automatically generate the parameters needed in the model. Compared with the common method, which requires a manual setting of parameters and is expensive to optimize these parameters, our method can save a lot of time while no reduction in performance. Besides, our method can be used not only for MTR tasks but also for multi-classification tasks. The experimental results show that our method performs well in different tasks and has a wide range of applications.},
journal = {Applied Intelligence},
month = oct,
pages = {6743–6751},
numpages = {9},
keywords = {Self-parameterized, Multi-classification, Lasso, Multi-target regression}
}

@inproceedings{10.1145/1882992.1883014,
author = {Zhang, Jintao and Huan, Jun},
title = {Novel biological network features discovery for in silico identification of drug targets},
year = {2010},
isbn = {9781450300308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882992.1883014},
doi = {10.1145/1882992.1883014},
abstract = {In silico identification of potential drug targets is a crucial task for drug discovery. Traditional approaches utilize only protein sequence or structural information to predict drug targets, and achieve limited successes. Since cellular proteins function in the context of interaction networks by interacting with other cellular macromolecules, analysis of topological features of proteins in such networks reveal important insights on the potential druggability of proteins. In this paper, we first introduced ten novel topological features extracted from the human protein-protein interaction network. When designing these new features, we specially emphasized the roles of three disease-related groups of proteins: known drug targets, disease genes, and essential genes. Based on these novel network features, we built highly accurate models with up to 80% classification accuracy using support vector machines, L1-regularized logistic regression, and k-nearest neighbors to predict drug target, and analyzed the relevance of each feature to the proteins' druggability. Moreover, we combined our network features with a set of protein sequence features, and achieved more robust experimental performance. With the framework of integrating both network and sequence features, our method can also be used to prioritize multiple candidate proteins according to their predicted druggability.},
booktitle = {Proceedings of the 1st ACM International Health Informatics Symposium},
pages = {144–152},
numpages = {9},
keywords = {network feature, machine learning, human protein interactome, drug target},
location = {Arlington, Virginia, USA},
series = {IHI '10}
}

@inproceedings{10.1007/978-3-030-00308-1_5,
author = {Rizzi, Caroline and Johnson, Colin G. and Vargas, Patricia A.},
title = {Fear Learning for Flexible Decision Making in RoboCup: A Discussion},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_5},
doi = {10.1007/978-3-030-00308-1_5},
abstract = {In this paper, we address the stagnation of RoboCup competitions in the fields of contextual perception, real-time adaptation and flexible decision-making, mainly in regards to the Standard Platform League (SPL). We argue that our Situation-Aware FEar Learning (SAFEL) model has the necessary tools to leverage the SPL competition in these fields of research, by allowing robot players to learn the behaviour profile of the opponent team at runtime. Later, players can use this knowledge to predict when an undesirable outcome is imminent, thus having the chance to act towards preventing it. We discuss specific scenarios where SAFEL’s associative learning could help to increase the positive outcomes of a team during a soccer match by means of contextual adaptation.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {59–70},
numpages = {12},
keywords = {Affective computing, Brain emotional model, Contextual fear conditioning, Cognitive learning, RoboCup},
location = {Nagoya, Japan}
}

@inproceedings{10.5555/2997189.2997322,
author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
title = {Self-paced learning for latent variable models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1189–1197},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@inproceedings{10.5555/2033313.2033323,
author = {Kiefer, Christoph and Bernstein, Abraham},
title = {Application and evaluation of inductive reasoning methods for the semantic web and software analysis},
year = {2011},
isbn = {9783642230318},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for SemanticWeb data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting four sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for almost any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines even shows that our approach is superior in terms of classification accuracy.},
booktitle = {Proceedings of the 7th International Conference on Reasoning Web: Semantic Technologies for the Web of Data},
pages = {460–503},
numpages = {44},
keywords = {semantic web, machine learning, inductive reasoning, evaluation, SPARQL},
location = {Galway, Ireland},
series = {RW'11}
}

@inproceedings{10.1007/978-3-030-66823-5_24,
author = {Campari, Tommaso and Eccher, Paolo and Serafini, Luciano and Ballan, Lamberto},
title = {Exploiting Scene-Specific Features for Object Goal Navigation},
year = {2020},
isbn = {978-3-030-66822-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66823-5_24},
doi = {10.1007/978-3-030-66823-5_24},
abstract = {Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {406–421},
numpages = {16},
keywords = {Reinforcement Learning, ObjectGoal Navigation, Visual Navigation},
location = {Glasgow, United Kingdom}
}

@article{10.5555/3190793.3190806,
author = {Shi, W. Y. and Chiao, J. -C.},
title = {Neural network based real-time heart sound monitor using a wireless wearable wrist sensor},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {94},
number = {3},
issn = {0925-1030},
abstract = {A new method is presented using a wearable wrist sensor to estimate acoustic parameters S1 and S2 of the heart sounds based on the neural network technique. Using the signal processing method, the heart conditions can be analyzed and monitored in real time and potentially in a long term with a wrist device. The velocities and time delays of the cardiac pulse waves in blood vessels were experimentally acquired and calculated at different artery locations on the human body. Signal attenuation of the pulses from the heart to the wrist radial artery was analyzed and a pulse-waveform travel model in blood vessels was proposed. A band-pass filter is applied to the pulse waves at various artery locations to reveal the heart sound features S1 and S2 existed in the pulse waves. In order to obtain accurate acoustic parameters, a neural network with two layers and 500 nonlinear tansig neurons was employed to estimate the heart sounds using the pulse waveforms from the wrist radial artery. It is encouraging to find that the acoustic parameters of estimated heart sounds by the trained neural network have only 1% average errors compared with the original heart sounds. The effects of various analog-to-digital conversion resolutions and sample rates were empirically analyzed. When the maximum value of errors is allowed within 2.15%, a 10,000-Hz sample rate and 12-bit resolution should be an appropriate selection for lower power consumption. Using the trained neural network, the new estimation method has been verified by a sensor with Bluetooth communication strapped on the wrist, thus mobility is not limited for the person whose heart sounds need to be monitored.},
journal = {Analog Integr. Circuits Signal Process.},
month = mar,
pages = {383–393},
numpages = {11},
keywords = {Wireless sensor networks, Stethoscope, Neural network, Digital signal processing}
}

@article{10.5555/1466818.1466820,
author = {Zak, Andrzej},
title = {Ships classification basing on acoustic signatures},
year = {2008},
issue_date = {April 2008},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {4},
number = {4},
issn = {1790-5052},
abstract = {The paper presents the technique of artificial neural networks used as classifier of hydroacoustic signatures generated by moving ship. The main task of proposed solution is to classify the objects which made the underwater noises. Firstly, the measurements were carried out dynamically by running ship past stationary hydrophones, mounted on tripods 1 m above the sea bottom. Secondly to identify the source of noise the level of vibration were measured on board by accelerometers, which were installed on important components of machinery. On the base of this measurement there was determined the sound pressure level, noise spectra and spectograms, transmission of acoustic energy via the hull into water. More over it was checked by using coherence function that components of underwater noise has its origin in vibrations of ship's mechanisms. Basing on this research it was possible to create the hydroacoustic signature or so called "acoustic portrait" of moving ship. Next during the complex ships' measurements on Polish Navy Test and Evaluation Acoustic Range hydroacoustic noises generated by moving ship were acquired. Basing on these results the classifier of acoustic signatures using artificial neural network was worked out. From the technique of artificial neural networks the Kohonen networks which belongs to group of self organizing networks where chosen to solve the research problem of classification. The choice was caused by some advantages of mentioned kind of neural networks like: they are ideal for finding relationships amongst complex sets of data, they have possibility to self expand the set of answers for new input vectors. To check the correctness of classifier work the research in which the number of right classification for presented and not presented before hydroacoustic signatures were made. Some results of research were presented on this paper. Described method actually is extended and its application is provided as assistant subsystem for hydrolocations systems of Polish Naval ships.},
journal = {WSEAS Trans. Sig. Proc.},
month = apr,
pages = {137–149},
numpages = {13},
keywords = {self-organizing map, hydroacousitc signatures, classification, Kohonen's neural networks}
}

@article{10.1145/3340962,
author = {Zhang, Xiao and Lyu, Yongqiang and Qu, Tong and Qiu, Pengfei and Luo, Xiaomin and Zhang, Jingyu and Fan, Shunjie and Shi, Yuanchun},
title = {Photoplethysmogram-based Cognitive Load Assessment Using Multi-Feature Fusion Model},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/3340962},
doi = {10.1145/3340962},
abstract = {Cognitive load assessment is crucial for user studies and human--computer interaction designs. As a noninvasive and easy-to-use category of measures, current photoplethysmogram- (PPG) based assessment methods rely on single or small-scale predefined features to recognize responses induced by people’s cognitive load, which are not stable in assessment accuracy. In this study, we propose a machine-learning method by using 46 kinds of PPG features together to improve the measurement accuracy for cognitive load. We test the method on 16 participants through the classical n-back tasks (0-back, 1-back, and 2-back). The accuracy of the machine-learning method in differentiating different levels of cognitive loads induced by task difficulties can reach 100% in 0-back vs. 2-back tasks, which outperformed the traditional HRV-based and single-PPG-feature-based methods by 12--55%. When using “leave-one-participant-out” subject-independent cross validation, 87.5% binary classification accuracy was reached, which is at the state-of-the-art level. The proposed method can also support real-time cognitive load assessment by beat-to-beat classifications with better performance than the traditional single-feature-based real-time evaluation method.},
journal = {ACM Trans. Appl. Percept.},
month = sep,
articleno = {19},
numpages = {17},
keywords = {real-time assessment, photoplethysmogram, multi-feature fusion, Cognitive load}
}

@article{10.1016/j.patrec.2021.06.029,
author = {Chaudhary, Sachin and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Talbar, Sanjay},
title = {Motion estimation in hazy videos},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.029},
doi = {10.1016/j.patrec.2021.06.029},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {130–138},
numpages = {9},
keywords = {Motion estimation, Scene understanding, 65D17, 65D05, 41A10, 41A05}
}

@article{10.1016/j.jss.2009.02.011,
author = {White, Jules and Dougherty, Brian and Schmidt, Douglas C.},
title = {Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.02.011},
doi = {10.1016/j.jss.2009.02.011},
abstract = {Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1268–1284},
numpages = {17},
keywords = {Resource constraints, Optimization, Feature modeling, Approximation algorithm}
}

@article{10.1007/s11263-011-0430-6,
author = {Taylor, Simon and Drummond, Tom},
title = {Binary Histogrammed Intensity Patches for Efficient and Robust Matching},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {94},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-011-0430-6},
doi = {10.1007/s11263-011-0430-6},
abstract = {This paper describes a method for feature-based matching which offers very fast runtime performance due to the simple quantised patches used for matching and a tree-based lookup scheme which prevents the need for exhaustively comparing each query patch against the entire feature database. The method enables seven independently moving targets in a test sequence to be localised in an average total processing time of 6.03 ms per frame.A training phase is employed to identify the most repeatable features from a particular range of viewpoints and to learn a model for the patches corresponding to each feature. Feature models consist of independent histograms of quantised intensity for each pixel in the patch, which we refer to as Histogrammed Intensity Patches (HIPs). The histogram values are thresholded and the feature model is stored in a compact binary representation which requires under 60 bytes of memory per feature and permits the rapid computation of a matching score using bitwise operations.The method achieves better matching robustness than the state-of-the-art fast localisation schemes introduced by Wagner et al. (IEEE International Symposium on Mixed and Augmented Reality, 2008). Additionally both the runtime memory usage and computation time are reduced by a factor of more than four.},
journal = {Int. J. Comput. Vision},
month = sep,
pages = {241–265},
numpages = {25},
keywords = {Real-time system, Pose estimation, Matching by classification, Keypoint recognition, Keypoint matching, Image matching, Feature extraction}
}

@article{10.1134/S1054661820030086,
author = {Gurevich, I. B. and Yashina, V. V.},
title = {Descriptive Image Analysis: Part III. Multilevel Model for Algorithms and Initial Data Combining in Pattern Recognition},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3},
issn = {1054-6618},
url = {https://doi.org/10.1134/S1054661820030086},
doi = {10.1134/S1054661820030086},
journal = {Pattern Recognit. Image Anal.},
month = jul,
pages = {328–341},
numpages = {14},
keywords = {image recognition algorithms, support sets, multiple classifiers, combinatorial structures of local neighborhoods, dual representation of images, descriptive image models, descriptive image representations, image models, image representations, models of image analysis procedures, mathematical theory of image analysis, descriptive image analysis, image analysis}
}

@inproceedings{10.1145/3460231.3478885,
author = {Di Sipio, Claudio and Di Rocco, Juri and Di Ruscio, Davide and Nguyen, Dr. Phuong Thanh},
title = {A Low-Code Tool Supporting the Development of Recommender Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3478885},
doi = {10.1145/3460231.3478885},
abstract = {The design of recommender systems (RSs) to support software development encompasses the fulfillment of different steps, including data preprocessing, choice of the most appropriate algorithms, item delivery. Though RSs can alleviate the curse of information overload, existing approaches resemble black-box systems, in which the end-user is not expected to fine-tune or personalize the overall process. In this work, we propose LEV4REC, a low-code environment to assist developers in designing, configuring, and delivering recommender systems. The first step supported by the proposed tool includes defining an initial model that allows for the configuration of the crucial components of the wanted RS. Then, a subsequent phase is performed to finalize the RS design, e.g., to specify configuration parameters. LEV4REC is eventually capable of generating source code for the desired RS. To evaluate the capabilities of the approach, we used LEV4REC to specify two existing RSs built on top of two different recommendation algorithms, i.e., collaborative filtering and supervised machine learning.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {741–744},
numpages = {4},
keywords = {recommender systems, model-driven, lowcode},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3377930.3390167,
author = {Anjum, Muhammad Sheraz and Ryan, Conor},
title = {Scalability analysis of grammatical evolution based test data generation},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390167},
doi = {10.1145/3377930.3390167},
abstract = {Heuristic-based search techniques have been increasingly used to automate different aspects of software testing. Several studies suggest that variable interdependencies may exist in branching conditions of real-life programs, and these dependencies result in the need for highly precise data values (such as of the form i=j=k) for code coverage analysis. This requirement makes it very difficult for Genetic Algorithm (GA)-based approach to successfully search for the required test data from vast search spaces of real-life programs.Ariadne is the only Grammatical Evolution (GE)-based test data generation system, proposed to date, that uses grammars to exploit variable interdependencies to improve code coverage. Ariadne has been compared favourably to other well-known test data generation techniques in the literature; however, its scalability has not yet been tested for increasingly complex programs.This paper presents the results of a rigorous analysis performed to examine Ariadne's scalability. We also designed and employed a large set of highly scalable 18 benchmark programs for our experiments. Our results suggest that Ariadne is highly scalable as it exhibited 100% coverage across all the programs of increasing complexity with significantly smaller search costs than GA-based approaches, which failed even with huge search budgets.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1213–1221},
numpages = {9},
keywords = {variable interdependencies, software testing, search based software testing, scalability, grammatical evolution, evolutionary testing, code coverage analysis, automatic test data generation},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.3233/JIFS-169890,
author = {Li, Hao and Pu, Bin and Kang, Yan and Lu, Chen Yang and Hsieh, Wen-Hsiang},
title = {Research on massive ECG data in XGBoost},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169890},
doi = {10.3233/JIFS-169890},
abstract = {&nbsp;There exists a huge amount of ECG data available in heart disease diagnosis which is found difficult in handing. Recently, many researchers focused on mining disease diagnosis to innovate the hidden patterns and their relevant features. Mining bio-medical data is one of the predominant research areas where clustering techniques are emphasized in heart disease diagnosis. But few people deal with large heart disease datasets and then classify disease data sets according to heart disease feature. We propose a method of anomaly threshold based on multiple classifiers can be well suited to datasets containing abnormal data, and use XGBoost algorithm as a sub-classifier to process massive ECG data. This research focuses on the heart disease classification problem. The data set is divided into two categories, and then it was classified into more specific categories, experimental results show that this method can improve classification accuracy. The experiments are conducted on massive instances of different heart disease obtained from the hospital actual cases and two data sets of UCI. In fact, we compared SVM, C4.5, Naive Bayes, Logistic, RandomForest and XGBoost algorithms, and found that tree-based model classifier is the best fit to predict arrhythmia. The method proposed in this paper is of great significance to the processing and forecasting system of large medical data sets, and promote the development of wisdom medical care.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1161–1169},
numpages = {9},
keywords = {intelligent medical, heart disease, heart disease diagnosis, ECG}
}

@inproceedings{10.1145/3395351.3399421,
author = {Acar, Abbas and Fereidooni, Hossein and Abera, Tigist and Sikder, Amit Kumar and Miettinen, Markus and Aksu, Hidayet and Conti, Mauro and Sadeghi, Ahmad-Reza and Uluagac, Selcuk},
title = {Peek-a-boo: i see your smart home activities, even encrypted!},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3399421},
doi = {10.1145/3395351.3399421},
abstract = {A myriad of IoT devices such as bulbs, switches, speakers in a smart home environment allow users to easily control the physical world around them and facilitate their living styles through the sensors already embedded in these devices. Sensor data contains a lot of sensitive information about the user and devices. However, an attacker inside or near a smart home environment can potentially exploit the innate wireless medium used by these devices to exfiltrate sensitive information from the encrypted payload (i.e., sensor data) about the users and their activities, invading user privacy. With this in mind, in this work, we introduce a novel multi-stage privacy attack against user privacy in a smart environment. It is realized utilizing state-of-the-art machine-learning approaches for detecting and identifying the types of IoT devices, their states, and ongoing user activities in a cascading style by only passively sniffing the network traffic from smart home devices and sensors. The attack effectively works on both encrypted and unencrypted communications. We evaluate the efficiency of the attack with real measurements from an extensive set of popular off-the-shelf smart home IoT devices utilizing a set of diverse network protocols like WiFi, ZigBee, and BLE. Our results show that an adversary passively sniffing the traffic can achieve very high accuracy (above 90%) in identifying the state and actions of targeted smart home devices and their users. To protect against this privacy leakage, we also propose a countermeasure based on generating spoofed traffic to hide the device states and demonstrate that it provides better protection than existing solutions.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {207–218},
numpages = {12},
keywords = {wifi, smart-home, privacy, network traffic, ZigBee, BLE},
location = {Linz, Austria},
series = {WiSec '20}
}

@inproceedings{10.5555/3044805.3045094,
author = {Heaukulani, Creighton and Knowles, David A. and Ghahramani, Zoubin},
title = {Beta diffusion trees},
year = {2014},
publisher = {JMLR.org},
abstract = {We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles &amp; Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1809–II–1817},
location = {Beijing, China},
series = {ICML'14}
}

@article{10.1016/j.specom.2007.07.006,
author = {Yapanel, Umit H. and Hansen, John H. L.},
title = {A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {2},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2007.07.006},
doi = {10.1016/j.specom.2007.07.006},
abstract = {Acoustic feature extraction from speech constitutes a fundamental component of automatic speech recognition (ASR) systems. In this paper, we propose a novel feature extraction algorithm, perceptual-MVDR (PMVDR), which computes cepstral coefficients from the speech signal. This new feature representation is shown to better model the speech spectrum compared to traditional feature extraction approaches. Experimental results for small (40-word digits) to medium (5k-word dictation) size vocabulary tasks show varying degree of consistent improvements across different experiments; however, the new front-end is most effective in noisy car environments. The PMVDR front-end uses the minimum variance distortionless response (MVDR) spectral estimator to represent the upper envelope of the speech signal. Unlike Mel frequency cepstral coefficients (MFCCs), the proposed front-end does not utilize a filterbank. The effectiveness of the PMVDR approach is demonstrated by comparing speech recognition accuracies with the traditional MFCC front-end and recently proposed PMCC front-end in both noise-free and real adverse environments. For speech recognition in noisy car environments, a 40-word vocabulary task, PMVDR front-end provides a 36% relative decrease in word error rate (WER) over the MFCC front-end. Under simulated speaker stress conditions, a 35-word vocabulary task, the PMVDR front-end yields a 27% relative decrease in the WER. For a noise-free dictation task, a 5k-word vocabulary task, again a relative 8% reduction in the WER is reported. Finally, a novel analysis technique is proposed to quantify noise robustness of an acoustic front-end. This analysis is conducted for the acoustic front-ends analyzed in the paper and results are presented.},
journal = {Speech Commun.},
month = feb,
pages = {142–152},
numpages = {11},
keywords = {Robust speech recognition, Noise-robustness analysis, Acoustic feature extraction}
}

@inproceedings{10.5555/3104482.3104581,
author = {Zhang, XianXing and Dunson, David B. and Carin, Lawrence},
title = {Tree-structured infinite sparse factor model},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {A tree-structured multiplicative gamma process (TMGP) is developed, for inferring the depth of a tree-based factor-analysis model. This new model is coupled with the nested Chinese restaurant process, to nonparametrically infer the depth and width (structure) of the tree. In addition to developing the model, theoretical properties of the TMGP are addressed, and a novel MCMC sampler is developed. The structure of the inferred tree is used to learn relationships between high-dimensional data, and the model is also applied to compressive sensing and interpolation of incomplete images.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {785–792},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{10.1145/3240508.3240582,
author = {Jiang, Yangbangyan and Yang, Zhiyong and Xu, Qianqian and Cao, Xiaochun and Huang, Qingming},
title = {When to Learn What: Deep Cognitive Subspace Clustering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240582},
doi = {10.1145/3240508.3240582},
abstract = {Subspace clustering aims at clustering data points drawn from a union of low-dimensional subspaces. Recently deep neural networks are introduced into this problem to improve both representation ability and precision for non-linear data. However, such models are sensitive to noise and outliers, since both difficult and easy samples are treated equally. On the contrary, in the human cognitive process, individuals tend to follow a learning paradigm from easy to hard and less to more. In other words, human beings always learn from simple concepts, then absorb more complicated ones gradually. Inspired by such learning scheme, in this paper, we propose a robust deep subspace clustering framework based on the principle of human cognitive process. Specifically, we measure the easinesses of samples dynamically so that our proposed method could gradually utilize instances from easy to more complex ones in a robust way. Meanwhile, a promising solution is designed to update the weights and parameters using an alternative optimization strategy, followed by a theoretical analysis to demonstrated the rationality of the proposed method. Experimental results on three popular benchmark datasets demonstrate the validity of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {718–726},
numpages = {9},
keywords = {subspace clustering, self-paced learning, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1016/j.csi.2019.103389,
author = {Bukhsh, Faiza Allah and Bukhsh, Zaharah Allah and Daneva, Maya},
title = {A systematic literature review on requirement prioritization techniques and their empirical evaluation},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.103389},
doi = {10.1016/j.csi.2019.103389},
journal = {Comput. Stand. Interfaces},
month = mar,
numpages = {18},
keywords = {Systematic literature review, Empirical research method, Empirical study, Requirements prioritization, Requirements engineering}
}

@article{10.1016/j.compbiomed.2019.103380,
author = {Kakati, Tulika and Bhattacharyya, Dhruba K. and Barah, Pankaj and Kalita, Jugal K.},
title = {Comparison of Methods for Differential Co-expression Analysis for Disease Biomarker Prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103380},
doi = {10.1016/j.compbiomed.2019.103380},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {12},
keywords = {Disease biomarkers, Parkinson's disease, Alzheimer's disease, miRNA expression, Gene expression, Empirical study, Differential co-expression analysis}
}

@inproceedings{10.5555/795666.796550,
author = {Alon, N. and Dar, S. and Parnas, M. and Ron, D.},
title = {Testing of clustering},
year = {2000},
isbn = {0769508502},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A set X of points in /spl Rfr//sup d/ is (k,b)-clusterable if X can be partitioned into k subsets (clusters) so that the diameter (alternatively, the radius) of each cluster is at most b. We present algorithms that by sampling from a set X, distinguish between the case that X is (k,b)-clusterable and the case that X is /spl epsiv/-far from being (k,b')-clusterable for any given 0&gt;/spl epsiv//spl les/1 and for b'/spl ges/b. In /spl epsiv/-far from being (k,b')-clusterable we mean that more than /spl epsiv/.|X| points should be removed from X so that it becomes (k,b')-clusterable. We give algorithms for a variety of cost measures that use a sample of size independent of |X|, and polynomial in k and 1//spl epsiv/. Our algorithms can also be used to find approximately good clusterings. Namely, these are clusterings of all but an /spl epsiv/-fraction of the points in X that have optimal (or close to optimal) cost. The benefit of our algorithms is that they construct an implicit representation of such clusterings in time independent of |X|. That is, without actually having to partition all points in X, the implicit representation can be used to answer queries concerning the cluster any given point belongs to.},
booktitle = {Proceedings of the 41st Annual Symposium on Foundations of Computer Science},
pages = {240},
keywords = {statistical analysis, sampling, pattern clustering, optimal cost, lower bounds, cost measures, computational complexity, clustering testing},
series = {FOCS '00}
}

@inproceedings{10.1145/3175516.3175532,
author = {Mujalli, Randa Oqab and L\'{o}pez, Griselda and Garach, Laura},
title = {Modeling Injury Severity of Vehicular Traffic Crashes},
year = {2017},
isbn = {9781450363501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175516.3175532},
doi = {10.1145/3175516.3175532},
abstract = {Data mining techniques constitute an alternative approach that has received increasing attention from researchers in recent years in road safety analysis field. In this paper Bayesian networks were used to develop models in order to identify factors that affect the injury severity of a crash within urban areas based on traffic crashes data on Jordanian roads collected for three years (2009-2011). The following variables were found to have a significant effect on classifying crashes according to their injury severity: lighting of roadway, crash type, road type, crash manner, surface condition, number of lanes, number of vehicles, gradient, type of pavement, traffic control devices, and speed limit. The results of this research can be used to determine the factors that should be taken into consideration when designing a new roadway or for improving safety of existing roads.},
booktitle = {Proceedings of the 2017 International Conference on Automation, Control and Robots},
pages = {51–55},
numpages = {5},
keywords = {traffic crashes, injury severity, Data mining},
location = {Wuhan, China},
series = {ICACR 2017}
}

@inproceedings{10.5555/3172077.3172256,
author = {Ren, Yazhou and Zhao, Peng and Sheng, Yongpan and Yao, Dezhong and Xu, Zenglin},
title = {Robust softmax regression for multi-class classification with self-paced learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2641–2647},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1109/ICASSP.2001.941166,
author = {Taira, S.},
title = {Automatic classification of QAM signals by neural networks},
year = {2001},
isbn = {0780370414},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICASSP.2001.941166},
doi = {10.1109/ICASSP.2001.941166},
abstract = {In this paper, automatic classification of QAM signals including 64-state QAM and 256-state QAM is discussed. Three layer neural networks whose input data are the histogram distribution of instantaneous amplitude at symbol points are used for the classification. The evaluation of the classification performance is carried out for both cases in which the synchronization of symbol timing is assured at the receiver and not assured. Good classification results are obtained by the computer simulations at SNR/spl ges/10 dB. The influence of the number of symbol points which are used for the calculation of the histogram is also discussed.},
booktitle = {Proceedings of the Acoustics, Speech, and Signal Processing, 200. on IEEE International Conference - Volume 02},
pages = {1309–1312},
numpages = {4},
series = {ICASSP '01}
}

@article{10.1016/j.jcp.2007.01.021,
author = {Waterson, N. P. and Deconinck, H.},
title = {Design principles for bounded higher-order convection schemes - a unified approach},
year = {2007},
issue_date = {May, 2007},
publisher = {Academic Press Professional, Inc.},
address = {USA},
volume = {224},
number = {1},
issn = {0021-9991},
url = {https://doi.org/10.1016/j.jcp.2007.01.021},
doi = {10.1016/j.jcp.2007.01.021},
abstract = {The design of bounded, higher-order convection schemes is considered with a view to selecting those discretizations giving good resolution of sharp gradients, while at the same time providing competitive accuracy and convergence behaviour when applied to smooth, recirculating flows. The present work contains a detailed classification and analysis and extensive tables of most non-linear scalar convection schemes so far proposed within the cell-centred, finite-volume framework. The analysis includes a review and comparison of the two most frequently-used non-linear approaches, flux limiters (FL) and normalized variables (NV), and the three major boundedness criteria typically employed: total-variation diminishing (TVD), positivity and the convection-boundedness criterion (CBC). All NV schemes considered are converted to FL form to allow direct comparison and classification of a wide range of schemes. Several specific design principles for positive non-linear schemes are considered and it is shown how these can be applied to understand the relative performance of different approaches. Finally the performance of many existing schemes is compared and ranked on the basis of two scalar convection test cases, one smooth and one discontinuous, which demonstrates the wide variation in both accuracy and convergence behaviour of the various schemes and the benefits of the design principles considered.},
journal = {J. Comput. Phys.},
month = may,
pages = {182–207},
numpages = {26},
keywords = {TVD, Normalized variables, Non-linear, Higher-order, Flux limiters, Discretization, Convection, Bounded}
}

@inproceedings{10.5555/1779459.1779474,
author = {Yao, Dan and Lu, Hong and Xue, Xiangyang and Zhou, Zhongyi},
title = {Local dual closed loop model based Bayesian face tracking},
year = {2007},
isbn = {3540772545},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a new Bayesian face tracking method under particle filter framework. First, two adaptive feature models are proposed to extract face features from image sequences. Then the robustness of face tracking is reinforced via building a local dual closed loop model (LDCLM). Meanwhile, trajectory analysis, which helps to avoid unnecessary restarting of detection module, is introduced to keep tracked faces' identity as consistent as possible. Experimental results demonstrate the efficacy of our method.},
booktitle = {Proceedings of the Multimedia 8th Pacific Rim Conference on Advances in Multimedia Information Processing},
pages = {98–107},
numpages = {10},
location = {Hong Kong, China},
series = {PCM'07}
}

@article{10.1016/j.patrec.2008.12.005,
author = {Hu, Zhilan and Wang, Guijin and Lin, Xinggang and Yan, Hong},
title = {Recovery of upper body poses in static images based on joints detection},
year = {2009},
issue_date = {April, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {30},
number = {5},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2008.12.005},
doi = {10.1016/j.patrec.2008.12.005},
abstract = {Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {503–512},
numpages = {10},
keywords = {Torso detection, Pose estimation, Markov chain Monte Carlo, Gaussian mixture model}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@article{10.1007/s11042-020-09907-1,
author = {Zhong, Yuanhong and Zhang, Jing and Zhou, Zhaokun and Cheng, Xinyu and Huang, Guan and Li, Qiang},
title = {Recovery of image and video based on compressive sensing via tensor approximation and Spatio-temporal correlation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09907-1},
doi = {10.1007/s11042-020-09907-1},
abstract = {In recent years, block-based compressive sensing (BCS) has been extensively studied because it can reduce computational complexity and data storage by dividing the image into smaller patches, but the performance of the reconstruction algorithm is not satisfactory. In this paper, a new reconstruction model for image and video is proposed. The model makes full use of spatio-temporal correlation and utilizes low-rank tensor approximation to improve the quality of the reconstructed image and video. For image recovery, the proposed model obtains a low-rank approximation of a tensor formed by non-local similar patches, and improves the reconstruction quality from a spatial perspective by combining non-local similarity and low-rank property. For video recovery, the reconstruction process is divided into two phases. In the first phase, each frame of the video sequence is regarded as an independent image to be reconstructed by taking advantage of spatial property. The second phase performs tensor approximation through searching similar patches within frames near the target frame, to achieve reconstruction by putting the spatio-temporal correlation into full play. The resulting model is solved by an efficient Alternating Direction Method of Multipliers (ADMM) algorithm. A series of experiments show that the quality of the proposed model is comparable to the current state-of-the-art recovery methods.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {7433–7450},
numpages = {18},
keywords = {Spatio-temporal correlation, High order singular value decomposition (HOSVD), Low-rank tensor approximation, Image and video recovery, Block-based compressive sensing}
}

@article{10.1016/j.eswa.2019.06.016,
author = {Huang, Hai-Hui and Liang, Yong},
title = {An integrative analysis system of gene expression using self-paced learning and SCAD-Net},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.06.016},
doi = {10.1016/j.eswa.2019.06.016},
journal = {Expert Syst. Appl.},
month = nov,
pages = {102–112},
numpages = {11},
keywords = {SPS-NL, SPS-Net, SCAD-Net, NSCLC, Gene expression, Variable selection, Regularization, Meta-analysis, Integrative analysis system}
}

@inproceedings{10.1007/978-3-030-58604-1_7,
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
title = {Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_7},
doi = {10.1007/978-3-030-58604-1_7},
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at 
.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {104–120},
numpages = {17},
keywords = {Embodied agents, Vision-and-Language Navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3045118.3045249,
author = {Liu, Chunchen and Feng, Lu and Fujimaki, Ryohei and Muraoka, Yusuke},
title = {Scalable model selection for large-scale factorial relational models},
year = {2015},
publisher = {JMLR.org},
abstract = {With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1227–1235},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1155/2020/6681391,
author = {Li, Chenpu and Xing, Qianjian and Ma, Zhenguo and Zang, Ke and Tolba, Amr},
title = {MFCFSiam: A Correlation-Filter-Guided Siamese Network with Multifeature for Visual Tracking},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/6681391},
doi = {10.1155/2020/6681391},
abstract = {With the development of deep learning, trackers based on convolutional neural networks (CNNs) have made significant achievements in visual tracking over the years. The fully connected Siamese network (SiamFC) is a typical representation of those trackers. SiamFC designs a two-branch architecture of a CNN and models’ visual tracking as a general similarity-learning problem. However, the feature maps it uses for visual tracking are only from the last layer of the CNN. Those features contain high-level semantic information but lack sufficiently detailed texture information. This means that the SiamFC tracker tends to drift when there are other same-category objects or when the contrast between the target and the background is very low. Focusing on addressing this problem, we design a novel tracking algorithm that combines a correlation filter tracker and the SiamFC tracker into one framework. In this framework, the correlation filter tracker can use the Histograms of Oriented Gradients (HOG) and color name (CN) features to guide the SiamFC tracker. This framework also contains an evaluation criterion which we design to evaluate the tracking result of the two trackers. If this criterion finds the SiamFC tracker fails in some cases, our framework will use the tracking result from the correlation filter tracker to correct the SiamFC. In this way, the defects of SiamFC’s high-level semantic features are remedied by the HOG and CN features. So, our algorithm provides a framework which combines two trackers together and makes them complement each other in visual tracking. And to the best of our knowledge, our algorithm is also the first one which designs an evaluation criterion using correlation filter and zero padding to evaluate the tracking result. Comprehensive experiments are conducted on the Online Tracking Benchmark (OTB), Temple Color (TC128), Benchmark for UAV Tracking (UAV-123), and Visual Object Tracking (VOT) Benchmark. The results show that our algorithm achieves quite a competitive performance when compared with the baseline tracker and several other state-of-the-art trackers.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {19}
}

@inproceedings{10.5555/3044805.3044851,
author = {Lian, Wenzhao and Rao, Vinayak and Eriksson, Brian and Carin, Lawrence},
title = {Modeling correlated arrival events with latent semi-Markov processes},
year = {2014},
publisher = {JMLR.org},
abstract = {The analysis of correlated point process data has wide applications, ranging from biomedical research to network analysis. In this work, we model such data as generated by a latent collection of continuous-time binary semi-Markov processes, corresponding to external events appearing and disappearing. A continuous-time modeling framework is more appropriate for multichannel point process data than a binning approach requiring time discretization, and we show connections between our model and recent ideas from the discrete-time literature. We describe an efficient MCMC algorithm for posterior inference, and apply our ideas to both synthetic data and a real-world biometrics application.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {I–396–I–404},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.5555/3045118.3045252,
author = {Yuan, Xin and Henao, Ricardo and Tsalik, Ephraim L. and Langley, Raymond J. and Carin, Lawrence},
title = {Non-Gaussian discriminative factor models via the max-margin rank-likelihood},
year = {2015},
publisher = {JMLR.org},
abstract = {We consider the problem of discriminative factor analysis for data that are in general nonGaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a new max-margin version of the ranklikelihood. A discriminative factor model is then developed, integrating the max-margin ranklikelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1254–1263},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.5555/3044805.3045049,
author = {Bartunov, Sergey and Vetrov, Dmitry P.},
title = {Variational inference for sequential distance dependent chinese restaurant process},
year = {2014},
publisher = {JMLR.org},
abstract = {Recently proposed distance dependent Chinese Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP) by accounting for dependencies between data points. Its posterior is intractable and so far only MCMC methods were used for inference. Because of very different nature of ddCRP no prior developments in variational methods for Bayesian nonparametrics are appliable. In this paper we propose novel variational inference for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate its efficiency comparing to Gibbs sampler. We also apply our variational approximation to CRP-equivalent seqddCRP-mixture model, where it could be considered as alternative to one based on truncated stick-breaking representation. This allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1404–II–1412},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1007/978-3-030-59605-7_1,
author = {Ji, Chunyan and Basodi, Sunitha and Xiao, Xueli and Pan, Yi},
title = {Infant Sound Classification on Multi-stage CNNs with Hybrid Features and Prior Knowledge},
year = {2020},
isbn = {978-3-030-59604-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59605-7_1},
doi = {10.1007/978-3-030-59605-7_1},
abstract = {We propose an approach of generating a hybrid feature set and using prior knowledge in a multi-stage CNNs for robust infant sound classification. The dominant and auxiliary features within the set are beneficial to enlarge the coverage as well as keeping a good resolution for modeling the diversity of variations within infant sound. The novel multi-stage CNNs method work together with prior knowledge constraints in decision making to overcome the limited data problem in infant sound classification. Prior knowledge either from rules or from statistical results provides a good guidance for searching and classification. The effectiveness of proposed method is evaluated on commonly used Dustan Baby Language Database and Baby Chillanto Database. It gives an encouraging reduction of 4.14% absolute classification error rate compared with the results from the best model using one-stage CNN. In addition, on Baby Chillanto Database, a significant absolute error reduction of 5.33% is achieved compared to one-stage CNN and it outperforms all other existing related studies.},
booktitle = {Artificial Intelligence and Mobile Services – AIMS 2020: 9th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18-20, 2020, Proceedings},
pages = {3–16},
numpages = {14},
keywords = {Hybrid features, Multi-stage CNNs, Prior knowledge},
location = {Honolulu, HI, USA}
}

@article{10.4018/IJBDCN.2019070106,
author = {Patil, Vilas K and Nagarale, P.P.},
title = {Prediction of L10 and Leq Noise Levels Due to Vehicular Traffic in Urban Area Using ANN and Adaptive Neuro-Fuzzy Interface System (ANFIS) Approach},
year = {2019},
issue_date = {Jul 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-0631},
url = {https://doi.org/10.4018/IJBDCN.2019070106},
doi = {10.4018/IJBDCN.2019070106},
abstract = {Recently in urban areas, road traffic noise is one of the primary sources of noise pollution. Variation in noise level is impacted by the synthesis of traffic and the percentage of heavy vehicles. Presentation to high noise levels may cause serious impact on the health of an individual or community residing near the roadside. Thus, predicting the vehicular traffic noise level is important. The present study aims at the formulation of regression, an artificial neural network (ANN) and an adaptive neuro-fuzzy interface system (ANFIS) model using the data of observed noise levels, traffic volume, and average speed of vehicles for the prediction of L10 and Leq. Measured noise levels are compared to the noise levels predicted by the experimental model. It is observed that the ANFIS approach is more superior when compared to output given by regression and an ANN model. Also, there exists a positive correlation between measured and predicted noise levels. The proposed ANFIS model can be utilized as a tool for traffic direction and planning of new roads in zones of similar land use pattern.},
journal = {Int. J. Bus. Data Commun. Netw.},
month = jul,
pages = {92–105},
numpages = {14},
keywords = {Vehicular Traffic Noise Prediction, Regression, Modeling, Artificial Neural Network, ANFIS}
}

@inproceedings{10.1145/3387168.3387201,
author = {Li, Zhong and Xiong, Jiulong and Ye, Xiangbin},
title = {Gait Energy Image Based on Static Region Alignment for Pedestrian Gait Recognition},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387201},
doi = {10.1145/3387168.3387201},
abstract = {The Gait Energy Image (GEI) spatially aligns, accumulates, and averages all the frames of a gait cycle, so there is a very high requirement for the registration of moving targets. Accurate registration of moving targets is important for the synthesis of Gait Energy Image (GEI).In this paper, we propose a new Gait Energy Image to improve the registration effect: Gait Energy Image based on static region alignment (SRA-GEI). Firstly, we select the minimum circumscribed rectangle containing the moving human body from the gait sequence. Secondly, we scale the minimum circumscribed rectangle to the specified height and calculate the gait cycle by analyzing the distance between the two feet. Finally, we propose a new registration method to generate Gait Energy Image by calculating and aligning the centroid of the static region of the gait image. This paper explores the performance of SRA-GEI with KNN based on the CASIA Dataset B. The experimental results have shown that the proposed method achieves better recognition rate compared with GEI which aligned by overall centroid.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {49},
numpages = {6},
keywords = {Static Region Alignment, KNN, Gait Recognition, GEI},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@article{10.1016/j.neucom.2015.11.101,
author = {Gu, Ke and Zhai, Guangtao and Lin, Weisi and Yang, Xiaokang and Zhang, Wenjun},
title = {Learning a blind quality evaluation engine of screen content images},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {196},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.11.101},
doi = {10.1016/j.neucom.2015.11.101},
abstract = {We in this paper investigate how to blindly predict the visual quality of a screen content image (SCI). With the popularity of multi-client and remote-controlling systems, SCIs and the relevant applications have been a hot research topic. In general, SCIs contain texts or graphics in cartoons, ebooks or captures of computer screens. As for blind quality assessment (QA) of natural scene images (NSIs), it has been well established since NSIs possess certain statistical properties. SCIs however do not have reliable statistic models so far and thus the associated blind QA task is hard to be addressed. Aiming at solving this problem, we first extract 13 perceptual-inspired features with the free energy based brain theory and structural degradation model. In order to avoid the overfitting and guarantee the independence of training and testing samples, we then collect 100,000 images and use their objective quality scores computed via a high-accuracy full-reference QA method for SCIs as labels, before learning a new blind quality measure from aforementioned 13 features to the objective quality score. Experimental results performed on a large-scale screen image quality assessment database (SIQAD) demonstrate that the proposed blind quality metric has a good correlation with human perception of quality, even superior to state-of-the-art full-, reduced- and no-reference QA algorithms.},
journal = {Neurocomput.},
month = jul,
pages = {140–149},
numpages = {10},
keywords = {Statistical model, Screen content images (SCIs), Machine learning, Image quality assessment (IQA), Blind/no-reference (NR)}
}

@inproceedings{10.1145/3412841.3442106,
author = {Liu, Tianen and Khuri, Natalia},
title = {Classification of drug prescribing information using long short-term memory networks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442106},
doi = {10.1145/3412841.3442106},
abstract = {Information about drug's safety and efficacy is publicly available to healthcare providers and consumers in the United States. Yet, it remains challenging to find this information for special populations of patients. These populations include pregnant, lactating, nursing women, elderly, and pediatric patients. Motivated by the unmet need for the accurate and efficient extraction of information, we trained a multi-class Long Short-Term Memory classifier with over 90,000 semi-structured labeled texts. The classifier achieved excellent performance when tested on an unseen dataset of 20,000 texts, reaching between 95% to 99% accuracy for the five classes. The classifier significantly outperformed the baseline model trained using Na\"{\i}ve Bayes algorithm, especially in the classification of texts containing information relevant to nursing mothers.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1086–1089},
numpages = {4},
keywords = {biomedical text classification, drug labels, long short-term memory network, na\"{\i}ve bayes classifier},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1016/j.eswa.2008.08.062,
author = {Suh, Jong Hwan and Park, Sang Chan},
title = {Service-oriented Technology Roadmap (SoTRM) using patent map for R&amp;D strategy of service industry},
year = {2009},
issue_date = {April, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.08.062},
doi = {10.1016/j.eswa.2008.08.062},
abstract = {As a consequence of the service economy, R&amp;D of the service industry has become more essential nowadays. Therefore, technology roadmaps are required for selection and concentration of services and related technologies. However, now there are problems and challenging issues as follows. First, there is no objective and systematic method or analysis tool to evaluate emerging technologies for services. Second, current technology roadmaps do not provide technology's priority oriented to the service side. Therefore, we propose a patent map and a Service-oriented Technology Roadmap using the patent map, i.e. SoTRM. Our patent map is a three-dimensional visualization method and analysis tool based on keywords, which contributes to evaluating emerging technologies for services. It does not only overcome the subjectivity of experts, but it also discovers technologies missed out by experts initially. And SoTRM is a technology roadmap customized for the service industry. Based on four layers of patents, keywords, technologies, and services, the layer of service-oriented technologies provides the order of technologies in a service-oriented aspect. It also gives guidelines to assign roles in R&amp;D to public and private sectors. As a result, we provide an objective and systematic framework required to form a technology roadmap oriented to services for R&amp;D strategy of the service industry. Eventually, it helps decision makers from public and private sectors to select and concentrate on the first things among services and the related technologies in R&amp;D of the service industry, and thereby to find the direction of distributing investment funds into technologies for services.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {6754–6772},
numpages = {19},
keywords = {Technology roadmap, Service-oriented, Portfolio matrix, Patent analysis, Clustering}
}

@inproceedings{10.1109/CEC-EEE.2006.2,
author = {Fantinato, Marcelo and de Toledo, Maria Beatriz F. and Gimenes, Itana Maria de S.},
title = {A Feature-based Approach to Electronic Contracts},
year = {2006},
isbn = {0769525113},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CEC-EEE.2006.2},
doi = {10.1109/CEC-EEE.2006.2},
abstract = {E-contracts are used to describe the supply and the consumption details of e-services within a business process. The establishment of e-contracts in a given application domain usually involves a set of welldefined common and variation points. This paper proposes a feature-based approach in order to decrease the complexity in e-contract establishment and to foster inter-organizational cooperation. Feature modeling is a technique that has been widely used for capturing and managing commonalities and variabilities of product families in the software product line context. The feasibility of the approach is shown by a case study carried out within the telecom context and based on experimental software engineering concepts.},
booktitle = {Proceedings of the The 8th IEEE International Conference on E-Commerce Technology and The 3rd IEEE International Conference on Enterprise Computing, E-Commerce, and E-Services},
pages = {34},
series = {CEC-EEE '06}
}

@inproceedings{10.1145/2970276.2970327,
author = {Moonen, Leon and Di Alesio, Stefano and Binkley, David and Rolfsnes, Thomas},
title = {Practical guidelines for change recommendation using association rule mining},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970327},
doi = {10.1145/2970276.2970327},
abstract = {Association rule mining is an unsupervised learning technique that infers relationships among items in a data set. This technique has been successfully used to analyze a system's change history and uncover evolutionary coupling between system artifacts. Evolutionary coupling can, in turn, be used to recommend artifacts that are potentially affected by a given set of changes to the system. In general, the quality of such recommendations is affected by (1) the values selected for various parameters of the mining algorithm, (2) characteristics of the set of changes used to derive a recommendation, and (3) characteristics of the system's change history for which recommendations are generated.In this paper, we empirically investigate the extent to which certain choices for these factors affect change recommendation. Specifically, we conduct a series of systematic experiments on the change histories of two large industrial systems and eight large open source systems, in which we control the size of the change set for which to derive a recommendation, the measure used to assess the strength of the evolutionary coupling, and the maximum size of historical changes taken into account when inferring these couplings. We use the results from our study to derive a number of practical guidelines for applying association rule mining for change recommendation.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {732–743},
numpages = {12},
keywords = {parameter tuning, change recommendations, change impact analysis, association rule mining, Evolutionary coupling},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/2838739.2838753,
author = {Arshad, Syed Z. and Zhou, Jianlong and Bridon, Constant and Chen, Fang and Wang, Yang},
title = {Investigating User Confidence for Uncertainty Presentation in Predictive Decision Making},
year = {2015},
isbn = {9781450336734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838739.2838753},
doi = {10.1145/2838739.2838753},
abstract = {Machine Learning (ML) based decision support systems are often like a black box to non-expert users. Here user's confidence becomes critical for effective decision making and maintaining trust in the system. We find that user confidence varies significantly depending on supplementary material presented on screen. We investigate change in user confidence (in the context of ML based decision making) by varying level of uncertainty presented (in an online water-pipe failure prediction case study) and find that all 26 subjects rated higher uncertainty task to be most difficult and had lowest user confidence in predictive decisions of the same. This agrees with our expectation that increased uncertainty would reduce user confidence in predictive decision making. However, ML-researchers subgroup reported being most confident when uncertainty with known probability was presented, whereas other subgroups (viz. general staff and non-ML researchers) appeared most confident when uncertainty was not at all presented. This is an original research to improve understanding of user's decision making confidence with respect to uncertainty presented in machine learning context.},
booktitle = {Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction},
pages = {352–360},
numpages = {9},
keywords = {User Uncertainty, User Confidence, Uncertainty Presentation, Model Uncertainty, Decision Making},
location = {Parkville, VIC, Australia},
series = {OzCHI '15}
}

@inproceedings{10.5555/3540261.3541282,
author = {Zhang, Jiwen and Wei, Zhongyu and Fan, Jianqing and Peng, Jiajie},
title = {Curriculum learning for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1021},
numpages = {12},
series = {NIPS '21}
}

@article{10.1007/s11042-020-09252-3,
author = {Wang, Jihua and Yan, Wei and Huang, Chao},
title = {Surface shape-based clustering for B-rep models},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {35–36},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09252-3},
doi = {10.1007/s11042-020-09252-3},
abstract = {Matching, clustering, and retrieving 3D CAD models of mechanical components based on their shape are useful for many CAD/CAM applications such as design reuse, variant process planning and group technology. Surfaces are the prominent elements of the B-Rep (Boundary Representation) CAD model, but the current methods of similarity assessment are not centered on the surfaces and lack an accurate description of their geometric features. In order to solve the problem of retrieval and clustering of B-Rep models more efficiently, the concept of “most crucial surface” is proposed and its corresponding characteristics are studied in detail. The contribution of our approach is that surfaces are the major shape determinants of the B-Rep model, and the distribution of Carosati curvatures is the optimum shape features of surfaces. First, the surface elements are extracted from the STEP (Standard for Exchange of Product Model) files of the B-rep models, and the distribution of minimum, Gauss and Carosati surface curvatures are converted into the shape feature space by the wavelet transform, the Fourier transform, and the grouping calculation. Thus we characterize the B-Rep model as a histogram with surfaces as bins, and then compare and cluster the B-Rep models by the bipartite matching algorithm or the earth mover’s distance. The surface-based methods are evaluated with the four effectiveness indices in the clustering experiment of the NDR (National Design Reservoir) data, and the results indicated that the grouping method for the surface Carosati curvatures has a highly competent matching and clustering performance.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {25747–25761},
numpages = {15},
keywords = {Earth mover’s distance, Bipartite graph matching, STEP(standard for exchange of product model), 3D model clustering, Surface curvature, B-rep model, Surface shape}
}

@inproceedings{10.5555/3157382.3157479,
author = {Niepert, Mathias},
title = {Discriminative Gaifman models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present discriminative Gaifman models, a novel family of relational machine learning models. Gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (KBs). Considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of over-fitting, and facilitates weight sharing. Gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world KBs. We present the core ideas of Gaifman models and apply them to large-scale relational learning problems. We also discuss the ways in which Gaifman models relate to some existing relational machine learning approaches.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3413–3421},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1007/978-3-030-71827-5_11,
author = {Estienne, Th\'{e}o and Vakalopoulou, Maria and Battistella, Enzo and Carr\'{e}, Alexandre and Henry, Th\'{e}ophraste and Lerousseau, Marvin and Robert, Charlotte and Paragios, Nikos and Deutsch, Eric},
title = {Deep Learning Based Registration Using&nbsp;Spatial Gradients and Noisy Segmentation Labels},
year = {2020},
isbn = {978-3-030-71826-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71827-5_11},
doi = {10.1007/978-3-030-71827-5_11},
abstract = {Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of 0.64 for task 3 and 0.85 for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at  and .},
booktitle = {Segmentation, Classification, and Registration of Multi-Modality Medical Imaging Data: MICCAI 2020 Challenges, ABCs 2020, L2R 2020, TN-SCUI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {87–93},
numpages = {7},
location = {Lima, Peru}
}

@inproceedings{10.1145/3474124.3474171,
author = {Gupta, Manas and Chandra, Satish},
title = {Speech Emotion Recognition Using MFCC and Wide Residual Network},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474171},
doi = {10.1145/3474124.3474171},
abstract = {Emotion recognition from speech has been a topic of research from many years due to its importance in human-computer interaction. While a lot of work has been done upon recognizing emotions through facial expressions, recognition of emotions through speech is still a challenging task in Machine Learning due to the obscure knowledge about the effectiveness of different speech features. In this work, Mel-frequency cepstral coefficients (MFCCs) has been used as a feature extractor for speech files. Further, classification of speech signals has been done using Convolution Neural Network (CNN) in the form of Wide Residual Network (WRN) followed by a Dense Neural Network (DNN). To train and test this approach we used Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Toronto Emotional Speech Set (TESS) databases together. Results show that the proposed approach is gives an accuracy of 90.09% in recognizing emotions from speech into 8 categories.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {320–327},
numpages = {8},
keywords = {Wide Residual Network, Neural Networks, MFCC},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1007/978-3-030-58607-2_18,
author = {Qi, Yuankai and Pan, Zizheng and Zhang, Shengping and van den Hengel, Anton and Wu, Qi},
title = {Object-and-Action Aware Model for Visual Language Navigation},
year = {2020},
isbn = {978-3-030-58606-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58607-2_18},
doi = {10.1007/978-3-030-58607-2_18},
abstract = {Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of visible environments. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., ‘go straight’, ‘turn left’) which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a 50% SPL score on the R2R dataset and a 40% CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X},
pages = {303–317},
numpages = {15},
keywords = {Reward shaping, Modular network, Vision-and-Language Navigation},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.jss.2017.01.031,
author = {Lucas, Edson M. and Oliveira, Toacy C. and Farias, Kleinner and Alencar, Paulo S.C.},
title = {CollabRDL},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.031},
doi = {10.1016/j.jss.2017.01.031},
abstract = {Extends the Reuse Description Language (RDL) to address collaborative reuse processesIncludes three new commands in RDL, including ROLE, PARALLEL and DOPARALLELCollabRDL can represent parallelism, synchronization, multiple-choice and roleCollabRDL is capable of representing critical workflow patterns Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.},
journal = {J. Syst. Softw.},
month = sep,
pages = {505–527},
numpages = {23},
keywords = {Software reuse, Reuse process, Language, Framework, Collaboration}
}

@inproceedings{10.5555/3044805.3045093,
author = {Rai, Piyush and Wang, Yingjian and Guo, Shengbo and Chen, Gary and Dunson, David and Carin, Lawrence},
title = {Scalable Bayesian low-rank decomposition of incomplete multiway tensors},
year = {2014},
publisher = {JMLR.org},
abstract = {We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the P\'{o}lya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1800–II–1808},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1007/978-3-030-30446-1_7,
author = {Bittner, Paul Maximilian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SAT Encodings of the At-Most-k Constraint: A Case Study on Configuring University Courses},
year = {2019},
isbn = {978-3-030-30445-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30446-1_7},
doi = {10.1007/978-3-030-30446-1_7},
abstract = {At universities, some fields of study offer multiple branches to graduate in. These branches are defined by mandatory and optional courses. Configuring a branch manually can be a difficult task, especially if some courses have already been attended. Hence, a tool providing guidance on choosing courses is desired. Feature models enable modelling such behaviour, as they are designed to define valid configurations from a set of features. Unfortunately, the branches contain constraints instructing to choose at least k out of n courses in essence. Encoding some of these constraints na\"{\i}vely in propositional calculus is practically infeasible. We develop a new encoding by combining existing approaches. Furthermore, we report on our experience of encoding the constraints of the computer science master at TU Braunschweig and discuss the impact for research on configurability.},
booktitle = {Software Engineering and Formal Methods: 17th International Conference, SEFM 2019, Oslo, Norway, September 18–20, 2019, Proceedings},
pages = {127–144},
numpages = {18},
location = {Oslo, Norway}
}

@inproceedings{10.5555/645654.665656,
author = {Pal, Sankar K.},
title = {Soft Computing Pattern Recognition: Principles, Integrations, and Data Mining},
year = {2001},
isbn = {3540430709},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Relevance of fuzzy logic, artificial neural networks, genetic algorithms and rough sets to pattern recognition and image processing problems is described through examples. Different integrations of these soft computing tools are illustrated. Evolutionary rough fuzzy network which is based on modular principle is explained, as an example of integrating all the four tools for efficient classification and rule generation, with its various characterstics. Significance of soft computing approach in data mining and knowledge discovery is finally discussed along with the scope of future research.},
booktitle = {Proceedings of the Joint JSAI 2001 Workshop on New Frontiers in Artificial Intelligence},
pages = {261–271},
numpages = {11}
}

@inproceedings{10.5555/3042573.3042627,
author = {Palla, Konstantina and Knowles, David A. and Ghahramani, Zoubin},
title = {An infinite latent attribute model for network data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a "flat" clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {395–402},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{10.1145/2746237,
author = {Firouzi, Farshad and Ye, Fangming and Chakrabarty, Krishnendu and Tahoori, Mehdi B.},
title = {Aging- and Variation-Aware Delay Monitoring Using Representative Critical Path Selection},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/2746237},
doi = {10.1145/2746237},
abstract = {Process together with runtime variations in temperature and voltage, as well as transistor aging, degrade path delay and may eventually induce circuit failure due to timing variations. Therefore, in-field tracking of path delays is essential, and to respond to this need, several delay sensor designs have been proposed in the literature. However, due to the significant overhead of these sensors and the large number of critical paths in today's IC, it is infeasible to monitor the delay of every critical path in silicon. We present an aging- and variationaware representative path selection technique based on machine learning that allows to measure the delay of a small set of paths and infer the delay of a larger pool of paths that are likely to fail due to delay variations. Simulation results for benchmark circuits highlight the accuracy of the proposed approach for predicting critical-path delay based on the selected representative paths.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {39},
numpages = {23},
keywords = {variations, transistor aging, performance, monitoring, Reliability}
}

@inproceedings{10.5555/2034117.2034146,
author = {Menon, Aditya Krishna and Elkan, Charles},
title = {Link prediction via matrix factorization},
year = {2011},
isbn = {9783642237829},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose to solve the link prediction problem in graphs using a supervised matrix factorization approach. The model learns latent features from the topological structure of a (possibly directed) graph, and is shown to make better predictions than popular unsupervised scores. We show how these latent features may be combined with optional explicit features for nodes or edges, which yields better performance than using either type of feature exclusively. Finally, we propose a novel approach to address the class imbalance problem which is common in link prediction by directly optimizing for a ranking loss. Our model is optimized with stochastic gradient descent and scales to large graphs. Results on several datasets show the efficacy of our approach.},
booktitle = {Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II},
pages = {437–452},
numpages = {16},
keywords = {side information, ranking loss, matrix factorization, link prediction},
location = {Athens, Greece},
series = {ECML PKDD'11}
}

@inproceedings{10.5555/3042817.3043046,
author = {Xu, Minjie and Zhu, Jun and Zhang, Bo},
title = {Fast max-margin matrix factorization with data augmentation},
year = {2013},
publisher = {JMLR.org},
abstract = {Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–978–III–986},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.5555/3042817.3042859,
author = {Wulsin, Drausin F. and Fox, Emily B. and Litt, Brian},
title = {Parsing epileptic events using a Markov switching process for correlated time series},
year = {2013},
publisher = {JMLR.org},
abstract = {Patients with epilepsy can manifest short, sub-clinical epileptic "bursts" in addition to full-blown clinical seizures. We believe the relationship between these two classes of events--something not previously studied quantitatively--could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes. A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients. We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes. We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data. We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–356–I–364},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1016/S0165-1684(98)00044-9,
author = {Marques, Jorge S.},
title = {A link between image-based and feature-based active contours},
year = {1998},
issue_date = {June 1998},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {67},
number = {3},
issn = {0165-1684},
url = {https://doi.org/10.1016/S0165-1684(98)00044-9},
doi = {10.1016/S0165-1684(98)00044-9},
journal = {Signal Process.},
month = jun,
pages = {271–278},
numpages = {8},
keywords = {shape, neural networks, feature models, deformable models, active contours, Kohonen maps}
}

@article{10.1145/3287058,
author = {Maruri, H\'{e}ctor A. Cordourier and Lopez-Meyer, Paulo and Huang, Jonathan and Beltman, Willem Marco and Nachman, Lama and Lu, Hong},
title = {V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287058},
doi = {10.1145/3287058},
abstract = {Smart glasses are often used in public environments or industrial scenarios that are relatively noisy. Background noise and sound from competing speakers deteriorate voice communication or performance of automatic speech recognition (ASR). Typically, signal processing techniques are used to reduce noise and enhance voice quality, but they have limitations in performance, hardware and/or computing resources. Voice capturing techniques using bone conducting on the head have been proposed in some experimental and commercial devices, with good robustness against environmental noise, but limited by signal distortions inherent to the capturing method. We present V-Speech, a novel sensing and signal processing solution that enables speech recognition and human-to-human communication in very noisy environments. It captures the voice signal with a vibration sensor located in the nasal pads of smart glasses and performs a transformation to the sensor signal in order to mimic that of a regular microphone in low noise conditions. The signal transformation is key, as it eliminates the "nasal distortion" that is introduced for nasal phonemes in the speech induced vibrations of the nasal bone. The output of V-Speech has low noise, sounds natural, and can be used in voice communication or as input to an off-the-shelf ASR service. We evaluated V-Speech in noise-free and noisy conditions with 30 volunteer speakers uttering 145 phrases and validated its performance on ASR engines and with assessments of voice quality using the Perceptual Evaluation of Speech Quality (PESQ) metric. The results show in extreme noise conditions a mean improvement of 50% for Word Error Rate (WER), and 1.0 on a scale of 5.0 for PESQ. In addition, real life recordings were made under various representative noise conditions, some with sound pressure levels of 93 dBA, which require hearing protection. Subjective listening tests were conducted according to a modified ITU P.835 approach to determine intelligibility, naturalness and overall quality. Under these extreme conditions, where V-Speech achieved 30 dB SNR, subjective results show the speech is intelligible, and the naturalness of the speech is rated as fair to good. This enables clear voice communication in challenging work environments, for example in places with industrial, factory, mining and construction noise. With our proposed smart switching technique between a regular microphone signal and V-Speech, the optimal quality can be maintained from low to high noise conditions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {180},
numpages = {23},
keywords = {Voice capturing, Vibration sensing, Smart glasses, Head worn devices, Accelerometer}
}

@article{10.1016/j.artmed.2006.06.003,
author = {Botros, Andrew and van Dijk, Bas and Killian, Matthijs},
title = {AutoNRTTM: An automated system that measures ECAP thresholds with the Nucleus® FreedomTM cochlear implant via machine intelligence},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2006.06.003},
doi = {10.1016/j.artmed.2006.06.003},
abstract = {Objective: AutoNRT(TM) is an automated system that measures electrically evoked compound action potential (ECAP) thresholds from the auditory nerve with the Nucleus^(R) Freedom(TM) cochlear implant. ECAP thresholds along the electrode array are useful in objectively fitting cochlear implant systems for individual use. This paper provides the first detailed description of the AutoNRT algorithm and its expert systems, and reports the clinical success of AutoNRT to date. Methods: AutoNRT determines thresholds by visual detection, using two decision tree expert systems that automatically recognise ECAPs. The expert systems are guided by a dataset of 5393 neural response measurements. The algorithm approaches threshold from lower stimulus levels, ensuring recipient safety during postoperative measurements. Intraoperative measurements use the same algorithm but proceed faster by beginning at stimulus levels much closer to threshold. When searching for ECAPs, AutoNRT uses a highly specific expert system (specificity of 99% during training, 96% during testing; sensitivity of 91% during training, 89% during testing). Once ECAPs are established, AutoNRT uses an unbiased expert system to determine an accurate threshold. Throughout the execution of the algorithm, recording parameters (such as implant amplifier gain) are automatically optimised when needed. Results: In a study that included 29 intraoperative and 29 postoperative subjects (a total of 418 electrodes), AutoNRT determined a threshold in 93% of cases where a human expert also determined a threshold. When compared to the median threshold of multiple human observers on 77 randomly selected electrodes, AutoNRT performed as accurately as the 'average' clinician. Conclusions: AutoNRT has demonstrated a high success rate and a level of performance that is comparable with human experts. It has been used in many clinics worldwide throughout the clinical trial and commercial launch of Nucleus Custom Sound(TM) Suite, significantly streamlining the clinical procedures associated with cochlear implant use.},
journal = {Artif. Intell. Med.},
month = may,
pages = {15–28},
numpages = {14},
keywords = {Threshold estimation, Pattern recognition, Neural response telemetry, Machine learning, Electrically evoked compound action potential, Decision trees, Cochlear implants, Automated systems}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Time shift, Spatio-temporal pyramids, Temporal sequence matching, Video matching, Gait recognition, Action recognition, Person re-identification}
}

@inproceedings{10.5555/3020419.3020482,
author = {Weng, Paul},
title = {Axiomatic foundations for a class of generalized expected utility: algebraic expected utility},
year = {2006},
isbn = {0974903922},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von Neumann-Morgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty.},
booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Cambridge, MA, USA},
series = {UAI'06}
}

@article{10.1016/j.neunet.2015.09.005,
author = {Beyeler, Michael and Oros, Nicolas and Dutt, Nikil and Krichmar, Jeffrey L.},
title = {A GPU-accelerated cortical neural network model for visually guided robot navigation},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {72},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.09.005},
doi = {10.1016/j.neunet.2015.09.005},
abstract = {Humans and other terrestrial animals use vision to traverse novel cluttered environments with apparent ease. On one hand, although much is known about the behavioral dynamics of steering in humans, it remains unclear how relevant perceptual variables might be represented in the brain. On the other hand, although a wealth of data exists about the neural circuitry that is concerned with the perception of self-motion variables such as the current direction of travel, little research has been devoted to investigating how this neural circuitry may relate to active steering control. Here we present a cortical neural network model for visually guided navigation that has been embodied on a physical robot exploring a real-world environment. The model includes a rate based motion energy model for area V1, and a spiking neural network model for cortical area MT. The model generates a cortical representation of optic flow, determines the position of objects based on motion discontinuities, and combines these signals with the representation of a goal location to produce motor commands that successfully steer the robot around obstacles toward the goal. The model produces robot trajectories that closely match human behavioral data. This study demonstrates how neural signals in a model of cortical area MT might provide sufficient motion information to steer a physical robot on human-like paths around obstacles in a real-world environment, and exemplifies the importance of embodiment, as behavior is deeply coupled not only with the underlying model of brain function, but also with the anatomical constraints of the physical body it controls.},
journal = {Neural Netw.},
month = dec,
pages = {75–87},
numpages = {13},
keywords = {Spiking neural network, Robot navigation, Obstacle avoidance, Motion energy, MT, GPU}
}

@article{10.1145/3385398,
author = {Shaikhha, Amir and Elseidy, Mohammed and Mihaila, Stephan and Espino, Daniel and Koch, Christoph},
title = {Synthesis of Incremental Linear Algebra Programs},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3385398},
doi = {10.1145/3385398},
abstract = {This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations.},
journal = {ACM Trans. Database Syst.},
month = aug,
articleno = {12},
numpages = {44},
keywords = {materialized views, incremental linear algebra, domain-specific languages, compilation, abstract interpretation, Incremental view maintenance (IVM)}
}

@inproceedings{10.5555/3042817.3042850,
author = {Heaukulani, Creighton and Ghahramani, Zoubin},
title = {Dynamic probabilistic models for latent feature propagation in social networks},
year = {2013},
publisher = {JMLR.org},
abstract = {Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected. In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks. We demonstrate our model's capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–275–I–283},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1016/j.procs.2019.08.201,
author = {Lysenko, Anton and Shikov, Egor and Bochenina, Klavdiya},
title = {Temporal point processes for purchase categories forecasting},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {156},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.08.201},
doi = {10.1016/j.procs.2019.08.201},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {255–263},
numpages = {9},
keywords = {transactional data, purchase forecasting, temporal point processes, events forecasting}
}

@article{10.1016/j.jksuci.2017.02.002,
author = {Juneja, Kapil},
title = {Multiple feature descriptors based model for individual identification in group photos},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {2},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2017.02.002},
doi = {10.1016/j.jksuci.2017.02.002},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = apr,
pages = {185–207},
numpages = {23},
keywords = {Group photos, Situational, LBP, Structural, Partial view, Full view}
}

@inproceedings{10.1145/2430502.2430531,
author = {Wulf-Hadash, Ora and Reinhartz-Berger, Iris},
title = {Cross product line analysis},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430531},
doi = {10.1145/2430502.2430531},
abstract = {Due to increase in market competition and merger and acquisition of companies, different software product lines (SPLs) may exist under the same roof. These SPLs may be developed applying different domain analysis processes, but are likely not disjoint. Cross product line analysis aims to examine the common and variable aspects of different SPLs for improving maintenance and future development of related SPLs. Currently different SPL artifacts, or more accurately feature models, are compared, matched, and merged for supporting scalability, increasing modularity and reuse, synchronizing feature model versions, and modeling multiple SPLs for software supply chains. However, in all these cases the focus is on creating valid merged models from the input feature models. Furthermore, the terminology used in all the input feature models is assumed to be the same, namely similar features are named the same. As a result these methods cannot be simply applied to feature models that represent different SPLs. In this work we offer adapting similarity metrics and text clustering techniques in order to enable cross product line analysis. This way analysis of feature models that use different terminologies in the same domain can be done in order to improve the management of the involved SPLs. Preliminary results reveal that the suggested method helps systematically analyze the commonality and variability between related SPLs, potentially suggesting improvements to existing SPLs and to the maintenance of sets of SPLs.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {8},
keywords = {feature similarity, feature diagram merging, feature diagram matching, feature clustering, empirical evaluation},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1007/11527886_77,
author = {Wang, Yang},
title = {Constraint-Sensitive privacy management for personalized web-based systems},
year = {2005},
isbn = {3540278850},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527886_77},
doi = {10.1007/11527886_77},
abstract = {This research aims at reconciling web personalization with privacy constraints imposed by legal restrictions and by users' privacy preferences. We propose a software product line architecture approach, where our privacyenabling user modeling architecture can dynamically select personalization methods that satisfy current privacy constraints to provide personalization services. A feasibility study is being carried out with the support of an existing user modeling server and a software architecture based development environment.},
booktitle = {Proceedings of the 10th International Conference on User Modeling},
pages = {524–526},
numpages = {3},
location = {Edinburgh, UK},
series = {UM'05}
}

@article{10.1007/s11257-011-9114-8,
author = {Wang, Yang and Kobsa, Alfred},
title = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-011-9114-8},
doi = {10.1007/s11257-011-9114-8},
abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data.},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {41–82},
numpages = {42},
keywords = {User modeling, User experiment, Product line architecture, Privacy preferences, Privacy laws, Performance evaluation, Disclosure behavior, Compliance}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {question quality, latent semantics, classification, association rules, Community-based question answering}
}

@article{10.1016/j.patrec.2019.03.021,
author = {Ding, Songtao and Qu, Shiru and Xi, Yuling and Sangaiah, Arun Kumar and Wan, Shaohua},
title = {Image caption generation with high-level image features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {123},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.03.021},
doi = {10.1016/j.patrec.2019.03.021},
journal = {Pattern Recogn. Lett.},
month = may,
pages = {89–95},
numpages = {7},
keywords = {Faster R-CNN, Bottom-up attention mechanism, Language model, Image captioning}
}

@inproceedings{10.5555/3042573.3042696,
author = {Kim, Myunghwan and Leskovec, Jure},
title = {Latent multi-group membership graph model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop the Latent Multi-group Membership Graph (LMMG) model, a model of networks with rich node feature structure. In the LMMG model, each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure. The LMMG can be used to summarize the network structure, to predict links between the nodes, and to predict missing features of a node. We derive efficient inference and learning algorithms and evaluate the predictive performance of the LMMG on several social and document network datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {947–954},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Systems of systems, Systematic literature review, Requirements monitoring}
}

@article{10.1016/j.specom.2021.05.009,
author = {Avila, Anderson R. and O’Shaughnessy, Douglas and Falk, Tiago H.},
title = {Automatic speaker verification from affective speech using Gaussian mixture model based estimation of neutral speech characteristics},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.05.009},
doi = {10.1016/j.specom.2021.05.009},
journal = {Speech Commun.},
month = sep,
pages = {21–31},
numpages = {11},
keywords = {Transfer learning, Intra-speaker variability, Affective speech, Speaker verification}
}

@inproceedings{10.1145/2339530.2339556,
author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes},
title = {Intelligible models for classification and regression},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339556},
doi = {10.1145/2339530.2339556},
abstract = {Complex models for regression and classification have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users.We present the first large-scale empirical comparison of existing methods for learning GAMs. Our study includes existing spline and tree-based methods for shape functions and penalized least squares, gradient boosting, and backfitting for learning GAMs. We also present a new method based on tree ensembles with an adaptive number of leaves that consistently outperforms previous work. We complement our experimental results with a bias-variance analysis that explains how different shape models influence the additive model. Our experiments show that shallow bagged trees with gradient boosting distinguish itself as the best method on low- to medium-dimensional datasets.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {150–158},
numpages = {9},
keywords = {regression, intelligible models, classification},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.5555/3042573.3042738,
author = {Passos, Alexandre and Rai, Piyush and Wainer, Jacques and Daum\'{e}, Hal},
title = {Flexible modeling of latent task structures in multitask learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the "right" latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, mean-regularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and real-world datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1283–1290},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/1153922.1154326,
author = {Qi, Yanjun and Hauptmann, A. and Liu, Ting},
title = {Supervised classification for video shot segmentation},
year = {2003},
isbn = {0780379659},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we explore supervised classification methods for video shot segmentation. We transform the temporal segmentation problem into a multi-class categorization issue. This approach provides a uniform framework for using different kinds of features extracted from the video and for detecting various types of shot boundaries. The approach utilizes manual labeled training data and a simple classification structure, which eliminates arbitrary thresholds and achieves more reliable estimation than previous threshold-based methods. Contrastive experiments on 13 videos (/spl sim/4 hours) show excellent performance on the 2001 TREC video track shot classification task in terms of precision and recall.},
booktitle = {Proceedings of the 2003 International Conference on Multimedia and Expo - Volume 1},
pages = {689–692},
numpages = {4},
series = {ICME '03}
}

@inproceedings{10.1007/978-3-642-24955-6_8,
author = {Mutoh, Yoshitaka and Kashimori, Yoshiki},
title = {Neural model of auditory cortex for binding sound intensity and frequency information in bat's echolocation},
year = {2011},
isbn = {9783642249549},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-24955-6_8},
doi = {10.1007/978-3-642-24955-6_8},
abstract = {Most species of bats making echolocation use the sound pressure level (SPL) and Doppler-shifted frequency of ultrasonic echo pulse to measure the size and velocity of target. The neural circuits for detecting these target features are specialized for amplitude and frequency analysis of the second harmonic constant frequency (CF2) component of Doppler-shifted echoes. The neuronal circuits involved in detecting these features have been well established. However, it is not yet clear the neural mechanism by which these neuronal circuits detect the amplitude and frequency of echo signals. We present here neural models for detecting SPL amplitude and Doppler-shifted frequency of echo sound reflecting a target. Using the model, we show that the tuning property of frequency is changed depending on the feedback connections between cortical and subcortical neurons. We also show SPL amplitude is detected by integrating input signals emanating from ipsi and contralatreal subcortical neurons.},
booktitle = {Proceedings of the 18th International Conference on Neural Information Processing - Volume Part I},
pages = {62–69},
numpages = {8},
keywords = {neural model, frequency tuning, echolocation, auditory system, SPL amplitude},
location = {Shanghai, China},
series = {ICONIP'11}
}

@inproceedings{10.5555/3042573.3042788,
author = {Salazar, Esther and Cain, Matthew S. and Darling, Elise F. and Mitroff, Stephen F. and Carin, Lawrence},
title = {Inferring latent structure from mixed real and categorical relational data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider analysis of relational data (a matrix), in which the rows correspond to subjects (e.g., people) and the columns correspond to attributes. The elements of the matrix may be a mix of real and categorical. Each subject and attribute is characterized by a latent binary feature vector, and an inferred matrix maps each row-column pair of binary feature vectors to an observed matrix element. The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low-rank covariance matrix, and the Gaussian random variables are mapped to latent binary features via a probit link. The same type construction is applied jointly to the columns. The model infers latent, low-dimensional binary features associated with each row and each column, as well correlation structure between all rows and between all columns.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1683–1690},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.1145/3371158.3371189,
author = {Pal, Bithika and Jenamani, Mamata},
title = {Personalized Ranking in Collaborative Filtering: Exploiting l-th Order Transitive Relations of Social Ties},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371189},
doi = {10.1145/3371158.3371189},
abstract = {The use of social information in collaborative filtering is highly encouraged, as it can improve the recommendation accuracy by handling the cold start issue. The intuition of social recommendation is to reflect one's personal choice by its social neighbors. Though there exists a considerable amount of studies in this domain, no attention is paid to incorporate the transitive relationships of social ties in the ranking problem. In this paper, we exploit the lth order transitive relations of a user and extend the popular Social Bayesian Personalized Ranking (SBPR) model. The use of transitive relation creates a more granular pairwise ranking of items for a particular user and levels the user's personal choice based on the order of its social neighbors. We implement the model and conduct experiments on two real-world recommendation datasets with different values of l. We show that our model outperforms state-of-the-art pairwise ranking techniques.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {214–218},
numpages = {5},
keywords = {Social Network, Recommendation, Ranking, Personalization},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.1007/s10009-017-0466-1,
author = {Chadli, Mounir and Kim, Jin H. and Larsen, Kim G. and Legay, Axel and Naujokat, Stefan and Steffen, Bernhard and Traonouez, Louis-Marie},
title = {High-level frameworks for the specification and verification of scheduling problems},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-017-0466-1},
doi = {10.1007/s10009-017-0466-1},
abstract = {Over the years, schedulability of Cyber-Physical Systems (CPS) has mainly been performed by analytical methods. These techniques are known to be effective but limited to a few classes of scheduling policies. In a series of recent work, we have shown that schedulability analysis of CPS could be performed with a model-based approach and extensions of verification tools such as UPPAAL. One of our main contributions has been to show that such models are flexible enough to embed various types of scheduling policies, which goes beyond those in the scope of analytical tools. However, the specification of scheduling problems with model-based approaches requires a substantial modeling effort, and a deep understanding of the techniques employed in order to understand their results. In this paper we propose simplicity-driven high-level specification and verification frameworks for various scheduling problems. These frameworks consist of graphical and user-friendly languages for describing scheduling problems. The high-level specifications are then automatically translated to formal models, and results are transformed back into the comprehensible model view. To construct these frameworks we exploit a meta-modeling approach based on the tool generator Cinco . Additionally we propose in this paper two new techniques for scheduling analysis. The first performs runtime monitoring using the CUSUM algorithm to detect alarming change in the system. The second performs optimization using efficient statistical techniques. We illustrate our frameworks and techniques on two case studies.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = aug,
pages = {397–422},
numpages = {26},
keywords = {Statistical model-checking, Scheduling, Meta-modeling, High-level language, Hierarchical scheduling, Formal methods, Energy}
}

@inproceedings{10.1007/978-3-030-90370-1_17,
author = {Huang, Linan and Zhu, Quanyan},
title = {Combating Informational Denial-of-Service (IDoS) Attacks: Modeling and Mitigation of&nbsp;Attentional Human Vulnerability},
year = {2021},
isbn = {978-3-030-90369-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90370-1_17},
doi = {10.1007/978-3-030-90370-1_17},
abstract = {This work proposes a new class of proactive attacks called the Informational Denial-of-Service (IDoS) attacks that exploit the attentional human vulnerability. By generating a large volume of feints, IDoS attacks deplete the cognitive resources of human operators to prevent humans from identifying the real attacks hidden among feints. This work aims to formally define IDoS attacks, quantify their consequences, and develop human-assistive security technologies to mitigate the severity level and risks of IDoS attacks. To this end, we use the semi-Markov process to model the sequential arrivals of feints and real attacks with category labels attached in the associated alerts. The assistive technology strategically manages human attention by highlighting selective alerts periodically to prevent the distraction of other alerts. A data-driven approach is applied to evaluate human performance under different Attention Management (AM) strategies. Under a representative special case, we establish the computational equivalency between two dynamic programming representations to reduce the computation complexity and enable online learning with samples of reduced size and zero delays. A case study corroborates the effectiveness of the learning framework. The numerical results illustrate how AM strategies can alleviate the severity level and the risk of IDoS attacks. Furthermore, the results show that the minimum risk is achieved with a proper level of intentional inattention to alerts, which we refer to as the law of rational risk-reduction inattention.},
booktitle = {Decision and Game Theory for Security: 12th International Conference, GameSec 2021, Virtual Event, October 25–27, 2021, Proceedings},
pages = {314–333},
numpages = {20},
keywords = {Cognitive load, Attention management, Risk analysis, Temporal-difference learning, Cyber feint attack, Alert fatigue, Human vulnerability}
}

@article{10.1504/ijbra.2020.104853,
author = {Deshpande, Sumukh and James, Anne and Franklin, Chris and Leach, Lindsey and Yang, Jianhua},
title = {Identification of novel flowering genes using RNA-Seq pipeline employing combinatorial approach in Arabidopsis thaliana time-series apical shoot meristem data},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {1},
issn = {1744-5485},
url = {https://doi.org/10.1504/ijbra.2020.104853},
doi = {10.1504/ijbra.2020.104853},
abstract = {Floral transition is a crucial event in the reproductive cycle of a flowering plant during which many genes are expressed that govern the transition phase. Identification of additional genes connected to flowering genes is vital since they may regulate flowering genes and vice versa. Through our study, expression values of additional genes have been found similar to flowering genes FLC and LFY in the transition phase. The presented approach plays a crucial role in this discovery. An RNA-Seq computational pipeline was developed for identification of novel genes involved in floral transition from A. thaliana apical shoot meristem time-series data. By intersecting differentially expressed genes (DEGs) from Cuffdiff, DEseq and edgeR, we identified 30 genes as principle regulators in the transition phase. Additionally, expression profiles of highly connected genes from network analysis revealed 76 genes with non-functional association and high correlation to FLC and LFY suggesting their potential role in floral regulation.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {25–58},
numpages = {33},
keywords = {Arabidopsis thaliana, enrichment, differential expression, step analysis, Cuffdiff, pipeline, flowering, apical shoot}
}

@inproceedings{10.5555/3104482.3104594,
author = {Paisley, John and Carin, Lawrence and Blei, David},
title = {Variational inference for stick-breaking beta process priors},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a variational Bayesian inference algorithm for the stick-breaking construction of the beta process. We derive an alternate representation of the beta process that is amenable to variational inference, and present a bound relating the truncated beta process to its infinite counterpart. We assess performance on two matrix factorization problems, using a non-negative factorization model and a linear-Gaussian model.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {889–896},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@article{10.1016/j.rcim.2016.01.008,
author = {Luo, Hao and Wang, Kai and Kong, Xiang T.R. and Lu, Shaoping and Qu, Ting},
title = {Synchronized production and logistics via ubiquitous computing technology},
year = {2017},
issue_date = {June 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.01.008},
doi = {10.1016/j.rcim.2016.01.008},
abstract = {The integration of manufacturing and logistics has drawn widespread research attentions in recent years. This paper focuses on the Synchronized Production and Logistics (SPL), which is operational level integration. SPL\'{z}is defined as synchronizing the processing, moving and storing of raw material, WIP and finished product within one manufacturing unit by high level information sharing and joint scheduling to achieve synergic decision, execution and overall performance improvement. Through analysing the requirements and challenges in real life industry, the ubiquitous computing is adopted as an enabling technology and an Ubi-SPL (Synchronized Production and Logistics via Ubiquitous Technology) framework is proposed. This framework is consists of four layers, which creates a close decision-execution loop by linking the frontline real time data, user feedback and optimized decision together. A real life case study of applying Ubi-SPL solution in a chemical industry has been conducted. The implementation results show that the proposed Ubi-SPL solution can significantly improve the overall performance in both production and logistics service. This paper focuses on the Synchronized Production and Logistics.The ubiquitous computing is adopted as an enabling technology.Synchronized Production &amp; Logistics via Ubiquitous Technics framework is proposed.A real life case study in a chemical industry has been conducted.},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
pages = {99–115},
numpages = {17},
keywords = {Ubiquitous manufacturing, Synchronized production and logistics, RFID}
}

@article{10.1016/j.compag.2019.02.007,
author = {Sultan Mahmud, Md. and Zaman, Qamar U. and Esau, Travis J. and Price, Gordon W. and Prithiviraj, Balakrishnan},
title = {Development of an artificial cloud lighting condition system using machine vision for strawberry powdery mildew disease detection},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {158},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.02.007},
doi = {10.1016/j.compag.2019.02.007},
journal = {Comput. Electron. Agric.},
month = mar,
pages = {219–225},
numpages = {7},
keywords = {Working depth, Acquisition speed, Lighting condition, Powdery mildew, Machine vision}
}

@inproceedings{10.5555/3104482.3104540,
author = {Virtanen, Seppo and Klami, Arto and Kaski, Samuel},
title = {Bayesian CCA via group sparsity},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Bayesian treatments of Canonical Correlation Analysis (CCA) -type latent variable models have been recently proposed for coping with overfitting in small sample sizes, as well as for producing factorizations of the data sources into correlated and non-shared effects. However, all of the current implementations of Bayesian CCA and its extensions are computationally inefficient for high-dimensional data and, as shown in this paper, break down completely for high-dimensional sources with low sample count. Furthermore, they cannot reliably separate the correlated effects from non-shared ones. We propose a new Bayesian CCA variant that is computationally efficient and works for high-dimensional data, while also learning the factorization more accurately. The improvements are gained by introducing a group sparsity assumption and an improved variational approximation. The method is demonstrated to work well on multi-label prediction tasks and in analyzing brain correlates of naturalistic audio stimulation.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {457–464},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{10.1145/3184558.3191523,
author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
title = {Towards Interpretation of Node Embeddings},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191523},
doi = {10.1145/3184558.3191523},
abstract = {Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {945–952},
numpages = {8},
keywords = {graph representation, model interpretability, neural networks},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1007/s00371-017-1442-1,
author = {Dehshibi, Mohammad Mahdi and Shanbehzadeh, Jamshid},
title = {Cubic norm and kernel-based bi-directional PCA: toward age-aware facial kinship verification},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {1},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-017-1442-1},
doi = {10.1007/s00371-017-1442-1},
abstract = {A recent challenge in computer vision is exploring the cardinality of a relationship among multiple visual entities to answer questions like whether the subjects in a photograph have a kin relationship. This paper tackles kinship recognition from the aging viewpoint in which the system could find the parent of a child where the input image of the parent belongs to the age range that is lower than the child is. Technical contributions of this research are twofold. (1) An efficient discriminative feature space is constructed by proposing kernelized bi-directional PCA to form a topological cubic feature space. Cubic feature space in conjunction with the introduced cubic norm is used to solve the kinship problem. (2) To fill the gap of aging effect in finding a kin relation, a semi-supervised learning paradigm is proposed. To do this, first, the pooling layer of a convolutional neural network is modified to do a soft pooling. Then, the last pooling layer, as a rich feature vector, is fed into density-based spatial clustering of applications with noise algorithm. This pre-classification phase would be useful when there is no aggregation on how many classes should be used in the age group estimation task. Finally, by adding kernel computation to sparse representation classifier, the age classification is done. Evaluation of the proposed method on five publicly available facial kinship datasets shows the superiority of the proposed method over both the state-of-the-art kinship verification methods and what is known as human decision-making.},
journal = {Vis. Comput.},
month = jan,
pages = {23–40},
numpages = {18},
keywords = {Kinship verification, Kernelized bi-directional PCA, Feature extraction, Cube norm, Convolutional neural network, Age estimation, Age clustering}
}

@article{10.1007/s42979-021-00705-6,
author = {Jagdhuber, Rudolf and Rahnenf\"{u}hrer, J\"{o}rg},
title = {Implications on Feature Detection When Using the Benefit–Cost Ratio},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {4},
url = {https://doi.org/10.1007/s42979-021-00705-6},
doi = {10.1007/s42979-021-00705-6},
abstract = {In many practical machine learning applications, there are two objectives: one is to maximize predictive accuracy and the other is to minimize costs of the resulting model. These costs of individual features may be financial costs, but can also refer to other aspects, for example, evaluation time. Feature selection addresses both objectives, as it reduces the number of features and can improve the generalization ability of the model. If costs differ between features, the feature selection needs to trade-off the individual benefit and cost of each feature. A popular trade-off choice is the ratio of both, the benefit–cost ratio (BCR). In this paper, we analyze implications of using this measure with special focus to the ability to distinguish relevant features from noise. We perform simulation studies for different cost and data settings and obtain detection rates of relevant features and empirical distributions of the trade-off ratio. Our simulation studies exposed a clear impact of the cost setting on the detection rate. In situations with large cost differences and small effect sizes, the BCR missed relevant features and preferred cheap noise features. We conclude that a trade-off between predictive performance and costs without a controlling hyperparameter can easily overemphasize very cheap noise features. While the simple benefit–cost ratio offers an easy solution to incorporate costs, it is important to be aware of its risks. Avoiding costs close to 0, rescaling large cost differences, or using a hyperparameter trade-off are ways to counteract the adverse effects exposed in this paper.},
journal = {SN Comput. Sci.},
month = jun,
numpages = {10},
keywords = {Cost-sensitive learning, Feature selection, Benefit–cost ratio, Feature detection, Feature costs}
}

@article{10.1109/TPAMI.2008.240,
author = {Sun, Zhenan and Tan, Tieniu},
title = {Ordinal Measures for Iris Recognition},
year = {2009},
issue_date = {December 2009},
publisher = {IEEE Computer Society},
address = {USA},
volume = {31},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2008.240},
doi = {10.1109/TPAMI.2008.240},
abstract = {Images of a human iris contain rich texture information useful for identity authentication. A key and still open issue in iris recognition is how best to represent such textural information using a compact set of features (iris features). In this paper, we propose using ordinal measures for iris feature representation with the objective of characterizing qualitative relationships between iris regions rather than precise measurements of iris image structures. Such a representation may lose some image-specific information, but it achieves a good trade-off between distinctiveness and robustness. We show that ordinal measures are intrinsic features of iris patterns and largely invariant to illumination changes. Moreover, compactness and low computational complexity of ordinal measures enable highly efficient iris recognition. Ordinal measures are a general concept useful for image analysis and many variants can be derived for ordinal feature extraction. In this paper, we develop multilobe differential filters to compute ordinal measures with flexible intralobe and interlobe parameters such as location, scale, orientation, and distance. Experimental results on three public iris image databases demonstrate the effectiveness of the proposed ordinal feature models.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {2211–2226},
numpages = {16},
keywords = {ordinal measures., multilobe differential filter, iris recognition, feature representation, Iris Recognition, Biometrics}
}

@inproceedings{10.1145/1806799.1806872,
author = {Rastkar, Sarah and Murphy, Gail C. and Murray, Gabriel},
title = {Summarizing software artifacts: a case study of bug reports},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806872},
doi = {10.1145/1806799.1806872},
abstract = {Many software artifacts are created, maintained and evolved as part of a software development project. As software developers work on a project, they interact with existing project artifacts, performing such activities as reading previously filed bug reports in search of duplicate reports. These activities often require a developer to peruse a substantial amount of text. In this paper, we investigate whether it is possible to summarize software artifacts automatically and effectively so that developers could consult smaller summaries instead of entire artifacts. To provide focus to our investigation, we consider the generation of summaries for bug reports. We found that existing conversation-based generators can produce better results than random generators and that a generator trained specifically on bug reports can perform statistically better than existing conversation-based generators. We demonstrate that humans also find these generated summaries reasonable indicating that summaries might be used effectively for many tasks.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {505–514},
numpages = {10},
keywords = {machine learning, human-centric software engineering},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3139958.3140044,
author = {Jiang, Zhe and Li, Yan and Shekhar, Shashi and Rampi, Lian and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity: A Summary of Results},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140044},
doi = {10.1145/3139958.3140044},
abstract = {Class ambiguity refers to the phenomenon whereby samples with similar features belong to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. SEL problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost (finding an optimal zone partition is NP-hard). Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble), and thus cannot effectively minimize class ambiguity. In contrast, our spatial ensemble framework explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {23},
numpages = {10},
keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1007/978-3-030-34585-3_18,
author = {Serra, Angela and della Pietra, Antonio and Herdener, Marcus and Tagliaferri, Roberto and Esposito, Fabrizio},
title = {Automatic Discrimination of Auditory Stimuli Perceived by the Human Brain},
year = {2018},
isbn = {978-3-030-34584-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34585-3_18},
doi = {10.1007/978-3-030-34585-3_18},
abstract = {Humans are able to perceive small difference of sound frequency but it is still unknown how the difference in frequency information is represented at the level of the primary sensory cortex. Indeed, analysis of fMRI imaging identified tonotopic maps through the auditory pathways to the primary sensory cortex. These maps are unfortunately too coarse to show ultra-fine discrimination. Then, the hypothesis is that this small frequency differences are recognised thanks to the information coming from a large set of auditory neurons. To investigate this possibility, a multi-voxel pattern discriminating analysis of the response of BOLD-fMRI in the bilateral auditory cortex to tonal stimuli with different shift in frequency was performed. Our results suggest that small shifts in the frequency are easily classified compared with big shifts and that multiple areas of the auditory cortex are involved in the tone recognition.},
booktitle = {Computational Intelligence Methods for Bioinformatics and Biostatistics: 15th International Meeting, CIBB 2018, Caparica, Portugal, September 6–8, 2018, Revised Selected Papers},
pages = {205–211},
numpages = {7},
keywords = {BOLD-fMRI, Auditory cortex, Tonotonic maps},
location = {Caparica, Portugal}
}

@inproceedings{10.1007/11668855_4,
author = {Cho, Eun Sook and Cha, Jung Eun and Yang, Young Jong},
title = {MARMI-RE: a method and tools for legacy system modernization},
year = {2004},
isbn = {3540321330},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11668855_4},
doi = {10.1007/11668855_4},
abstract = {Software evolution is the process of adapting an existing software system to conform to an enhanced set of requirements. Software reengineering is software evolution performed in systematic way. Especially software system is fundamentally different from developing one from scratch. Consequently, tools to support evolution must go beyond forward engineering tools. This paper presents a reengineering method and tools for software evolution or modernization. The paper briefly describes MARMI-RE methodology before presenting the individual tools and how they interoperate to support legacy system modernization. We expect that our proposed methodology can be used flexibly because it presents various scenarios of migration process.},
booktitle = {Proceedings of the Second International Conference on Software Engineering Research, Management and Applications},
pages = {42–57},
numpages = {16},
location = {Los Angeles, CA},
series = {SERA'04}
}

@inproceedings{10.1109/ICComm.2016.7528326,
author = {Irofti, Paul and Dumitrescu, Bogdan},
title = {Regularized algorithms for dictionary learning},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICComm.2016.7528326},
doi = {10.1109/ICComm.2016.7528326},
abstract = {Dictionary learning (DL) for sparse representation is a difficult optimization problem for which several successful algorithms exist, although none can be claimed the best. A common problem is a possible stall in the evolution of the algorithm, due to nearly linearly dependent atoms. The proposed cure was to regularize the error criterion using either the norm of the representations or an atom coherence measure. However, only gradient-based algorithms have been proposed for the regularized problems. We give here regularized versions of Approximate K-SVD and other algorithms related to it and investigate numerically their behavior. The experiments show that the new regularized algorithms are able to reduce the representation error, and thus produce better dictionaries, when the imposed sparsity is not very high.},
booktitle = {2016 International Conference on Communications (COMM)},
pages = {439–442},
numpages = {4},
location = {Bucharest, Romania}
}

