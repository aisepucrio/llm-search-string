@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-86486-6_46,
author = {Louiset, Robin and Gori, Pietro and Dufumier, Benoit and Houenou, Josselin and Grigis, Antoine and Duchesnay, Edouard},
title = {UCSL : A Machine Learning Expectation-Maximization Framework for&nbsp;Unsupervised Clustering Driven by&nbsp;Supervised Learning},
year = {2021},
isbn = {978-3-030-86485-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86486-6_46},
doi = {10.1007/978-3-030-86486-6_46},
abstract = {Subtype Discovery consists in finding interpretable and consistent sub-parts of a dataset, which are also relevant to a certain supervised task. From a mathematical point of view, this can be defined as a clustering task driven by supervised learning in order to uncover subgroups in line with the supervised prediction. In this paper, we propose a general Expectation-Maximization ensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised Learning). Our method is generic, it can integrate any clustering method and can be driven by both binary classification and regression. We propose to construct a non-linear model by merging multiple linear estimators, one per cluster. Each hyperplane is estimated so that it correctly discriminates - or predict - only one cluster. We use SVC or Logistic Regression for classification and SVR for regression. Furthermore, to perform cluster analysis within a more suitable space, we also propose a dimension-reduction algorithm that projects the data onto an orthonormal space relevant to the supervised task. We analyze the robustness and generalization capability of our algorithm using synthetic and experimental datasets. In particular, we validate its ability to identify suitable consistent sub-types by conducting a psychiatric-diseases cluster analysis with known ground-truth labels. The gain of the proposed method over previous state-of-the-art techniques is about +1.9 points in terms of balanced accuracy. Finally, we make codes and examples available in a scikit-learn-compatible Python package. .},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part I},
pages = {755–771},
numpages = {17},
keywords = {Clustering, Subtype discovery, Expectation-maximization, Machine learning, Neuroimaging},
location = {Bilbao, Spain}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1007/978-3-031-17587-9_1,
author = {Shahbaz, Ajmal and Khan, Salman and Hossain, Mohammad Asiful and Lomonaco, Vincenzo and Cannons, Kevin and Xu, Zhan and Cuzzolin, Fabio},
title = {International Workshop on&nbsp;Continual Semi-Supervised Learning: Introduction, Benchmarks and&nbsp;Baselines},
year = {2021},
isbn = {978-3-031-17586-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17587-9_1},
doi = {10.1007/978-3-031-17587-9_1},
abstract = {The aim of this paper is to formalise a new continual semi-supervised learning (CSSL) paradigm, proposed to the attention of the machine learning community via the IJCAI 2021 International Workshop on Continual Semi-Supervised Learning (CSSL@IJCAI) (), with the aim of raising the field’s awareness about this problem and mobilising its effort in this direction. After a formal definition of continual semi-supervised learning and the appropriate training and testing protocols, the paper introduces two new benchmarks specifically designed to assess CSSL on two important computer vision tasks: activity recognition and crowd counting. We describe the Continual Activity Recognition (CAR) and Continual Crowd Counting (CCC) challenges built upon those benchmarks, the baseline models proposed for the challenges, and describe a simple CSSL baseline which consists in applying batch self-training in temporal sessions, for a limited number of rounds. The results show that learning from unlabelled data streams is extremely challenging, and stimulate the search for methods that can encode the dynamics of the data stream.},
booktitle = {Continual Semi-Supervised Learning: First International Workshop, CSSL 2021, Virtual Event, August 19–20, 2021, Revised Selected Papers},
pages = {1–14},
numpages = {14},
keywords = {Continual learning, Semi-supervised learning, Artificial intelligence}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-87626-5_5,
author = {Davidson, Padraig and Buckermann, Florian and Steininger, Michael and Krause, Anna and Hotho, Andreas},
title = {Semi-unsupervised Learning: An In-depth Parameter Analysis},
year = {2021},
isbn = {978-3-030-87625-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87626-5_5},
doi = {10.1007/978-3-030-87626-5_5},
abstract = {Creating datasets for supervised learning is a very challenging and expensive task, in which each input example has to be annotated with its expected output (e.g. object class). By combining unsupervised and semi-supervised learning, semi-unsupervised learning proposes a new paradigm for partially labeled datasets with additional unknown classes. In this paper we focus on a better understanding of this new learning paradigm and analyze the impact of the amount of labeled data, the number of augmented classes and the selection of hidden classes on the quality of prediction. Especially the number of augmented classes highly influences classification accuracy, which needs tuning for each dataset, since too few and too many augmented classes are detrimental to classifier performance. We also show that we can improve results on a large variety of datasets when using convolutional networks as feature extractors while applying output driven entropy regularization instead of a simple weight based L2 norm.},
booktitle = {KI 2021: Advances in Artificial Intelligence: 44th German Conference on AI, Virtual Event, September 27 – October 1, 2021, Proceedings},
pages = {51–66},
numpages = {16},
keywords = {Semi-unsupervised learning, Deep generative models, Classification}
}

@article{10.1007/s00521-020-05529-8,
author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco},
title = {Machine learning for landslides prevention: a survey},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05529-8},
doi = {10.1007/s00521-020-05529-8},
abstract = {Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {10881–10907},
numpages = {27},
keywords = {Natural disasters, Landslides prevention, Machine learning, Supervised learning, Unsupervised learning, Deep learning}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.5555/3540261.3541394,
author = {Balcan, Maria-Florina and Sharma, Dravyansh},
title = {Data driven semi-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a novel data driven approach for designing semi-supervised learning algorithms that can effectively learn with only a small number of labeled examples. We focus on graph-based techniques, where the unlabeled examples are connected in a graph under the implicit assumption that similar nodes likely have similar labels. Over the past two decades, several elegant graph-based semi-supervised learning algorithms for inferring the labels of the unlabeled examples given the graph and a few labeled examples have been proposed. However, the problem of how to create the graph (which impacts the practical usefulness of these methods significantly) has been relegated to heuristics and domain-specific art, and no general principles have been proposed. In this work we present a novel data driven approach for learning the graph and provide strong formal guarantees in both the distributional and online learning formalizations. We show how to leverage problem instances coming from an underlying problem domain to learn the graph hyperparameters for commonly used parametric families of graphs that provably perform well on new instances from the same domain. We obtain low regret and efficient algorithms in the online setting, and generalization guarantees in the distributional setting. We also show how to combine several very different similarity metrics and learn multiple hyperparameters, our results hold for large classes of problems. We expect some of the tools and techniques we develop along the way to be of independent interest, for data driven algorithms more generally.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1133},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.engappai.2021.104504,
author = {Vaish, Rachna and Dwivedi, U.D. and Tewari, Saurabh and Tripathi, S.M.},
title = {Machine learning applications in power system fault diagnosis: Research advancements and perspectives},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104504},
doi = {10.1016/j.engappai.2021.104504},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {33},
keywords = {Machine learning (ML), Reinforcement learning, Supervised learning, Transfer learning, Unsupervised learning}
}

@inproceedings{10.5555/3540261.3541402,
author = {Yang, Longqi and Zhang, Liangliang and Yang, Wenjing},
title = {Graph adversarial self-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper studies a long-standing problem of learning the representations of a whole graph without human supervision. The recent self-supervised learning methods train models to be invariant to the transformations (views) of the inputs. However, designing these views requires the experience of human experts. Inspired by adversarial training, we propose an adversarial self-supervised learning (GASSL) framework for learning unsupervised representations of graph data without any handcrafted views. GASSL automatically generates challenging views by adding perturbations to the input and are adversarially trained with respect to the encoder. Our method optimizes the min-max problem and utilizes a gradient accumulation strategy to accelerate the training process. Experimental on ten graph classification datasets show that the proposed approach is superior to state-of-the-art self-supervised learning baselines, which are competitive with supervised models.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1141},
numpages = {13},
series = {NIPS '21}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.5555/3524938.3525621,
author = {Nock, Richard and Menon, Aditya Krishna},
title = {Supervised learning: no loss no cry},
year = {2020},
publisher = {JMLR.org},
abstract = {Supervised learning requires the specification of a loss function to minimise. While the theory of admissible losses from both a computational and statistical perspective is well-developed, these offer a panoply of different choices. In practice, this choice is typically made in an ad hoc manner. In hopes of making this procedure more principled, the problem of learning the loss function for a downstream task (e.g., classification) has garnered recent interest. However, works in this area have been generally empirical in nature.In this paper, we revisit the SLISOTRON algorithm of Kakade et al. (2011) through a novel lens, derive a generalisation based on Bregman divergences, and show how it provides a principled procedure for learning the loss. In detail, we cast SLISOTRON as learning a loss from a family of composite square losses. By interpreting this through the lens of proper losses, we derive a generalisation of SLISOTRON based on Bregman divergences. The resulting BREGMANTRON algorithm jointly learns the loss along with the classifier. It comes equipped with a simple guarantee of convergence for the loss it learns, and its set of possible outputs comes with a guarantee of agnostic approximability of Bayes rule. Experiments indicate that the BREGMANTRON outperforms the SLISOTRON, and that the loss it learns can be minimized by other algorithms for different tasks, thereby opening the interesting problem of loss transfer between domains.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {683},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.neunet.2021.08.003,
author = {Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
title = {Hebbian semi-supervised learning in a sample efficiency setting},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {143},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.08.003},
doi = {10.1016/j.neunet.2021.08.003},
journal = {Neural Netw.},
month = nov,
pages = {719–731},
numpages = {13},
keywords = {Convolutional Neural Networks, Computer vision, Semi-supervised learning, Hebbian learning, Sample efficiency}
}

@inproceedings{10.5555/3327345.3327407,
author = {Garg, Vikas K. and Kalai, Adam},
title = {Supervising unsupervised learning},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm, remove the outliers, and provably circumvent Kleinberg's impossibility result. Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4996–5006},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1007/978-3-030-47426-3_39,
author = {Jawed, Shayan and Grabocka, Josif and Schmidt-Thieme, Lars},
title = {Self-supervised Learning for Semi-supervised Time Series Classification},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_39},
doi = {10.1007/978-3-030-47426-3_39},
abstract = {Self-supervised learning is a promising new technique for learning representative features in the absence of manual annotations. It is particularly efficient in cases where labeling the training data is expensive and tedious, naturally linking it to the semi-supervised learning paradigm. In this work, we propose a new semi-supervised time series classification model that leverages features learned from the self-supervised task on unlabeled data. The idea is to exploit the unlabeled training data with a forecasting task which provides a strong surrogate supervision signal for feature learning. We draw from established multi-task learning approaches and model forecasting as an auxiliary task to be optimized jointly with the main task of classification. We evaluate our proposed method on benchmark time series classification datasets in semi-supervised setting and are able to show that it significantly outperforms the state-of-the-art baselines.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {499–511},
numpages = {13},
keywords = {Self-supervised features, Semi-supervised classification, Auxiliary tasks, Convolutional Neural Networks},
location = {Singapore, Singapore}
}

@article{10.1613/jair.1.12839,
author = {Jurewicz, Mateusz and Derczynski, Leon},
title = {Set-to-Sequence Methods in Machine Learning: A Review},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12839},
doi = {10.1613/jair.1.12839},
abstract = {Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modelling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {885–924},
numpages = {40},
keywords = {neural networks, machine learning}
}

@inproceedings{10.5555/3540261.3542199,
author = {Araslanov, Nikita and Schaub-Meyer, Simone and Roth, Stefan},
title = {Dense unsupervised learning for video segmentation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1938},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/3540261.3542376,
author = {Wang, Yu and Lin, Jingyang and Zou, Jingjing and Pan, Yingwei and Yao, Ting and Mei, Tao},
title = {Improving self-supervised learning with automated unsupervised outlier arbitration},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Our work reveals a structured shortcoming of the existing mainstream selfsupervised learning methods. Whereas self-supervised learning frameworks usually take the prevailing perfect instance level invariance hypothesis for granted, we carefully investigate the pitfalls behind. Particularly, we argue that the existing augmentation pipeline for generating multiple positive views naturally introduces out-of-distribution (OOD) samples that undermine the learning of the downstream tasks. Generating diverse positive augmentations on the input does not always pay off in benefiting downstream tasks. To overcome this inherent deficiency, we introduce a lightweight latent variable model UOTA, targeting the view sampling issue for self-supervised learning. UOTA adaptively searches for the most important sampling region to produce views, and provides viable choice for outlier-robust self-supervised learning approaches. Our method directly generalizes to many mainstream self-supervised learning approaches, regardless of the loss's nature contrastive or not. We empirically show UOTA's advantage over the state-of-the-art self-supervised paradigms with evident margin, which well justifies the existence of the OOD sample issue embedded in the existing approaches. Especially, we theoretically prove that the merits of the proposal boil down to guaranteed estimator variance and bias reduction.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2115},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.5555/3540261.3542417,
author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
title = {Fair sequential selection using supervised learning models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classifica-tion problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfes the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions. We introduce a new fairness notion, "Equal Selection (ES)," suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the perfect ES fairness can still be attained under certain conditions.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2156},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-36718-3_53,
author = {Rastin, Parisa and Cabanes, Gu\'{e}na\"{e}l and Verde, Rosanna and Bennani, Youn\`{e}s and Couronne, Thierry},
title = {Generative Histogram-Based Model Using Unsupervised Learning},
year = {2019},
isbn = {978-3-030-36717-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-36718-3_53},
doi = {10.1007/978-3-030-36718-3_53},
abstract = {This paper presents a new generative unsupervised learning algorithm based on a representation of the clusters distribution by histograms. The main idea is to reduce the model complexity through cluster-defined projections of the data on independent axes. The results show that the proposed approach performs efficiently compared with other algorithms. In addition, it is more efficient to generate new instances with the same distribution than the training data.},
booktitle = {Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III},
pages = {634–646},
numpages = {13},
keywords = {Unsupervised learning, Clustering, Generative model, Histogram distribution},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1007/978-3-030-58799-4_57,
author = {Nakao, Eduardo K. and Levada, Alexandre L. M.},
title = {Unsupervised Learning and Feature Extraction in Hyperspectral Imagery},
year = {2020},
isbn = {978-3-030-58798-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58799-4_57},
doi = {10.1007/978-3-030-58799-4_57},
abstract = {Remotely sensed hyperspectral scenes are typically defined by large area coverage and hundreds of spectral bands. Those characteristics imply smooth transitions in the spectral-spatio domains. As consequence, subtle differences in the scene are evidenced, benefiting precision applications, but values in neighboring locations and wavelengths are highly correlated. Nondiagonal covariance matrices and wide autocorrelation functions can be observed this way, implying increased intraclass and decreased interclass variation, in both spectral and spatial domains. This leads to lower interpretation accuracies and makes it reasonable to investigate if hyperspectral imagery suffer from Curse of Dimensionality. Moreover, as this Curse can compromise linear method’s Euclidean behavior assumption, it is relevant to compare linear and nonlinear dimensionality reduction performance. So, in this work we verify these two aspects empirically using multiple nonparametric statistical comparisons of Gaussian Mixture Model clustering performances in the cases of: absence, linear and nonlinear unsupervised feature extraction. Experimental results indicate Curse of Dimensionality presence and nonlinear adequacy.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part I},
pages = {792–806},
numpages = {15},
keywords = {Hyperspectral imagery, Curse of dimensionality, Unsupervised feature extraction, Nonlinear dimensionality reduction, Unsupervised learning},
location = {Cagliari, Italy}
}

@article{10.1007/s00521-020-05343-2,
author = {Erkan, U\u{g}ur},
title = {A precise and stable machine learning algorithm: eigenvalue classification (EigenClass)},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05343-2},
doi = {10.1007/s00521-020-05343-2},
abstract = {In this study, a precise and efficient eigenvalue-based machine learning algorithm, particularly denoted as Eigenvalue Classification (EigenClass) algorithm, has been presented to deal with classification problems. The EigenClass algorithm is constructed by exploiting an eigenvalue-based proximity evaluation. To appreciate the classification performance of EigenClass, it is compared with the well-known algorithms, such as k-nearest neighbours, fuzzy k-nearest neighbours, random forest (RF) and multi-support vector machines. Number of 20 different datasets with various attributes and classes are used for the comparison. Every algorithm is trained and tested for 30 runs through 5-fold cross-validation. The results are then compared among each other in terms of the most used measures, such as accuracy, precision, recall, micro-F-measure, and macro-F-measure. It is demonstrated that EigenClass exhibits the best classification performance for 15 datasets in terms of every metric and, in a pairwise comparison, outperforms the other algorithms for at least 16 datasets in consideration of each metric. Moreover, the algorithms are also compared through statistical analysis and computational complexity. Therefore, the achieved results show that EigenClass is a precise and stable algorithm as well as the most successful algorithm considering the overall classification performances.},
journal = {Neural Comput. Appl.},
month = may,
pages = {5381–5392},
numpages = {12},
keywords = {Data classification, Eigenvalues, Learning algorithm, Machine learning, Supervised learning}
}

@article{10.1007/s00500-018-3655-2,
author = {Aldana-Bobadila, Edwin and Kuri-Morales, Angel and Lopez-Arevalo, Ivan and Rios-Alvarado, Ana B.},
title = {An unsupervised learning approach for multilayer perceptron networks},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3655-2},
doi = {10.1007/s00500-018-3655-2},
abstract = {Multilayer perceptron networks have been designed to solve supervised learning problems in which there is a set of known labeled training feature vectors. The resulting model allows us to infer adequate labels for unknown input vectors. Traditionally, the optimal model is the one that minimizes the error between the known labels and those inferred labels via such a model. The training process results in those weights that achieve the most adequate labels. Training implies a search process which is usually determined by the descent gradient of the error. In this work, we propose to replace the known labels by a set of such labels induced by a validity index. The validity index represents a measure of the adequateness of the model relative only to intrinsic structures and relationships of the set of feature vectors and not to previously known labels. Since, in general, there is no guarantee of the differentiability of such an index, we resort to heuristic optimization techniques. Our proposal results in an unsupervised learning approach for multilayer perceptron networks that allows us to infer the best model relative to labels derived from such a validity index which uncovers the hidden relationships of an unlabeled dataset.},
journal = {Soft Comput.},
month = nov,
pages = {11001–11013},
numpages = {13},
keywords = {Neural networks, Clustering, Unsupervised learning}
}

@inproceedings{10.5555/3540261.3542103,
author = {Kaku, Aakash and Upadhya, Sahana and Razavian, Narges},
title = {Intermediate layers matter in momentum contrastive self supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that bringing intermediate layers' representations of two augmented versions of an image closer together in self supervised learning helps to improve the momentum contrastive (MoCo) method. To this end, in addition to the con-trastive loss, we minimize the mean squared error between the intermediate layer representations or make their cross-correlation matrix closer to an identity matrix. Both loss objectives either outperform standard MoCo, or achieve similar performances on three diverse medical imaging datasets: NIH-Chest Xrays, Breast Cancer Histopathology, and Diabetic Retinopathy. The gains of the improved MoCo are especially large in a low-labeled data regime (e.g. 1% labeled data) with an average gain of 5% across three datasets. We analyze the models trained using our novel approach via feature similarity analysis and layer-wise probing. Our analysis reveals that models trained via our approach have higher feature reuse compared to a standard MoCo and learn informative features earlier in the network. Finally, by comparing the output probability distribution of models fine tuned on small versus large labeled data, we conclude that our proposed method of pre-training leads to lower Kolmogorov–Smirnov distance, as compared to a standard MoCo. This provides additional evidence that our proposed method learns more informative features in the pre-training phase which could be leveraged in a low-labeled data regime.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1842},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-59861-7_38,
author = {Zhao, Fenqiang and Wu, Zhengwang and Wang, Li and Lin, Weili and Xia, Shunren and Shen, Dinggang and Li, Gang},
title = {Unsupervised Learning for Spherical Surface Registration},
year = {2020},
isbn = {978-3-030-59860-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59861-7_38},
doi = {10.1007/978-3-030-59861-7_38},
abstract = {Current spherical surface registration methods achieve good performance on alignment and spatial normalization of cortical surfaces across individuals in neuroimaging analysis. However, they are computationally intensive, since they have to optimize an objective function independently for each pair of surfaces. In this paper, we present a fast learning-based algorithm that makes use of the recent development in spherical Convolutional Neural Networks (CNNs) for spherical cortical surface registration. Given a set of surface pairs without supervised information such as ground truth deformation fields or anatomical landmarks, we formulate the registration as a parametric function and learn its parameters by enforcing the feature similarity between one surface and the other one warped by the estimated deformation field using the function. Then, given a new pair of surfaces, we can quickly infer the spherical deformation field registering one surface to the other one. We model this parametric function using three orthogonal Spherical U-Nets and use spherical transform layers to warp the spherical surfaces, while imposing smoothness constraints on the deformation field. All the layers in the network are well-defined and differentiable, thus the parameters can be effectively learned. We show that our method achieves accurate cortical alignment results on 102 subjects, comparable to two state-of-the-art methods: Spherical Demons and MSM, while runs much faster.},
booktitle = {Machine Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {373–383},
numpages = {11},
keywords = {Spherical U-Net, Cortical surface registration},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-61616-8_67,
author = {Axenie, Cristian and Kurz, Daria},
title = {Tumor Characterization Using Unsupervised Learning of Mathematical Relations Within Breast Cancer Data},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_67},
doi = {10.1007/978-3-030-61616-8_67},
abstract = {Despite the variety of imaging, genetic and histopathological data used to assess tumors, there is still an unmet need for patient-specific tumor growth profile extraction and tumor volume prediction, for use in surgery planning. Models of tumor growth predict tumor size and require tumor biology-dependent parametrization, which hardly generalizes to cope with tumor variability among patients. In addition, the datasets are limited in size, owing to the restricted or single-time measurements. In this work, we address the shortcomings that incomplete biological specifications, the inter-patient variability of tumors, and the limited size of the data bring to mechanistic tumor growth models. We introduce a machine learning model that alleviates these shortcomings and is capable of characterizing a tumor’s growth pattern, phenotypical transitions, and volume. The model learns without supervision, from different types of breast cancer data the underlying mathematical relations describing tumor growth curves more accurate than three state-of-the-art models. Experiments performed on publicly available clinical breast cancer datasets, demonstrate the versatility of the approach among breast cancer types. Moreover, the model can also, without modification, learn the mathematical relations among, for instance, histopathological and morphological parameters of the tumor and, combined with the growth curve, capture the (phenotypical) growth transitions of the tumor from a small amount of data. Finally, given the tumor growth curve and its transitions, our model can learn the relation among tumor proliferation-to-apoptosis ratio, tumor radius, and tumor nutrient diffusion length, used to estimate tumor volume. Such a quantity can be readily incorporated within current clinical practice, for surgery planning.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {838–849},
numpages = {12},
keywords = {Artificial neural networks, Breast cancer, Unsupervised learning, Prediction algorithms},
location = {Bratislava, Slovakia}
}

@article{10.1007/s00521-021-05968-x,
author = {Esqu\'{\i}vel, Manuel L. and Krasii, Nadezhda P.},
title = {A wavelet-based neural network scheme for supervised and unsupervised learning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05968-x},
doi = {10.1007/s00521-021-05968-x},
abstract = {We introduce a scheme for supervised and unsupervised learning based on successive decomposition of random inputs by means of wavelet basis. To each successive layer corresponds a different wavelet basis with more null moments than its predecessor. We consider two types of operations in the scheme: firstly, a input signal treatment phase—the awake phase—and, secondly, a reorganizing phase of the random wavelet coefficients obtained in the previous awake phase—the asleep phase. The next awake phase input treatment will include a feedback derived from the previous asleep phase. The set of random wavelet coefficients of the deepest layer—at each stage of the learning process—is supposed to be Gaussian distributed, and the corresponding sequence of Gaussian distributions constitutes the inner representation of the world in the scheme. We show that in the case of a constant average value of the inputs in each successive awake phases, the mean value of the feedback converges to a multiple of the constant expected value of the inputs. We show a general result on the stabilization of the Gaussian distribution corresponding to the deepest layer, and we show that when the estimated means and covariances converge, then the sequence of Gaussian distributions of the inner representation of the world in the scheme also converges. We present an example of a neural network scheme for supervised learning corresponding to extracting a signal from data corrupted with Gaussian noise.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {13433–13448},
numpages = {16},
keywords = {Neural networks, Supervised and unsupervised learning, Wavelet theory}
}

@inproceedings{10.1007/978-3-031-17587-9_3,
author = {Monorchio, Luca and Capotondi, Marco and Corsanici, Mario and Villa, Wilson and De Luca, Alessandro and Puja, Francesco},
title = {Transfer and&nbsp;Continual Supervised Learning for&nbsp;Robotic Grasping Through Grasping Features},
year = {2021},
isbn = {978-3-031-17586-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-17587-9_3},
doi = {10.1007/978-3-031-17587-9_3},
abstract = {We present a Transfer and Continual Learning method for robotic grasping tasks, based on small vision-depth (RGBD) datasets and realized through the use of Grasping Features. Given a network architecture composed by a CNN (Convolutional Neural Network) followed by a FCC (Fully Connected Cascade Neural Network), we exploit high-level features specific of the grasping tasks, as extracted by the convolutional network from RGBD images. These features are more descriptive of a grasping task than just visual ones, and thus more efficient for transfer learning purposes. Being datasets for visual grasping less common than those for image recognition, we also propose an efficient way to generate these data using only simple geometric structures. This reduces the computational burden of the FCC and allows to obtain a better performance with the same amount of data. Simulation results using the collaborative UR-10 robot and a jaw gripper are reported to show the quality of the proposed method.},
booktitle = {Continual Semi-Supervised Learning: First International Workshop, CSSL 2021, Virtual Event, August 19–20, 2021, Revised Selected Papers},
pages = {33–47},
numpages = {15},
keywords = {Transfer Learning, Continual Learning, Robotic grasping}
}

@article{10.1016/j.eswa.2021.115662,
author = {Wiwatcharakoses, Chayut and Berrar, Daniel},
title = {A self-organizing incremental neural network for continual supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115662},
doi = {10.1016/j.eswa.2021.115662},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {9},
keywords = {Catastrophic forgetting, Concept drift, Continual learning, Incremental learning, Supervised learning}
}

@inproceedings{10.5555/3327345.3327355,
author = {Sun, Haitian and Bing, Lidong and Cohen, William W.},
title = {Semi-supervised learning with declaratively specified entropy constraints},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4430–4440},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1007/978-3-030-70866-5_3,
author = {Mohammedi, El-Heithem and Lavinal, Emmanuel and Fleury, Guillaume},
title = {Configuration Faults Detection in IP Virtual Private Networks Based on Machine Learning},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_3},
doi = {10.1007/978-3-030-70866-5_3},
abstract = {Network incidents are largely due to configuration errors, particularly within network service providers who manage large complex networks. Such providers offer virtual private networks to their customers to interconnect their remote sites and provide Internet access. The growing demand for virtual private networks leads service providers to search for novel scalable approaches to locate incidents arising from configuration faults. In this paper, we propose a machine learning approach that aims to locate customer connectivity issues coming from configurations errors, in a BGP/MPLS IP virtual private network architecture. We feed the learning model with valid and faulty configuration data and train it using three algorithms: decision tree, random forest and multi-layer perceptron. Since failures can occur on several routers, we consider the learning problem as a supervised multi-label classification problem, where each customer router is represented by a unique label. We carry out our experiments on three network sizes containing different types of configuration errors. Results show that multi-layer perceptron has a better accuracy in detecting faults than the other algorithms, making it a potential candidate to validate offline network configurations before online deployment.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {40–56},
numpages = {17},
keywords = {Configuration faults detection, Machine learning, Virtual private networks, BGP/MPLS networks},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-031-08147-7_13,
author = {Vitorino, Jo\~{a}o and Andrade, Rui and Pra\c{c}a, Isabel and Sousa, Orlando and Maia, Eva},
title = {A Comparative Analysis of Machine Learning Techniques for IoT Intrusion Detection},
year = {2021},
isbn = {978-3-031-08146-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08147-7_13},
doi = {10.1007/978-3-031-08147-7_13},
abstract = {The digital transformation faces tremendous security challenges. In particular, the growing number of cyber-attacks targeting Internet of Things (IoT) systems restates the need for a reliable detection of malicious network activity. This paper presents a comparative analysis of supervised, unsupervised and reinforcement learning techniques on nine malware captures of the IoT-23 dataset, considering both binary and multi-class classification scenarios. The developed models consisted of Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), Isolation Forest (iForest), Local Outlier Factor (LOF) and a Deep Reinforcement Learning (DRL) model based on a Double Deep Q-Network (DDQIN), adapted to the intrusion detection context. The most reliable performance was achieved by LightGBM. Nonetheless, iForest displayed good anomaly detection results and the DRL model demonstrated the possible benefits of employing this methodology to continuously improve the detection. Overall, the obtained results indicate that the analyzed techniques are well suited for IoT intrusion detection.},
booktitle = {Foundations and Practice of Security: 14th International Symposium, FPS 2021, Paris, France, December 7–10, 2021, Revised Selected Papers},
pages = {191–207},
numpages = {17},
keywords = {Internet of Things, Intrusion detection, Supervised learning, Unsupervised learning, Reinforcement learning},
location = {Paris, France}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1007/978-3-030-93944-1_8,
author = {Goldsteen, Abigail and Ezov, Gilad and Shmelkin, Ron and Moffie, Micha and Farkash, Ariel},
title = {Anonymizing Machine Learning Models},
year = {2021},
isbn = {978-3-030-93943-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93944-1_8},
doi = {10.1007/978-3-030-93944-1_8},
abstract = {There is a known tension between the need to analyze personal data to drive business and the need to preserve the privacy of data subjects. Many data protection regulations, including the EU General Data Protection Regulation (GDPR) and the California Consumer Protection Act (CCPA), set out strict restrictions and obligations on the collection and processing of personal data. Moreover, machine learning models themselves can be used to derive personal information, as demonstrated by recent membership and attribute inference attacks. Anonymized data, however, is exempt from the obligations set out in these regulations. It is therefore desirable to be able to create models that are anonymized, thus also exempting them from those obligations, in addition to providing better protection against attacks.Learning on anonymized data typically results in significant degradation in accuracy. In this work, we propose a method that is able to achieve better model accuracy by using the knowledge encoded within the trained model, and guiding our anonymization process to minimize the impact on the model’s accuracy, a process we call accuracy-guided anonymization. We demonstrate that by focusing on the model’s accuracy rather than generic information loss measures, our method outperforms state of the art k-anonymity methods in terms of the achieved utility, in particular with high values of k and large numbers of quasi-identifiers.We also demonstrate that our approach has a similar, and sometimes even better ability to prevent membership inference attacks as approaches based on differential privacy, while averting some of their drawbacks such as complexity, performance overhead and model-specific implementations. In addition, since our approach does not rely on making modifications to the training algorithm, it can even work with “black-box” models where the data owner does not have full control over the training process, or within complex machine learning pipelines where it may be difficult to replace existing learning algorithms with new ones. This makes model-guided anonymization a legitimate substitute for such methods and a practical approach to creating privacy-preserving models.},
booktitle = {Data Privacy Management, Cryptocurrencies and Blockchain Technology: ESORICS 2021 International Workshops, DPM 2021 and CBT 2021, Darmstadt, Germany, October 8, 2021, Revised Selected Papers},
pages = {121–136},
numpages = {16},
keywords = {GDPR, Anonymization, k-anonymity, Compliance, Privacy, Machine learning},
location = {Darmstadt, Germany}
}

@inproceedings{10.1145/3383455.3422517,
author = {Chew, Peter},
title = {Unsupervised-learning financial reconciliation: a robust, accurate approach inspired by machine translation},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422517},
doi = {10.1145/3383455.3422517},
abstract = {Financial reconciliation (cross-checking independent sources of data) is a time-honored and widespread function in finance and audit. Its objectives are to ensure completeness, timeliness, and accuracy of recording of transactions.With the proliferation of data over recent decades, the ways in which reconciliation is approached have evolved. There are now a number of software products that promise to automate the process of detailed transaction matching. However, the 'state of the art' in these products is rules-based reconciliation. Rules-based systems in any domain tend to be brittle in that any change in the data streams causes the rules to have to be debugged and rebuilt, itself often a time-consuming process. And to make matters worse, this might be caused not just by data schema changes, but the contents of what is in the data fields themselves. Either of these can occur if a third party such as a bank changes its internal processes.In a sense, automated reconciliation is where machine translation was almost 70 years ago; IBM's 1954 Georgetown experiment approached Russian-to-English translation using rules, but it took another 4 decades for researchers in data-driven Artificial Intelligence (AI) to realize why machine translation did not initially live up to its promises and develop a truly robust methodology for machine translation based on unsupervised learning.It turns out that financial reconciliation can be cast as a machine translation problem based on unsupervised learning. To our knowledge, we are the first to propose this. Here, we demonstrate via experiments on real-life (albeit small-scale) financial data that this way of approaching the problem demonstrates promise in terms of accuracy, as well as solving the problem of lack of robustness inherent in rules-based approaches.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {46},
numpages = {12},
keywords = {computing methodologies, machine translation, reconciliation, unsupervised learning},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3395035.3425191,
author = {Wu, Yujin and Daoudi, Mohamed and Amad, Ali and Sparrow, Laurent and D'Hondt, Fabien},
title = {Unsupervised Learning Method for Exploring Students' Mental Stress in Medical Simulation Training},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425191},
doi = {10.1145/3395035.3425191},
abstract = {So far, stress detection technology usually uses supervised learning methods combined with a series of physiological, physical, or behavioral signals and has achieved promising results. However, the problem of label collection such as the latency of stress response and subjective uncertainty introduced by the questionnaires has not been effectively solved. This paper proposes an unsupervised learning method with K-means clustering for exploring students' autonomic responses to medical simulation training in an ambulant environment. With the use of wearable sensors, features of electrodermal activity and heart rate variability of subjects are extracted to train the K-means model. The Silhouette Score of 0.49 with two clusters was reached, proving the difference in students' mental stress between baseline stage and simulation stage. Besides, with the aid of external ground truth which could be associated with either the baseline phase or simulation phase, four evaluation metrics were calculated and provided comparable results concerning supervised and unsupervised learning methods. The highest classification performance of 70% was reached with the measure of precision. In the future, we will integrate context information or facial image to provide more accurate stress detection.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {165–170},
numpages = {6},
keywords = {EDA, HRV, k-means, mental stress, physiological signal, silhouette score, unsupervised learning},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1561/2200000081,
author = {Holden, Sean B.},
title = {Machine Learning for Automated Theorem Proving: Learning to Solve SAT and QSAT},
year = {2021},
issue_date = {Nov 2021},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {14},
number = {6},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000081},
doi = {10.1561/2200000081},
abstract = {The decision problem for Boolean satisfiability, generally
    referred to as SAT, is the archetypal NP-complete problem,
    and encodings of many problems of practical interest exist
    allowing them to be treated as SAT problems. Its generalization
    to quantified SAT (QSAT) is PSPACE-complete, and
    is useful for the same reason. Despite the computational
    complexity of SAT and QSAT, methods have been developed
    allowing large instances to be solved within reasonable
    resource constraints. These techniques have largely exploited
    algorithmic developments; however machine learning also
    exerts a significant influence in the development of state-ofthe-
    art solvers. Here, the application of machine learning
    is delicate, as in many cases, even if a relevant learning
    problem can be solved, it may be that incorporating the
    result into a SAT or QSAT solver is counterproductive, because
    the run-time of such solvers can be sensitive to small
    implementation changes. The application of better machine
    learning methods in this area is thus an ongoing challenge,
    with characteristics unique to the field. This work provides
    a comprehensive review of the research to date on incorporating
    machine learning into SAT and QSAT solvers, as a
    resource for those interested in further advancing the field.},
journal = {Found. Trends Mach. Learn.},
month = nov,
pages = {807–989},
numpages = {187}
}

@inproceedings{10.1145/3450267.3452001,
author = {Vardhan, Harsh and Volgyesi, Peter and Sztipanovits, Janos},
title = {Machine learning assisted propeller design},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450267.3452001},
doi = {10.1145/3450267.3452001},
abstract = {Propellers are one of the most widely used propulsive devices for generating thrust from rotational engine motion both in marine vehicles and subsonic air-crafts. Due to their simplicity, robustness and high efficiency, propellers remained the mainstream design choice over the last hundred years. On the other hand, finding the optimal application-specific geometry is still challenging. This work in progress report describes application of modern and rapidly developing Machine Learning (ML) techniques to gain novel designs. We rely on a rich set of preexisting parametric design patterns and accumulated engineering knowledge supplemented by high-fidelity simulation models to formulate the design process as a supervised learning problem.The aim of our work is to develop and evaluate machine learning models for the parametric design of propellers based on application-specific constraints. While the application of ML techniques in optimal propeller design is at a very nascent level, we believe that our early results are promising with a potentially significant impact on the overall design process. The ML-assisted design flow allows for a more automated design space exploration process with less dependency on human intuition and engineering guidance.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {227–228},
numpages = {2},
keywords = {OpenProp, design-space exploration, evolutionary algorithm, machine learning, propeller, random forest regression},
location = {Nashville, Tennessee},
series = {ICCPS '21}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@article{10.1145/3360601,
author = {Cambronero, Jos\'{e} P. and Rinard, Martin C.},
title = {AL: autogenerating supervised learning programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360601},
doi = {10.1145/3360601},
abstract = {We present AL, a novel automated machine learning system that learns to generate new supervised learning pipelines from an existing corpus of supervised learning programs. In contrast to existing automated machine learning tools, which typically implement a search over manually selected machine learning functions and classes, AL learns to identify the relevant classes in an API by analyzing dynamic program traces that use the target machine learning library. AL constructs a conditional probability model from these traces to estimate the likelihood of the generated supervised learning pipelines and uses this model to guide the search to generate pipelines for new datasets. Our evaluation shows that AL can produce successful pipelines for datasets that previous systems fail to process and produces pipelines with comparable predictive performance for datasets that previous systems process successfully.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {175},
numpages = {28},
keywords = {automated machine learning, program analysis for machine learning}
}

@article{10.1016/j.knosys.2019.105426,
author = {He, Hongshun and Han, Deqiang and Dezert, Jean},
title = {Disagreement based semi-supervised learning approaches with belief functions},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105426},
doi = {10.1016/j.knosys.2019.105426},
journal = {Know.-Based Syst.},
month = apr,
numpages = {29},
keywords = {Machine learning, Semi-supervised learning, Belief functions, Uncertainty}
}

@article{10.1016/j.neunet.2021.07.023,
author = {Tonin, Francesco and Patrinos, Panagiotis and Suykens, Johan A.K.},
title = {Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.07.023},
doi = {10.1016/j.neunet.2021.07.023},
journal = {Neural Netw.},
month = oct,
pages = {661–679},
numpages = {19},
keywords = {Kernel methods, Unsupervised learning, Manifold learning, Learning disentangled representations}
}

@inproceedings{10.1007/978-3-030-87897-9_16,
author = {Kalina, Jan and Matonoha, Ctirad},
title = {Robustness of Supervised Learning Based on Combined Centroids},
year = {2021},
isbn = {978-3-030-87896-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87897-9_16},
doi = {10.1007/978-3-030-87897-9_16},
abstract = {Recently, we proposed a novel sparse centroid-based supervised learning method, allowing to optimize a single centroid and its corresponding weights. The method is especially useful for localizing objects in images. Here, we extend the method to the task of joint localization of several objects in a&nbsp;2D-image by means of combining several centroids. The novel approach, i.e. joint optimization of several centroids and a&nbsp;subsequent optimization of their weights, is illustrated on the task of localizing the mouth and both eyes in facial images. Because we are particularly interested in studying the robustness of the method to various modifications of the images, we evaluate the performance of the methods also over images artificially modified by additional noise, occlusion, changed illumination, or rotation. The novel centroid-based method is successful in the localization task, and the optimization turns out to ensure robustness with respect to the presence of noise or occlusion in the images. Moreover, combining the optimized centroids yields more robust results than a method using simple centroids with a highly robust correlation coefficient (with a high breakdown point).},
booktitle = {Artificial Intelligence and Soft Computing: 20th International Conference, ICAISC 2021, Virtual Event, June 21–23, 2021, Proceedings, Part II},
pages = {171–182},
numpages = {12},
keywords = {Machine learning, Sparsity, Regularization, Robust optimization, Outliers}
}

@inproceedings{10.1145/3453800.3453803,
author = {Thanh Trieu, Ngoan and Pottier, Bernard and Rodin, Vincent and Xuan Huynh, Hiep},
title = {Interpretable Machine Learning for Meteorological Data},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453803},
doi = {10.1145/3453800.3453803},
abstract = {Weather forecasting is the task to predict the state of the atmosphere in a given location. In the past, the weather forecast has been done through physical models of the atmosphere as a fluid. It becomes the problem of solving sophisticated equations of fluid dynamics. In recent years, machine learning algorithms have been used to speed up weather data modeling, a computationally intensive task. Machine learning algorithms learn from data and produce relevant predictions. In addition to prediction, there is a need of providing knowledge about domain relationships inside the data. This paper provides a new approach using interpretable machine learning for explaining the characteristic variables of meteorological data. Interpretable machine learning is the use of machine learning models for the extraction of knowledge in the data. An illustration is shown on characteristic variables of meteorological data.},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {11–17},
numpages = {7},
keywords = {BUFR/Express, Environment Simulations, Interpretable Machine Learning, Weather Data},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@inproceedings{10.1145/3380446.3430690,
author = {Khailany, Brucek},
title = {Accelerating Chip Design with Machine Learning},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430690},
doi = {10.1145/3380446.3430690},
abstract = {As Moore's law has provided an exponential increase in chip transistor density, the unique features we can now include in large chips are no longer predominantly limited by area constraints. Instead, new capabilities are increasingly limited by the engineering effort associated with digital design, verification, and implementation. As applications demand more performance and energy efficiency from specialization in the post-Moore's-law era, we expect required complexity and design effort to increase.Historically, these challenges have been met through levels of abstraction and automation. Over the last few decades, Electronic Design Automation (EDA) algorithms and methodologies were developed for all aspects of chip design - design verification and simulation, logic synthesis, place-and-route, and timing and physical signoff analysis. With each increase in automation, total work per chip has increased, but more work has also been offloaded from manual effort to software. Just as machine learning (ML) has transformed software in many domains, we expect advancements in ML will also transform EDA software and as a result, chip design workflows.In this talk, we highlight work from our research group and the community applying ML to various chip design prediction tasks [1]. We show how deep convolutional neural networks [2] and graph-based neural networks [3] can be used in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted chip design workflow to automate optimization tasks. In this future vision, GPU acceleration, neural-network predictors, and deep reinforcement learning techniques combine to automate VLSI design and optimization.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {33},
numpages = {1},
keywords = {VLSI, design methodology, machine learning},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@article{10.1016/j.patrec.2020.12.012,
author = {Zeng, Shaofeng and Liu, Zhiyong and Yang, Xu},
title = {Supervised learning for parameterized Koopmans–Beckmann’s graph matching},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2020.12.012},
doi = {10.1016/j.patrec.2020.12.012},
journal = {Pattern Recogn. Lett.},
month = mar,
pages = {8–13},
numpages = {6},
keywords = {Graph matching, Koopmans–Beckmann, Supervised learning, Structured SVM}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1007/978-3-030-68780-9_13,
author = {Kanevski, Mikhail and Laib, Mohamed},
title = {Unsupervised Learning of High Dimensional Environmental Data Using Local Fractality Concept},
year = {2021},
isbn = {978-3-030-68779-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68780-9_13},
doi = {10.1007/978-3-030-68780-9_13},
abstract = {The research deals with an exploration of high dimensional environmental data using unsupervised learning algorithms and the concept of local fractality. The proposed methodology is applied to geospatial data used for the wind speed prediction in a complex mountainous region. It is shown, that the approach provides important additional information on data manifold useful in data analysis, data visualisation and predictive modelling.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part VI},
pages = {130–138},
numpages = {9},
keywords = {Unsupervised learning, Environmental data, Fractals}
}

@article{10.1016/j.jksuci.2019.02.003,
author = {Aamir, Muhammad and Ali Zaidi, Syed Mustafa},
title = {Clustering based semi-supervised machine learning for DDoS attack classification},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2019.02.003},
doi = {10.1016/j.jksuci.2019.02.003},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = may,
pages = {436–446},
numpages = {11},
keywords = {Clustering, DDoS attacks, Machine learning, Semi-supervised}
}

@article{10.1007/s00521-021-05951-6,
author = {Badr, Assem},
title = {Awesome back-propagation machine learning paradigm},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05951-6},
doi = {10.1007/s00521-021-05951-6},
abstract = {For a better future in machine learning (ML), it is necessary to modify our current concepts to get the fastest ML. Many designers had attempted to find the optimal learning rates in their applications through many algorithms over the past decades, but they have not yet achieved their target of highest speed of back-propagation (BP). This research proposes a novel BP rule called the Instant Learning Ratios-Machine Learning (ILR-ML) or (ILRML). Unlike the traditional BP algorithms, the ILR-ML offers its learning without the concepts of the learning rate(s). The ILR-ML has a new concept called the "Learning Ratio" and indicated by a sign (Δℓ). The ILR-ML performs the full BP algorithm with 100% accuracy per each learning iteration. The ILR-ML is more suitable for the online machine learning.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {13225–13249},
numpages = {25},
keywords = {Neural network, Machine learning algorithms, Back-propagation}
}

@article{10.1007/s11227-018-02737-x,
author = {Lee, Hyeonseo and Lee, Nakyeong and Seo, Harim and Song, Min},
title = {Developing a supervised learning-based social media business sentiment index: Developing a supervised learning-based social media…},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {5},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-02737-x},
doi = {10.1007/s11227-018-02737-x},
abstract = {The fast-growing digital data generation leads to the emergence of the era of big data, which become particularly more valuable because approximately 70% of the collected data in the world comes from social media. Thus, the investigation of online social network services is of paramount importance. In this paper, we use the sentiment analysis, which detects attitudes and emotions toward issues of society posted in social media, to understand the actual economic situation. To this end, two steps are suggested. In the first step, after training the sentiment classifiers with several big data sources of social media datasets, we consider three types of feature sets: feature vector, sequence vector and a combination of dictionary-based feature and sequence vectors. Then, the performance of six classifiers is assessed: MaxEnt-L1, C4.5 decision tree, SVM-kernel, Ada-boost, Na\"{\i}ve Bayes and MaxEnt. In the second step, we collect datasets that are relevant to several economic words that the public use to explicitly express their opinions. Finally, we use a vector auto-regression analysis to confirm our hypothesis. The results show the statistically significant relationship between public sentiment and economic performance. That is, “depression” and “unemployment” lead to KOSPI. Also, it shows that the extracted keywords from the sentiment analysis, such as “price,” “year-end-tax” and “budget deficit,” cause the exchange rates.},
journal = {J. Supercomput.},
month = may,
pages = {3882–3897},
numpages = {16},
keywords = {Sentiment analysis, Social media, Machine learning, Supervised learning}
}

@article{10.1016/j.eswa.2021.114820,
author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
title = {Machine Learning for industrial applications: A comprehensive literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114820},
doi = {10.1016/j.eswa.2021.114820},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {29},
keywords = {Literature review, Industrial applications, Deep Learning, Machine Learning, Operation management}
}

@article{10.1007/s10994-021-06030-6,
author = {Watson, David S. and Wright, Marvin N.},
title = {Testing conditional independence in supervised learning algorithms},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {8},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06030-6},
doi = {10.1007/s10994-021-06030-6},
abstract = {We propose the conditional predictive impact (CPI), a consistent and unbiased estimator of the association between one or several features and a given outcome, conditional on a reduced feature set. Building on the knockoff framework of Cand\`{e}s et al. (J R Stat Soc Ser B 80:551–577, 2018), we develop a novel testing procedure that works in conjunction with any valid knockoff sampler, supervised learning algorithm, and loss function. The CPI can be efficiently computed for high-dimensional data without any sparsity constraints. We demonstrate convergence criteria for the CPI and develop statistical inference procedures for evaluating its magnitude, significance, and precision. These tests aid in feature and model selection, extending traditional frequentist and Bayesian techniques to general supervised learning tasks. The CPI may also be applied in causal discovery to identify underlying multivariate graph structures. We test our method using various algorithms, including linear regression, neural networks, random forests, and support vector machines. Empirical results show that the CPI compares favorably to alternative variable importance measures and other nonparametric tests of conditional independence on a diverse array of real and synthetic datasets. Simulations confirm that our inference procedures successfully control Type I error with competitive power in a range of settings. Our method has been implemented in an R package, cpi, which can be downloaded from .},
journal = {Mach. Learn.},
month = aug,
pages = {2107–2129},
numpages = {23},
keywords = {Knockoffs, Machine learning, Conditional independence, Markov blanket, Variable importance}
}

@inproceedings{10.1007/978-3-030-86365-4_47,
author = {Liao, Kaimin and Gan, Ziyu and Yang, Xuan},
title = {Semi-supervised Learning Based Right Ventricle Segmentation Using Deep Convolutional Boltzmann Machine Shape&nbsp;Model},
year = {2021},
isbn = {978-3-030-86364-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86365-4_47},
doi = {10.1007/978-3-030-86365-4_47},
abstract = {Automated Right Ventricle (RV) segmentation is a challenge due to the RV’s variable shape and the lack of labelled data. This paper proposes a semi-supervised learning method based on a convolutional deep Boltzmann machine (CDBM). A CDBM is constructed to learn the complex shape of RV using the short-run MCMC. Next, a semi-supervised learning network composing of a CDBM and two CNNs is proposed. The CNNs and the CDBM have trained alternatively; labelled data are used to train the CNNs, and CDBM reconstructs the predicted results of unlabelled data using the CNNs to guide the training of CNNs further. During this procedure, the CDBM is trained at the same time. Our approach’s main idea is to extract the shape information of RV and use the shape information to improve CNN’s performance. Our approach takes advantage of avoiding overfitting and requiring less labelled data. Besides, our approach does not increase any extra computational cost and parameters during inference. The experiment results show that our approach can improve segmentation accuracy when the labelled training data is small.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part III},
pages = {585–597},
numpages = {13},
keywords = {Deep Boltzmann Machine, Right ventricle segmentation, Semi-supervised learning, Convolutional Neural Network},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.artmed.2021.102198,
author = {Peralta, Maxime and Jannin, Pierre and Baxter, John S.H.},
title = {Machine learning in deep brain stimulation: A systematic review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {122},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102198},
doi = {10.1016/j.artmed.2021.102198},
journal = {Artif. Intell. Med.},
month = dec,
numpages = {13},
keywords = {Systematic review, Deep brain stimulation, Machine learning}
}

@article{10.1016/j.ipm.2021.102642,
author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
title = {Machine learning fairness notions: Bridging the gap with real-world applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102642},
doi = {10.1016/j.ipm.2021.102642},
journal = {Inf. Process. Manage.},
month = sep,
numpages = {32},
keywords = {Fairness, Machine learning, Discrimination, Survey, Systemization of Knowledge (SoK)}
}

@article{10.1016/j.datak.2021.101909,
author = {Maass, Wolfgang and Storey, Veda C.},
title = {Pairing conceptual modeling with machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101909},
doi = {10.1016/j.datak.2021.101909},
journal = {Data Knowl. Eng.},
month = jul,
numpages = {35},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@article{10.1145/3459664,
author = {Talbi, El-Ghazali},
title = {Machine Learning into Metaheuristics: A Survey and Taxonomy},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459664},
doi = {10.1145/3459664},
abstract = {During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {129},
numpages = {32},
keywords = {ML-supported metaheuristics, Metaheuristics, machine learning, optimization}
}

@inproceedings{10.1145/3474370.3485662,
author = {Chhabra, Anshuman and Mohapatra, Prasant},
title = {Moving Target Defense against Adversarial Machine Learning},
year = {2021},
isbn = {9781450386586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474370.3485662},
doi = {10.1145/3474370.3485662},
abstract = {As Machine Learning (ML) models are increasingly employed in a number of applications across a multitude of fields, the threat of adversarial attacks against ML models is also increasing. Adversarial samples crafted via specialized attack algorithms have been shown to significantly decrease the performance of ML models. Furthermore, it has also been found that adversarial samples generated for a particular model can transfer across other models, and decrease accuracy and other performance metrics for a model they were not originally crafted for. In recent research, many different defense approaches have been proposed for making ML models robust, ranging from adversarial input re-training to defensive distillation, among others. While these approaches operate at the model-level, we propose an alternate approach to defending ML models against adversarial attacks, using Moving Target Defense (MTD). We formulate the problem and provide preliminary results to showcase the validity of the proposed approach.},
booktitle = {Proceedings of the 8th ACM Workshop on Moving Target Defense},
pages = {29–30},
numpages = {2},
keywords = {adversarial attacks, adversarial machine learning, moving target defense, reinforcement learning},
location = {Virtual Event, Republic of Korea},
series = {MTD '21}
}

@inproceedings{10.1145/3468891.3468896,
author = {Xu, Chenghan},
title = {Image-based Candlestick Pattern Classification with Machine Learning},
year = {2021},
isbn = {9781450389402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468891.3468896},
doi = {10.1145/3468891.3468896},
abstract = {Financial markets, such as the stock market, bond market and foreign exchange market, are important channels for fund transfer. As a graphical analysis tool, candlestick charts use graphs to display the open, high, low, and close prices in a specific period. In the past, there have been attempts to identify the characteristics of candlesticks based on Gramian Angular Field (GAF) images, but they are not perfect. In this study, we implemented Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), AdaBoost, Random Forest (RF) and XGBoost models, we found that the use of deep learning models is not the best choice for the recognition of candlestick features based on GAF images. Comparing these models, MLP and CNN are better than AdaBoost and RF, but worse than XGBoost. Our results show that for the candlestick pattern classification problem based on GAF images, it is unnecessary to use complex CNNs and traditional machine learning models can also achieve satisfactory results with much less computation resources.},
booktitle = {Proceedings of the 2021 6th International Conference on Machine Learning Technologies},
pages = {26–33},
numpages = {8},
keywords = {Candlestick, Deep Learning, Machine Learning, Pattern Classification},
location = {Jeju Island, Republic of Korea},
series = {ICMLT '21}
}

@inproceedings{10.5555/3540261.3541456,
author = {Du, Yilun and Li, Shuang and Sharma, Yash and Tenenbaum, Joshua B. and Mordatch, Igor},
title = {Unsupervised learning of compositional energy concepts},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Humans are able to rapidly understand scenes by utilizing concepts extracted from prior experience. Such concepts are diverse, and include global scene descriptors, such as the weather or lighting, as well as local scene descriptors, such as the color or size of a particular object. So far, unsupervised discovery of concepts has focused on either modeling the global scene-level or the local object-level factors of variation, but not both. In this work, we propose COMET, which discovers and represents concepts as separate energy functions, enabling us to represent both global concepts as well as objects under a unified framework. COMET discovers energy functions through recomposing the input image, which we find captures independent factors without additional supervision. Sample generation in COMET is formulated as an optimization process on underlying energy functions, enabling us to generate images with permuted and composed concepts. Finally, discovered visual concepts in COMET generalize well, enabling us to compose concepts between separate modalities of images as well as with other concepts discovered by a separate instance of COMET trained on a different dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1195},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.asoc.2021.107280,
author = {Cheng, Chen-Yang and Pourhejazy, Pourya and Ying, Kuo-Ching and Lin, Chen-Fang},
title = {Unsupervised Learning-based Artificial Bee Colony for minimizing non-value-adding operations},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107280},
doi = {10.1016/j.asoc.2021.107280},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {10},
keywords = {Lean manufacturing, Scheduling, Unsupervised learning, Unrelated parallel machines, Metaheuristics}
}

@inproceedings{10.1007/978-3-030-87196-3_4,
author = {Li, Hongwei and Xue, Fei-Fei and Chaitanya, Krishna and Luo, Shengda and Ezhov, Ivan and Wiestler, Benedikt and Zhang, Jianguo and Menze, Bjoern},
title = {Imbalance-Aware Self-supervised Learning for 3D Radiomic Representations},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_4},
doi = {10.1007/978-3-030-87196-3_4},
abstract = {Radiomics can quantify the properties of regions of interest in medical image data. Classically, they account for pre-defined statistics of shape, texture, and other low-level image features. Alternatively, deep learning-based representations are derived from supervised learning but require expensive annotations and often suffer from overfitting and data imbalance issues. In this work, we address the challenge of learning the representation of a 3D medical image for an effective quantification under data imbalance. We propose a self-supervised representation learning framework to learn high-level features of 3D volumes as a complement to existing radiomics features. Specifically, we demonstrate how to learn image representations in a self-supervised fashion using a 3D Siamese network. More importantly, we deal with data imbalance by exploiting two unsupervised strategies: a) sample re-weighting, and b) balancing the composition of training batches. When combining the learned self-supervised feature with traditional radiomics, we show significant improvement in brain tumor classification and lung cancer staging tasks covering MRI and CT imaging modalities. Codes are available in .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {36–46},
numpages = {11},
location = {Strasbourg, France}
}

@article{10.1145/3475167,
author = {Molino, Piero and R\'{e}, Christopher},
title = {Declarative machine learning systems},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3475167},
doi = {10.1145/3475167},
abstract = {The future of machine learning will depend on it being in the hands of the rest of us.},
journal = {Commun. ACM},
month = dec,
pages = {42–49},
numpages = {8}
}

@inproceedings{10.1145/3459637.3482114,
author = {Li, Chen and Peng, Xutan and Peng, Hao and Wu, Jia and Wang, Lihong and Yu, Philip S. and Li, Jianxin and Sun, Lichao},
title = {Graph-based Semi-Supervised Learning by Strengthening Local Label Consistency},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482114},
doi = {10.1145/3459637.3482114},
abstract = {Graph-based algorithms have drawn much attention thanks to their impressive success in semi-supervised setups. For better model performance, previous studies have learned to transform the topology of the input graph. However, these works only focus on optimizing the original nodes and edges, leaving the direction of augmenting existing data insufficiently explored. In this paper, we propose a novel heuristic pre-processing technique, namelyLocal Label Consistency Strengthening (\L{}LCS), which automatically expands new nodes and edges to refine the label consistency within a dense subgraph. Our framework can effectively benefit downstream models by substantially enlarging the original training set with high-quality generated labeled data and refining the original graph topology. To justify the generality and practicality of \L{}LCS, we couple it with the popular graph convolution network and graph attention network to perform extensive evaluations on three standard datasets. In all setups tested, our method boosts the average accuracy by a large margin of 4.7% and consistently outperforms the state-of-the-art.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3201–3205},
numpages = {5},
keywords = {node classification, semi-supervised learning, topology enhanced transformation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1109/MILCOM52596.2021.9652947,
author = {Abdou, Ahmed and Sheatsley, Ryan and Beugin, Yohan and Shipp, Tyler and McDaniel, Patrick},
title = {HoneyModels: Machine Learning Honeypots},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM52596.2021.9652947},
doi = {10.1109/MILCOM52596.2021.9652947},
abstract = {Machine Learning is becoming a pivotal aspect of many systems today, offering newfound performance on classification and prediction tasks, but this rapid integration also comes with new unforeseen vulnerabilities. To harden these systems the ever-growing field of Adversarial Machine Learning has proposed new attack and defense mechanisms. However, a great asymmetry exists as these defensive methods can only provide security to certain models and lack scalability, computational efficiency, and practicality due to overly restrictive constraints. Moreover, newly introduced attacks can easily bypass defensive strategies by making subtle alterations. In this paper, we study an alternate approach inspired by honeypots to detect adversaries. Our approach yields learned models with an embedded watermark. When an adversary initiates an interaction with our model, attacks are encouraged to add this predetermined watermark stimulating detection of adversarial examples. We show that HoneyModels can reveal 69.5% of adversaries attempting to attack a Neural Network while preserving the original functionality of the model. HoneyModels offer an alternate direction to secure Machine Learning that slightly affects the accuracy while encouraging the creation of watermarked adversarial samples detectable by the HoneyModel but indistinguishable from others for the adversary.},
booktitle = {MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM)},
pages = {886–891},
numpages = {6},
location = {San Diego, CA, USA}
}

@inproceedings{10.5555/3507788.3507807,
author = {Ara\'{u}jo, Rodrigo and Holmes, Reid},
title = {Lightweight self-adaptive configuration using machine learning},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Modern distributed systems are comprised of many components that often have complex configuration parameters to allow them to be tuned to differing runtime requirements. Engineers must manually adjust many of these parameters to achieve their desired runtime behaviours. Unfortunately, static configurations are often insufficient, but ad hoc configuration modifications can unexpectedly degrade overall system quality.In this work, we describe Finch, a tool for injecting a machine learning-based MAPE-K feedback loop into existing REST-based systems to automate configuration tuning. Finch configures and optimizes systems according to service-level agreements under uncertain workloads and usage patterns. Rather than changing the core infrastructure of a target system to fit the feedback loop, Finch asks the user to perform a small set of actions: adding limited instrumentation to the code and configuration parameters and defining service-level objectives and agreements. With these changes, Finch learns how to dynamically configure the system at runtime to self-adapt to dynamic workloads.We provide a proof-of-concept evaluation to demonstrate how Finch can provide an automated self-adaptive system that replaces the trial-and-error engineering effort that otherwise would be spent manually optimizing a system's wide array of configuration parameters.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {machine learning, self adaptation, software configuration},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.5555/3540261.3542590,
author = {Cabannes, Vivien and Pillaud-Vivien, Loucas and Bach, Francis and Rudi, Alessandro},
title = {Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As annotations of data can be scarce in large-scale practical problems, leveraging unlabelled examples is one of the most important aspects of machine learning. This is the aim of semi-supervised learning. To benefit from the access to unlabelled data, it is natural to diffuse smoothly knowledge of labelled data to unlabelled one. This induces to the use of Laplacian regularization. Yet, current implementations of Laplacian regularization suffer from several drawbacks, notably the well-known curse of dimensionality. In this paper, we provide a statistical analysis to overcome those issues, and unveil a large body of spectral filtering methods that exhibit desirable behaviors. They are implemented through (reproducing) kernel methods, for which we provide realistic computational guidelines in order to make our method usable with large amounts of data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2329},
numpages = {13},
series = {NIPS '21}
}

@article{10.1007/s10614-018-9803-z,
author = {Kao, Ying-Fang and Venkatachalam, Ragupathy},
title = {Human and Machine Learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {3},
issn = {0927-7099},
url = {https://doi.org/10.1007/s10614-018-9803-z},
doi = {10.1007/s10614-018-9803-z},
abstract = {In this paper, we consider learning by human beings and machines in the light of Herbert Simon’s pioneering contributions to the theory of Human Problem Solving. Using board games of perfect information as a paradigm, we explore differences in human and machine learning in complex strategic environments. In doing so, we contrast theories of learning in classical game theory with computational game theory proposed by Simon. Among theories that invoke computation, we make a further distinction between computable and computational or machine learning theories. We argue that the modern machine learning algorithms, although impressive in terms of their performance, do not necessarily shed enough light on human learning. Instead, they seem to take us further away from Simon’s lifelong quest to understand the mechanics of actual human behaviour.},
journal = {Comput. Econ.},
month = mar,
pages = {889–909},
numpages = {21},
keywords = {Machine learning, Human problem solving, Herbert Simon, Learning, Artificial intelligence, Go}
}

@article{10.1007/s10489-020-02048-w,
author = {Nerurkar, Pranav and Bhirud, Sunil and Patel, Dhiren and Ludinard, Romaric and Busnel, Yann and Kumari, Saru},
title = {Supervised learning model for identifying illegal activities in Bitcoin},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02048-w},
doi = {10.1007/s10489-020-02048-w},
abstract = {Since its inception in 2009, Bitcoin is mired in controversies for providing a haven for illegal activities. Several types of illicit users hide behind the blanket of anonymity. Uncovering these entities is key for forensic investigations. Current methods utilize machine learning for identifying these illicit entities. However, the existing approaches only focus on a limited category of illicit users. The current paper proposes to address the issue by implementing an ensemble of decision trees for supervised learning. More parameters allow the ensemble model to learn discriminating features that can categorize multiple groups of illicit users from licit users. To evaluate the model, a dataset of 1216 real-life entities on Bitcoin was extracted from the Blockchain. Nine Features were engineered to train the model for segregating 16 different licit-illicit categories of users. The proposed model provided a reliable tool for forensic study. Empirical evaluation of the proposed model vis-a-vis three existing benchmark models was performed to highlight its efficacy. Experiments showed that the specificity and sensitivity of the proposed model were comparable to other models. Due to higher parameters of the ensemble tree model, the classification accuracy was 0.91, with 95% CI - 0.8727, 0.9477. This was better than SVM and Logistic Regression, the two popular models in the literature and comparable to the Random Forest and XGBOOST model. CPU and RAM utilization were also monitored to demonstrate the usefulness of the proposed work for real-world deployment. RAM utilization for the proposed model was higher by 30-45% compared to the other three models. Hence, the proposed model is resource-intensive as it has higher parameters than the other three models. Higher parameters also result in higher accuracy of predictions.},
journal = {Applied Intelligence},
month = jun,
pages = {3824–3843},
numpages = {20},
keywords = {Bitcoin, Fraud detection, Exploratory data analysis}
}

@article{10.1007/s10772-021-09808-0,
author = {Bhangale, Kishor Barasu and Mohanaprasad, K.},
title = {A review on speech processing using machine learning paradigm},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {2},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-021-09808-0},
doi = {10.1007/s10772-021-09808-0},
abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human–machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.},
journal = {Int. J. Speech Technol.},
month = jun,
pages = {367–388},
numpages = {22},
keywords = {Speech processing, Speech recognition, Machine learning, Speech feature extraction, Speech classification, Speech emotion recognition}
}

@article{10.1016/j.ins.2021.01.045,
author = {Li, Hao and Wang, Yongli and Li, Yanchao and Xiao, Gang and Hu, Peng and Zhao, Ruxin and Li, Bo},
title = {Learning adaptive criteria weights for active semi-supervised learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {561},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.01.045},
doi = {10.1016/j.ins.2021.01.045},
journal = {Inf. Sci.},
month = jun,
pages = {286–303},
numpages = {18},
keywords = {Batch mode active learning, Adaptive criteria weights, Submodular function, Semi-supervised classification, Semi-supervised clustering}
}

@article{10.1007/s00607-021-00914-0,
author = {Menegazzo, Jeferson and von Wangenheim, Aldo},
title = {Road surface type classification based on inertial sensors and machine learning: A comparison between classical and deep machine learning approaches for multi-contextual real-world scenarios},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {10},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-00914-0},
doi = {10.1007/s00607-021-00914-0},
abstract = {The demand for several sources of situational data from the traffic environment has intensified in recent years, through the development of applications in intelligent transport systems (ITS), such as autonomous vehicles and advanced driver assistance systems. Among these situational data, the road surface type classification is one of the most important and can be used throughout the ITS domain. However, in order to have a wide application, the development of a safe and reliable model is necessary. Therefore, in addition to the application of safe technology, the model developed must operate correctly in different vehicles, with different driving styles and in different environments in which vehicles can travel to. For this purpose, in this work we collect nine datasets with contextual variations using inertial sensors, represented by accelerometers and gyroscopes. These data were produced in three different vehicles, with three different drivers, in three different environments in which there are three different surface types, in addition to variations in conservation state and presence of obstacles and anomalies, such as speed bumps and potholes. After a pre-processing step, these data were used in 34 different computational models for road surface type classification, employing both Classical Machine Learning and Deep Learning techniques. Through several experiments, we analyze the learning and generalization capacity of each technique. The best model developed was a CNN-based deep neural network, which obtained validation accuracy of 93.17%, classifying surfaces between segments of dirt, cobblestone or asphalt roads.},
journal = {Computing},
month = oct,
pages = {2143–2170},
numpages = {28},
keywords = {Road conditions, Road surface type, Intelligent transport systems, Inertial sensors, Pattern recognition, Machine learning, 68Q07, 68T05, 68T07, 68T10, 68T40}
}

@article{10.1145/3467477,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3467477},
doi = {10.1145/3467477},
abstract = {Evolutionary Computation (EC) approaches are inspired by nature and solve optimization problems in a stochastic manner. They can offer a reliable and effective approach to address complex problems in real-world applications. EC algorithms have recently been used to improve the performance of Machine Learning (ML) models and the quality of their results. Evolutionary approaches can be used in all three parts of ML: preprocessing (e.g., feature selection and resampling), learning (e.g., parameter setting, membership functions, and neural network topology), and postprocessing (e.g., rule optimization, decision tree/support vectors pruning, and ensemble learning). This article investigates the role of EC algorithms in solving different ML challenges. We do not provide a comprehensive review of evolutionary ML approaches here; instead, we discuss how EC algorithms can contribute to ML by addressing conventional challenges of the artificial intelligence and ML communities. We look at the contributions of EC to ML in nine sub-fields: feature selection, resampling, classifiers, neural networks, reinforcement learning, clustering, association rule mining, and ensemble methods. For each category, we discuss evolutionary machine learning in terms of three aspects: problem formulation, search mechanisms, and fitness value computation. We also consider open issues and challenges that should be addressed in future work.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {161},
numpages = {35},
keywords = {Evolutionary computation, learning optimization, swarm intelligence}
}

@inproceedings{10.1007/978-3-030-59430-5_14,
author = {Trinh, Nam H. and O’Brien, Darragh},
title = {Generative Adversarial Network-Based Semi-supervised Learning for Pathological Speech Classification},
year = {2020},
isbn = {978-3-030-59429-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59430-5_14},
doi = {10.1007/978-3-030-59430-5_14},
abstract = {A challenge in applying machine learning algorithms to pathological speech classification is the labelled data shortage problem. Labelled data acquisition often requires significant human effort and time-consuming experimental design. Further, for medical applications, privacy and ethical issues must be addressed where patient data is collected. While labelled data are expensive and scarce, unlabelled data are typically inexpensive and plentiful. In this paper, we propose a semi-supervised learning approach that employs a generative adversarial network to incorporate both labelled and unlabelled data into training. We observe a promising accuracy gain with this approach compared to a baseline convolutional neural network trained only on labelled pathological speech data.},
booktitle = {Statistical Language and Speech Processing: 8th International Conference, SLSP 2020, Cardiff, UK, October 14–16, 2020, Proceedings},
pages = {169–181},
numpages = {13},
keywords = {Semi-supervised learning, Generative adversarial networks, Pathological speech classification},
location = {Cardiff, United Kingdom}
}

@article{10.1007/s11227-019-02805-w,
author = {Choi, Hyunseung and Kim, Mintae and Lee, Gyubok and Kim, Wooju},
title = {Unsupervised learning approach for network intrusion detection system using autoencoders},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-02805-w},
doi = {10.1007/s11227-019-02805-w},
abstract = {Network intrusion detection systems are useful tools that support system administrators in detecting various types of intrusions and play an important role in monitoring and analyzing network traffic. In particular, anomaly detection-based network intrusion detection systems are widely used and are mainly implemented in two ways: (1) a supervised learning approach trained using labeled data and (2) an unsupervised learning approach trained using unlabeled data. Most studies related to intrusion detection systems focus on supervised learning. However, the process of acquiring labeled data is expensive, requiring manual labeling by network experts. Therefore, it is worthwhile investigating the development of unsupervised learning approaches for intrusion detection systems. In this study, we developed a network intrusion detection system using an unsupervised learning algorithm autoencoder and verified its performance. As our results show, our model achieved an accuracy of 91.70%, which outperforms previous studies that achieved 80% accuracy using cluster analysis algorithms. Our results provide a practical guideline for developing network intrusion detection systems based on autoencoders and significantly contribute to the exploration of unsupervised learning techniques for various network intrusion detection systems.},
journal = {J. Supercomput.},
month = sep,
pages = {5597–5621},
numpages = {25},
keywords = {Intrusion detection system, Unsupervised learning, Autoencoder, Anomaly detection, NSL-KDD}
}

@inproceedings{10.1007/978-3-030-61616-8_51,
author = {Lin, Xianghong and Du, Pangao},
title = {Spike-Train Level Unsupervised Learning Algorithm for Deep Spiking Belief Networks},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_51},
doi = {10.1007/978-3-030-61616-8_51},
abstract = {Deep spiking belief network (DSBN) uses unsupervised layer-wise pre-training method to train the network weights, it is stacked with the spike neural machine (SNM) modules. However, the synaptic weights of SNMs are difficult to pre-training through simple and effective approach for spike-train driven networks. This paper proposes a new algorithm that uses unsupervised multi-spike learning rule to train SNMs, which can implement the complex spatio-temporal pattern learning of spike trains. The spike signals first propagate in the forward direction, and then are reconstructed in the reverse direction, and the synaptic weights are adjusted according to the reconstruction error. The algorithm is successfully applied to spike train patterns, the module parameters are analyzed, such as the neuron number and learning rate in the SNMs. In addition, the low reconstruction errors of DSBNs are shown by the experimental results.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {634–645},
numpages = {12},
keywords = {Deep spiking belief networks, Unsupervised learning, Spike neural machines, Reconstruction error},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.patcog.2021.108140,
author = {Liu, Lu and Tan, Robby T.},
title = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108140},
doi = {10.1016/j.patcog.2021.108140},
journal = {Pattern Recogn.},
month = dec,
numpages = {11},
keywords = {Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels}
}

@article{10.1016/j.future.2021.06.036,
author = {Janjua, Faisal and Masood, Asif and Abbas, Haider and Rashid, Imran and Khan, Malik M. Zaki Murtaza},
title = {Textual analysis of traitor-based dataset through semi supervised machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.06.036},
doi = {10.1016/j.future.2021.06.036},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {652–660},
numpages = {9},
keywords = {Malicious emails, Insider threat, Machine learning, Enron dataset, TWOS dataset, Text classification}
}

@article{10.1016/j.compbiolchem.2021.107529,
author = {Druchok, Maksym and Yarish, Dzvenymyra and Garkot, Sofiya and Nikolaienko, Tymofii and Gurbych, Oleksandr},
title = {Ensembling machine learning models to boost molecular affinity prediction},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {1476-9271},
url = {https://doi.org/10.1016/j.compbiolchem.2021.107529},
doi = {10.1016/j.compbiolchem.2021.107529},
journal = {Comput. Biol. Chem.},
month = aug,
numpages = {11},
keywords = {Binding affinity, Human thrombin, Ensembled prediction, Machine learning, Deep neural networks}
}

@inproceedings{10.1145/3461702.3462611,
author = {Perrier, Elija},
title = {Quantum Fair Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462611},
doi = {10.1145/3461702.3462611},
abstract = {In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {843–853},
numpages = {11},
keywords = {fair, learning, machine, quantum},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1609/aaai.v33i01.33014237,
author = {Li, Yu-Feng and Wang, Hai and Wei, Tong and Tu, Wei-Wei},
title = {Towards automated semi-supervised learning},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014237},
doi = {10.1609/aaai.v33i01.33014237},
abstract = {Automated Machine Learning (AutoML) aims to build an appropriate machine learning model for any unseen dataset automatically, i.e., without human intervention. Great efforts have been devoted on AutoML while they typically focus on supervised learning. In many applications, however, semi-supervised learning (SSL) are widespread and current AutoML systems could not well address SSL problems. In this paper, we propose to present an automated learning system for SSL (AUTO-SSL). First, meta-learning with enhanced meta-features is employed to quickly suggest some instantiations of the SSL techniques which are likely to perform quite well. Second, a large margin separation method is proposed to fine-tune the hyperparameters and more importantly, alleviate performance deterioration. The basic idea is that, if a certain hyperparameter owns a high quality, its predictive results on unlabeled data may have a large margin separation. Extensive empirical results over 200 cases demonstrate that our proposal on one side achieves highly competitive or better performance compared to the state-of-the-art AutoML system AUTO-SKLEARN and classical SSL techniques, on the other side unlike classical SSL techniques which often significantly degenerate performance, our proposal seldom suffers from such deficiency.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {520},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.ijinfomgt.2019.07.003,
author = {Lee, Keon Myung and Yoo, Jaesoo and Kim, Sang-Wook and Lee, Jee-Hyong and Hong, Jiman},
title = {Autonomic machine learning platform},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2019.07.003},
doi = {10.1016/j.ijinfomgt.2019.07.003},
journal = {Int. J. Inf. Manag.},
month = dec,
pages = {491–501},
numpages = {11},
keywords = {Autonomic machine learning platform, Autonomic level, Machine learning, Smart City}
}

@article{10.1016/j.ijar.2021.06.003,
author = {Cozman, Fabio Gagliardi and Munhoz, Hugo Neri},
title = {Some thoughts on knowledge-enhanced machine learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.06.003},
doi = {10.1016/j.ijar.2021.06.003},
journal = {Int. J. Approx. Reasoning},
month = sep,
pages = {308–324},
numpages = {17},
keywords = {Knowledge representation, Machine learning}
}

@inproceedings{10.1145/3487923.3487938,
author = {Chindove, Hatitye and Brown, Dane},
title = {Adaptive Machine Learning Based Network Intrusion Detection},
year = {2021},
isbn = {9781450385756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487923.3487938},
doi = {10.1145/3487923.3487938},
abstract = {Network intrusion detection system (NIDS) adoption is essential for mitigating computer network attacks in various scenarios. However, the increasing complexity of computer networks and attacks make it challenging to classify network traffic. Machine learning (ML) techniques in a NIDS can be affected by different scenarios, and thus the recency, size and applicability of datasets are vital factors to consider when selecting and tuning a machine learning classifier. The proposed approach evaluates relatively new datasets constructed such that they depict real-world scenarios. It includes analyses of dataset balancing and sampling, feature engineering and systematic ML-based NIDS model tuning focused on the adaptive improvement of intrusion detection. A comparison between machine learning classifiers forms part of the evaluation process. Results on the proposed approach model effectiveness for NIDS are discussed. Recurrent neural networks and random forests models consistently achieved high f1-score results with macro f1-scores of 0.73 and 0.87 for the CICIDS 2017 dataset; and 0.73 and 0.72 against the CICIDS 2018 dataset, respectively.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Its Applications},
articleno = {15},
numpages = {6},
location = {Virtual Event, Mauritius},
series = {icARTi '21}
}

@article{10.1007/s10586-021-03313-4,
author = {Lin, Frank Yeong-Sung and Hsiao, Chiu-Han and Zhang, Si-Yuan and Rung, Yi-Ping and Chen, Yu-Xuan},
title = {Cross-device matching approaches: word embedding and supervised learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03313-4},
doi = {10.1007/s10586-021-03313-4},
abstract = {Due to the rapid development of diversified technology, people may use multiple electronic devices, such as personal computers, tablets, and smartphones, to connect to the Internet in their daily lives. Switching between devices enables a user to use e-commerce on various platforms. The complexity of consumer behavior is directly proportional to the number of involved devices. Additionally, since the personal privacy regulations nowadays are getting more strict, the user data on the Internet starts to be anonymous. Thus, determining how the devices are related is an indispensable step in achieving precision marketing or developing customized applications. In this research, the dataset provided by the CIKM Cup 2016 Challenge is used. The representation of a device is created by extracting features from browsing logs. The computation cost is reduced by filtering candidates of a target device instead of comparing them in pairs. Latent semantic indexing representations and techniques of supervised learning are used to accomplish filtering. Performing word embedding can turn literature semantic into vectors through an unsupervised neural ensemble. The addition of feature engineering on the input vectors of supervised classification can enhance the classifier’s discrimination. The classification is used to determine the probability of any two instances belonging to the same user. The significant benefit of the implementation is to form the sequences mentioned above by a cross-device linking mechanism to provide a baseline for aligning with the computation limitation and boosting the performance.},
journal = {Cluster Computing},
month = dec,
pages = {3043–3053},
numpages = {11},
keywords = {Cross-device tracking, Latent semantic indexing, Word embedding, Supervised learning}
}

@article{10.1007/s11042-021-11100-x,
author = {Hsu, Chih-Yu and Wang, Shuai and Qiao, Yu},
title = {Intrusion detection by machine learning for multimedia platform},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {19},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-11100-x},
doi = {10.1007/s11042-021-11100-x},
abstract = {The multimedia service company, Netflix, increased the number of new subscribers during the Coronavirus pandemic age. Intrusion detection systems for multimedia platforms can prevent the platform from network attacks. An intelligent intrusion detection system is proposed for the security IP Multimedia Subsystem (IMS) based on machine learning technology. For increasing the accuracy of the classifiers, it is vital to select the critical features to construct the intrusion detection system. Two-class classifiers, including the Decision Tree, Support Vector Machine, and Naive Bayesian, are selected to evaluate intrusion detection accuracy. According to the three classifiers’ accuracy values, the most critical features are selected based on the features’ ranking orders. Six critical features are selected:Service, dst_host_same_srv_rate, Flag, Protocol Type, Dst_host_rerror_rate, and Count. Numerical comparison with state_of_the_art shows that critical features improve intrusion detection accuracy, which can be better than the deep learning method.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {29643–29656},
numpages = {14},
keywords = {Intrusion detection, Support vector machine, Decision tree, Naive Bayesian classifier, Machine learning, Streaming service, Coronavirus pandemic}
}

@article{10.1016/j.adhoc.2021.102667,
author = {Afaq, Amir and Haider, Noman and Baig, Muhammad Zeeshan and Khan, Komal S. and Imran, Muhammad and Razzak, Imran},
title = {Machine learning for 5G security: Architecture, recent advances, and challenges},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2021.102667},
doi = {10.1016/j.adhoc.2021.102667},
journal = {Ad Hoc Netw.},
month = dec,
numpages = {9},
keywords = {5G network security, Threat landscape, Vulnerabilities, Threat intelligence, Machine learning, Federated learning, Threat classification}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.compbiomed.2021.104672,
author = {Ali, Md Mamun and Paul, Bikash Kumar and Ahmed, Kawsar and Bui, Francis M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
title = {Heart disease prediction using supervised machine learning algorithms: Performance analysis and comparison},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104672},
doi = {10.1016/j.compbiomed.2021.104672},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {10},
keywords = {Cardiovascular disease, Machine learning, Random forest, Decision tree, KNN}
}

@article{10.1007/s10489-020-01689-1,
author = {Zhu, Qiu-yu and Li, Tian-tian},
title = {Semi-supervised learning method based on predefined evenly-distributed class centroids},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {9},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01689-1},
doi = {10.1007/s10489-020-01689-1},
abstract = {Compared to supervised learning, semi-supervised learning reduces the dependence of deep learning on a large number of labeled samples. In this work, we use a small number of labeled samples and perform data augmentation on unlabeled samples to achieve image classification. Our method constrains all samples to the predefined evenly-distributed class centroids (PEDCC) by the corresponding loss function. Specifically, the PEDCC-Loss for labeled samples, and the maximum mean discrepancy loss for unlabeled samples are used to make the feature distribution closer to the distribution of PEDCC. Our method ensures that the inter-class distance is large and the intra-class distance is small enough to make the classification boundaries between different classes clearer. Meanwhile, for unlabeled samples, we also use KL divergence to constrain the consistency of the network predictions between unlabeled and augmented samples. Our semi-supervised learning method achieves the state-of-the-art results, with 4000 labeled samples on CIFAR10 and 1000 labeled samples on SVHN, and the accuracy is 95.10% and 97.58% respectively. Code is available in .},
journal = {Applied Intelligence},
month = sep,
pages = {2770–2778},
numpages = {9},
keywords = {Semi–supervised learning, Predefined class centroids, PEDCC-loss, Maximum mean discrepancy, Data augmentation}
}

@article{10.1016/j.cmpb.2021.106220,
author = {Martinez, Oscar and Martinez, Carol and Parra, Carlos A. and Rugeles, Saul and Suarez, Daniel R.},
title = {Machine learning for surgical time prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {208},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106220},
doi = {10.1016/j.cmpb.2021.106220},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {8},
keywords = {Machine learning, Surgical time prediction, Linear regression, Support vector machine, Regression trees, Assembly methods}
}

@article{10.1145/3379499,
author = {Suaboot, Jakapan and Fahad, Adil and Tari, Zahir and Grundy, John and Mahmood, Abdun Naser and Almalawi, Abdulmohsen and Zomaya, Albert Y. and Drira, Khalil},
title = {A Taxonomy of Supervised Learning for IDSs in SCADA Environments},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379499},
doi = {10.1145/3379499},
abstract = {Supervisory Control and Data Acquisition (SCADA) systems play an important role in monitoring industrial processes such as electric power distribution, transport systems, water distribution, and wastewater collection systems. Such systems require a particular attention with regards to security aspects, as they deal with critical infrastructures that are crucial to organizations and countries. Protecting SCADA systems from intrusion is a very challenging task because they do not only inherit traditional IT security threats but they also include additional vulnerabilities related to field components (e.g., cyber-physical attacks). Many of the existing intrusion detection techniques rely on supervised learning that consists of algorithms that are first trained with reference inputs to learn specific information, and then tested on unseen inputs for classification purposes. This article surveys supervised learning from a specific security angle, namely SCADA-based intrusion detection. Based on a systematic review process, existing literature is categorized and evaluated according to SCADA-specific requirements. Additionally, this survey reports on well-known SCADA datasets and testbeds used with machine learning methods. Finally, we present key challenges and our recommendations for using specific supervised methods for SCADA systems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {40},
numpages = {37},
keywords = {SCADA security, machine learning, network intrusion, supervised learning}
}

@inproceedings{10.1007/978-3-030-86514-6_25,
author = {Forman, George},
title = {Getting Your Package to the Right Place: Supervised Machine Learning for Geolocation},
year = {2021},
isbn = {978-3-030-86513-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86514-6_25},
doi = {10.1007/978-3-030-86514-6_25},
abstract = {Amazon Last Mile strives to learn an accurate delivery point for each address by using the noisy GPS locations reported from past deliveries. Centroids and other center-finding methods do not serve well, because the noise is consistently biased. The problem calls for supervised machine learning, but how? We addressed it with a novel adaptation of learning to rank from the information retrieval domain. This also enabled information fusion from map layers. Offline experiments show outstanding reduction in error distance, and online experiments estimated millions in annualized savings.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part IV},
pages = {403–419},
numpages = {17},
keywords = {Learning to rank, Geospatial supervised learning},
location = {Bilbao, Spain}
}

@inproceedings{10.1007/978-3-030-90370-1_7,
author = {Jin, Kun and Yin, Tongxin and Kamhoua, Charles A. and Liu, Mingyan},
title = {Network Games with Strategic Machine Learning},
year = {2021},
isbn = {978-3-030-90369-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90370-1_7},
doi = {10.1007/978-3-030-90370-1_7},
abstract = {In this paper, we study the strategic machine learning problem with a planner (decision maker) and multiple agents. The planner is the first-mover, who designs, publishes, and commits to a decision rule. The agents then best-respond by manipulating their input features to obtain a desirable decision outcome so as to maximize their utilities. Earlier works in strategic machine learning assume that every agent’s strategic action is independent of others’. By contrast, we consider a different case where agents are connected in a network and can either benefit from their neighbors’ positive decision outcomes from the planner or benefit from their neighbors’ actions. We study the Stackelberg equilibrium in this new setting and highlight the similarities and differences between this model and the literature on network/graphical games and strategic machine learning.},
booktitle = {Decision and Game Theory for Security: 12th International Conference, GameSec 2021, Virtual Event, October 25–27, 2021, Proceedings},
pages = {118–137},
numpages = {20},
keywords = {Stackelberg game, Strategic machine learning, Mechanism design}
}

@inproceedings{10.1007/978-3-030-61377-8_24,
author = {Mello, Claudio D. and Messias, Lucas R. V. and Drews-Jr, Paulo Lilles Jorge and Botelho, Silvia S. C.},
title = {Unsupervised Learning Method for Encoder-Decoder-Based Image Restoration},
year = {2020},
isbn = {978-3-030-61376-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61377-8_24},
doi = {10.1007/978-3-030-61377-8_24},
abstract = {The restoration of a corrupted image is a challenge to computer vision and image processing. In hazy, underwater and medical images, the lack of paired images lead the state of the art to synthesize datasets. The Generative Adversarial Networks (GANs) are widely used in these cases. However, computational cost and training instability are current concerns. We present an unsupervised learning algorithm that does not requires paired dataset to train encoder-decoder-like neural network for image restoration. An encoder-decoder learn to represent its input data in a latent representation and reconstruct then in the output. During the training stage, our algorithm applies the encoder-decoder output image to a degradation block that reinforces its degradation. The degraded and input images are matched using a loss function. After the training process, we obtain a restored image from the decoder. We used ill-exposed images to evaluate and validate our algorithm.},
booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
pages = {348–360},
numpages = {13},
keywords = {Unsupervised learning, Image restoration, Neural network},
location = {Rio Grande, Brazil}
}

@article{10.1016/j.procs.2021.09.044,
author = {Magboo, Vincent Peter C. and Magboo, Ma. Sheila A.},
title = {Machine Learning Classifiers on Breast Cancer Recurrences},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.044},
doi = {10.1016/j.procs.2021.09.044},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {2742–2752},
numpages = {11},
keywords = {breast cancer, data mining, machine learning, Na\"{\i}ve Vayes, Logistic Regression, K-Nearest Neighbors}
}

@inproceedings{10.1145/3459637.3481952,
author = {Yao, Tiansheng and Yi, Xinyang and Cheng, Derek Zhiyuan and Yu, Felix and Chen, Ting and Menon, Aditya and Hong, Lichan and Chi, Ed H. and Tjoa, Steve and Kang, Jieqi (Jay) and Ettinger, Evan},
title = {Self-supervised Learning for Large-scale Item Recommendations},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481952},
doi = {10.1145/3459637.3481952},
abstract = {Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse. Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4321–4330},
numpages = {10},
keywords = {contrastive learning, neural networks, recommender systems, self-supervised learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1016/j.imavis.2019.06.011,
author = {Conze, Pierre-Henri and Tilquin, Florian and Lamard, Mathieu and Heitz, Fabrice and Quellec, Gwenol\'{e}},
title = {Unsupervised learning-based long-term superpixel tracking},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {89},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2019.06.011},
doi = {10.1016/j.imavis.2019.06.011},
journal = {Image Vision Comput.},
month = sep,
pages = {289–301},
numpages = {13},
keywords = {Superpixel matching, Unsupervised learning, Superpixel tracking, Multi-step integration, Random forests, Forward-backward consistency}
}

@article{10.1007/s00521-020-05030-2,
author = {Punitha, V. and Mala, C.},
title = {Traffic classification in server farm using supervised learning techniques},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05030-2},
doi = {10.1007/s00521-020-05030-2},
abstract = {Server farms used in web hosting and commercial applications connect multiple servers. Edge computing being a realm of cloud technology is orchestrated with server farms to enhance network efficiency. Edge computing increases the availability of cloud resources and Internet services. The higher availability of services and their ease of access deeply affect the user’s requesting behavior. The anomalous requesting behavior is creating malicious traffic, and enormous amount of such traffics at server farm denies the services to the legitimate users. Categorizing the incoming traffic into malicious and non-malicious traffic at server farm is the foremost criteria to eliminate the attacks, which in turn improves the QoS of the server farm. In the light of preventing the biased usage of the server farm, this paper proposes a SVM classifier based on requesting statistics. The proposed classifier discovers the attacks that deny services to legitimate users in two levels, based on the user’s request behavior. The pattern of arrival, its statistical characteristics and security misbehaviors are investigated at both levels. An incremental learning algorithm is proposed to enhance the learning plasticity of the proposed classifier. The experimental results illustrate that the performance of the proposed two-level classifier with respect to classification accuracy is competently improved with incremental learning.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1279–1296},
numpages = {18},
keywords = {Traffic classification, Denial of service, Support vector machine, Machine learning}
}

@inproceedings{10.1007/978-3-030-30487-4_30,
author = {Genkin, Alexander and Sengupta, Anirvan M. and Chklovskii, Dmitri},
title = {A Neural Network for Semi-supervised Learning on Manifolds},
year = {2019},
isbn = {978-3-030-30486-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30487-4_30},
doi = {10.1007/978-3-030-30487-4_30},
abstract = {Semi-supervised learning algorithms typically construct a weighted graph of data points to represent a manifold. However, an explicit graph representation is problematic for neural networks operating in the online setting. Here, we propose a feed-forward neural network capable of semi-supervised learning on manifolds without using an explicit graph representation. Our algorithm uses channels that represent localities on the manifold such that correlations between channels represent manifold structure. The proposed neural network has two layers. The first layer learns to build a representation of low-dimensional manifolds in the input data as proposed recently in [8]. The second learns to classify data using both occasional supervision and similarity of the manifold representation of the data. The channel carrying label information for the second layer is assumed to be “silent” most of the time. Learning in both layers is Hebbian, making our network design biologically plausible. We experimentally demonstrate the effect of semi-supervised learning on non-trivial manifolds.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Theoretical Neural Computation: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part I},
pages = {375–386},
numpages = {12},
keywords = {Semi-supervised learning, Online learning, Manifold learning},
location = {Munich, Germany}
}

@article{10.1007/s11265-021-01656-0,
author = {Zhou, Feng and Qu, Hua and Liu, Hailong and Liu, Hong and Li, Bo},
title = {Fingerprinting IIoT Devices Through Machine Learning Techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {7},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01656-0},
doi = {10.1007/s11265-021-01656-0},
abstract = {From a security perspective, identifying Industrial Internet of Things (IIoT) devices connected to a network has multiple applications such as penetration testing, vulnerability assessment, etc. In this work, we propose a feature-based methodology to perform device-type fingerprinting. A device fingerprint consists of the TCP/IP header features and port-based features extracted from the network traffic of the device. These features are collected by a hybrid mechanism which has a negligible impact on device functionality and can avoid the problem of the long TCP connection. Once the fingerprint of a device is generated, it will be fed to the classifiers based on Gradient Boosting to predict its type details. Based on our proposed method, we implement a prototype application called IIoT Device Type Fingerprinting (IDTF) which capable of automatically identifying the types of devices being connected to an IIoT network. We collect a dataset consisting of 19,174 fingerprints from real-world Internet-facing IIoT devices indexed by Shodan to train and evaluate the classifiers using ten-fold cross-validation. And we conduct comparative experiments in an IIoT testbed to compare the effectiveness of IDTF with two famous fingerprinting tools. The experimental result shows that the ability of our approach is confirmed by a high mean F-Measure of 95.76%. It also demonstrates that IDTF achieves the highest identification rate in the testbed and is non-intrusive for IIoT devices. Compared with existing works, our approach is more generic as it does not rely on a specific protocol or deep packet inspection and can distinguish almost all IIoT device-types.},
journal = {J. Signal Process. Syst.},
month = jul,
pages = {779–794},
numpages = {16},
keywords = {Industrial Internet of Things (IIoT), Device-type fingerprinting, Machine learning}
}

@article{10.1007/s10676-021-09608-9,
author = {Scantamburlo, Teresa},
title = {Non-empirical problems in fair machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-021-09608-9},
doi = {10.1007/s10676-021-09608-9},
abstract = {The problem of fair machine learning has drawn much attention over the last few years and the bulk of offered solutions are, in principle, empirical. However, algorithmic fairness also raises important conceptual issues that would fail to be addressed if one relies entirely on empirical considerations. Herein, I will argue that the current debate has developed an empirical framework that has brought important contributions to the development of algorithmic decision-making, such as new techniques to discover and prevent discrimination, additional assessment criteria, and analyses of the interaction between fairness and predictive accuracy. However, the same framework has also suggested higher-order issues regarding the translation of fairness into metrics and quantifiable trade-offs. Although the (empirical) tools which have been developed so far are essential to address discrimination encoded in data and algorithms, their integration into society elicits key (conceptual) questions such as: What kind of assumptions and decisions underlies the empirical framework? How do the results of the empirical approach penetrate public debate? What kind of reflection and deliberation should stakeholders have over available fairness metrics? I will outline the empirical approach to fair machine learning, i.e. how the problem is framed and addressed, and suggest that there are important non-empirical issues that should be tackled. While this work will focus on the problem of algorithmic fairness, the lesson can extend to other conceptual problems in the analysis of algorithmic decision-making such as privacy and explainability.},
journal = {Ethics and Inf. Technol.},
month = dec,
pages = {703–712},
numpages = {10},
keywords = {Machine learning, Fairness, Empirical approach, Assessment of machine learning}
}

@article{10.14778/3476311.3476405,
author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
title = {Machine learning for databases},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476405},
doi = {10.14778/3476311.3476405},
abstract = {Machine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously selecting optimization strategy. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and prediction problems (e.g., query workload prediction). We review existing machine learning based techniques to address these problems and provide research challenges.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3190–3193},
numpages = {4}
}

@inproceedings{10.1007/978-3-030-91669-5_13,
author = {Liu, Tingyi and Iwaihara, Mizuho},
title = {Supervised Learning of Keyphrase Extraction Utilizing Prior Summarization},
year = {2021},
isbn = {978-3-030-91668-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91669-5_13},
doi = {10.1007/978-3-030-91669-5_13},
abstract = {Keyphrase extraction is the task of selecting a set of phrases that can best represent a given document. Keyphrase extraction is utilized in document indexing and categorization, thus being one of core technologies of digital libraries. Supervised keyphrase extraction based on pretrained language models are advantageous thorough their contextualized text representations. In this paper, we show an adaptation of the pertained language model BERT to keyphrase extraction, called BERT Keyphrase-Rank (BK-Rank), based on a cross-encoder architecture. However, the accuracy of BK-Rank alone is suffering when documents contain a large amount of candidate phrases, especially in long documents. Based on the notion that keyphrases are more likely to occur in representative sentences of the document, we propose a new approach called Keyphrase-Focused BERT Summarization (KFBS), which extracts important sentences as a summary, from which BK-Rank can more easily find keyphrases. Training of KFBS is by distant supervision such that sentences lexically similar to the keyphrase set are chosen as positive samples. Our experimental results show that the combination of KFBS + BK-Rank show superior performance over the compared baseline methods on well-known four benchmark collections, especially on long documents.},
booktitle = {Towards Open and Trustworthy Digital Societies: 23rd International Conference on Asia-Pacific Digital Libraries, ICADL 2021, Virtual Event, December 1–3, 2021, Proceedings},
pages = {157–166},
numpages = {10},
keywords = {Keyphrase extraction, Supervised learning, Pretrained language model, Extractive summarization, Document indexing}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@article{10.3233/THC-202237,
author = {Choudhury, Avishek},
title = {Predicting cancer using supervised machine learning: Mesothelioma},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-202237},
doi = {10.3233/THC-202237},
journal = {Technol. Health Care},
month = jan,
pages = {45–58},
numpages = {14},
keywords = {Mesothelioma, predictive modeling, decision support system, machine learning, artificial intelligence, lung cancer}
}

@inproceedings{10.1007/978-3-030-86517-7_4,
author = {Wendlinger, Lorenz and Berndl, Emanuel and Granitzer, Michael},
title = {Methods for Automatic Machine-Learning Workflow Analysis},
year = {2021},
isbn = {978-3-030-86516-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86517-7_4},
doi = {10.1007/978-3-030-86517-7_4},
abstract = {Developing real-world Machine Learning-based Systems goes beyond algorithm development. ML algorithms are usually embedded in complex pre-processing steps and consider different stages like development, testing or deployment. Managing workflows poses several challenges, such as workflow versioning, sharing pipeline elements or optimizing individual workflow elements - tasks which are usually conducted manually by data scientists. A dataset containing 16 035 real-world Machine Learning and Data Science Workflows extracted from the ONE DATA platform () is explored and made available. Based on our analysis, we develop a representation learning algorithm using a graph-level Graph Convolutional Network with explicit residuals which exploits workflow versioning history. Moreover, this method can easily be adapted to supervised tasks and outperforms state-of-the-art approaches in NAS-bench-101 performance prediction. Another interesting application is the suggestion of component types, for which a classification baseline is presented. A slightly adapted GCN using both graph- and node-level information further improves upon this baseline. The used codebase as well as all experimental setups with results are available at .},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part V},
pages = {52–67},
numpages = {16},
keywords = {Graph neural networks, Structured prediction, Neural Architecture Search},
location = {Bilbao, Spain}
}

@inbook{10.1145/3447404.3447414,
author = {Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros},
title = {Machine Learning Basics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447414},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {143–193},
numpages = {51}
}

@article{10.1016/j.eswa.2021.115782,
author = {Alhajjar, Elie and Maxwell, Paul and Bastian, Nathaniel},
title = {Adversarial machine learning in Network Intrusion Detection Systems},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115782},
doi = {10.1016/j.eswa.2021.115782},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {13},
keywords = {Network Intrusion Detection Systems, Adversarial machine learning, Evolutionary computation, Deep learning, Monte Carlo simulation}
}

@article{10.1016/j.eswa.2021.115656,
author = {Espinheira, Patr\'{\i}cia L. and Silva, Luana C.M. and Cribari-Neto, Francisco},
title = {Bias and variance residuals for machine learning nonlinear simplex regressions},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115656},
doi = {10.1016/j.eswa.2021.115656},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {13},
keywords = {Double bounded variable, Machine learning, Outlier, Simplex regression}
}

@inproceedings{10.1145/3461778.3462163,
author = {Scurto, Hugo and Caramiaux, Baptiste and Bevilacqua, Frederic},
title = {Prototyping Machine Learning Through Diffractive Art Practice},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462163},
doi = {10.1145/3461778.3462163},
abstract = {In this paper, we outline a diffractive practice of machine learning (ML) in the frame of material-centered interaction design. To this aim, we review related work in ML, HCI, design, new interfaces for musical expression, and computational art, and introduce two practice-based studies of music performance and robotic art based on interactive machine learning tools, with the hope of revealing the computational materiality of ML, and the potential of embodiment to craft prototypes of ML that reconfigure conceptual or technical approaches to ML. We derive five interference conditions for such art-based ML prototypes—situational whole, small data, shallow model, learnable algorithm, and somaesthetic behaviour—and describe their widening of design and engineering practices of ML prototyping. Finally, we sketch how a process of intra-active machine learning could complement that of interactive machine learning to take materiality as an entry point for ML design within HCI.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {2013–2025},
numpages = {13},
keywords = {Art Practice, Design, Diffractive Methods., Machine Learning},
location = {Virtual Event, USA},
series = {DIS '21}
}

@article{10.1007/s00034-019-01173-3,
author = {Geng, Mingyang and Shang, Suning and Ding, Bo and Wang, Huaimin and Zhang, Pengfei},
title = {Unsupervised Learning-Based Depth Estimation-Aided Visual SLAM Approach},
year = {2020},
issue_date = {Feb 2020},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {39},
number = {2},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-019-01173-3},
doi = {10.1007/s00034-019-01173-3},
abstract = {Simultaneous localization and map construction (SLAM) tasks have been proven to benefit greatly from the depth information of the environment. In this paper, we first present an unsupervised end-to-end learning framework for the task of monocular depth and camera motion estimation from video sequences. The difference between our work and the existing unsupervised methods is that we not only use image reconstruction for supervising but also exploit the pose estimation method used in traditional SLAM approaches to enhance the supervised signal and add extra training constraints for the task of monocular depth and camera motion estimation. Furthermore, we successfully exploit our unsupervised learning framework to assist the traditional ORB-SLAM system when the initialization module of ORB-SLAM method could not match enough features. Qualitative and quantitative experiments have shown that our unsupervised learning framework performs the depth estimation task superior to the supervised methods and outperforms the previous state-of-the-art unsupervised approach by 13.5% on KITTI dataset. For the pose estimation task, our method performs comparably to the supervised methods that use ground-truth pose data for training. Besides, our unsupervised learning framework can significantly accelerate the initialization process of the traditional ORB-SLAM system and effectively improve the accuracy of environmental mapping in strong lighting and weak texture scenes.},
journal = {Circuits Syst. Signal Process.},
month = feb,
pages = {543–570},
numpages = {28},
keywords = {Monocular depth estimation, Pose estimation, Unsupervised learning, Visual SLAM system}
}

@inproceedings{10.1007/978-3-030-59719-1_31,
author = {Song, Youyi and Zhou, Teng and Teoh, Jeremy Yuen-Chun and Zhang, Jing and Qin, Jing},
title = {Unsupervised Learning for CT Image Segmentation via Adversarial Redrawing},
year = {2020},
isbn = {978-3-030-59718-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59719-1_31},
doi = {10.1007/978-3-030-59719-1_31},
abstract = {We propose a novel adversarial learning framework for unsupervised training of CNNs in CT image segmentation. It is motivated by difficulties in collecting voxel-wise annotations, which is laborious, time-consuming and expensive. It is conceptually simple, allowing us to train an effective segmentation network without any human annotation. Specifically, we design the generator with a CNN producing the segmentation results and a decoder redrawing the CT volume based on the segmentation results. The CNN is then implicitly trained in the adversarial learning framework where a discriminator gradually enforcing the generator to generate CT volumes whose distribution well matches the distribution of the training data. We further propose two constrains as regularization schemes for the training procedure to drive the model towards optimal segmentation by avoiding some unreasonable results. We conducted extensive experiments to evaluate the proposed method on a famous publicly available dataset, and the experimental results demonstrate the effectiveness of the proposed method.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part IV},
pages = {309–320},
numpages = {12},
keywords = {Unsupervised learning, Adversarial redrawing, CNNs, CT image segmentation.},
location = {Lima, Peru}
}

@phdthesis{10.5555/AAI28717041,
author = {Zhu, Pengkai and Brian, Kulis, and Kate, Saenko, and Francesco, Orabona,},
advisor = {Venkatesh, Saligrama,},
title = {Machine Learning Under Limited Resources},
year = {2021},
isbn = {9798460478361},
publisher = {Boston University},
address = {USA},
abstract = {Deep learning methods have led to substantial improvement of performance in many computer vision applications. However, these methods require massive resources, including data collection, label annotation, and computation, which may be insufficient in real-world applications. The constraints of resources limit the deployment of powerful deep models, resulting in degraded performance. Therefore, research topics that address resource limitations, such as few-shot/zero-shot recognition, have recently drawn attention. In this thesis, we develop machine learning methods that reduce the requirement of resources while keeping the prediction accuracy on par with resource-rich models. Specifically, we consider three different settings: label-limited recognition, zero-shot detection, and reducing energy consumption for IoT systems at inference time.We first propose a novel image encoding method that decomposes an image into a few semantic parts and represents each part in a compact vocabulary of a few concepts. Because the concepts learned by our model generalize well to novel objects, this encoding shows competent results in label-limited classification tasks like few-shot/zero-shot recognition and unsupervised domain adaptation. The encoding also demonstrates extraordinary robustness to adversarial image perturbations, and we found the encoding is interpretable by humans through crowd-sourcing evaluations. Next, we propose a statistical model that represents the structural information of an object. Each object is described by the part location and location-independent signatures. They form a latent space on which a structural constraint is imposed. At inference time, the model produces the representations that maximize the posterior probability. We show that the new representation can achieve state-of-the-art performance for few-shot recognition on benchmark datasets.We then study the problem of zero-shot detection. We propose an evaluation protocol and develop two algorithms to address the problem. One algorithm seamlessly integrates semantic attribute predictions into visual features to produce bounding boxes with visual and semantic information. In the second algorithm, we take an approach of data augmentation. First, a conditional variational auto-encoder is employed to produce synthetic features for unseen classes by leveraging the semantic attributes. The confidence predictor is then trained on the real data along with the synthetic features to predict higher confidence scores for unseen objects. Both algorithms show significant improvement in the detection of unseen objects through empirical evaluations on complex datasets.Finally, we present a novel learning framework that associates each edge device in an IoT system with a gating function. The gating function can stop the device from transmitting redundant features to the central inference model for some instances at inference time. This framework can significantly reduce the energy cost by reducing the transmission counts with negligible accuracy degradation in our evaluations on real-world datasets.},
note = {AAI28717041}
}

@article{10.1016/j.compeleceng.2021.107527,
author = {Mohammad, Abdul Salam and Pradhan, Manas Ranjan},
title = {Machine learning with big data analytics for cloud security},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {PA},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107527},
doi = {10.1016/j.compeleceng.2021.107527},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {15},
keywords = {Big data, Cloud computing, Cloud security, Data security, Data management, Data storage, Machine learning}
}

@article{10.1016/j.patcog.2018.11.006,
author = {Li, Yanchao and Wang, Yongli and Liu, Qi and Bi, Cheng and Jiang, Xiaohui and Sun, Shurong},
title = {Incremental semi-supervised learning on streaming data},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.006},
doi = {10.1016/j.patcog.2018.11.006},
journal = {Pattern Recogn.},
month = apr,
pages = {383–396},
numpages = {14},
keywords = {Semi-supervised learning, Dynamic feature learning, Streaming data, Classification}
}

@article{10.1016/j.ins.2018.12.057,
author = {Li, Yang and Pan, Quan and Wang, Suhang and Peng, Haiyun and Yang, Tao and Cambria, Erik},
title = {Disentangled Variational Auto-Encoder for semi-supervised learning},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {482},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.12.057},
doi = {10.1016/j.ins.2018.12.057},
journal = {Inf. Sci.},
month = may,
pages = {73–85},
numpages = {13},
keywords = {Semi-supervised learning, Variational Auto-encoder, Disentangled representation, Neural networks}
}

@inproceedings{10.1145/3473938.3474509,
author = {Liu, Che-Yu and Chen, Xiaoliang and Proietti, Roberto and Li, Zhaohui and Yoo, S. J. Ben},
title = {Reconfigurable Optical Datacom Networks by Self-supervised Learning},
year = {2021},
isbn = {9781450386500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473938.3474509},
doi = {10.1145/3473938.3474509},
abstract = {This paper presents a self-supervised machine learning approach for cognitive reconfiguration in a Hyper-X-like flexible-bandwidth optical interconnect architecture. The proposed approach makes use of a clustering algorithm to learn the traffic patterns from historical traces. A heuristic algorithm is developed for optimizing the connectivity graph for each identified traffic pattern. Further, to mitigate the scalability issue induced by frequent clustering operations, we parameterize the learned traffic patterns by a deep neural network classifier. The classifier is trained offline by supervised learning to enable classification of traffic matrices during online operations, thereby facilitating cognitive reconfiguration decision making. Simulation results show that compared with a static all-to-all interconnection, the proposed approach can improve throughput by up to 1.76\texttimes{} while reducing end-to-end packet latency and flow completion time by up to 2.8\texttimes{} and 25\texttimes{}, respectively.},
booktitle = {Proceedings of the ACM SIGCOMM 2021 Workshop on Optical Systems},
pages = {23–27},
numpages = {5},
keywords = {Clustering, Deep neural network, Flexible bandwidth optical interconnect, Reconfigurable datacom networks},
location = {Virtual Event, USA},
series = {OptSys '21}
}

@phdthesis{10.5555/AAI27833863,
author = {Nguyen, Thanh Van and Wong, Raymond and Liu, Jia and Sarkar, Soumik and Vaswani, Namara and Wang, Zhengdao},
advisor = {Chinmay, Hegde,},
title = {Provable Surrogate Gradient-Based Optimization for Unsupervised Learning},
year = {2020},
isbn = {9798662371071},
publisher = {Iowa State University},
address = {USA},
abstract = {Gradient-based optimization lies at the core of modern machine learning and deep learning, with (stochastic) gradient descent algorithms being employed as the main workhorse. The unprecedented success of deep learning over the last decade has arguably been tied with the popularity and mysterious success of these algorithms --- particularly for supervised learning.This success is despite the fact that objective functions in deep learning are extremely non-linear and non-convex.The past few years have witnessed great theoretical advances in analyzing optimization and inductive biases of gradient descent for supervised learning. However, the majority of existing work only applies to settings such as classification and regression. In contrast, the role of gradient descent in the unsupervised setting has gained far less attention. In this work, we make concrete contributions to the understanding of gradient-based optimization in unsupervised learning.We start with dictionary learning, an unsupervised feature learning mechanism widely used in signal processing and machine learning. The primary goal of dictionary learning is to learn sparse, linear representations of the data by minimizing the reconstruction loss. In this problem, the objective function is coupled with an intractable sparse coding step due to the latent representations. Therefore, the gradient of the loss with respect to the model parameters can not be obtained exactly but is only a noisy estimate of the true gradient. However, gradient-based alternating minimization for dictionary learning works surprisingly well in practice while theoretical understanding of the success has lagged behind. We will refer to this method as "surrogate" gradient-based optimization.In Chapter 1 and Chapter 2, we introduce two surrogate gradient descent algorithms for sparse coding. The first algorithm learns a double-sparsity model where the dictionary is the product of a fixed, known basis and a learnable sparse component. The second algorithm provably learns a dictionary from samples with missing entries. In each case, we provide a spectral initialization subroutine that gives a coarse estimate of the true dictionary. Then, starting from this estimate, we prove that the surrogate descent algorithm linearly converges to the true dictionary. We analyze the algorithm and demonstrate superior sample complexity and computational complexity bounds over existing provable approaches.While sparse coding is still widely used, its computational cost is prohibitive for high-dimensional data. Autoencoders have instead emerged as an efficient and flexible alternative for feature learning using neural networks. In Chapter 3, we build upon our theory of surrogate gradient developed in the previous chapters to provide a series of results for autoencoder learning. For several generative models of data, we prove that when trained with gradient descent, two-layer weight-tied autoencoders can successfully recover the ground-truth parameters of the corresponding models. Our analysis establishes theoretical evidence that shallow autoencoder modules can indeed be powerful feature learning mechanisms for a variety of data models. In Chapter 4, we go beyond the local analysis in Chapter 3 and analyze the gradient dynamics of over-parameterized autoencoders. Under a few mild assumptions about the given training dataset, we rigorously prove the linear convergence of gradient descent for randomly initialized autoencoder networks. Our analysis mirrors the recent advances in the emerging theory of neural tangent kernels.Chapter 5 considers a black-box optimization problem where the objective and constraints are specified as solutions to expensive PDE solvers. We pose this optimization as sampling from a Gibbs distribution with a black-box energy function and perform Langevin sampling by using surrogate gradients of the black-box functions learned by deep neural networks. We prove the convergence of the surrogate Langevin dynamics when the target distribution is log-concave and smooth. Finally, in Chapter 6, we lay out several potential directions that merge two lines of research in gradient flow analysis and Langevin dynamics, as well as inverse problems with generative priors.},
note = {AAI27833863}
}

@article{10.1007/s00245-019-09637-3,
author = {Calder, Jeff and Slep\v{c}ev, Dejan},
title = {Properly-Weighted Graph Laplacian for Semi-supervised Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {82},
number = {3},
issn = {0095-4616},
url = {https://doi.org/10.1007/s00245-019-09637-3},
doi = {10.1007/s00245-019-09637-3},
abstract = {The performance of traditional graph Laplacian methods for semi-supervised learning degrades substantially as the ratio of labeled to unlabeled data decreases, due to a degeneracy in the graph Laplacian. Several approaches have been proposed recently to address this, however we show that some of them remain ill-posed in the large-data limit. In this paper, we show a way to correctly set the weights in Laplacian regularization so that the estimator remains well posed and stable in the large-sample limit. We prove that our semi-supervised learning algorithm converges, in the infinite sample size limit, to the smooth solution of a continuum variational problem that attains the labeled values continuously. Our method is fast and easy to implement.},
journal = {Appl. Math. Optim.},
month = dec,
pages = {1111–1159},
numpages = {49},
keywords = {Semi-supervised learning, Label propagation, Asymptotic consistency, PDEs on graphs, Gamma-convergence, 49J55, 35J20, 35B65, 62G20, 65N12}
}

@phdthesis{10.5555/AAI28262460,
author = {Lavania, Chandrashekhar and Kannan, Sreeram and Hajishirzi, Hannaneh},
advisor = {Jeffrey, Bilmes,},
title = {Towards Unsupervised Learning of Submodular Functions for Summarization},
year = {2020},
isbn = {9798569995592},
publisher = {University of Washington},
abstract = {In the information age, vast volumes of data are generated daily. There exist a plethora of data sources, including text, videos, and sensor networks. The large size of data can make it difficult to process. Furthermore, the generated data often has considerable redundancy. Therefore, extracting meaningful information from the data can make it easier to process for downstream tasks. Summarization is one way to extract this information. In the past, submodular functions have been successfully used for summarizing data. These functions can be defined based on domain knowledge or can be learned from the data itself. However, supervised learning of submodular functions faces an obstacle as there is often a lack of known good summaries of the data for training. Therefore, it would be beneficial if the functions can be learned in an unsupervised manner. This work proposes an approach towards learning a mixture of submodular functions in an unsupervised manner. It is achieved through a two-part process. First, an autoencoder neural network is trained in a constrained manner. The aim is to produce features such that a larger feature value implies that the input data sample has a larger amount of the corresponding learned property.  It is analogous to bag-of-words features, with the ``words''  learned automatically.   Next, a mixture of submodular functions is instantiated using the learned features. Each component of the mixture consists of a concave composed with a modular function. The mixture weights are learned through an approach that does not directly utilize supervised summary information.  It optimizes a set of meta-objectives, each of which corresponds to a likely necessary condition on what constitutes a good summarization objective.  Empirical results on different data modalities show that the proposed two-part process produces functions that perform significantly better than a variety of baseline methods.This work also explores other applications of the proposed features. The focus application is the summarization of video streams on the fly under a memory budget. The aim is to produce running summaries (within a budget) of incoming video streams at each time step. Any video snippet that is not part of a summary at a given time step is dropped to abide by the memory constraints. To this end, this work proposes two algorithms, one each for single-stream and multi-stream summarization. Empirical evaluations demonstrate that the algorithms, instantiated with the proposed features, can outperform the baselines.In addition to these explorations, it is also shown that the proposed constrained training can be used with different flavors of autoencoder architectures and losses. The influence of these different setups is also demonstrated for the task of summarization.},
note = {AAI28262460}
}

@article{10.1016/j.cosrev.2021.100370,
author = {Garg, Arunim and Mago, Vijay},
title = {Role of machine learning in medical research: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100370},
doi = {10.1016/j.cosrev.2021.100370},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {17},
keywords = {00-01, 99-00, Medical research, Machine learning, Deep learning, Medical data}
}

@article{10.1016/j.neucom.2019.09.039,
author = {Chen, Chuangquan and Gan, Yanfen and Vong, Chi-Man},
title = {Extreme semi-supervised learning for multiclass classification},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {376},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.09.039},
doi = {10.1016/j.neucom.2019.09.039},
journal = {Neurocomput.},
month = feb,
pages = {103–118},
numpages = {16},
keywords = {Multiclass classification, Semi-supervised support vector machine, Extreme learning machine, Approximate empirical kernel map, Alternating optimization}
}

@inproceedings{10.1145/3397271.3401128,
author = {Tzaban, Hen and Guy, Ido and Greenstein-Messica, Asnat and Dagan, Arnon and Rokach, Lior and Shapira, Bracha},
title = {Product Bundle Identification using Semi-Supervised Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401128},
doi = {10.1145/3397271.3401128},
abstract = {Many sellers on e-commerce platforms offer buyers product bundles, which package together two or more different items. The identification of such bundles is a necessary step to support a variety of related services, from recommendation to dynamic pricing. In this work, we present a comprehensive study of bundle identification on a large e-commerce website. Our analysis of bundle compared to non-bundle listed items reveals several key differentiating characteristics, spanning the listing's title, image, and attributes. Following, we experiment with a multi-modal classifier, which takes advantage of these characteristics as features. Our analysis also shows that a bundle indicator input by sellers tends to be highly noisy and carries only a weak signal. The bundle identification task therefore faces the challenge of having a small set of manually-labeled clean examples and a larger set of noisy-labeled examples, in conjunction with class imbalance due to the relative scarcity of bundles.Our experiments with basic supervised classifiers, using the manually-labeled and/or the noisy-labeled data for training, demonstrates only moderate performance. We therefore turn to a semisupervised approach and propose GREED, a self-training ensemblebased algorithm with a greedy model selection. Our evaluation over two different meta-categories shows a superior performance of semi-supervised approaches for the bundle identification task, with GREED outperforming several semi-supervised alternatives. The combination of textual, image, and some metadata features is shown to yield the best performance, reaching an AUC of 0.89 and 0.92 for the two meta-categories, respectively},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {791–800},
numpages = {10},
keywords = {electronic commerce, ensemble learning, product bundling, self-training, semi-supervised learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3453444,
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453444},
doi = {10.1145/3453444},
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {111},
numpages = {39},
keywords = {Machine learning lifecycle, assurance, assurance evidence, machine learning workflow, safety-critical systems}
}

@inproceedings{10.1007/978-3-030-87231-1_37,
author = {Hu, Chen and Li, Cheng and Wang, Haifeng and Liu, Qiegen and Zheng, Hairong and Wang, Shanshan},
title = {Self-supervised Learning for MRI Reconstruction with a Parallel Network Training Framework},
year = {2021},
isbn = {978-3-030-87230-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87231-1_37},
doi = {10.1007/978-3-030-87231-1_37},
abstract = {Image reconstruction from undersampled k-space data plays an important role in accelerating the acquisition of MR data, and a lot of deep learning-based methods have been exploited recently. Despite the achieved inspiring results, the optimization of these methods commonly relies on the fully-sampled reference data, which are time-consuming and difficult to collect. To address this issue, we propose a novel self-supervised learning method. Specifically, during model optimization, two subsets are constructed by randomly selecting part of k-space data from the undersampled data and then fed into two parallel reconstruction networks to perform information recovery. Two reconstruction losses are defined on all the scanned data points to enhance the network’s capability of recovering the frequency information. Meanwhile, to constrain the learned unscanned data points of the network, a difference loss is designed to enforce consistency between the two parallel networks. In this way, the reconstruction model can be properly trained with only the undersampled data. During the model evaluation, the undersampled data are treated as the inputs and either of the two trained networks is expected to reconstruct the high-quality results. The proposed method is flexible and can be employed in any existing deep learning-based method. The effectiveness of the method is evaluated on an open brain MRI dataset. Experimental results demonstrate that the proposed self-supervised method can achieve competitive reconstruction performance compared to the corresponding supervised learning method at high acceleration rates (4 and 8). The code is publicly available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VI},
pages = {382–391},
numpages = {10},
keywords = {Image reconstruction, Deep learning, Self-supervised learning, Parallel network},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3447548.3470799,
author = {Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong},
title = {Machine Learning Robustness, Fairness, and their Convergence},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470799},
doi = {10.1145/3447548.3470799},
abstract = {Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4046–4047},
numpages = {2},
keywords = {convergence, fairness, machine learning, robustness},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3417313.3429378,
author = {Disabato, Simone and Roveri, Manuel},
title = {Incremental On-Device Tiny Machine Learning},
year = {2020},
isbn = {9781450381345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417313.3429378},
doi = {10.1145/3417313.3429378},
abstract = {Tiny Machine Learning (TML) is a novel research area aiming at designing and developing Machine Learning (ML) techniques meant to be executed on Embedded Systems and Internet-of-Things (IoT) units. Such techniques, which take into account the constraints on computation, memory, and energy characterizing the hardware platform they operate on, exploit approximation and pruning mechanisms to reduce the computational load and the memory demand of Machine and Deep Learning (DL) algorithms.Despite the advancement of the research, TML solutions present in the literature assume that Embedded Systems and IoT units support only the inference of ML and DL algorithms, whereas their training is confined to more-powerful computing units (due to larger computational load and memory demand). This also prevents such pervasive devices from being able to learn in an incremental way directly from the field to improve the accuracy over time or to adapt to new working conditions.The aim of this paper is to address such an open challenge by introducing an incremental algorithm based on transfer learning and k-nearest neighbor to support the on-device learning (and not only the inference) of ML and DL solutions on embedded systems and IoT units. Moreover, the proposed solution is general and can be applied to different application scenarios. Experimental results on image/audio benchmarks and two off-the-shelf hardware platforms show the feasibility and effectiveness of the proposed solution.},
booktitle = {Proceedings of the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things},
pages = {7–13},
numpages = {7},
keywords = {Deep Learning, Embedded Systems, Incremental Learning, Internet-of-Things, Tiny Machine Learning},
location = {Virtual Event, Japan},
series = {AIChallengeIoT '20}
}

@inproceedings{10.1145/3448016.3459240,
author = {Jiang, Jiawei and Gan, Shaoduo and Liu, Yue and Wang, Fanlin and Alonso, Gustavo and Klimovic, Ana and Singla, Ankit and Wu, Wentao and Zhang, Ce},
title = {Towards Demystifying Serverless Machine Learning Training},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3459240},
doi = {10.1145/3448016.3459240},
abstract = {The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over "serverful" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {857–871},
numpages = {15},
keywords = {machine learning, serverless computing},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1007/978-3-030-29726-8_22,
author = {Westphal, Florian and Lavesson, Niklas and Grahn, H\r{a}kan},
title = {A Case for Guided Machine Learning},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_22},
doi = {10.1007/978-3-030-29726-8_22},
abstract = {Involving humans in the learning process of a machine learning algorithm can have many advantages ranging from establishing trust into a particular model to added personalization capabilities to reducing labeling efforts. While these approaches are commonly summarized under the term interactive machine learning&nbsp;(iML), no unambiguous definition of iML exists to clearly define this area of research. In this position paper, we discuss the shortcomings of current definitions of iML and propose and define the term guided machine learning&nbsp;(gML) as an alternative.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {353–361},
numpages = {9},
keywords = {Guided machine learning, Interactive machine learning, Human-in-the-loop, Definition},
location = {Canterbury, United Kingdom}
}

@article{10.1007/s10270-021-00913-x,
author = {Nguyen, Phuong T. and Di&nbsp;Rocco, Juri and Iovino, Ludovico and Di&nbsp;Ruscio, Davide and Pierantonio, Alfonso},
title = {Evaluation of a machine learning classifier for metamodels},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-021-00913-x},
doi = {10.1007/s10270-021-00913-x},
abstract = {Modeling is a ubiquitous activity in the process of software development. In recent years, such an activity has reached a high degree of intricacy, guided by the heterogeneity of the components, data sources, and tasks. The democratized use of models has led to the necessity for suitable machinery for mining modeling repositories. Among others, the classification of metamodels into independent categories facilitates personalized searches by boosting the visibility of metamodels. Nevertheless, the manual classification of metamodels is not only a tedious but also an error-prone task. According to our observation, misclassification is the norm which leads to a reduction in reachability as well as reusability of metamodels. Handling such complexity requires suitable tooling to leverage raw data into practical knowledge that can help modelers with their daily tasks. In our previous work, we proposed AURORA as a machine learning classifier for metamodel repositories. In this paper, we present a thorough evaluation of the system by taking into consideration different settings as well as evaluation metrics. More importantly, we improve the original AURORA tool by changing its internal design. Experimental results demonstrate that the proposed amendment is beneficial to the classification of metamodels. We also compared our approach with two baseline algorithms, namely gradient boosted decision tree and support vector machines. Eventually, we see that AURORA outperforms the baselines with respect to various quality metrics.},
journal = {Softw. Syst. Model.},
month = dec,
pages = {1797–1821},
numpages = {25},
keywords = {Model-driven engineering, Machine learning, Neural networks, GBDT, SVM}
}

@inproceedings{10.1145/3448016.3457566,
author = {Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
title = {Production Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457566},
doi = {10.1145/3448016.3457566},
abstract = {Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industry-strength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50% without compromising the model deployment cadence.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2639–2652},
numpages = {14},
keywords = {data management, machine learning pipelines},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3474376.3487276,
author = {Koushanfar, Farinaz},
title = {Machine Learning on Encrypted Data: Hardware to the Rescue},
year = {2021},
isbn = {9781450386623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474376.3487276},
doi = {10.1145/3474376.3487276},
abstract = {Machine Learning on encrypted data is a yet-to-be-addressed challenge. Several recent key advances across different layers of the system, from cryptography and mathematics to logic synthesis and hardware are paving the way for practical realization of privacy preserving computing for certain target applications. This talk highlights the crucial role of hardware and advances in computing architecture in supporting the recent progresses in the field. I outline the main technologies and mixed computing models. I particularly center my talk on the recent progress in synthesis of Garbled Circuits that provide a leap in scalable realization of machine learning on encrypted data. I explore how hardware could pave the way for navigating the complex space of privacy-preserving computing in general, and enabling scalable future mixed protocol solutions. I conclude by briefly discussing the challenges and opportunities moving forward.},
booktitle = {Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security},
pages = {1},
numpages = {1},
keywords = {hardware security, machine learning},
location = {Virtual Event, Republic of Korea},
series = {ASHES '21}
}

@inproceedings{10.1145/3447548.3469463,
author = {Wang, Gang and Ciptadi, Arridhana and Ahmadzadeh, Ali},
title = {MLHat: Deployable Machine Learning for Security Defense},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469463},
doi = {10.1145/3447548.3469463},
abstract = {The MLHat workshop aims to bring together academic researchers and industry practitioners to discuss the open challenges, potential solutions, and best practices to deploy machine learning at scale for security defense. The workshop will discuss related topics from both defender perspectives (white-hat) and the attacker perspectives (black-hat). We call the workshop MLHats, to serve as a place for people who are interested in using machine learning to solve practical security problems. The workshop will focus on defining new machine learning paradigms under various security application contexts and identifying exciting new future research directions. At the same time, the workshop will also have a strong industry presence to provide insights into the challenges in deploying and maintaining machine learning models and the much-needed discussion on the capabilities that the state-of-the-arts failed to provide.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4161–4162},
numpages = {2},
keywords = {adversarial machine learning, deployable machine learning, security and privacy},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1007/978-3-030-70866-5_10,
author = {Elaeraj, Ouafae and Leghris, Cherkaoui and Renault, \'{E}ric},
title = {Performance Evaluation of Some Machine Learning Algorithms for Security Intrusion Detection},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_10},
doi = {10.1007/978-3-030-70866-5_10},
abstract = {The growth of the Internet and the opening of systems have led to an increasing number of attacks on computer networks. Security vulnerabilities are increasing, in the design of communication protocols as well as in their implementation. On another side, the knowledge, tools and scripts, to launch attacks, become readily available and more usable. Hence, the need for an intrusion detection system (IDS) is also more apparent. This technology consists in searching for a series of words or parameters characterizing an attack in a packet flow. Intrusion Detection Systems has become an essential and critical component in an IT security architecture. An IDS should be designed as part of a global security policy. The objective of an IDS is to detect any violation of the rules according to the local security policy, it thus makes it possible to report attacks. This last multi-faceted, difficult to pin down when not handled, but most of the work done in this area remains difficult to compare, that's why the aim of our article is to analyze and compare intrusion detection techniques with several machine learning algorithms. Our research indicates which algorithm offers better overall performance than the others with the IDS field.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {154–166},
numpages = {13},
keywords = {IDS, Machine learning, KNN, SVM and Decision tree},
location = {Paris, France}
}

@inproceedings{10.1145/3412841.3442027,
author = {Tsimpourlas, Foivos and Rajan, Ajitha and Allamanis, Miltiadis},
title = {Supervised learning over test executions as a test oracle},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442027},
doi = {10.1145/3412841.3442027},
abstract = {The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is a key remaining issue for automated testing. The paper aims at solving the test oracle problem in a scalable and accurate way. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program.We evaluate our approach using case studies from different application domains - 1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components and 4. Sed stream editor. We found the classification models for all subject programs resulted in high precision, recall and specificity, averaging to 89%, 88% and 92% respectively, while only training with an average 15% of the total traces. Our experiments show that the proposed NN model is promising as a test oracle and is able to learn runtime patterns to distinguish test executions for systems and tests from different application domains.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1521–1531},
numpages = {11},
keywords = {execution trace, neural networks, software testing, test oracle},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1007/978-3-030-78191-0_29,
author = {Chen, Huai and Li, Jieyu and Wang, Renzhen and Huang, Yijie and Meng, Fanrui and Meng, Deyu and Peng, Qing and Wang, Lisheng},
title = {Unsupervised Learning of Local Discriminative Representation for Medical Images},
year = {2021},
isbn = {978-3-030-78190-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78191-0_29},
doi = {10.1007/978-3-030-78191-0_29},
abstract = {Local discriminative representation is needed in many medical image analysis tasks such as identifying sub-types of lesion or segmenting detailed components of anatomical structures. However, the commonly applied supervised representation learning methods require a large amount of annotated data, and unsupervised discriminative representation learning distinguishes different images by learning a global feature, both of which are not suitable for localized medical image analysis tasks. In order to avoid the limitations of these two methods, we introduce local discrimination into unsupervised representation learning in this work. The model contains two branches: one is an embedding branch which learns an embedding function to disperse dissimilar pixels over a low-dimensional hypersphere; and the other is a clustering branch which learns a clustering function to classify similar pixels into the same cluster. These two branches are trained simultaneously in a mutually beneficial pattern, and the learnt local discriminative representations are able to well measure the similarity of local image regions. These representations can be transferred to enhance various downstream tasks. Meanwhile, they can also be applied to cluster anatomical structures from unlabeled medical images under the guidance of topological priors from simulation or other structures with similar topological characteristics. The effectiveness and usefulness of the proposed method are demonstrated by enhancing various downstream tasks and clustering anatomical structures in retinal images and chest X-ray images.},
booktitle = {Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event, June 28–June 30, 2021, Proceedings},
pages = {373–385},
numpages = {13},
keywords = {Unsupervised representation learning, Local discrimination, Topological priors}
}

@article{10.1016/j.knosys.2021.107474,
author = {Che, Feihu and Tao, Jianhua and Yang, Guohua and Liu, Tong and Zhang, Dawei},
title = {Multi-aspect self-supervised learning for heterogeneous information network▪},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107474},
doi = {10.1016/j.knosys.2021.107474},
journal = {Know.-Based Syst.},
month = dec,
numpages = {14},
keywords = {Heterogeneous information network, Self-supervised, Contrastive learning, Graph neural network}
}

@article{10.1007/s11063-021-10556-0,
author = {Zhou, Wei and Lian, Cheng and Zeng, Zhigang and Xu, Bingrong and Su, Yixin},
title = {Improve Semi-supervised Learning with Metric Learning Clusters and Auxiliary Fake Samples},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {5},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-021-10556-0},
doi = {10.1007/s11063-021-10556-0},
abstract = {Because it is very expensive to collect a large number of labeled samples to train deep neural networks in certain fields, semi-supervised learning (SSL) researcher has become increasingly important in recent years. There are many consistency regularization-based methods for solving SSL tasks, such as the Π model and mean teacher. In this paper, we first show through an experiment that the traditional consistency-based methods exist the following two problems: (1) as the size of unlabeled samples increases, the accuracy of these methods increases very slowly, which means they cannot make full use of unlabeled samples. (2) When the number of labeled samples is vary small, the performance of these methods will be very low. Based on these two findings, we propose two methods, metric learning clustering (MLC) and auxiliary fake samples, to alleviate these problems. The proposed methods achieve state-of-the-art results on SSL benchmarks. The error rates are 10.20%, 38.44% and 4.24% for CIFAR-10 with 4000 labels, CIFAR-100 with 10,000 labels and SVHN with 1000 labels by using MLC. For MNIST, the auxiliary fake samples method shows great results in cases with the very few labels.},
journal = {Neural Process. Lett.},
month = oct,
pages = {3427–3443},
numpages = {17},
keywords = {Semi-supervised learning, Metric learning, Variational auto-encoders, Very few labeled data}
}

@inproceedings{10.1145/3486011.3486469,
author = {V\'{a}zquez-Ingelmo, Andrea and Alonso-S\'{a}nchez, Julia and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Sampedro-G\'{o}mez, Jes\'{u}s and S\'{a}nchez-Puente, Antonio and Vicente-Palacios, V\'{\i}ctor and Dorado-D\'{\i}az, P. Ignacio and Sanchez, Pedro L.},
title = {Bringing machine learning closer to non-experts: proposal of a user-friendly machine learning tool in the healthcare domain},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486469},
doi = {10.1145/3486011.3486469},
abstract = {Applying Machine Learning to solve or support complex tasks is growing in popularity in a lot of different contexts. One of these contexts is the medical domain. Through Machine Learning, specific problems such as diagnosis, classification, disease detection, segmentation, assessment of organ functions, etc., can be eased by assisting physicians with useful models and their outcomes. However, understanding the application of Machine Learning and Artificial Intelligence algorithms requires expert knowledge and significant data science skills. This work presents a proposal for a user-friendly Machine Learning tool, focusing on providing a good user experience for physicians as well as an educative context for understanding the tasks involved in Machine Learning pipelines, their configuration, and their outputs.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {324–329},
numpages = {6},
keywords = {Health domain, Human-Computer Interaction, Machine Learning, User-centered design},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@article{10.1016/j.patrec.2019.08.025,
author = {Happy, S L and Dantcheva, Antitza and Bremond, Francois},
title = {A Weakly Supervised learning technique for classifying facial expressions},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.08.025},
doi = {10.1016/j.patrec.2019.08.025},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {162–168},
numpages = {7},
keywords = {Weakly supervised learning, Facial expression recognition, Label smoothing, 41A05, 41A10, 65D05, 65D17}
}

@article{10.4018/IJIIT.2021070105,
author = {Adel, Alti and Farid, Ayeche},
title = {Performance Evaluation of Machine Learning for Recognizing Human Facial Emotions},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {17},
number = {3},
issn = {1548-3657},
url = {https://doi.org/10.4018/IJIIT.2021070105},
doi = {10.4018/IJIIT.2021070105},
abstract = {Facial expression recognition is a human emotion classification problem attracting much attention from scientific research. Classifying human emotions can be a challenging task for machines. However, more accurate results and less execution time are still the issues when extracting features of human emotions. To cope with these challenges, the authors propose an automatic system that provides users with a well-adopted classifier for recognizing facial expressions in a more accurate manner. The system is based on two fundamental machine learning stages, namely feature selection and feature classification. Feature selection is realized by active shape model (ASM) composed of landmarks while the feature classification algorithm is based on seven well-known classifiers. The authors have used CK+ dataset, implemented and tested seven classifiers to find the best classifier. The experimental results show that quadratic classifier (DA) provides excellent performance, and it outperforms the other classifiers with the highest recognition rate of 100% for the same dataset.},
journal = {Int. J. Intell. Inf. Technol.},
month = jul,
pages = {1–17},
numpages = {17},
keywords = {Active Shape Model, Generalized Procrust Analysis, Human Facial Emotions, Machine Learning, Quadratic Classifier}
}

@article{10.3233/IDA-194528,
author = {de Paulo Faleiros, Thiago and Valejo, Alan and de Andrade Lopes, Alneu},
title = {Unsupervised learning of textual pattern based on Propagation in Bipartite Graph},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {3},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-194528},
doi = {10.3233/IDA-194528},
abstract = {Graph-based algorithms have aroused considerable interests in recent years by facilitating pattern recognition and learning via information propagation process through the graph. Here, we propose an unsupervised learning algorithm based on propagation on bipartite graph, referred to as Propagation in Bipartite Graph (PBG) algorithm. The contributions of this approach are threefold: 1) we present an iterative graph-based algorithm and a straight-forward bipartite representation for textual data, in which vertices represent documents and words, and edges between documents and words represent the occurrences of the words in the documents. Additionally, 2) we show that PBG is more flexible and easier to be adapted for different applications than the mathematical formalism of the generative models, and 3) we present a comprehensive evaluation and comparison of PBG to other topic extraction techniques. Here, we describe the strategy employed in PBG algorithm as a problem of maximization of similarity between latent vectors assigned to vertices and edges and demonstrate that the proposed strategy can be improved by assigning good initial values for the vectors. We notice that PBG can be parallelized by a simple adjustment in the algorithm. We also show that the proposed algorithm is competitive with LDA and NMF in the task of textual collection modelling, returning coherent topics, and in the dimensionality reduction task.},
journal = {Intell. Data Anal.},
month = jan,
pages = {543–565},
numpages = {23},
keywords = {Unsupervised learning, topic modelling, bipartite graph representation, dimensionality reduction, text mining}
}

@article{10.1016/j.cosrev.2021.100395,
author = {T.K., Balaji and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree},
title = {Machine learning algorithms for social media analysis: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100395},
doi = {10.1016/j.cosrev.2021.100395},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {32},
keywords = {Social Media, Machine learning, Social network analysis, Applications of social media analysis}
}

@inproceedings{10.1007/978-3-030-59861-7_49,
author = {Wodzinski, Marek and M\"{u}ller, Henning},
title = {Unsupervised Learning-Based Nonrigid Registration of High Resolution Histology Images},
year = {2020},
isbn = {978-3-030-59860-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59861-7_49},
doi = {10.1007/978-3-030-59861-7_49},
abstract = {The use of different dyes during histological sample preparation reveals distinct tissue properties and may improve the diagnosis. Nonetheless, the staining process deforms the tissue slides and registration is necessary before further processing. The importance of this problem led to organizing an open challenge named Automatic Non-rigid Histological Image Registration Challenge (ANHIR), organized jointly with the IEEE ISBI 2019 conference. The challenge organizers provided 481 image pairs and a server-side evaluation platform making it possible to reliably compare the proposed algorithms. The majority of the methods proposed for the challenge were based on the classical, iterative image registration, resulting in high computational load and arguable usefulness in clinical practice due to the long analysis time. In this work, we propose a deep learning-based unsupervised nonrigid registration method, that provides results comparable to the solutions of the best scoring teams, while being significantly faster during the inference. We propose a multi-level, patch-based training and inference scheme that makes it possible to register images of almost any size, up&nbsp;to the highest resolution provided by the challenge organizers. The median target registration error is close to 0.2% of the image diagonal while the average registration time, including the data loading and initial alignment, is below 3&nbsp;s. We freely release both the training and inference code making the results fully reproducible.},
booktitle = {Machine Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings},
pages = {484–493},
numpages = {10},
keywords = {Image registration, Deep learning, Histology, ANHIR},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-89817-5_3,
author = {Guedes, Gustavo Bartz and da Silva, Ana Estela Antunes},
title = {Supervised Learning Approach for Section Title Detection in PDF Scientific Articles},
year = {2021},
isbn = {978-3-030-89816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89817-5_3},
doi = {10.1007/978-3-030-89817-5_3},
abstract = {The majority of scientific articles is available in Portable Document Format (PDF). Although PDF format has the advantage of preserving layout across platforms it does not maintain the original metadata structure, making it difficult further text processing. Despite different layouts, depending on the applied template, articles have a hierarchical structure and are divided into sections, which represent topics of specific subjects, such as methodology and results. Hence, section segmentation serves as an important step for a contextualized text processing of scientific articles. Therefore, this work applies binary classification, a supervised learning task, for section title detection in PDF scientific articles. To train the classifiers, a large dataset (more than 5 millions samples from 7,302 articles) was created through an automated feature extraction approach, comprised by 17 features, where 4 were introduced in this work. Training and testing were made for ten different classifiers for which the best F1 score reached 0.94. Finally, we evaluated our results against CERMINE, an open-source system that extracts metadata from scientific articles, having an absolute improvement in section detection of 0.19 in F1 score.},
booktitle = {Advances in Computational Intelligence: 20th Mexican International Conference on Artificial Intelligence, MICAI 2021, Mexico City, Mexico, October 25–30, 2021, Proceedings, Part I},
pages = {44–54},
numpages = {11},
keywords = {Scientific article segmentation, Section title detection, Text segmentation, Supervised learning}
}

@article{10.1504/ijict.2021.113039,
author = {Haiying, Wang},
title = {Machine learning method based on improved drosophila optimisation algorithm},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {2},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2021.113039},
doi = {10.1504/ijict.2021.113039},
abstract = {Aiming at the problems of poor classification effect and high CPU ratio of traditional machine learning methods, a machine learning method based on improved drosophila optimisation algorithm was proposed. The rank one data mapping and the low order data are established. In low rank support vector set, CP rank organisation of traditional support vector machine is used to improve data security. The traditional drosophila algorithm was improved and optimised to increase the number of data iterations, ensure the compatibility of rank one data, improve the optimal calculation of drosophila, and increase the density clustering. The decomposition process is designed to evaluate the objective function value of the optimal solution. In the evaluation process, support vector machine is used to complete the label classification of learning data. Experimental data show that this method performs well in data classification effect, low-rank data storage dimension characteristic performance and CPU operation proportion performance.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {142–159},
numpages = {17},
keywords = {machine learning, drosophila algorithm, low rank data, support vector machine}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3341105.3374013,
author = {de Aquino Afonso, Bruno Klaus and Berton, Lilian},
title = {Analysis of label noise in graph-based semi-supervised learning},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374013},
doi = {10.1145/3341105.3374013},
abstract = {In machine learning, one must acquire labels to help supervise a model that will be able to generalize to unseen data. However, the labeling process can be tedious, long, costly, and error-prone. It is often the case that most of our data is unlabeled. Semi-supervised learning (SSL) alleviates that by making strong assumptions about the relation between the labels and the input data distribution. This paradigm has been successful in practice, but most SSL algorithms end up fully trusting the few available labels. In real life, both humans and automated systems are prone to mistakes; it is essential that our algorithms are able to work with labels that are both few and also unreliable. Our work aims to perform an extensive empirical evaluation of existing graph-based semi-supervised algorithms, like Gaussian Fields and Harmonic Functions, Local and Global Consistency, Laplacian Eigenmaps, Graph Transduction Through Alternating Minimization. To do that, we compare the accuracy of classifiers while varying the amount of labeled data and label noise for many different samples. Our results show that, if the dataset is consistent with SSL assumptions, we are able to detect the noisiest instances, although this gets harder when the number of available labels decreases. Also, the Laplacian Eigenmaps algorithm performed better than label propagation when the data came from high-dimensional clusters.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1127–1134},
numpages = {8},
keywords = {classification, graph-based algorithms, label noise, machine learning, semi-supervised learning},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@phdthesis{10.5555/AAI28498593,
author = {Feng, Zhe and V., Conitzer, and Y., Chen, and S, Kominers,},
advisor = {D, Parkes,},
title = {Machine Learning-Aided Economic Design},
year = {2021},
isbn = {9798534671513},
publisher = {Harvard University},
address = {USA},
abstract = {Nowadays, online markets (e.g. online advertising market and online two-sided markets) grow larger and larger everyday. Designing an efficient and near-optimal market is an intricate task. Market designers are facing challenges not only in regard to scalability, but also coming from the use of data to better understand the behavior of strategic participants. At the same time, these participants are trying to understand how these markets work and to maximize reward. For these reasons, we continue to need improved frameworks for the design of online markets. One challenge for market design is to make effective use of data in order to design better markets. For the players, a central problem is how to optimize their strategy, adaptively learning from feedback and incorporating this along with other side information.To handle these challenges, my thesis focuses on two topics, Economic Design via Machine Learning and Learning in Online Markets. For the first topic, I propose a unified computational framework for data-driven mechanism design that can help a mechanism designer to automatically design a good mechanism to satisfy incentive constraints and achieve a desired objective (e.g. revenue, social welfare). I provide different approaches to guarantee Incentive Compatibility and prove the generalization bounds. This deep-learning framework is very general and can be extended to handle other constraints, e.g., private budget constraints. In addition, I investigate how to transform an approximately incentive compatible mechanism to a fully BIC mechanism without loss of welfare and with only negligible loss of revenue. For the second topic, I analyze the convergence of the outcome achieved by strategic bidders when they adopt mean-based learning algorithms to bid in repeated auctions. I also propose a new online learning algorithm for a bidder to use when bidding in repeated auctions, where the bidder's own value, evolving in an arbitrary manner, and observed only if the bidder wins an auction. This algorithm has exponentially faster convergence in terms of its dependence on the action space than the generic bandit algorithm.},
note = {AAI28498593}
}

@inproceedings{10.1145/3380446.3430691,
author = {Khandelwal, Vishal},
title = {Machine-Learning Enabled Next-Generation Physical Design - An EDA Perspective},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430691},
doi = {10.1145/3380446.3430691},
abstract = {Physical design is an ensemble of NP-complete problems that P&amp;R tools attempt to solve in (pseudo) linear time. Advanced process nodes and complex signoff requirements bring in new physical and timing constraints into the implementation flow, making it harder for physical design algorithms to deliver industry-leading power, performance, area (PPA), without giving up design turn-around-time. The relentless pursuit for low-power high-performance designs is putting constant pressure to limit any over-design, creating an acute need to have better models/predictions and advanced analytics to drive implementation flows. Given the advancements in supervised and reinforcement learning, combined with the availability of large-scale compute, Machine Learning (ML) has the potential to become a disruptive paradigm change for EDA tools. In this talk, I would like to share some of the challenges and opportunities for innovation in next-generation physical design using ML.Biography: Vishal leads the physical optimization team for the Digital Implementation products at Synopsys. He has 15 years of R&amp;D experience in building state-of-the-art optimization engines and P&amp;R flows targeting advanced-node low-power high-performance designs. More recently, he has been looking at bringing machine-learning paradigms into digital implementation tools to improve power, performance, area and productivity. Vishal has a B.Tech. from Indian Institute of Technology, Kanpur and a Ph.D. from University of Maryland, College Park. He has won a best paper award at ISPD, co-authored several patents and over 20 IEEE/ACM publications.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {135},
numpages = {1},
keywords = {PPA, machine learning, physical design},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@article{10.1007/s10489-021-02210-y,
author = {Dornaika, F.},
title = {Flexible data representation with feature convolution for semi-supervised learning},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02210-y},
doi = {10.1007/s10489-021-02210-y},
abstract = {Data representation plays a crucial role in semi-supervised learning. This paper proposes a framework for semi-supervised data representation. It introduces a flexible nonlinear embedding model that integrates graph-based data convolutions. The proposed approach exploits structured data in order to estimate a nonlinear data representation as well as a linear transformation, enabling an inductive semi-supervised model. The introduced approach exploits data graphs at two different levels. First, it integrates manifold regularization that is encoded by the graph itself. Second, it optimizes a flexible linear transformation that maps the convolved data samples to their nonlinear representations. These convolved data are generated by the joint use of the graph and data. The proposed semi-supervised model overcomes some challenges related to some samples distributions in the original spaces. The proposed Graph Convolution based Semi-supervised Embedding (GCSE) provides flexible models which can improve both the data representation and the final performance of the learning model. Experiments are run on six image datasets for comparing the proposed approach with several state-of-art semi-supervised methods. These results show the effectiveness of the proposed framework.},
journal = {Applied Intelligence},
month = nov,
pages = {7690–7704},
numpages = {15},
keywords = {Graph-based embedding, Semi-supervised learning, Graph convolutions, Discriminant embedding, Pattern recognition}
}

@inproceedings{10.1145/3431379.3464450,
author = {Doudali, Thaleia Dimitra and Gavrilovska, Ada},
title = {Machine Learning Augmented Hybrid Memory Management},
year = {2021},
isbn = {9781450382175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431379.3464450},
doi = {10.1145/3431379.3464450},
abstract = {The integration of emerging non volatile memory hardware technologies into the main memory substrate, enables massive memory capacities at a reasonable cost in return for slower access speeds. This heterogeneity, along with the greater irregularity in the behavior of emerging workloads, render existing memory management approaches ineffective. This creates a significant gap between the realized vs. achievable performance and efficiency. At the same time, resource management solutions augmented with machine learning show great promise for fine-tuning system configuration knobs and predicting future behaviors. This thesis builds novel system-level mechanisms and reveals new insights towards the practical integration of machine learning in hybrid memory management. The specific contributions of this thesis is a machine learning augmented memory manager, coupled with insightful mechanisms to reduce the associated learning overheads and fine-tune critical operational parameters. The impact of this thesis is realizing an average of 3x application performance improvements and setting the new state-of-the-art in hybrid memory management.},
booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {253–254},
numpages = {2},
keywords = {clustering, emerging memory technologies, heterogeneous memory systems, hybrid memory systems, keywords{data tiering, long short term memory networks, machine intelligence, machine learning, non volatile memory, page scheduler, persistent memory, recurrent neural networks, }},
location = {Virtual Event, Sweden},
series = {HPDC '21}
}

@article{10.1007/s11063-018-9794-8,
author = {Ngoc, Minh Tran and Park, Dong-Chul},
title = {Centroid Neural Network with Pairwise Constraints for Semi-supervised Learning},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-018-9794-8},
doi = {10.1007/s11063-018-9794-8},
abstract = {A clustering algorithm for datasets with pairwise constraints using the Centroid Neural Network (Cent.NN) is proposed in this paper. The proposed algorithm, referred to as the Centroid Neural Network with Pairwise Constraints (Cent. NN-PC) algorithm, utilizes Cent.NN as its backbone algorithm for data clustering and adopts a semi-supervised learning process for pairwise constraints. A newly formulated energy function is adopted from the original Cent.NN algorithm for the proposed Cent.NN-PC algorithm, introducing penalty terms for violating constraints. The weight update procedure of the proposed Cent.NN-PC algorithm finds optimal prototypes for the given dataset that minimize the quantization error while minimizing the number of violated constraints. In order to evaluate the performance of the proposed Cent.NN-PC algorithm, experiments on six different datasets from the UCI database and two bioinformatics datasets from the KEEL repository are carried out. The performance of the proposed algorithm is compared to that of the the Linear Constrained Vector Quantization Error (LCVQE) algorithm, one of the most commonly used algorithms for data clustering with pairwise constraints. In the experiments, five different numbers of pairwise constraints are utilized to evaluate the clustering performance with constraints of different sizes. The results show that the proposed Cent.NN-PC algorithm outperforms the LCVQE algorithm on most performance criteria, including the total quantization error, the number of violated constraints, and on the three performance metrics of the classification accuracy rate, F-score, and NMI measure outcome. The experiments also show that Cent.NN-PC provides much more stable clustering results at an improved operational speed compared to LCVQE.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1721–1747},
numpages = {27},
keywords = {Classification, Clustering, Learning algorithm, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-98682-7_27,
author = {Olthuis, J. J. and van&nbsp;der Meer, N. B. and Kempers, S. T. and van Hoof, C. A. and Beumer, R. M. and Kuijpers, W. J. P. and Kokkelmans, A. A. and Houtman, W. and van Eijck, J. J. F. J. and Kon, J. J. and Peijnenburg, A. T. A. and van&nbsp;de Molengraft, M. J. G.},
title = {Vision-Based Machine Learning in Robot Soccer},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_27},
doi = {10.1007/978-3-030-98682-7_27},
abstract = {Robots need to perceive their environment in order to properly interact with it. In the RoboCup Soccer Middle Size League&nbsp;(MSL) this happens primarily through cameras mounted on the robots. Machine Learning can be used to extract relevant features from camera imagery. The real-time analysis of camera data is a challenge for both traditional and Machine Learning algorithms, since all computations in the MSL have to be performed on the robot itself.This contribution shows that it is possible to process camera imagery in real-time using Machine Learning. It does this by presenting the current state of Machine Learning in MSL and providing two examples that won the Scientific and Technical Challenges at RoboCup 2021. Both examples focus on semantic detection of objects and humans in imagery. The Scientific Challenge winner presents how YOLOv5 can be used for object detection in the MSL. The Technical Challenge winner demonstrates how to improve interaction between robots and humans in soccer using OpenPose. This contributes towards the goal of RoboCup to arrive at robots that can beat the human soccer world champion by 2050.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {327–339},
numpages = {13},
keywords = {RoboCup soccer, Middle size league, Machine learning, Object detection, People detection, Real-time},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1007/978-3-030-88007-1_11,
author = {Lin, Huibin and Wang, Shiping and Liu, Zhanghui and Xiao, Shunxin and Du, Shide and Guo, Wenzhong},
title = {FMixAugment for Semi-supervised Learning with Consistency Regularization},
year = {2021},
isbn = {978-3-030-88006-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88007-1_11},
doi = {10.1007/978-3-030-88007-1_11},
abstract = {Consistency regularization has witnessed tremendous success in the area of semi-supervised deep learning for image classification, which leverages data augmentation on unlabeled examples to encourage the model outputting the invariant predicted class distribution as before augmented. These methods have been made considerable progress in this area, but most of them are at the cost of utilizing more complex models. In this work, we propose a simple and efficient method FMixAugment, which combines the proposed MixAugment with Fourier space-based data masking and applies it on unlabeled examples to generate a strongly-augmented version. Our approach first generates a hard pseudo-label by employing a weakly-augmented version and minimizes the cross-entropy between it and the strongly-augmented version. Furthermore, to improve the robustness and uncertainty measurement of the model, we also enforce consistency constraints between the mixed augmented version and the weakly-augmented version. Ultimately, we introduce a dynamic growth of the confidence threshold for pseudo-labels. Extensive experiments are tested on CIFAR-10/100, SVHN, and STL-10 datasets, which indicate that our method outperforms the previous state-of-the-art methods. Specifically, with 40 labeled examples on CIFAR-10, we achieve 90.21% accuracy, and exceed 95% accuracy with 1000 labeled examples on STL-10.},
booktitle = {Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29 – November 1, 2021, Proceedings, Part II},
pages = {127–139},
numpages = {13},
keywords = {Semi-supervised learning, Image classification, Consistency regularization, Data augmentation},
location = {Beijing, China}
}

@article{10.1007/s00521-020-05609-9,
author = {Zheng, Xiaohan and Zhang, Li and Xu, Zhiqiang},
title = {L1-norm Laplacian support vector machine for data reduction in semi-supervised learning},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05609-9},
doi = {10.1007/s00521-020-05609-9},
abstract = {As a semi-supervised learning method, Laplacian support vector machine (LapSVM) is popular. Unfortunately, the model generated by LapSVM has a poor sparsity. A sparse decision model has always been fascinating because it could implement data reduction and improve performance. To generate a sparse model of LapSVM, we propose an ℓ1-norm Laplacian support vector machine (ℓ1-norm LapSVM), which replaces the ℓ2-norm with the ℓ1-norm in LapSVM. The ℓ1-norm LapSVM has two techniques that can induce sparsity: the ℓ1-norm regularization and the hinge loss function. We discuss two situations for the ℓ1-norm LapSVM, linear and nonlinear ones. In the linear ℓ1-norm LapSVM, the sparse decision model implies that features with nonzero coefficients are contributive. In other words, the linear ℓ1-norm LapSVM can perform feature selection to achieve the goal of data reduction. Moreover, the nonlinear (kernel) ℓ1-norm LapSVM can also implement data reduction in terms of sample selection. In addition, the optimization problem of the ℓ1-norm LapSVM is a convex quadratic programming one. That is, the ℓ1-norm LapSVM has a unique and global solution. Experimental results on semi-supervised classification tasks have shown a comparable performance of our ℓ1-norm LapSVM.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {12343–12360},
numpages = {18},
keywords = {Semi-supervised learning, Support vector machine, ℓ1-norm regularization, Laplacian regularization}
}

@article{10.1016/j.neucom.2021.01.138,
author = {Mihaljevi\'{c}, Bojan and Bielza, Concha and Larra\~{n}aga, Pedro},
title = {Bayesian networks for interpretable machine learning and optimization},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {456},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.01.138},
doi = {10.1016/j.neucom.2021.01.138},
journal = {Neurocomput.},
month = oct,
pages = {648–665},
numpages = {18},
keywords = {Interpretability, Explainable machine learning, Probabilistic graphical models}
}

@article{10.1007/s10115-020-01439-2,
author = {Lyaqini, Soufiane and Quafafou, Mohamed and Nachaoui, Mourad and Chakib, Abdelkrim},
title = {Supervised learning as an inverse problem based on non-smooth loss function},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {8},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01439-2},
doi = {10.1007/s10115-020-01439-2},
abstract = {This paper is concerned by solving supervised machine learning problem as an inverse problem. Recently, many works have focused on defining a relationship between supervised learning and the well-known inverse problems. However, this connection between the learning problem and the inverse one has been done in the particular case where the inverse problem is reformulated as a minimization problem with a quadratic cost functional (L2 cost functional). Although, it is well known that the cost functional can be L1, L2 or any positive function that measures the gap between the predicted data and the observed one. Indeed, the use of L1 loss function for supervised learning problem gives more consistent results (see Rosasco et al. in Neural Comput 16:1063–1076, 2004). This strengthens the idea of reformulating the inverse problem, associated to machine learning problem, into a minimization problem using L1 functional. However, the L1 loss function is non-differentiable, which precludes the use of standard optimization tools. To overcome this difficulty, we propose in this paper a new technique of approximation based on the reformulation of the associated inverse problem into a minimizing one of a slanting cost functional Chen et al. (MIS Q Manag Inf Syst 36:1165–1188, 2012), which is solved using Tikhonov regularization and Newton’s method. This approach leads to an efficient numerical algorithm allowing us to solve supervised learning problem in the most general framework. To confirm this, we present some numerical results showing the efficiency of the proposed approach. Furthermore, the numerical experiment validation is made through academic and real-life data. Thus, the comparison with existing methods and numerical stability of the algorithm is presented in order to show that our approach is better in terms of convergence speed and quality of predicted models.},
journal = {Knowl. Inf. Syst.},
month = aug,
pages = {3039–3058},
numpages = {20},
keywords = {Inverse problem, Supervised learning, Non-smooth loss function, Optimization, Slanting function, Airfoil self-noise, ECG signals}
}

@inproceedings{10.1007/978-3-030-70866-5_11,
author = {Phillipson, Frank and Wezeman, Robert S. and Chiscop, Irina},
title = {Three Quantum Machine Learning Approaches for Mobile User Indoor-Outdoor Detection},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_11},
doi = {10.1007/978-3-030-70866-5_11},
abstract = {There is a growing trend in using machine learning techniques for detecting environmental context in communication networks. Machine learning is one of the promising candidate areas where quantum computing can show a quantum advantage over their classical algorithmic counterpart on near term Noisy Intermediate-Scale Quantum (NISQ) devices. The goal of this paper is to give a practical overview of (supervised) quantum machine learning techniques to be used for indoor-outdoor detection. Due to the small number of qubits in current quantum hardware, real application is not yet feasible. Our work is intended to be a starting point for further explorations of quantum machine learning techniques for indoor-outdoor detection.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {167–183},
numpages = {17},
keywords = {Quantum machine learning, Mobile devices, Indoor-outdoor detection, Hybrid quantum-classical, Variational quantum classifier, Quantum classification, Quantum SVM},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-030-87240-3_4,
author = {Zheng, Kang and Wang, Yirui and Zhou, Xiao-Yun and Wang, Fakai and Lu, Le and Lin, Chihung and Huang, Lingyun and Xie, Guotong and Xiao, Jing and Kuo, Chang-Fu and Miao, Shun},
title = {Semi-supervised Learning for Bone Mineral Density Estimation in Hip X-Ray Images},
year = {2021},
isbn = {978-3-030-87239-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87240-3_4},
doi = {10.1007/978-3-030-87240-3_4},
abstract = {Bone mineral density (BMD) is a clinically critical indicator of osteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due to the limited accessibility of DEXA machines and examinations, osteoporosis is often under-diagnosed and under-treated, leading to increased fragility fracture risks. Thus it is highly desirable to obtain BMDs with alternative cost-effective and more accessible medical imaging examinations such as X-ray plain films. In this work, we formulate the BMD estimation from plain hip X-ray images as a regression problem. Specifically, we propose a new semi-supervised self-training algorithm to train a BMD regression model using images coupled with DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are generated and refined iteratively for unlabeled images during self-training. We also present a novel adaptive triplet loss to improve the model’s regression accuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD estimation method achieves a high Pearson correlation coefficient of 0.8805 to ground-truth BMDs. It offers good feasibility to use the more accessible and cheaper X-ray imaging for opportunistic osteoporosis screening.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27 – October 1, 2021, Proceedings, Part V},
pages = {33–42},
numpages = {10},
keywords = {Bone mineral density estimation, Hip X-ray, Semi-supervised learning},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-73280-6_12,
author = {Bregu, Ornela and Zamzami, Nuha and Bouguila, Nizar},
title = {Mixture-Based Unsupervised Learning for Positively Correlated Count Data},
year = {2021},
isbn = {978-3-030-73279-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73280-6_12},
doi = {10.1007/978-3-030-73280-6_12},
abstract = {The Multinomial distribution has been widely used to model count data. However, its Naive Bayes assumption usually degrades clustering performance especially when correlation between features is imminent, i.e., text documents. In this paper, we use the Negative Multinomial distribution to perform clustering based on finite mixture models, where the mixture parameters are to be estimated using a novel minorization-maximization algorithm, thriving in high-dimensionality optimization settings. Furthermore, we integrate a model-based feature selection approach to determine the optimal number of components in the mixture. To evaluate the clustering performance of the proposed model, three real-world applications are considered, namely, COVID-19 analysis, Web page clustering and facial expression recognition.},
booktitle = {Intelligent Information and Database Systems: 13th Asian Conference, ACIIDS 2021, Phuket, Thailand, April 7–10, 2021, Proceedings},
pages = {144–154},
numpages = {11},
keywords = {Negative multinomial distribution, Positive correlation, Minorization-maximization, Minimum message length, Overdispersion},
location = {Phuket, Thailand}
}

@inproceedings{10.5555/3495724.3496560,
author = {Han, Tao and Gao, Junyu and Yuan, Yuan and Wang, Qi},
title = {Unsupervised semantic aggregation and deformable template matching for semi-supervised learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unlabeled data learning has attracted considerable attention recently. However, it is still elusive to extract the expected high-level semantic feature with mere unsu-pervised learning. In the meantime, semi-supervised learning (SSL) demonstrates a promising future in leveraging few samples. In this paper, we combine both to propose an Unsupervised Semantic Aggregation and Deformable Template Matching (USADTM) framework for SSL, which strives to improve the classification performance with few labeled data and then reduce the cost in data annotating. Specifically, unsupervised semantic aggregation based on Triplet Mutual Information (T-MI) loss is explored to generate semantic labels for unlabeled data. Then the semantic labels are aligned to the actual class by the supervision of labeled data. Furthermore, a feature pool that stores the labeled samples is dynamically updated to assign proxy labels for unlabeled data, which are used as targets for cross-entropy minimization. Extensive experiments and analysis across four standard semi-supervised learning benchmarks validate that USADTM achieves top performance (e.g., 90.46% accuracy on CIFAR-10 with 40 labels and 95.20% accuracy with 250 labels).},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {836},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1007/978-3-030-81242-3_11,
author = {Ozga, Wojciech and Quoc, Do Le and Fetzer, Christof},
title = {Perun: Confidential Multi-stakeholder Machine Learning Framework with Hardware Acceleration Support},
year = {2021},
isbn = {978-3-030-81241-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-81242-3_11},
doi = {10.1007/978-3-030-81242-3_11},
abstract = {Confidential multi-stakeholder machine learning (ML) allows multiple parties to perform collaborative data analytics while not revealing their intellectual property, such as ML source code, model, or datasets. State-of-the-art solutions based on homomorphic encryption incur a large performance overhead. Hardware-based solutions, such as trusted execution environments (TEEs), significantly improve the performance in inference computations but still suffer from low performance in training computations, e.g., deep neural networks model training, because of limited availability of protected memory and lack of GPU support.To address this problem, we designed and implemented Perun, a framework for confidential multi-stakeholder machine learning that allows users to make a trade-off between security and performance. Perun executes ML training on hardware accelerators (e.g., GPU) while providing security guarantees using trusted computing technologies, such as trusted platform module and integrity measurement architecture. Less compute-intensive workloads, such as inference, execute only inside TEE, thus at a lower trusted computing base. The evaluation shows that during the ML training on CIFAR-10 and real-world medical datasets, Perun achieved a 161\texttimes{} to 1560\texttimes{} speedup compared to a pure TEE-based approach.},
booktitle = {Data and Applications Security and Privacy XXXV: 35th Annual IFIP WG 11.3 Conference, DBSec 2021, Calgary, Canada, July 19–20, 2021, Proceedings},
pages = {189–208},
numpages = {20},
keywords = {Multi-stakeholder computation, Machine learning, Confidential computing, Trusted computing, Trust management},
location = {Calgary, AB, Canada}
}

@inproceedings{10.1145/3448016.3457295,
author = {Neutatz, Felix and Biessmann, Felix and Abedjan, Ziawasch},
title = {Enforcing Constraints for Machine Learning Systems via Declarative Feature Selection: An Experimental Study},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457295},
doi = {10.1145/3448016.3457295},
abstract = {Responsible usage of Machine Learning (ML) systems in practice does not only require enforcing high prediction quality, but also accounting for other constraints, such as fairness, privacy, or execution time. One way to address multiple user-specified constraints on ML systems is feature selection. Yet, optimizing feature selection strategies for multiple metrics is difficult to implement and has been underrepresented in previous experimental studies. Here, we propose Declarative Feature Selection (DFS) to simplify the design and validation of ML systems satisfying diverse user-specified constraints. We benchmark and evaluate a representative series of feature selection algorithms. From our extensive experimental results, we derive concrete suggestions on when to use which strategy and show that a meta-learning-driven optimizer can accurately predict the right strategy for an ML task at hand. These results demonstrate that feature selection can help to build ML systems that meet combinations of user-specified constraints, independent of the ML methods used.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1345–1358},
numpages = {14},
keywords = {DFS, bias, declarative feature selection, declarative machine learning, declarative ml, fairness, feature selection, machine learning, meta learning, privacy, robustness},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3449726.3459523,
author = {Pfisterer, Florian and van Rijn, Jan N. and Probst, Philipp and M\"{u}ller, Andreas C. and Bischl, Bernd},
title = {Learning multiple defaults for machine learning algorithms},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459523},
doi = {10.1145/3449726.3459523},
abstract = {Modern machine learning methods highly depend on their hyper-parameter configurations for optimal performance. A widely used approach to selecting a configuration is using default settings, often proposed along with the publication of a new algorithm. Those default values are usually chosen in an ad-hoc manner to work on a wide variety of datasets. Different automatic hyperparameter configuration algorithms which select an optimal configuration per dataset have been proposed, but despite its importance, tuning is often skipped in applications because of additional run time, complexity, and experimental design questions. Instead, the learner is often applied in its defaults. This principled approach usually improves performance but adds additional algorithmic complexity and computational costs to the training procedure. We propose and study using a set of complementary default values, learned from a large database of prior empirical results as an alternative. Selecting an appropriate configuration on a new dataset then requires only a simple, efficient, and embarrassingly parallel search over this set. To demonstrate the effectiveness and efficiency of the approach, we compare learned sets of configurations to random search and Bayesian optimization. We show that sets of defaults can improve performance while being easy to deploy in comparison to more complex methods.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {241–242},
numpages = {2},
keywords = {AutoML, hyperparameter optimization, metalearning},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3383972.3384042,
author = {Pree, Wolfgang and Hoerbinger, Felix},
title = {Applying Machine Learning to a Conventional Data Processing Task: A Quantitative Evaluation},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384042},
doi = {10.1145/3383972.3384042},
abstract = {Though machine learning (ML) can be applied to a wide spectrum of applications, it has been hardly used and evaluated in the context of conventional data processing tasks. Such conventional data processing tasks are characterized by a set of calculations that follow strict rules, such as in accounting or banking applications. This paper quantitatively evaluates how software which is automatically generated by ML methods and tools compares to software programmed by hand. The assessment of poker hands according to Texas Hold'em rules is a representative example for conventional data processing tasks, because of the various exceptions how to assess and compare hands. For some hand values, the rank (two, three, ... king, ace) of the cards is relevant and the suit (club, diamond, heart, spade) irrelevant, and vice versa. This paper shows how an accuracy of 100% can be achieved for assessing poker hands according to Texas Hold'em rules, with a small set of labeled training data compared to the number of possible hands. We also evaluate quantitatively the effect of the labeling quality on accuracy.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {111–115},
numpages = {5},
keywords = {Machine learning, convolutional neural networks, data labeling, feed-forward neural networks, labeling quality, neural networks, robustness, supervised learning},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1007/978-3-030-59725-2_37,
author = {Xu, Junshen and Lala, Sayeri and Gagoski, Borjan and Abaci Turk, Esra and Grant, P. Ellen and Golland, Polina and Adalsteinsson, Elfar},
title = {Semi-supervised Learning for Fetal Brain MRI Quality Assessment with ROI Consistency},
year = {2020},
isbn = {978-3-030-59724-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59725-2_37},
doi = {10.1007/978-3-030-59725-2_37},
abstract = {Fetal brain MRI is useful for diagnosing brain abnormalities but is challenged by fetal motion. The current protocol for T2-weighted fetal brain MRI is not robust to motion so image volumes are degraded by inter- and intra- slice motion artifacts. Besides, manual annotation for fetal MR image quality assessment are usually time-consuming. Therefore, in this work, a semi-supervised deep learning method that detects slices with artifacts during the brain volume scan is proposed. Our method is based on the mean teacher model, where we not only enforce consistency between student and teacher models on the whole image, but also adopt an ROI consistency loss to guide the network to focus on the brain region. The proposed method is evaluated on a fetal brain MR dataset with 11,223 labeled images and more than 200,000 unlabeled images. Results show that compared with supervised learning, the proposed method can improve model accuracy by about 6% and outperform other state-of-the-art semi-supervised learning methods. The proposed method is also implemented and evaluated on an MR scanner, which demonstrates the feasibility of online image quality assessment and image reacquisition during fetal MR scans.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI},
pages = {386–395},
numpages = {10},
keywords = {Image quality assessment, Fetal magnetic resonance imaging (MRI), Semi-supervised learning, Convolutional neural network (CNN)},
location = {Lima, Peru}
}

@article{10.1016/j.eswa.2021.114767,
author = {Kim, Misuk},
title = {Adaptive trading system integrating machine learning and back-testing: Korean bond market case},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114767},
doi = {10.1016/j.eswa.2021.114767},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {10},
keywords = {Korean treasury bond market, Treasury bond spread prediction, Treasury futures trading, Back-testing, Machine learning}
}

@article{10.1145/3451210,
author = {Adnan, Md Musabbir and Sayyaparaju, Sagarvarma and Brown, Samuel D. and Shawkat, Mst Shamim Ara and Schuman, Catherine D. and Rose, Garrett S.},
title = {Design of a Robust Memristive Spiking Neuromorphic System with Unsupervised Learning in Hardware},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3451210},
doi = {10.1145/3451210},
abstract = {Spiking neural networks (SNN) offer a power efficient, biologically plausible learning paradigm by encoding information into spikes. The discovery of the memristor has accelerated the progress of spiking neuromorphic systems, as the intrinsic plasticity of the device makes it an ideal candidate to mimic a biological synapse. Despite providing a nanoscale form factor, non-volatility, and low-power operation, memristors suffer from device-level non-idealities, which impact system-level performance. To address these issues, this article presents a memristive crossbar-based neuromorphic system using unsupervised learning with twin-memristor synapses, fully digital pulse width modulated spike-timing-dependent plasticity, and homeostasis neurons. The implemented single-layer SNN was applied to a pattern-recognition task of classifying handwritten-digits. The performance of the system was analyzed by varying design parameters such as number of training epochs, neurons, and capacitors. Furthermore, the impact of memristor device non-idealities, such as device-switching mismatch, aging, failure, and process variations, were investigated and the resilience of the proposed system was demonstrated.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jun,
articleno = {56},
numpages = {26},
keywords = {Memristor, synapse, neuron, STDP, homeostasis, unsupervised learning}
}

@inproceedings{10.5555/3507788.3507841,
author = {Kamali, Amin and Zuzarte, Calisto and Kantere, Verena},
title = {Emerging applications of machine learning in modern data management},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {In recent years, the applications of machine learning (ML) have proliferated in most aspects of traditional computer science. Data management discipline is no exception in this regard. Rule-based modules are being replaced by ML-based counterparts that effectively 'mine the rules' from experience. Approaches that rely on crude statistics are rapidly being outdated by the ones that 'learn' the functional dependencies, correlations, and skewness from the underlying data. These learning-based methods have an upper hand on many different fronts. On one hand, they promise to reduce the cost of development and maintenance of the highly complex classical modules. On the other hand, they avoid the 'one solution fits all' approach by effectively tailoring the behavior to fit the requirements of individual system instances.This workshop brought together leaders of cutting-edge research projects in the area and audience from academia and industry, to discuss some examples of using machine learning for modernizing different aspects of data management. The discussed examples covered four different areas: Query Optimization, Data Partitioning, Database Knobs Tuning, and Data Caching.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {284–286},
numpages = {3},
keywords = {data caching, data partitioning, database tuning, machine learning, query optimization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1007/978-3-030-45231-5_1,
author = {Mehmood, Usama and Roy, Shouvik and Grosu, Radu and Smolka, Scott A. and Stoller, Scott D. and Tiwari, Ashish},
title = {Neural Flocking: MPC-Based Supervised Learning of Flocking Controllers},
year = {2020},
isbn = {978-3-030-45230-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45231-5_1},
doi = {10.1007/978-3-030-45231-5_1},
abstract = {We show how a symmetric and fully distributed flocking controller can be synthesized using Deep Learning from a centralized flocking controller. Our approach is based on Supervised Learning, with the centralized controller providing the training data, in the form of trajectories of state-action pairs. We use Model Predictive Control (MPC) for the centralized controller, an approach that we have successfully demonstrated on flocking problems. MPC-based flocking controllers are high-performing but also computationally expensive. By learning a symmetric and distributed neural flocking controller from a centralized MPC-based one, we achieve the best of both worlds: the neural controllers have high performance (on par with the MPC controllers) and high efficiency. Our experimental results demonstrate the sophisticated nature of the distributed controllers we learn. In particular, the neural controllers are capable of achieving myriad flocking-oriented control objectives, including flocking formation, collision avoidance, obstacle avoidance, predator avoidance, and target seeking. Moreover, they generalize the behavior seen in the training data to achieve these objectives in a significantly broader range of scenarios. In terms of verification of our neural flocking controller, we use a form of statistical model checking to compute confidence intervals for its convergence rate and time to convergence.},
booktitle = {Foundations of Software Science and Computation Structures: 23rd International Conference, FOSSACS 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {1–16},
numpages = {16},
keywords = {Flocking, Model Predictive Control, Distributed Neural Controller, Deep Neural Network, Supervised Learning},
location = {Dublin, Ireland}
}

@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {bias, fairness, feature selection, machine learning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1007/s11277-021-08355-w,
author = {Liwei, Zhang},
title = {Predictive Analysis of Machine Learning Error Classification Based on Bayesian Network},
year = {2021},
issue_date = {Nov 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08355-w},
doi = {10.1007/s11277-021-08355-w},
abstract = {With the emergence of various types of wireless communication products and the explosive growth of wireless network services, spectrum resources have become increasingly scarce. As the basis of cognitive radio technology, cognitive devices need to have real-time spectrum sensing capabilities, and classify and predict the perceived radio signals. Introducing Bayesian network for model construction, and using genetic algorithm to optimize and improve the model structure can more accurately establish the optimal network directed graph in structural learning, so as to better explore the high-risk predictive factors of machine learning. This paper makes a comparative analysis of the basic methods of the Bayesian network model in structural learning and parameter learning, and discusses the algorithm models that may be used to improve the structural learning. The adaptive search scoring function genetic algorithm is introduced into the network topology map construction part of the Bayesian network model structure learning, and a Bayesian network machine learning prediction model based on genetic optimization is proposed. The prediction algorithm of the maximum posterior probability of the perceptual signal considers which system the signal is most likely to come from at the next moment, and uses the estimation of the signal characteristics of the system as the signal prediction result. This paper further derives the upper bound of signal feature estimation error. Based on the prediction algorithm, the cognitive device can avoid the communication channel that will be occupied by the main user system in advance, thereby saving channel switching overhead. Simulation analysis verifies that the prediction algorithm designed in this paper has better prediction accuracy, and shows that as the number of signal samples sensed increases, the accuracy of the prediction algorithm is also improved accordingly. The classification accuracy rate based on Bayesian network is above 83% and the fluctuation rate is reduced to 2.5%.},
journal = {Wirel. Pers. Commun.},
month = apr,
pages = {615–634},
numpages = {20},
keywords = {Bayesian network, Dirichlet hybrid process, Machine learning, Genetic algorithm, Structural learning, Scoring function, Prediction algorithm}
}

@inproceedings{10.1007/978-3-030-61380-8_28,
author = {Cardoso, Lucas F. F. and Santos, Vitor C. A. and Franc\^{e}s, Regiane S. Kawasaki and Prud\^{e}ncio, Ricardo B. C. and Alves, Ronnie C. O.},
title = {Decoding Machine Learning Benchmarks},
year = {2020},
isbn = {978-3-030-61379-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61380-8_28},
doi = {10.1007/978-3-030-61380-8_28},
abstract = {Despite the availability of benchmark machine learning (ML) repositories (e.g., UCI, OpenML), there is no standard evaluation strategy yet capable of pointing out which is the best set of datasets to serve as gold standard to test different ML algorithms. In recent studies, Item Response Theory (IRT) has emerged as a new approach to elucidate what should be a good ML benchmark. This work applied IRT to explore the well-known OpenML-CC18 benchmark to identify how suitable it is on the evaluation of classifiers. Several classifiers ranging from classical to ensembles ones were evaluated using IRT models, which could simultaneously estimate dataset difficulty and classifiers’ ability. The Glicko-2 rating system was applied on the top of IRT to summarize the innate ability and aptitude of classifiers. It was observed that not all datasets from OpenML-CC18 are really useful to evaluate classifiers. Most datasets evaluated in this work (84%) contain easy instances in general (e.g., around 10% of difficult instances only). Also, 80% of the instances in half of this benchmark are very discriminating ones, which can be of great use for pairwise algorithm comparison, but not useful to push classifiers abilities. This paper presents this new evaluation methodology based on IRT as well as the tool decodIRT, developed to guide IRT estimation over ML benchmarks.},
booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part II},
pages = {412–425},
numpages = {14},
keywords = {IRT, Machine learning, Benchmarking, OpenML, Classification},
location = {Rio Grande, Brazil}
}

@article{10.1007/s00778-021-00690-5,
author = {Mohamed, Aisha and Abuoda, Ghadeer and Ghanem, Abdurrahman and Kaoudi, Zoi and Aboulnaga, Ashraf},
title = {RDFFrames: knowledge graph access for machine learning tools},
year = {2021},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {2},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00690-5},
doi = {10.1007/s00778-021-00690-5},
abstract = {Knowledge graphs represented as RDF datasets are integral to many machine learning applications. RDF is supported by a rich ecosystem of data management systems and tools, most notably RDF database systems that provide a SPARQL query interface. Surprisingly, machine learning tools for knowledge graphs do not use SPARQL, despite the obvious advantages of using a database system. This is due to the mismatch between SPARQL and machine learning tools in terms of data model and programming style. Machine learning tools work on data in tabular format and process it using an imperative programming style, while SPARQL is declarative and has as its basic operation matching graph patterns to RDF triples. We posit that a good interface to knowledge graphs from a machine learning software stack should use an imperative, navigational programming paradigm based on graph traversal rather than the SPARQL query paradigm based on graph patterns. In this paper, we present RDFFrames, a framework that provides such an interface. RDFFrames provides an imperative Python API that gets internally translated to SPARQL, and it is integrated with the PyData machine learning software stack. RDFFrames enables the user to make a sequence of Python calls to define the data to be extracted from a knowledge graph stored in an RDF database system, and it translates these calls into a compact SPQARL query, executes it on the database system, and returns the results in a standard tabular format. Thus, RDFFrames is a useful tool for data preparation that combines the usability of PyData with the flexibility and performance of RDF database systems.},
journal = {The VLDB Journal},
month = aug,
pages = {321–346},
numpages = {26},
keywords = {Knowledge graphs, RDF, SPARQL, PyData, Data preparation, Machine learning}
}

@article{10.1287/trsc.2021.1045,
author = {Morabit, Mouad and Desaulniers, Guy and Lodi, Andrea},
title = {Machine-Learning–Based Column Selection for Column Generation},
year = {2021},
issue_date = {July-August 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {4},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2021.1045},
doi = {10.1287/trsc.2021.1045},
abstract = {Column generation (CG) is widely used for solving large-scale optimization problems. This article presents a new approach based on a machine learning (ML) technique to accelerate CG. This approach, called column selection, applies a learned model to select a subset of the variables (columns) generated at each iteration of CG. The goal is to reduce the computing time spent reoptimizing the restricted master problem at each iteration by selecting the most promising columns. The effectiveness of the approach is demonstrated on two problems: the vehicle and crew scheduling problem and the vehicle routing problem with time windows. The ML model was able to generalize to instances of different sizes, yielding a gain in computing time of up to 30%.},
journal = {Transportation Science},
month = jul,
pages = {815–831},
numpages = {17},
keywords = {column generation, machine learning, column selection}
}

@inproceedings{10.1007/978-3-030-33391-1_18,
author = {Tan, Jeremy and Au, Anselm and Meng, Qingjie and Kainz, Bernhard},
title = {Semi-supervised Learning of Fetal Anatomy from Ultrasound},
year = {2019},
isbn = {978-3-030-33390-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33391-1_18},
doi = {10.1007/978-3-030-33391-1_18},
abstract = {Semi-supervised learning methods have achieved excellent performance on standard benchmark datasets using very few labelled images. Anatomy classification in fetal 2D ultrasound is an ideal problem setting to test whether these results translate to non-ideal data. Our results indicate that inclusion of a challenging background class can be detrimental and that semi-supervised learning mostly benefits classes that are already distinct, sometimes at the expense of more similar classes.},
booktitle = {Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data: First MICCAI Workshop, DART 2019, and First International Workshop, MIL3ID 2019, Shenzhen, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings},
pages = {157–164},
numpages = {8},
keywords = {Semi-supervised learning, Fetal ultrasound},
location = {Shenzhen, China}
}

@article{10.1177/10943420211029302,
author = {Germann, Tim and Alexander, Francis J and Ang, James and Bilbrey, Jenna A and Balewski, Jan and Casey, Tiernan and Chard, Ryan and Choi, Jong and Choudhury, Sutanay and Debusschere, Bert and DeGennaro, Anthony M and Dryden, Nikoli and Ellis, J Austin and Foster, Ian and Cardona, Cristina Garcia and Ghosh, Sayan and Harrington, Peter and Huang, Yunzhi and Jha, Shantenu and Johnston, Travis and Kagawa, Ai and Kannan, Ramakrishnan and Kumar, Neeraj and Liu, Zhengchun and Maruyama, Naoya and Matsuoka, Satoshi and McCarthy, Erin and Mohd-Yusof, Jamaludin and Nugent, Peter and Oyama, Yosuke and Proffen, Thomas and Pugmire, David and Rajamanickam, Sivasankaran and Ramakrishniah, Vinay and Schram, Malachi and Seal, Sudip K and Sivaraman, Ganesh and Sweeney, Christine and Tan, Li and Thakur, Rajeev and Van Essen, Brian and Ward, Logan and Welch, Paul and Wolf, Michael and Xantheas, Sotiris S and Yager, Kevin G and Yoo, Shinjae and Yoon, Byung-Jun},
title = {Co-design Center for Exascale Machine Learning Technologies (ExaLearn)},
year = {2021},
issue_date = {Nov 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {35},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/10943420211029302},
doi = {10.1177/10943420211029302},
abstract = {Rapid growth in data, computational methods, and computing power is driving a remarkable revolution in what variously is termed machine learning (ML), statistical learning, computational learning, and artificial intelligence. In addition to highly visible successes in machine-based natural language translation, playing the game Go, and self-driving cars, these new technologies also have profound implications for computational and experimental science and engineering, as well as for the exascale computing systems that the Department of Energy (DOE) is developing to support those disciplines. Not only do these learning technologies open up exciting opportunities for scientific discovery on exascale systems, they also appear poised to have important implications for the design and use of exascale computers themselves, including high-performance computing (HPC) for ML and ML for HPC. The overarching goal of the ExaLearn co-design project is to provide exascale ML software for use by Exascale Computing Project (ECP) applications, other ECP co-design centers, and DOE experimental facilities and leadership class computing facilities.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {598–616},
numpages = {19},
keywords = {Machine learning, exascale computing, reinforcement learning, active learning, high-performance computing for machine learning, machine learning for high-performance computing}
}

@article{10.1016/j.knosys.2021.107340,
author = {Zhang, Sheng and Chen, Min and Chen, Jincai and Li, Yuan-Fang and Wu, Yiling and Li, Minglei and Zhu, Chuanbo},
title = {Combining cross-modal knowledge transfer and semi-supervised learning for speech emotion recognition},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {229},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107340},
doi = {10.1016/j.knosys.2021.107340},
journal = {Know.-Based Syst.},
month = oct,
numpages = {10},
keywords = {Semi-supervised learning, Cross-modal knowledge transfer, Speech emotion recognition}
}

@article{10.1016/j.parco.2021.102813,
author = {Lawson, John and Goli, Mehdi},
title = {Performance portability through machine learning guided kernel selection in SYCL libraries},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {107},
number = {C},
issn = {0167-8191},
url = {https://doi.org/10.1016/j.parco.2021.102813},
doi = {10.1016/j.parco.2021.102813},
journal = {Parallel Comput.},
month = oct,
numpages = {12},
keywords = {Auto-tuning, SYCL, GPGPU, Machine learning, Performance portability}
}

@inproceedings{10.1007/978-3-030-92121-7_8,
author = {Di Dio, Riccardo and Galligo, Andr\'{e} and Mantzaflaris, Angelos and Mauroy, Benjamin},
title = {Spirometry-Based Airways Disease Simulation and&nbsp;Recognition Using Machine Learning Approaches},
year = {2021},
isbn = {978-3-030-92120-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92121-7_8},
doi = {10.1007/978-3-030-92121-7_8},
abstract = {The purpose of this study is to provide means to physicians for automated and fast recognition of airways diseases. In this work, we mainly focus on measures that can be easily recorded using a spirometer. The signals used in this framework are simulated using the linear bi-compartment model of the lungs. This allows us to simulate ventilation under the hypothesis of ventilation at rest (tidal breathing). By changing the resistive and elastic parameters, data samples are realized simulating healthy, fibrosis and asthma breathing. On this synthetic data, different machine learning models are tested and their performance is assessed. All but the Naive bias classifier show accuracy of at least 99%. This represents a proof of concept that Machine Learning can accurately differentiate diseases based on manufactured spirometry data. This paves the way for further developments on the topic, notably testing the model on real data.},
booktitle = {Learning and Intelligent Optimization: 15th International Conference, LION 15, Athens, Greece, June 20–25, 2021, Revised Selected Papers},
pages = {98–112},
numpages = {15},
keywords = {Lung disease, Machine Learning, Mathematical modeling},
location = {Athens, Greece}
}

@article{10.1002/int.22390,
author = {Petkovi\'{c}, Matej and Kocev, Dragi and \v{S}krlj, Bla\v{z} and D\v{z}eroski, Sa\v{s}o},
title = {Ensemble‐ and distance‐based feature ranking for unsupervised learning},
year = {2021},
issue_date = {July 2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {36},
number = {7},
issn = {0884-8173},
url = {https://doi.org/10.1002/int.22390},
doi = {10.1002/int.22390},
abstract = {In this study, we propose two novel (groups of) methods for unsupervised feature ranking and selection. The first group includes feature ranking scores (Genie3 score, RandomForest score) that are computed from ensembles of predictive clustering trees. The second method is URelief, the unsupervised extension of the Relief family of feature ranking algorithms. Using 26 benchmark data sets and 5 baselines, we show that both the Genie3 score (computed from the ensemble of extra trees) and the URelief method outperform the existing methods and that Genie3 performs best overall, in terms of predictive power of the top‐ranked features. Additionally, we analyze the influence of the hyper‐parameters of the proposed methods on their performance and show that for the Genie3 score the highest quality is achieved by the most efficient parameter configuration. Finally, we propose a way of discovering the location of the features in the ranking, which are the most relevant in reality.},
journal = {Int. J. Intell. Syst.},
month = may,
pages = {3068–3086},
numpages = {19},
keywords = {extra trees, feature ranking, relief, tree ensembles, unsupervised learning}
}

@article{10.1016/j.cageo.2021.104696,
author = {Charifi, Rajaa and Es-sbai, Najia and Zennayi, Yahya and Hosni, Taha and Bourzeix, Fran\c{c}ois and Mansouri, Anass},
title = {Sedimentary phosphate classification based on spectral analysis and machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2021.104696},
doi = {10.1016/j.cageo.2021.104696},
journal = {Comput. Geosci.},
month = may,
numpages = {16},
keywords = {Phosphate, Classification, Spectral analysis, Feature selection, Bhattacharyya distance, Machine learning}
}

@inbook{10.5555/3454287.3455069,
author = {Stretcu, Otilia and Viswanathan, Krishnamurthy and Movshovitz-Attias, Dana and Platanios, Emmanouil Antonios and Tomkins, Andrew and Ravi, Sujith},
title = {Graph agreement models for semi-supervised learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Graph-based algorithms are among the most successful paradigms for solving semi-supervised learning tasks. Recent work on graph convolutional networks and neural graph learning methods has successfully combined the expressiveness of neural networks with graph structures. We propose a technique that, when applied to these methods, achieves state-of-the-art results on semi-supervised learning datasets. Traditional graph-based algorithms, such as label propagation, were designed with the underlying assumption that the label of a node can be imputed from that of the neighboring nodes. However, real-world graphs are either noisy or have edges that do not correspond to label agreement. To address this, we propose Graph Agreement Models (GAM), which introduces an auxiliary model that predicts the probability of two nodes sharing the same label as a learned function of their features. The agreement model is used when training a node classification model by encouraging agreement only for the pairs of nodes it deems likely to have the same label, thus guiding its parameters to better local optima. The classification and agreement models are trained jointly in a co-training fashion. Moreover, GAM can also be applied to any semi-supervised classification problem, by inducing a graph whenever one is not provided. We demonstrate that our method achieves a relative improvement of up to 72% for various node classification models, and obtains state-of-the-art results on multiple established datasets.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {782},
numpages = {11}
}

@article{10.1145/3451179,
author = {Huang, Guyue and Hu, Jingbo and He, Yifan and Liu, Jialong and Ma, Mingyuan and Shen, Zhaoyang and Wu, Juejian and Xu, Yuanfan and Zhang, Hengrui and Zhong, Kai and Ning, Xuefei and Ma, Yuzhe and Yang, Haoyu and Yu, Bei and Yang, Huazhong and Wang, Yu},
title = {Machine Learning for Electronic Design Automation: A Survey},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3451179},
doi = {10.1145/3451179},
abstract = {With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {40},
numpages = {46},
keywords = {Electronic design automation, machine learning, neural networks}
}

@inproceedings{10.1145/3418981.3418984,
author = {Nerurkar, Pranav and Busnel, Yann and Ludinard, Romaric and Shah, Kunjal and Bhirud, Sunil and Patel, Dhiren},
title = {Detecting Illicit Entities in Bitcoin using Supervised Learning of Ensemble Decision Trees},
year = {2020},
isbn = {9781450387705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418981.3418984},
doi = {10.1145/3418981.3418984},
abstract = {Since its inception in 2009, Bitcoin has been mired in controversies for providing a haven for illegal activities. Several types of illicit users hide behind the blanket of anonymity. Uncovering these entities is key for forensic investigations. Current methods utilize machine learning for identifying these illicit entities. However, the existing approaches only focus on a limited category of illicit users. The current paper proposes to address the issue by implementing an ensemble of decision trees for supervised learning. More parameters allow the ensemble model to learn discriminating features that can categorize multiple groups of illicit users from licit users. To evaluate the model, a dataset of 2059 real-life entities on Bitcoin was extracted from the Blockchain. Nine features were engineered to train the model for segregating 28 different licit-illicit categories of users. The proposed model provided a reliable tool for forensic study. Empirical evaluation of the proposed model vis-a-vis three existing benchmark models was performed to highlight its efficacy. Experiments showed that the specificity and sensitivity of the proposed model were comparable to other models.},
booktitle = {Proceedings of the 10th International Conference on Information Communication and Management},
pages = {25–30},
numpages = {6},
keywords = {Bitcoin, Data Mining, Fraud detection, Supervised Learning},
location = {Paris, France},
series = {ICICM '20}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning, Machine Learning Platform, Machine Learning as a Service},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3479239.3485672,
author = {Garc\'{\i}a Mart\'{\i}, Dolores and Badini, Damiano and De Donno, Danilo and Widmer, Joerg},
title = {Scalable Machine Learning Algorithms to Design Massive MIMO Systems},
year = {2021},
isbn = {9781450390774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479239.3485672},
doi = {10.1145/3479239.3485672},
abstract = {Machine learning is a highly promising tool to design the physical layer of wireless communication systems, but its scaling properties for this purpose have not been widely studied. Machine learning algorithms are typically evaluated to learn SISO communications and low modulation orders, whereas current wireless standards use MIMO and high-order modulation schemes to increase capacity. The memory requirements of current Machine learning algorithms for wireless communications increase exponentially with the number of antennas and thus they cannot be used for advanced physical layers and massive MIMO. In this paper, we study the requirements of end-to-end Machine learning models for large-scale MIMO systems, determine the bottlenecks of the architecture, and design different solutions that vastly reduce overhead and allow training higher MIMO and modulation orders. We show that by training the autoencoder in a bit-wise manner, the memory requirements are reduced by several orders of magnitude, which is a critical step for Machine learning-based physical layer design in practical scenarios. Additionally, our design also improves performance over the classical autoencoder for MIMO.},
booktitle = {Proceedings of the 24th International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {167–171},
numpages = {5},
keywords = {MIMO, machine learning, neural networks, physical layer},
location = {Alicante, Spain},
series = {MSWiM '21}
}

@article{10.1007/s42979-021-00592-x,
author = {Sarker, Iqbal H.},
title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00592-x},
doi = {10.1007/s42979-021-00592-x},
abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated&nbsp;applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity&nbsp;systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers&nbsp;in various real-world situations and&nbsp;application areas, particularly from the technical point of view.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {21},
keywords = {Machine learning, Deep learning, Artificial intelligence, Data science, Data-driven decision-making, Predictive analytics, Intelligent applications}
}

@article{10.1145/3451163,
author = {Abououf, Menatalla and Singh, Shakti and Otrok, Hadi and Mizouni, Rabeb and Damiani, Ernesto},
title = {Machine Learning in Mobile Crowd Sourcing: A Behavior-Based Recruitment Model},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3451163},
doi = {10.1145/3451163},
abstract = {With the advent of mobile crowd sourcing (MCS) systems and its applications, the selection of the right crowd is gaining utmost importance. The increasing variability in the context of MCS tasks makes the selection of not only the capable but also the willing workers crucial for a high task completion rate. Most of the existing MCS selection frameworks rely primarily on reputation-based feedback mechanisms to assess the level of commitment of potential workers. Such frameworks select workers having high reputation scores but without any contextual awareness of the workers, at the time of selection, or the task. This may lead to an unfair selection of workers who will not perform the task. Hence, reputation on its own only gives an approximation of workers’ behaviors since it assumes that workers always behave consistently regardless of the situational context. However, following the concept of cross-situational consistency, where people tend to show similar behavior in similar situations and behave differently in disparate ones, this work proposes a novel recruitment system in MCS based on behavioral profiling. The proposed approach uses machine learning to predict the probability of the workers performing a given task, based on their learned behavioral models. Subsequently, a group-based selection mechanism, based on the genetic algorithm, uses these behavioral models in complementation with a reputation-based model to recruit a group of workers that maximizes the quality of recruitment of the tasks. Simulations based on a real-life dataset show that considering human behavior in varying situations improves the quality of recruitment achieved by the tasks and their completion confidence when compared with a benchmark that relies solely on reputation.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {16},
numpages = {28},
keywords = {Machine learning, behavioral profiling, mobile crowd sourcing, selection management, quality of recruitment}
}

@article{10.1007/s00530-020-00733-x,
author = {Lou, Ranran and Lv, Zhihan and Dang, Shuping and Su, Tianyun and Li, Xinfang},
title = {Application of machine learning in ocean data},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {3},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-020-00733-x},
doi = {10.1007/s00530-020-00733-x},
abstract = {In recent years, machine learning has become a hot research method in various fields and has been applied to every aspect of our life, providing an intelligent solution to problems that could not be solved or difficult to be solved before. Machine learning is driven by data. It learns from a part of the input data and builds a model. The model is used to predict and analyze another part of the data to get the results people want. With the continuous advancement of ocean observation technology, the amount of ocean data and data dimensions are rising sharply. The use of traditional data analysis methods to analyze massive amounts of data has revealed many shortcomings. The development of machine learning has solved these shortcomings. Nowadays, the use of machine learning technology to analyze and apply ocean data becomes the focus of scientific research. This method has important practical and long-term significance for protecting the ocean environment, predicting ocean elements, exploring the unknown, and responding to extreme weather. This paper focuses on the analysis of the state of the art and specific practices of machine learning in ocean data, review the application examples of machine learning in various fields such as ocean sound source identification and positioning, ocean element prediction, ocean biodiversity monitoring, and deep-sea resource monitoring. We also point out some constraints that still exist in the research and put forward the future development direction and application prospects.},
journal = {Multimedia Syst.},
month = feb,
pages = {1815–1824},
numpages = {10},
keywords = {Ocean, Data, Ocean data, Machine learning}
}

@inproceedings{10.1007/978-3-030-91452-3_13,
author = {Lee, Chi Hong and Hall, Tracy},
title = {Using Machine Learning to&nbsp;Recognise Novice and&nbsp;Expert Programmers},
year = {2021},
isbn = {978-3-030-91451-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91452-3_13},
doi = {10.1007/978-3-030-91452-3_13},
abstract = {Understanding and recognising the difference between novice and expert programmers could be beneficial in a wide range of scenarios, such as to screen programming job applicants. In this paper, we explore the identification of code author attributes to enable novice/expert differentiation via machine learning models. Our iteratively developed model is based on data from HackerRank, a competitive programming website. Multiple experiments were carried using 10-fold cross-validation. Our final model performed well by differentiating novice coders from expert coders with 71.3% accuracy.},
booktitle = {Product-Focused Software Process Improvement: 22nd International Conference, PROFES 2021, Turin, Italy, November 26, 2021, Proceedings},
pages = {199–206},
numpages = {8},
keywords = {Code, Authorship analysis, Novice programmers, Expert programmers},
location = {Turin, Italy}
}

@phdthesis{10.5555/AAI28722564,
author = {Lim, Robert and Boyana, Norris, and Dejing, Dou, and Camille, Coti, and William, Cresko,},
advisor = {Allen, Malony,},
title = {Accelerating Machine Learning via Multi-Objective Optimization},
year = {2021},
isbn = {9798492727826},
publisher = {University of Oregon},
address = {USA},
abstract = {This dissertation work presents various approaches toward accelerating training of deep neural networks with the use of high-performance computing resources, while balancing learning and systems utilization objectives.  Acceleration of machine learning is formulated as a multi-objective optimization problem that seeks to satisfy multiple objectives, based on its respective constraints.  In machine learning, the objective is to strive for a model that has high accuracy, while eliminating false positives and generalizing beyond the training set.  For systems execution performance, maximizing utilization of the underlying hardware resources within compute and power budgets are constraints that bound the problem.  In both scenarios, the search space is combinatorial and contains multiple local minima that in many cases satisfies the global optimum.  This dissertation work addresses the search of solutions in both performance tuning and neural network training.  Specifically, subgraph matching is proposed to bound the search problem and provide heuristics that guide the solver toward the optimal solution.  Mixed precision operations is also proposed for solving systems of linear equations and for training neural networks for image classification for evaluating the stability and robustness of the operations.  Use cases are presented with CUDA performance tuning and neural network training, demonstrating the effectiveness of the proposed technique.  The experiments were carried out on single and multi-node GPU clusters, and reveals opportunities for further exploration in this critical hardware/software co-design space of accelerated machine learning.},
note = {AAI28722564}
}

@inproceedings{10.1007/978-3-030-61746-2_15,
author = {Li, Xinran and Ma, Wenxing and Zhou, Zan and Xu, Changqiao},
title = {XSS Attack Detection Model Based on&nbsp;Semi-supervised Learning Algorithm with Weighted Neighbor Purity},
year = {2020},
isbn = {978-3-030-61745-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61746-2_15},
doi = {10.1007/978-3-030-61746-2_15},
abstract = {With the popularity of web applications, cyber security is becoming more and more important. The most common web attack is cross-site scripting (XSS), which can be easily constructed in malicious URLs. However, the existing methods of detecting XSS attacks are suffering from the lack of labeled data, and some semi-supervised methods still have the problem of mislabeling. In this paper, we propose a novel XSS attack detection model based on semi-supervised learning algorithm with weighted neighbor purity. Semi-supervised learning can make best use of little labeled data, and a simple mechanism of neighbor purity using weighted-kNN is applied to rectify mislabeled samples, improving classification accuracy. To verify the feasibility of our solution in real-world scenario, we collected real HTTP requests in the China Education and Research Network (CERNET) as training data. The comparison experiment shows that proposed method performs better than a well-known semi-supervised algorithm and a recently published ensemble learning method in different initially labeled rates.},
booktitle = {Ad-Hoc, Mobile, and Wireless Networks: 19th International Conference on Ad-Hoc Networks and Wireless, ADHOC-NOW 2020, Bari, Italy, October 19–21, 2020, Proceedings},
pages = {198–213},
numpages = {16},
keywords = {XSS attack, Machine learning, Semi-supervised learning, Binary classification},
location = {Bari, Italy}
}

@article{10.1007/s11263-019-01246-5,
author = {Spampinato, C. and Palazzo, S. and D’Oro, P. and Giordano, D. and Shah, M.},
title = {Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {5},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01246-5},
doi = {10.1007/s11263-019-01246-5},
abstract = {Human behavior understanding in videos is a complex, still unsolved problem and requires to accurately model motion at both the local (pixel-wise dense prediction) and global (aggregation of motion cues) levels. Current approaches based on supervised learning require large amounts of annotated data, whose scarce availability is one of the main limiting factors to the development of general solutions. Unsupervised learning can instead leverage the vast amount of videos available on the web and it is a promising solution for overcoming the existing limitations. In this paper, we propose an adversarial GAN-based framework that learns video representations and dynamics through a self-supervision mechanism in order to perform dense and global prediction in videos. Our approach synthesizes videos by (1) factorizing the process into the generation of static visual content and motion, (2) learning a suitable representation of a motion latent space in order to enforce spatio-temporal coherency of object trajectories, and (3) incorporating motion estimation and pixel-wise dense prediction into the training procedure. Self-supervision is enforced by using motion masks produced by the generator, as a co-product of its generation process, to supervise the discriminator network in performing dense prediction. Performance evaluation, carried out on standard benchmarks, shows that our approach is able to learn, in an unsupervised way, both local and global video dynamics. The learned representations, then, support the training of video object segmentation methods with sensibly less (about 50%) annotations, giving performance comparable to the state of the art. Furthermore, the proposed method achieves promising performance in generating realistic videos, outperforming state-of-the-art approaches especially on motion-related metrics.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {1378–1397},
numpages = {20},
keywords = {Generative adversarial networks, Video generation, Unsupervised learning, Video object segmentation}
}

@inproceedings{10.5555/3295222.3295276,
author = {Tung, Hsiao-Yu Fish and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina},
title = {Self-supervised learning of motion capture},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5242–5252},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{10.1145/3418034,
author = {Cummings, Mary L. and Li, Songpo},
title = {Subjectivity in the Creation of Machine Learning Models},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3418034},
doi = {10.1145/3418034},
abstract = {Transportation analysts are inundated with requests to apply popular machine learning modeling techniques to datasets to uncover never-before-seen relationships that could potentially revolutionize safety, congestion, and mobility. However, the results from such models can be influenced not just by biases in underlying data, but also through practitioner-induced biases. To demonstrate the significant number of subjective judgments made in the development and interpretation of machine learning models, we developed Logistic Regression and Neural Network models for transportation-focused datasets including those looking at driving injury/fatalities and pedestrian fatalities. We then developed five different representations of feature importance for each dataset, including different feature interpretations commonly used in the machine learning community. Twelve distinct judgments were highlighted in the development and interpretation of these models, which produced inconsistent results. Such inconsistencies can lead to very different interpretations of the results, which can lead to errors of commission and omission, with significant cost and safety implications if policies are erroneously adapted from such outcomes.},
journal = {J. Data and Information Quality},
month = may,
articleno = {7},
numpages = {19},
keywords = {Bias, interpretable machine learning, transportation, logistic regression, subjectivity}
}

@phdthesis{10.5555/AAI28644688,
author = {Zhang, Clark and Pratik, Chaudhari, and Dinesh, Jayaraman, and Szymon, Jakubczak,},
advisor = {Alejandro, Ribeiro,},
title = {Machine Learning for Robot Motion Planning},
year = {2021},
isbn = {9798535569383},
publisher = {University of Pennsylvania},
address = {USA},
abstract = {Robot motion planning is a field that encompasses many different problems and algorithms. From the traditional piano mover's problem to more complicated kinodynamic planning problems, motion planning requires a broad breadth of human expertise and time to design well functioning algorithms. A traditional motion planning pipeline consists of modeling a system and then designing a planner and planning heuristics. Each part of this pipeline can incorporate machine learning. Planners and planning heuristics can benefit from machine learned heuristics, while system modeling can benefit from model learning. Each aspect of the motion planning pipeline comes with trade offs between computational effort and human effort. This work explores algorithms that allow motion planning algorithms and frameworks to find a compromise between the two. First, a framework for learning heuristics for sampling-based planners is presented. The efficacy of the framework depends on human designed features and policy architecture. Next, a framework for learning system models is presented that incorporates human knowledge as constraints. The amount of human effort can be modulated by the quality of the constraints given. Lastly, semi-automatic constraint generation is explored to enable a larger range of trade-offs between human expert constraint generation and data driven constraint generation. We apply these techniques and show results in a variety of robotic systems.},
note = {AAI28644688}
}

@inproceedings{10.1145/3447548.3469451,
author = {Katariya, Sumeet and Rao, Nikhil and Reddy, Chandan K.},
title = {Workshop on Data-Efficient Machine Learning (DeMaL)},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469451},
doi = {10.1145/3447548.3469451},
abstract = {The recent increase in the size of neural networks has led to a proportional increase in the demands for high-quality human-annotated data. Labeling data is a costly and time-consuming endeavor, and the need for large data is often satiated through creative techniques such as data augmentation, transfer learning, self-supervised learning, active learning, to name a few. Many of these techniques are designed for specific data types such as images, text, and speech. The data in many data-mining applications however is multi-modal in nature, has implicit signals from user-interactions, and involves multiple agents. Given the uniqueness, importance, and growing interest in these problems, we feel that the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD) 2021 is an appropriate venue for running a workshop on Data-efficient Machine Learning. In this proposal, we discuss our vision for this workshop.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4135–4136},
numpages = {2},
keywords = {active learning, crowdsourcing, data augmentation, self-supervised, semi-supervised, unsupervised},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1007/978-3-030-37599-7_42,
author = {Fabris, Fabio and Freitas, Alex A.},
title = {Analysing the Overfit of the Auto-sklearn Automated Machine Learning Tool},
year = {2019},
isbn = {978-3-030-37598-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37599-7_42},
doi = {10.1007/978-3-030-37599-7_42},
abstract = {With the ever-increasing number of pre-processing and classification algorithms, manually selecting the best algorithm and their best hyper-parameter settings (i.e. the best classification workflow) is a daunting task. Automated Machine Learning (Auto-ML) methods have been recently proposed to tackle this issue. Auto-ML tools aim to automatically choose the best classification workflow for a given dataset. In this work we analyse the predictive accuracy and overfit of the state-of-the-art auto-sklearn tool, which iteratively builds a classification ensemble optimised for the user’s dataset. This work has 3 contributions. First, we measure 3 types of auto-sklearn’s overfit, involving the differences of predictive accuracies measured on different data subsets: two parts of the training set (for learning and internal validation of the model) and the hold-out test set used for final evaluation. Second, we analyse the distribution of types of classification models selected by auto-sklearn across all 17 datasets. Third, we measure correlations between predictive accuracies on different data subsets and different types of overfitting. Overall, substantial degrees of overfitting were found in several datasets, and decision tree ensembles were the most frequently selected types of models.},
booktitle = {Machine Learning, Optimization, and Data Science: 5th International Conference, LOD 2019, Siena, Italy, September 10–13, 2019, Proceedings},
pages = {508–520},
numpages = {13},
keywords = {Automated Machine Learning, Overfit, Classification},
location = {Siena, Italy}
}

@inproceedings{10.1007/978-3-030-97759-7_3,
author = {Liao, Chunhua and Wang, Anjia and Georgakoudis, Giorgis and de Supinski, Bronis R. and Yan, Yonghong and Beckingsale, David and Gamblin, Todd},
title = {Extending OpenMP for Machine Learning-Driven Adaptation},
year = {2021},
isbn = {978-3-030-97758-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97759-7_3},
doi = {10.1007/978-3-030-97759-7_3},
abstract = {OpenMP 5.0 introduced the 
[inline-graphic not available: see fulltext]
 directive to support compile-time selection from a set of directive variants based on OpenMP context. OpenMP 5.1 extended context information to include user-defined conditions that enable user-guided runtime adaptation. However, defining conditions that capture the complex interactions between applications and hardware platforms to select an optimized variant is challenging for programmers. This paper explores a novel approach to automate runtime adaptation through machine learning. We design a new 
[inline-graphic not available: see fulltext]
 directive to describe semantics for model-driven adaptation and also develop a prototype implementation. Using the Smith-Waterman algorithm as a use-case, our experiments demonstrate that the proposed adaptive OpenMP extension automatically chooses the code variants that deliver the best performance in heterogeneous platforms that consist of CPU and GPU processing capabilities. Using decision tree models for tuning has an accuracy of up&nbsp;to 93.1% in selecting the optimal variant, with negligible runtime overhead.},
booktitle = {Accelerator Programming Using Directives: 8th International Workshop, WACCPD 2021, Virtual Event, November 14, 2021, Proceedings},
pages = {49–69},
numpages = {21},
keywords = {OpenMP, Machine Learning, Runtime Adaptation}
}

@inproceedings{10.1007/978-3-030-88052-1_2,
author = {Wang, Gao and Wang, Gaoli},
title = {Improved Differential-ML Distinguisher: Machine Learning Based Generic Extension for Differential Analysis},
year = {2021},
isbn = {978-3-030-88051-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88052-1_2},
doi = {10.1007/978-3-030-88052-1_2},
abstract = {At CRYPTO 2019, Gohr first proposes a deep learning based differential analysis on round-reduced Speck32/64. Then Yadav etal. present a framework to construct the differential-ML (machine learning) distinguisher by combining the traditional differential distinguisher and the machine learning based differential distinguisher, which breaks the limit of the ML differential distinguisher on the number of attack rounds. However, the results obtained based on this method are not necessarily better than the results gained by traditional analysis. In this paper, we offer three novel greedy strategies (M1, M2 and M3) to solve this problem. The strategy M1 provides better differential-ML distinguishers by considering all combinations of classical differential distinguishers and ML differential distinguishers. And the strategy M2 uses the best ML differential distinguishers to splice classical differential distinguishers forward, while the strategy M3 adopts the best classical differential distinguishers to splice ML differential distinguishers. As proof of works, we apply our methods to round-reduced Speck32/64, Speck48/72 and Speck64/96 and get some improved cryptanalysis results. For the construction of differential-ML distinguishers, we can reach 11-round Speck32/64, 14-round Speck48/72 and 18-round Speck64/96 with 227, 245, 262 data respectively.},
booktitle = {Information and Communications Security: 23rd International Conference, ICICS 2021, Chongqing, China, November 19-21, 2021, Proceedings, Part II},
pages = {21–38},
numpages = {18},
keywords = {Differential analysis, Machine learning, Lightweight ciphers, Speck},
location = {Chongqing, China}
}

@inproceedings{10.1145/3447548.3470804,
author = {Wang, Xin and Zhu, Wenwu},
title = {Automated Machine Learning on Graph},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470804},
doi = {10.1145/3447548.3470804},
abstract = {Machine learning on graphs has been extensively studiedin both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attentions from the research community. In this tutorial, we discuss AutoML on graphs, primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. To the best of our knowledge, this tutorial is the first to systematically and comprehensively review automated machine learning on graphs, possessing a great potential to draw a large amount of interests in the community.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4082–4083},
numpages = {2},
keywords = {automl, graph representation learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1007/978-3-030-26142-9_6,
author = {Wu, William},
title = {Weakly Supervised Learning by a Confusion Matrix of Contexts},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_6},
doi = {10.1007/978-3-030-26142-9_6},
abstract = {Context consideration can help provide more background and related information for weakly supervised learning. The inclusion of less documented historical and environmental context in researching diabetes amongst Pima Indians uncovered reasons which were more likely to explain why some Pima Indians had much higher rates of diabetes than Caucasians, primarily due to historical, environmental and social causes rather than their specific genetic patterns or ethnicity as suggested by many medical studies.If historical and environmental factors are considered as external contexts when not included as part of a dataset for research, some forms of internal contexts may also exist inside the dataset without being declared. This paper discusses a context construction model that transforms a confusion matrix into a matrix of categorical, incremental and correlational context to emulate a kind of internal context to search for more informative patterns in order to improve weakly supervised learning from limited labeled samples for unlabeled data.When the negative and positive labeled samples and misclassification errors are compared to “happy families” and “unhappy families”, the contexts constructed by this model in the classification experiments reflected the Anna Karenina principle well - “Happy families are all alike; every unhappy family is unhappy in its own way”, an encouraging sign to further explore contexts associated with harmonizing patterns and divisive causes for knowledge discovery in a world of uncertainty.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {59–64},
numpages = {6},
keywords = {Weakly supervised learning, Context, Confusion matrix, Context construction, Contextual analysis},
location = {Macau, China}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@inproceedings{10.1145/3461702.3462521,
author = {Dai, Jessica and Fazelpour, Sina and Lipton, Zachary},
title = {Fair Machine Learning Under Partial Compliance},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462521},
doi = {10.1145/3461702.3462521},
abstract = {Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {55–65},
numpages = {11},
keywords = {distributive justice, fair machine learning, fairness, hiring, regulation, segregation, simulations},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1007/978-3-030-87231-1_12,
author = {Cheng, Kai and Ma, Yiting and Sun, Bin and Li, Yang and Chen, Xuejin},
title = {Depth Estimation for Colonoscopy Images with Self-supervised Learning from Videos},
year = {2021},
isbn = {978-3-030-87230-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87231-1_12},
doi = {10.1007/978-3-030-87231-1_12},
abstract = {Depth estimation in colonoscopy images provides geometric clues for downstream medical analysis tasks, such as polyp detection, 3D reconstruction, and diagnosis. Recently, deep learning technology has made significant progress in monocular depth estimation for natural scenes. However, without sufficient ground truth of dense depth maps for colonoscopy images, it is significantly challenging to train deep neural networks for colonoscopy depth estimation. In this paper, we propose a novel approach that makes full use of both synthetic data and real colonoscopy videos. We use synthetic data with ground truth depth maps to train a depth estimation network with a generative adversarial network model. Despite the lack of ground truth depth, real colonoscopy videos are used to train the network in a self-supervision manner by exploiting temporal consistency between neighboring frames. Furthermore, we design a masked gradient warping loss in order to ensure temporal consistency with more reliable correspondences. We conducted both quantitative and qualitative analysis on an existing synthetic dataset and a set of real colonoscopy videos, demonstrating the superiority of our method on more accurate and consistent depth estimation for colonoscopy images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VI},
pages = {119–128},
numpages = {10},
keywords = {Colonoscopy, Depth estimation, Self-supervised learning, Videos, Temporal consistency},
location = {Strasbourg, France}
}

@article{10.1016/j.patrec.2018.08.008,
author = {He, Fang and Wang, Rong and Jia, Weimin},
title = {Fast semi-supervised learning with anchor graph for large hyperspectral images},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.08.008},
doi = {10.1016/j.patrec.2018.08.008},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {319–326},
numpages = {8},
keywords = {Hyperspectral images (HSI) classification, Graph-based semi-supervised learning (SSL), Anchor graph}
}

@article{10.1109/MM.2020.3016551,
author = {Litz, Heiner and Hashemi, Milad},
title = {Machine Learning for Systems},
year = {2020},
issue_date = {Sept.-Oct. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {40},
number = {5},
issn = {0272-1732},
url = {https://doi.org/10.1109/MM.2020.3016551},
doi = {10.1109/MM.2020.3016551},
abstract = {The six papers in this special section focus on machine learning for computer systems. Specialized computer systems have driven the performance and capability of deep learning over the past decade.1 However, as machine learning models and systems improve, there is a growing opportunity to also use these models to improve how we design, architect, optimize, and automate computer systems and software. This is a challenging area, both from a learning and a systems perspective. Systems often impose tight size, latency, or reliability constraints on learning mechanisms that do not arise in other applications of machine learning, such as computer vision or natural language processing. From a learning perspective, systems is a challenging application, where input features are often large and sparse, action spaces are gigantic, and generalization is a key attribute.},
journal = {IEEE Micro},
month = sep,
pages = {6–7},
numpages = {2}
}

@inproceedings{10.1145/3368089.3418780,
author = {Gisi, Joshua},
title = {Synthesizing correct code for machine learning programs},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418780},
doi = {10.1145/3368089.3418780},
abstract = {Success using machine learning (ML) in numerous fields has created a new class of users, who are not experts in the data science domain but want to use ML as a means to solve their inference problems. Various automatic machine learning (AutoML) approaches attempt to make ML solutions accessible to such users. In this work, we present a system that automatically synthesizes correct code within the context of the user’s data using sketching. In sketching, insight is determined through a partial program; a sketch expresses the high-level structure of implementation but leaves holes in place of the low-level details. We use meta-learning on meta-features to approximately solve holes. We observe that the sketch-based approach is more expressive, easier to implement, and easier to optimize than existing AutoML frameworks. Our initial results are very promising. Our approach uses fewer resources and still produces comparable results to existing techniques.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1701–1703},
numpages = {3},
keywords = {Machine Learning, Program Synthesis, Sketch, autoML},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3468264.3468614,
author = {Cito, J\"{u}rgen and Dillig, Isil and Kim, Seohyun and Murali, Vijayaraghavan and Chandra, Satish},
title = {Explaining mispredictions of machine learning models using rule induction},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468614},
doi = {10.1145/3468264.3468614},
abstract = {While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {716–727},
numpages = {12},
keywords = {explainability, machine learning, rule induction},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@phdthesis{10.5555/AAI29248048,
author = {Liu, Fred},
advisor = {Lars, Stentoft, and Tim, Conley, and Charles, Saunders,},
title = {Essays in Financial Econometrics and Machine Learning},
year = {2021},
isbn = {9798845497796},
publisher = {The University of Western Ontario (Canada)},
abstract = {Financial econometrics is a highly interdisciplinary field that integrates finance, economics, probability, statistics, and applied mathematics. Machine learning is a growing area in finance that is particularly suitable for studying problems with many variables. My thesis contains three chapters that explore financial econometrics and machine learning in the fields of asset pricing and risk management.  Chapter 2 studies the implications of the new Basel 3 regulations. In 2019, the BCBS finalized the Basel 3 regulatory regime, which changes the regulatory measure of market risk and adds new complex calculations based on liquidity and risk factors. This chapter is motivated by these changes and seeks to answer the question of how regulation affects banks' choice of risk-management models, whether it incentivizes them to use correctly specified models, and if it results in more stable capital requirements.  Chapter 3 conducts, to our knowledge, the largest study ever of five-minute equity market returns using state-of-the-art machine learning models trained on the cross-section of lagged market index constituent returns, where we show that regularized linear models and nonlinear tree-based models yield significant market return predictability. Ensemble models perform the best across time and their predictability translates into economically significant Sharpe ratios of 0.98 after transaction costs. These results provide strong evidence that intraday market returns are predictable during short time horizons.  Chapter 4 studies the idiosyncratic tail risk premium and common factor. Stocks in the highest idiosyncratic tail risk decile earn 8% higher average annualized returns than in the lowest. I propose a risk-based explanation for this premium, in which shocks to intermediary funding cause idiosyncratic tail risk to follow a strong factor structure, and the factor, common idiosyncratic tail risk (CITR), comoves with intermediary funding. Consequently, firms with high idiosyncratic tail risk have high exposure to CITR shocks, and command a risk premium due to their low returns when intermediary constraints tighten. To test my explanation, I create a novel measure of idiosyncratic tail risk. Consistent with my explanation, CITR shocks are procyclical, are correlated to intermediary factors, are priced in assets, and explain the idiosyncratic tail risk premium.},
note = {AAI29248048}
}

@inproceedings{10.1007/978-3-030-86514-6_26,
author = {Zigrand, Louis and Alizadeh, Pegah and Traversi, Emiliano and Wolfler Calvo, Roberto},
title = {Machine Learning Guided Optimization for Demand Responsive Transport Systems},
year = {2021},
isbn = {978-3-030-86513-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86514-6_26},
doi = {10.1007/978-3-030-86514-6_26},
abstract = {Most of the time, objective functions used for solving static combinatorial optimization problems cannot deal efficiently with their real-time counterparts. It is notably the case of Shared Mobility Systems where the dispatching framework must adapt itself dynamically to the demand. More precisely, in the context of Demand Responsive Transport (DRT) services, various objective functions have been proposed in the literature to optimize the vehicles routes. However, these objective functions are limited in practice because they discard the dynamic evolution of the demand. To overcome such a limitation, we propose a Machine Learning Guided Optimization methodology to build a new objective function based on simulations and historical data. This way, we are able to take the demand’s dynamic evolution into account. We also present how to design the main components of the proposed framework to fit a DRT application: data generation and evaluation, training process and model optimization. We show the efficiency of our proposed methodology on real-world instances, obtained in a collaboration with Padam Mobility, an international company developing Shared Mobility Systems.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part IV},
pages = {420–436},
numpages = {17},
keywords = {Demand responsive transport, Surrogate modeling, Combinatorial optimization},
location = {Bilbao, Spain}
}

@article{10.1007/s10994-021-05975-y,
author = {Liang, Jiye and Cui, Junbiao and Wang, Jie and Wei, Wei},
title = {Graph-based semi-supervised learning via improving the quality of the graph dynamically},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {6},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-05975-y},
doi = {10.1007/s10994-021-05975-y},
abstract = {Graph-based semi-supervised learning (GSSL) is an important paradigm among semi-supervised learning approaches and includes the two processes of graph construction and label inference. In most traditional GSSL methods, the two processes are completed independently. Once the graph is constructed, the result of label inference cannot be changed. Therefore, the quality of the graph directly determines the GSSL’s performance. Most traditional graph construction methods make certain assumptions about the data distribution, resulting in the quality of the graph heavily depends on the correctness of these assumptions. Therefore, it is difficult to handle complex and various data distribution for traditional graph construction methods. To overcome such issues, this paper proposes a framework named Graph-based Semi-supervised Learning via Improving the Quality of the Graph Dynamically. In it, the graph construction based on the weighted fusion of multiple clustering results and the label inference are integrated into a unified framework to achieve their mutual guidance and dynamic improvement. Moreover, the proposed framework is a general framework, and most existing GSSL methods can be embedded into it so as to improve their performance. Finally, the working mechanism, the effectiveness in improving the performance of GSSL methods and the advantage compared with other GSSL methods based on dynamic graph construction methods of the proposal are verified through systematic experiments.},
journal = {Mach. Learn.},
month = jun,
pages = {1345–1388},
numpages = {44},
keywords = {Semi-supervised learning, Graph construction, Clustering, Label inference}
}

@inproceedings{10.1145/3439706.3446896,
author = {Dhar, Tonmoy and Kunal, Kishor and Li, Yaguang and Lin, Yishuang and Madhusudan, Meghna and Poojary, Jitesh and Sharma, Arvind K. and Burns, Steven M. and Harjani, Ramesh and Hu, Jiang and Mukherjee, Parijat and Yaldiz, Soner and Sapatnekar, Sachin S.},
title = {Machine Learning Techniques in Analog Layout Automation},
year = {2021},
isbn = {9781450383004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439706.3446896},
doi = {10.1145/3439706.3446896},
abstract = {The quality of layouts generated by automated analog design have traditionally not been able to match those from human designers over a wide range of analog designs. The ALIGN (Analog Layout, Intelligently Generated from Netlists) project [2, 3, 6] aims to build an open-source analog layout engine [1] that overcomes these challenges, using a variety of approaches. An important part of the toolbox is the use of machine learning (ML) methods, combined with traditional methods, and this talk overviews our efforts. The input to ALIGN is a SPICE-like netlist and a set of perfor- mance specifications, and the output is a GDSII layout. ALIGN automatically recognizes hierarchies in the input netlist. To detect variations of known blocks in the netlist, approximate subgraph iso- morphism methods based on graph convolutional networks can be used [5]. Repeated structures in a netlist are typically constrained by layout requirements related to symmetry or matching. In [7], we use a mix of graph methods and ML to detect symmetric and array structures, including the use of neural network based approximate matching through the use of the notion of graph edit distances. Once the circuit is annotated, ALIGN generates the layout, going from the lowest level cells to higher levels of the netlist hierarchy. Based on an abstraction of the process design rules, ALIGN builds parameterized cell layouts for each structure, accounting for the need for common centroid layouts where necessary [11]. These cells then undergo placement and routing that honors the geomet- ric constraints (symmetry, common-centroid). The chief parameter that changes during layout is the set of interconnect RC parasitics: excessively large RCs could result in an inability to meet perfor- mance. These values can be controlled by reducing the distance between blocks, or, in the case of R, by using larger effective wire widths (using multiple parallel connections in FinFET technologies where wire widths are quantized) to reduce the effective resistance. ALIGN has developed several approaches based on ML for this purpose [4, 8, 9] that rapidly predict whether a layout will meet the performance constraints that are imposed at the circuit level, and these can be deployed together with conventional algorithmic methods [10] to rapidly prune out infeasible layouts. This presentation overviews our experience in the use of ML- based methods in conjunction with conventional algorithmic ap- proaches for analog design. We will show (a) results from our efforts so far, (b) appropriate methods for mixing ML methods with tra- ditional algorithmic techniques for solving the larger problem of analog layout, (c) limitations of ML methods, and (d) techniques for overcoming these limitations to deliver workable solutions for analog layout automation.},
booktitle = {Proceedings of the 2021 International Symposium on Physical Design},
pages = {71–72},
numpages = {2},
keywords = {analog layout automation, machine learning},
location = {Virtual Event, USA},
series = {ISPD '21}
}

@article{10.1504/ijwgs.2021.118395,
author = {Mousavi, Mitra and Rezazadeh, Javad and Sianaki, Omid Ameri},
title = {Machine learning applications for fog computing in IoT: a survey},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {17},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2021.118395},
doi = {10.1504/ijwgs.2021.118395},
abstract = {Today, internet of things (IoT) has become an important paradigm. Everyday increasing number of IoT applications and services emerge. Smart devices connected by the IoT generate significant amounts of data. Analysis IoT sensor data using machine learning algorithms is a key to achieve useful information for prediction, classification, data association and data conceptualisation. Offloading input data to cloud servers leads to increased communication costs. Undertaking data analytics at the network edge using fog computing enables the rapid processing of incoming data for real-time response. In this paper, we examine the results of using different machine learning algorithms on fog nodes based on existing research. These results are low latency, high accuracy and low bandwidth. Also, this work presents the current fog computing architecture which consists of different layers that distribute computing, storage, control and networking and finally we investigate the challenges and open issues related to the deployment of machine learning on fog nodes.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {293–320},
numpages = {27},
keywords = {internet of things, IoT, fog computing, machine learning, fog-based machine learning}
}

@article{10.1145/3436755,
author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
title = {When Machine Learning Meets Privacy: A Survey and Outlook},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436755},
doi = {10.1145/3436755},
abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {31},
numpages = {36},
keywords = {Machine learning, deep learning, differential privacy, privacy}
}

@article{10.1016/j.procs.2021.01.127,
author = {Moroff, Nikolas Ulrich and Kurt, Ersin and Kamphues, Josef},
title = {Machine Learning and Statistics: A Study for assessing innovative Demand Forecasting Models},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {180},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.01.127},
doi = {10.1016/j.procs.2021.01.127},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {40–49},
numpages = {10},
keywords = {Demand Forecast, Machine Learning, Statistical Methods, Deep Learning}
}

@article{10.1145/3475965.3479315,
author = {Molino, Piero and R\'{e}, Christopher},
title = {Declarative Machine Learning Systems: The future of machine learning will depend on it being in the hands of the rest of us.},
year = {2021},
issue_date = {May-June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/3475965.3479315},
doi = {10.1145/3475965.3479315},
abstract = {The people training and using ML models now are typically experienced developers with years of study working within large organizations, but the next wave of ML systems should allow a substantially larger number of people, potentially without any coding skills, to perform the same tasks. These new ML systems will not require users to fully understand all the details of how models are trained and used for obtaining predictions, but will provide them a more abstract interface that is less demanding and more familiar. Declarative interfaces are well-suited for this goal, by hiding complexity and favoring separation of interest, and ultimately leading to increased productivity.},
journal = {Queue},
month = aug,
pages = {46–76},
numpages = {31}
}

@article{10.1016/j.ijcci.2021.100281,
author = {Vartiainen, Henriikka and Toivonen, Tapani and Jormanainen, Ilkka and Kahila, Juho and Tedre, Matti and Valtonen, Teemu},
title = {Machine learning for middle schoolers: Learning through data-driven design},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {C},
issn = {2212-8689},
url = {https://doi.org/10.1016/j.ijcci.2021.100281},
doi = {10.1016/j.ijcci.2021.100281},
journal = {Int. J. Child-Comp. Interact.},
month = sep,
numpages = {12},
keywords = {AI, Machine learning, K-12, Computational thinking, Design-oriented pedagogy, Design-based research}
}

@inproceedings{10.1007/978-3-030-88081-1_58,
author = {Ta\c{s}delen, Osman and \c{C}arkacioglu, Levent and T\"{o}reyin, Beh\c{c}et U\u{g}ur},
title = {Anomaly Detection on ADS-B Flight Data Using Machine Learning Techniques},
year = {2021},
isbn = {978-3-030-88080-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88081-1_58},
doi = {10.1007/978-3-030-88081-1_58},
abstract = {With the rapid increase in the number of flights all over the world, the management and control of flight operations has become difficult in recent years. Moreover, the expectations for the aviation sector indicate that this increase will continue in the upcoming years. Therefore, safer and systematic monitoring systems by eliminating the requirement of human-dependent tracking during the air travel of an aircraft and automating the detection of abnormal situations has become a major problem in aviation sector. With the recent advances in artificial intelligence, a safer and systematic tracking system for controlling the airspace by eliminating the need for human-dependent tracking during the flight of aircraft in the air has become possible.In this study, we aimed to create a system that detects and predicts movements to indicate abnormal, dangerous situations in the airspace by monitoring radar flight data using machine learning and deep learning techniques. We applied two different methods, i.e., Proximity Based kNN and Auto Encoder We used real-life historical radar flight data set which consists of Flight Radar 24 data were converted from ADS-B messages for learning. We created simulation data and used this data for testing and validation for our trained model. Within the scope of this project, we also developed a system to monitor air traffic through radar tracks with our model and present the abnormal situations to the user through a visual interface for decision support. In this visualization, we present the abnormal situations if one of the algorithms labeled as anomaly. Results for both methods have shown that our findings were similar to the real-life predictions.},
booktitle = {Computational Collective Intelligence: 13th International Conference, ICCCI 2021, Rhodes, Greece, September 29 – October 1, 2021, Proceedings},
pages = {771–783},
numpages = {13},
keywords = {Anomaly detection, Deep learning, Machine learning, Proximity based kNN, Auto encoder, Flight control, ADS-B, Air traffic management},
location = {Rhodos, Greece}
}

@article{10.1137/20M1332827,
author = {Bar, Leah and Sochen, Nir},
title = {Strong Solutions for PDE-Based Tomography by Unsupervised Learning},
year = {2021},
issue_date = {January 2021},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {14},
number = {1},
url = {https://doi.org/10.1137/20M1332827},
doi = {10.1137/20M1332827},
abstract = {We introduce a novel neural network-based PDEs solver for forward and inverse problems. The solver is grid free, mesh free, and shape free, and the solution is approximated by a neural network.
We employ an unsupervised approach such that the input to the network is a point set in an arbitrary domain, and the output is the
set of the corresponding function values.  The network is trained to minimize deviations of the learned function from the PDE solution and
satisfy the boundary conditions.
The resulting solution in turn is an explicit, smooth, differentiable function with a known analytical form. We solve the forward problem (observations given the underlying model's parameters), semi-inverse problem (model's parameters given the observations in the whole domain), and full tomography inverse problem (model's parameters given the observations on the boundary) by solving the forward and semi-inverse problems at the same time.
The optimized loss function consists of few elements: fidelity term of $L_2$ norm that enforces the PDE in the weak sense, an $L_infty$ norm term that enforces pointwise fidelity and thus promotes a strong solution, and boundary and initial conditions constraints. It further accommodates regularizers for the solution and/or the model's parameters of the differential operator. This setting is flexible in the sense that regularizers can be tailored to specific  problems. We demonstrate our method on several free shape two dimensional (2D) second order systems with application to electrical impedance tomography (EIT) and diffusion equation. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework enables,  in principle, the solution of high order and high dimensional nonlinear PDEs.},
journal = {SIAM J. Img. Sci.},
month = jan,
pages = {128–155},
numpages = {28},
keywords = {PDEs, forward problems, inverse problems, unsupervised learning, deep networks, EIT, 35C99, 65M32, 65N21}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1007/978-3-030-95470-3_22,
author = {Huber, Marc and Raidl, G\"{u}nther R.},
title = {Learning Beam Search: Utilizing Machine Learning to&nbsp;Guide Beam Search for&nbsp;Solving Combinatorial Optimization Problems},
year = {2021},
isbn = {978-3-030-95469-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95470-3_22},
doi = {10.1007/978-3-030-95470-3_22},
abstract = {Beam search (BS) is a well-known incomplete breadth-first-search variant frequently used to find heuristic solutions to hard combinatorial optimization problems. Its key ingredient is a guidance heuristic that estimates the expected length (cost) to complete a partial solution. While this function is usually developed manually for a specific problem, we propose a more general Learning Beam Search (LBS) that uses a machine learning model for guidance. Learning is performed by utilizing principles of reinforcement learning: LBS generates training data on its own by performing nested BS calls on many representative randomly created problem instances. The general approach is tested on two specific problems, the longest common subsequence problem and the constrained variant thereof. Results on established sets of benchmark instances indicate that the BS with models trained via LBS is highly competitive. On many instances new so far best solutions could be obtained, making the approach a new state-of-the-art method for these problems and documenting the high potential of this general framework.},
booktitle = {Machine Learning, Optimization, and Data Science: 7th International Conference, LOD 2021, Grasmere, UK, October 4–8, 2021, Revised Selected Papers, Part II},
pages = {283–298},
numpages = {16},
keywords = {Beam search, Combinatorial optimization, Machine learning, Longest common subsequence problem},
location = {Grasmere, United Kingdom}
}

@article{10.1007/s11633-020-1275-7,
author = {Alqwadri, Ahmad and Azzeh, Mohammad and Almasalha, Fadi},
title = {Application of Machine Learning for Online Reputation Systems},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-020-1275-7},
doi = {10.1007/s11633-020-1275-7},
abstract = {Users on the Internet usually require venues to provide better purchasing recommendations. This can be provided by a reputation system that processes ratings to provide recommendations. The rating aggregation process is a main part of reputation systems to produce global opinions about the product quality. Naive methods that are frequently used do not consider consumer profiles in their calculations and cannot discover unfair ratings and trends emerging in new ratings. Other sophisticated rating aggregation methods that use a weighted average technique focus on one or a few aspects of consumers’ profile data. This paper proposes a new reputation system using machine learning to predict reliability of consumers from their profile. In particular, we construct a new consumer profile dataset by extracting a set of factors that have a great impact on consumer reliability, which serve as an input to machine learning algorithms. The predicted weight is then integrated with a weighted average method to compute product reputation score. The proposed model has been evaluated over three MovieLens benchmarking datasets, using 10-folds cross validation. Furthermore, the performance of the proposed model has been compared to previous published rating aggregation models. The obtained results were promising which suggest that the proposed approach could be a potential solution for reputation systems. The results of the comparison demonstrated the accuracy of our models. Finally, the proposed approach can be integrated with online recommendation systems to provide better purchasing recommendations and facilitate user experience on online shopping markets.},
journal = {Int. J. Autom. Comput.},
month = jun,
pages = {492–502},
numpages = {11},
keywords = {Reputation system, rating aggregation, machine learning, consumer reliability, user trust}
}

@article{10.1016/j.compag.2019.05.051,
author = {Abdalla, Alwaseela and Cen, Haiyan and El-manawy, Ahmed and He, Yong},
title = {Infield oilseed rape images segmentation via improved unsupervised learning models combined with supreme color features},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.05.051},
doi = {10.1016/j.compag.2019.05.051},
journal = {Comput. Electron. Agric.},
month = jul,
pages = {1057–1068},
numpages = {12},
keywords = {Image segmentation, Unsupervised learning, Genetic algorithm, Field phenotyping}
}

@inproceedings{10.1007/978-3-030-50120-4_15,
author = {Ng, Eric and Ebrahimi, Mehran},
title = {An Unsupervised Learning Approach to Discontinuity-Preserving Image Registration},
year = {2020},
isbn = {978-3-030-50119-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50120-4_15},
doi = {10.1007/978-3-030-50120-4_15},
abstract = {Most traditional image registration algorithms aimed at aligning a pair of images impose well-established regularizers to guarantee smoothness of unknown deformation fields. Since these methods assume global smoothness within the image domain, they pose issues for scenarios where local discontinuities are expected, such as the sliding motion between the lungs and the chest wall during the respiratory cycle. Furthermore, an objective function must be optimized for each given pair of images, thus registering multiple sets of images become very time-consuming and scale poorly to higher resolution image volumes.Using recent advances in deep learning, we propose an unsupervised learning-based image registration model. The model is trained over a loss function with a custom regularizer that preserves local discontinuities, while simultaneously respecting the smoothness assumption in homogeneous regions of image volumes. Qualitative and quantitative validations on 3D pairs of lung CT datasets will be presented.},
booktitle = {Biomedical Image Registration: 9th International Workshop, WBIR 2020, Portoro\v{z}, Slovenia, December 1–2, 2020, Proceedings},
pages = {153–162},
numpages = {10},
location = {Portoro\v{z}, Slovenia}
}

@article{10.1016/j.jnca.2020.102576,
author = {Gu, Rentao and Yang, Zeyuan and Ji, Yuefeng},
title = {Machine learning for intelligent optical networks: A comprehensive survey},
year = {2020},
issue_date = {May 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {157},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2020.102576},
doi = {10.1016/j.jnca.2020.102576},
journal = {J. Netw. Comput. Appl.},
month = may,
numpages = {22},
keywords = {Optical networks, Machine learning, Resource management, Optical performance monitoring, Neural networks, Reinforcement learning}
}

@article{10.1016/j.patcog.2019.03.022,
author = {Ma, Qing and Bai, Cong and Zhang, Jinglin and Liu, Zhi and Chen, Shengyong},
title = {Supervised learning based discrete hashing for image retrieval},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {92},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.03.022},
doi = {10.1016/j.patcog.2019.03.022},
journal = {Pattern Recogn.},
month = aug,
pages = {156–164},
numpages = {9},
keywords = {Hashing, Supervised learning, Neural network, Optimization}
}

@inproceedings{10.1145/3474085.3475340,
author = {Wang, Li and Fan, Baoyu and Guo, Zhenhua and Zhao, Yaqian and Zhang, Runze and Li, Rengang and Gong, Weifeng and Wang, Endong},
title = {Knowledge-Supervised Learning: Knowledge Consensus Constraints for Person Re-Identification},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475340},
doi = {10.1145/3474085.3475340},
abstract = {The consensus of multiple views on the same data will provide extra regularization, thereby improving accuracy. Based on this idea, we proposed a novel Knowledge-Supervised Learning (KSL) method for person re-identification (Re-ID), which can improve the performance without introducing extra inference cost. Firstly, we introduce isomorphic auxiliary training strategy to conduct basic multiple views that simultaneously train multiple classifier heads of the same network on the same training data. The consensus constraints aim to maximize the agreement among multiple views. To introduce this regular constraint, inspired by knowledge distillation that paired branches can be trained collaboratively through mutual imitation learning. Three novel constraints losses are proposed to distill the knowledge that needs to be transferred across different branches: similarity of predicted classification probability for cosine space constraints, distance of embedding features for euclidean space constraints, hard sample mutual mining for hard sample space constraints. From different perspectives, these losses complement each other. Experiments on four mainstream Re-ID datasets show that a standard model with KSL method trained from scratch outperforms its ImageNet pre-training results by a clear margin. With KSL method, a lightweight model without ImageNet pre-training outperforms most large models. We expect that these discoveries can attract some attention from the current de facto paradigm of "pre-training and fine-tuning" in Re-ID task to the knowledge discovery during model training.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1866–1874},
numpages = {9},
keywords = {consensus constraints, isomorphic auxiliary training, knowledge distillation, person retrieval},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.5555/3326943.3327059,
author = {Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S.},
title = {Unsupervised learning of view-invariant action representations},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1262–1272},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1145/3447545.3451181,
author = {Kurz, Malte S.},
title = {Distributed Double Machine Learning with a Serverless Architecture},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451181},
doi = {10.1145/3447545.3451181},
abstract = {This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {27–33},
numpages = {7},
keywords = {AWS Lambda, causal machine learning, distributed computing, function-as-a-service (FAAS), machine learning, serverless computing},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1007/s00521-021-06299-7,
author = {Bile, Alessandro and Moratti, Francesca and Tari, Hamed and Fazio, Eugenio},
title = {Supervised and unsupervised learning using a fully-plastic all-optical unit of artificial intelligence based on solitonic waveguides},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06299-7},
doi = {10.1007/s00521-021-06299-7},
abstract = {The software implementations of neuronal systems have shown great effectiveness, even if the natural hardware separation between the processing and memory areas in computers slows down the analysis capacity. To overcome these limitations, new hardware configurations are moving towards neuromorphic models, capable of unifying the processing/memory dichotomy. Recently, integrated photonic X-junctions formed by waveguides written by spatial solitons have shown the ability to perform supervised learning. The solitonic technology, compared to the traditional one, offers the advantage of realizing plastic circuitry, a typical characteristic of biological neural networks. This work extensively studies both supervised and unsupervised learning of photonic soliton X-junctions. By exploiting the plasticity of the nonlinear refractive index at the base of the soliton formation, X-junctions can readdress their behaviours forwarding data to different outputs. In this article, we will extend the state-of-the-art: starting from supervised learning, for which all possible cases are now investigated, a material sensitive to the transported signals will be introduced to allow the junction to carry out unsupervised learning. In this way, the junction autonomously recognises the transported signals without the external intervention of the operator. Learning and memory now physically coincide in fact, learning means that the junction slowly switches based on the information sent; any further unknown information sent will find the junction in the modified state which corresponds to the learned information and will be recognised as well (reasoning based on comparison with stored information).},
journal = {Neural Comput. Appl.},
month = dec,
pages = {17071–17079},
numpages = {9},
keywords = {Neuromorphic, Supervised and unsupervised learning, Solitons, All-optical intelligent system}
}

@article{10.4018/IJAGR.2020100101,
author = {van Niekerk, Adriaan and Prins, Adriaan Jacobus},
title = {Regional Mapping of Vineyards Using Machine Learning and LiDAR Data},
year = {2020},
issue_date = {Oct 2020},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {4},
issn = {1947-9654},
url = {https://doi.org/10.4018/IJAGR.2020100101},
doi = {10.4018/IJAGR.2020100101},
abstract = {This study evaluates the use of LiDAR data and machine learning algorithms for mapping vineyards. Vineyards are planted in rows spaced at various distances, which can cause spectral mixing within individual pixels and complicate image classification. Four resolution where used for generating normalized digital surface model and intensity derivatives from the LiDAR data. In addition, texture measures with window sizes of 3x3 and 5x5 were generated from the LiDAR derivatives. The different combinations of the resolutions and window sizes resulted in eight data sets that were used as input to 11 machine learning algorithms. A larger window size was found to improve the overall accuracy for all the classifier–resolution combinations. The results showed that random forest with texture measures generated at a 5x5 window size outperformed the other experiments, regardless of the resolution used. The authors conclude that the random forest algorithm used on LiDAR derivatives with a resolution of 1.5m and a window size of 5x5 is the recommend configuration for vineyard mapping using LiDAR data.},
journal = {Int. J. Appl. Geosp. Res.},
month = oct,
pages = {1–22},
numpages = {22},
keywords = {Classification, Image Textures, LiDAR, Machine Learning, Vineyards}
}

@article{10.1016/j.eswa.2021.115102,
author = {Mancuso, Paolo and Piccialli, Veronica and Sudoso, Antonio M.},
title = {A machine learning approach for forecasting hierarchical time series},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115102},
doi = {10.1016/j.eswa.2021.115102},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {17},
keywords = {Hierarchical time series, Forecast, Machine learning, Deep neural network}
}

@inproceedings{10.1007/978-3-030-38085-4_51,
author = {D\'{\i}az-Montiel, Alan A. and Ruffini, Marco},
title = {A Performance Analysis of Supervised Learning Classifiers for QoT Estimation in ROADM-Based Networks},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_51},
doi = {10.1007/978-3-030-38085-4_51},
abstract = {Machine learning techniques for optimization purposes in the optical domain have been reviewed extensively in recent years. While several studies are pointing in the right direction towards building enhanced transport network control systems including estimation algorithms, the physical effects encountered in the optical domain raise several challenges that are hard to learn from and mitigate. In this paper, we provide a performance analysis of various supervised learning algorithms when predicting the Quality of Transmission (QoT), in terms of signal to noise ratio (OSNR), of lightpaths when erbium doped fiber amplifier (EDFA) power excursions and fiber nonlinearities are taken into account. The analysis considers F1-scores and computational training times as the main comparison metrics. A customized optical data network simulator was used for the generation of synthetic labeled data samples. Our results depict similar performance among groups of classifiers, and a correlation between the data sample size and the prediction accuracy.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {598–609},
numpages = {12},
keywords = {Supervised learning, Transport networks},
location = {Athens, Greece}
}

@inproceedings{10.1145/3318464.3386137,
author = {Fard, Arash and Le, Anh and Larionov, George and Dhillon, Waqas and Bear, Chuck},
title = {Vertica-ML: Distributed Machine Learning in Vertica Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386137},
doi = {10.1145/3318464.3386137},
abstract = {A growing number of companies rely on machine learning as a key element for gaining a competitive edge from their collected Big Data. An in-database machine learning system can provide many advantages in this scenario, e.g., eliminating the overhead of data transfer, avoiding the maintenance costs of a separate analytical system, and addressing data security and provenance concerns. In this paper, we present our distributed machine learning subsystem within the Vertica database. This subsystem, Vertica-ML, includes machine learning functionalities with SQL API which cover a complete data science workflow as well as model management. We treat machine learning models in Vertica as first-class database objects like tables and views; therefore, they enjoy a similar mechanism for archiving and managing. We explain the architecture of the subsystem, and present a set of experiments to evaluate the performance of the machine learning algorithms implemented on top of it.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {755–768},
numpages = {14},
keywords = {big data, database, distributed computing, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1016/j.csl.2019.04.004,
author = {Campos, Victor de Abreu and Pedronette, Daniel Carlos Guimar\~{a}es},
title = {A framework for speaker retrieval and identification through unsupervised learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {58},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2019.04.004},
doi = {10.1016/j.csl.2019.04.004},
journal = {Comput. Speech Lang.},
month = nov,
pages = {153–174},
numpages = {22},
keywords = {Speaker recognition, Speaker retrieval, Unsupervised learning, Vector quantization, Gaussian mixture model, i-vector}
}

@article{10.1007/s10506-020-09270-4,
author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Fr\'{e}nay, Beno\^{\i}t},
title = {Legal requirements on explainability in machine learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0924-8463},
url = {https://doi.org/10.1007/s10506-020-09270-4},
doi = {10.1007/s10506-020-09270-4},
abstract = {Deep learning and other black-box models are becoming more and more popular today. Despite their high performance, they may not be accepted ethically or legally because of their lack of explainability. This paper presents the increasing number of legal requirements on machine learning model interpretability and explainability in the context of private and public decision making. It then explains how those legal requirements can be implemented into machine-learning models and concludes with a call for more inter-disciplinary research on explainability.},
journal = {Artif. Intell. Law},
month = jun,
pages = {149–169},
numpages = {21},
keywords = {Interpretability, Explainability, Machine learning, Law}
}

@inproceedings{10.1007/978-3-030-13709-0_21,
author = {Jabbar, Eva and Besse, Philippe and Loubes, Jean-Michel and Roa, Nathalie Barbosa and Merle, Christophe and Dettai, R\'{e}mi},
title = {Supervised Learning Approach for Surface-Mount Device Production},
year = {2018},
isbn = {978-3-030-13708-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13709-0_21},
doi = {10.1007/978-3-030-13709-0_21},
abstract = {In this paper, we propose a decision-making tool based on supervised learning techniques that detects defects and proposes to the Surface-Mount Technology (SMT) operator a probability of being a false call. In this work, we compare four tree-based learning methods. The result of our experiments shows that a XGBoost model trained with our real-world dataset can accurately classify most real defects and false calls with an accuracy score of about 99.4% and a recall of about 98.6%. Moreover, we investigated the computing time of our prediction model and concluded that integration of our classification tool based on the XGBoost algorithm is realistic and feasible in the SMT production line. We believe that our tool will significantly improve the daily work of the SMT verify operator.},
booktitle = {Machine Learning, Optimization, and Data Science: 4th International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018, Revised Selected Papers},
pages = {254–263},
numpages = {10},
keywords = {Supervised learning, Industry 4.0, Decision-making tool, Big data analytics, Surface-Mount Technology},
location = {Volterra, Italy}
}

@inproceedings{10.1145/3240508.3240699,
author = {Wang, Lingjing and Qian, Cheng and Wang, Jifei and Fang, Yi},
title = {Unsupervised Learning of 3D Model Reconstruction from Hand-Drawn Sketches},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240699},
doi = {10.1145/3240508.3240699},
abstract = {3D objects modeling has gained considerable attention in the visual computing community. We propose a low-cost unsupervised learning model for 3D objects reconstruction from hand-drawn sketches. Recent advancements in deep learning opened new opportunities to learn high-quality 3D objects from 2D sketches via supervised networks. However, the limited availability of labeled 2D hand-drawn sketches data (i.e. sketches and its corresponding 3D ground truth models) hinders the training process of supervised methods. In this paper, driven by a novel design of combination of retrieval and reconstruction process, we developed a learning paradigm to reconstruct 3D objects from hand-drawn sketches, without the use of well-labeled hand-drawn sketch data during the entire training process. Specifically, the paradigm begins with the training of an adaption network via autoencoder with adversarial loss, embedding the unpaired 2D rendered image domain with the hand-drawn sketch domain to a shared latent vector space. Then from the embedding latent space, for each testing sketch image, we retrieve a few (e.g. five) nearest neighbors from the training 3D data set as prior knowledge for a 3D Generative Adversarial Network. Our experiments verify our network's robust and superior performance in handling 3D volumetric object generation from single hand-drawn sketch without requiring any 3D ground truth labels.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1820–1828},
numpages = {9},
keywords = {generative model, sketch modeling, unsupervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1016/j.patcog.2019.107076,
author = {Huang, Bin and Chen, Renwen and Zhou, Qinbang and Xu, Wang},
title = {Eye landmarks detection via weakly supervised learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107076},
doi = {10.1016/j.patcog.2019.107076},
journal = {Pattern Recogn.},
month = feb,
numpages = {11},
keywords = {Eye landmarks detection, Special format data, Weakly supervised learning, Object detection, Recurrent learning module}
}

@article{10.1016/j.knosys.2019.104982,
author = {Castellanos-Garz\'{o}n, Jos\'{e} A. and Costa, Ernesto and Jaimes S., Jos\'{e} Luis and Corchado, Juan M.},
title = {An evolutionary framework for machine learning applied to medical data},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {185},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104982},
doi = {10.1016/j.knosys.2019.104982},
journal = {Know.-Based Syst.},
month = dec,
numpages = {14},
keywords = {Machine learning, Logical rule induction, Data mining, Supervised learning, Evolutionary computation, Genetic programming, Ensemble classifier, Medical data}
}

@inproceedings{10.1007/978-3-030-66843-3_24,
author = {Anjum, Sadia and Hussain, Lal and Ali, Mushtaq and Abbasi, Adeel Ahmed},
title = {Automated Multi-class Brain Tumor Types Detection by Extracting RICA Based Features and Employing Machine Learning Techniques},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_24},
doi = {10.1007/978-3-030-66843-3_24},
abstract = {Brain tumor is the leading reason of mortality across the globe. It is obvious that the chances of survival can be increased if the tumor is identified and properly classified at an initial stage. Several factors such as type, texture and location help to categorize the brain tumor. In this study, we extracted reconstruction independent component analysis (RICA) base features from brain tumor types such as glioma, meningioma, pituitary and applied robust machine learning algorithms such as linear discriminant analysis (LDA) and support vector machine (SVM) with linear and quadratic kernels. The jackknife 10-fold cross validation was used for training and testing data validation. The SVM with quadratic kernel gives the highest multiclass detection performance. To detect pituitary, the highest detection performance was obtained with sensitivity (93.85%), specificity (100%), PPV (100%), NPV (97.27%), accuracy (98.07%) and AUC (96.92). To detect glioma, the highest detection performance was obtained with accuracy (94.35%), AUC (0.9508). To detect the meningioma, the highest was obtained with accuracy (96.18%), AUC (0.9095). The findings reveal that proposed methodology based on RICA features to detect multiclass brain tumor types will be very useful for treatment modification to achieve better clinical outcomes.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {249–258},
numpages = {10},
keywords = {Feature extraction, Machine learning, Glioma, Meningioma, Pituitary, Image analysis},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-72013-1_16,
author = {Scott, Joseph and Niemetz, Aina and Preiner, Mathias and Nejati, Saeed and Ganesh, Vijay},
title = {MachSMT: A Machine Learning-based Algorithm Selector for SMT Solvers},
year = {2021},
isbn = {978-3-030-72012-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72013-1_16},
doi = {10.1007/978-3-030-72013-1_16},
abstract = {In this paper, we present MachSMT, an algorithm selection tool for Satisfiability Modulo Theories (SMT) solvers. MachSMT supports the entirety of the SMT-LIB language. It employs machine learning (ML) methods to construct both empirical hardness models (EHMs) and pairwise ranking comparators (PWCs) over state-of-the-art SMT solvers. Given an SMT formula I as input, MachSMT leverages these learnt models to output a ranking of solvers based on predicted run time on the formula I. We evaluate MachSMT on the solvers, benchmarks, and data obtained from SMT-COMP 2019 and 2020. We observe MachSMT frequently improves on competition winners, winning 54 divisions outright and up to a 198.4% improvement in PAR-2 score, notably in logics that have broad applications (e.g., BV, LIA, NRA, etc.) in verification, program analysis, and software engineering. The MachSMT tool is designed to be easily tuned and extended to any suitable solver application by users. MachSMT is not a replacement for SMT solvers by any means. Instead, it is a tool that enables users to leverage the collective strength of the diverse set of algorithms implemented as part of these sophisticated solvers.},
booktitle = {Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021,  Luxembourg City, Luxembourg, March 27 – April 1, 2021, Proceedings, Part II},
pages = {303–325},
numpages = {23},
keywords = {SMT Solvers, Machine Learning, Algorithm Selection},
location = {Luxembourg City, Luxembourg}
}

@article{10.1007/s10586-021-03359-4,
author = {Yadav, Mahendra Pratap and Rohit and Yadav, Dharmendra Kumar},
title = {Maintaining container sustainability through machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03359-4},
doi = {10.1007/s10586-021-03359-4},
abstract = {Container-based virtualization is a new technology used by cloud providers to provide cloud services to end-user. This technology has various advantages (e.g. lightweight, quickly deployable, and efficient for resource utilization) for executing an application. It reduces the operating cost, carbon emission, and allocates the resources dynamically. Different cloud applications have different requirements. Deploying resources according to peak requirements always can be costly. On the other hand, always having minimum computing resources may not meet workload’s peak requirements, and may cause degraded system performance, less throughput, more response time and service level agreement violations. Hence, it becomes a challenge to maintain optimal level of resources to fulfill the SLA requirements for the applications. To address the above issues, we propose an auto-scaler which uses proactive approach (Support Vector Regression) to perform horizontal elasticity for Docker containers in response to fluctuating workload for real-time applications. As the workload increases, additional resources will be allocated dynamically supporting elasticity. The increase in capacity of a machine dynamically is termed as elasticity. The effective mechanism of elasticity avoids the violation of SLA and penalties in terms of user’s loss. The proposed auto-scaler uses the IBM computing model, MAPE-K principle to perform elasticity using the workload predictions made by the SVR model. The predicted workload helps auto-scaler to find out the minimum numbers of replicas needed for a container of a cluster so that it handles the future workload. The experimental results show that the results of SVM prediction keep the performance of the system sustainable with fluctuating workload.},
journal = {Cluster Computing},
month = dec,
pages = {3725–3750},
numpages = {26},
keywords = {Cloud computing, Elasticity, Container, Auto-scaling, MAPE model, Prediction models, Support vector machine, Machine learning}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {50},
numpages = {47},
keywords = {Data exfiltration, advanced persistent threat, data breach, data leakage, machine learning}
}

@inproceedings{10.1007/978-3-030-77977-1_26,
author = {Torres, Marcella},
title = {A Machine Learning Method for Parameter Estimation and Sensitivity Analysis},
year = {2021},
isbn = {978-3-030-77976-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77977-1_26},
doi = {10.1007/978-3-030-77977-1_26},
abstract = {We discuss the application of a supervised machine learning method, random forest algorithm (RF), to perform parameter space exploration and sensitivity analysis on ordinary differential equation models. Decision trees can provide complex decision boundaries and can help visualize decision rules in an easily digested format that can aid in understanding the predictive structure of a dynamic model and the relationship between input parameters and model output. We study a simplified process for model parameter tuning and sensitivity analysis that can be used in the early stages of model development.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part V},
pages = {330–343},
numpages = {14},
keywords = {Parameter estimation, Machine learning, Sensitivity analysis, Ordinary differential equations, Random forest},
location = {Krakow, Poland}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {14},
keywords = {Global development, developing countries}
}

@article{10.5555/3455716.3455925,
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R\"{a}tsch, Gunnar and Gelly, Sylvain and Sch\"{o}lkopf, Bernhard and Bachem, Olivier},
title = {A sober look at the unsupervised learning of disentangled representations and their evaluation},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {The idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train over 14 000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on eight data sets. We observe that while the different methods successfully enforce properties "encouraged" by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, different evaluation metrics do not always agree on what should be considered "disentangled" and exhibit systematic differences in the estimation. Finally, increased disentanglement does not seem to necessarily lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {209},
numpages = {62},
keywords = {disentangled representations, impossibility, evaluation, reproducibility, large scale experimental study}
}

@inproceedings{10.1145/3383972.3384061,
author = {Rasool, Abdur and Tao, Ran and Kashif, Kaleem and Khan, Waqas and Agbedanu, Promise and Choudhry, Neeta},
title = {Statistic Solution for Machine Learning to Analyze Heart Disease Data},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384061},
doi = {10.1145/3383972.3384061},
abstract = {Data crawling, collection and analysis have become a popular pillar for the business intelligence of big data analysis which is the latest hot-topic among the research association. Numerous tools and techniques to solve and analyze the structured and unstructured datasets are developing very quickly. The previous studies show the different approaches in the identification of the strengths and weaknesses of multiple machine learning algorithms. But, most of the approaches demand more expert knowledge base information to understand the concepts of given data. In this paper, we modernize the machine learning methods for the effective prediction of heart disease. This work deliberates the detailed process of implementation of our proposed system. The goal of this work is to find a strong and effective machine learning algorithm for disease prediction for the problem; how can doctors get fast and better results for their diagnosis of heart disease. We design a new system for disease prediction using machine learning prediction algorithms (LR, ANN and SVC) by utilizing an effective approach of ETL, OLAP and data mining. The results showed that the best machine learning algorithm is SVC with 92% accuracy for the risk prediction model. We found that subjects at 56-64 years old have a high risk of heart disease, as well as men, have more heart disease rate than women. This proposed study can be favorable for the medical practitioners in the field of healthcare, supportive practice and precautions to the heart disease patients.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {134–139},
numpages = {6},
keywords = {Machine learning, data mining, heart disease analysis},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1007/978-3-030-63833-7_26,
author = {You, Xuanke and Zhang, Lan and Yang, Linzhuo and Yu, Xiaojing and Liu, Kebin},
title = {Entropy Repulsion for Semi-supervised Learning Against Class Mismatch},
year = {2020},
isbn = {978-3-030-63832-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63833-7_26},
doi = {10.1007/978-3-030-63833-7_26},
abstract = {A series of semi-supervised learning (SSL) algorithms have been proposed to alleviate the need for labeled data by leveraging large amounts of unlabeled data. Those algorithms have achieved good performance on standard benchmark datasets, however, their performance can degrade drastically when there exists a class mismatch between the labeled and unlabeled data, which is common in practice. In this work, we propose a new technique, entropy repulsion for mismatch (ERCM), to improve SSL against a class mismatch situation. Specifically, we design an entropy repulsion loss and a batch annealing and reloading mechanism, which work together to prevent potentially mismatched unlabeled data from participating in the early training stages as well as facilitate the minimization of the unsupervised loss term of traditional SSL algorithms. ERCM can be adopted to enhance existing SSL algorithms with minor extra computation cost and no change to their network structures. Our extensive experiments demonstrate that ERCM can significantly improve the performance of state-of-the-art SSL algorithms, namely Mean Teacher, Virtual Adversarial Training (VAT) and Mixmatch in various class-mismatch cases.},
booktitle = {Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 23–27, 2020, Proceedings, Part II},
pages = {307–319},
numpages = {13},
keywords = {Semi-supervised learning, Class mismatch},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1145/3399595,
author = {Agnesina, Anthony and Lim, Sung Kyu and Lepercq, Etienne and Cid, Jose Escobedo Del},
title = {Improving FPGA-Based Logic Emulation Systems through Machine Learning},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3399595},
doi = {10.1145/3399595},
abstract = {We present a machine learning (ML) framework to improve the use of computing resources in the FPGA compilation step of a commercial FPGA-based logic emulation flow. Our ML models enable highly accurate predictability of the final place and route design qualities, runtime, and optimal mapping parameters. We identify key compilation features that may require aggressive compilation efforts using our ML models. Experiments based on our large-scale database from an industry’s emulation system show that our ML models help reduce the total number of jobs required for a given netlist by 33%. Moreover, our job scheduling algorithm based on our ML model reduces the overall time to completion of concurrent compilation runs by 24%. In addition, we propose a new method to compute “recommendations” from our ML model to perform re-partitioning of difficult partitions. Tested on a large-scale industry system on chip design, our recommendation flow provides additional 15% compile time savings for the entire system on chip. To exploit our ML model inside the time-critical multi-FPGA partitioning step, we implement it in an optimized multi-threaded representation.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jul,
articleno = {46},
numpages = {20},
keywords = {Field programmable gate array, SoC verification, emulation flow optimization with machine learning}
}

@inproceedings{10.1007/978-3-031-15116-3_7,
author = {Casimiro, Maria and Romano, Paolo and Garlan, David and Moreno, Gabriel A. and Kang, Eunsuk and Klein, Mark},
title = {Self-adaptive Machine Learning Systems: Research Challenges and&nbsp;Opportunities},
year = {2021},
isbn = {978-3-031-15115-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-15116-3_7},
doi = {10.1007/978-3-031-15116-3_7},
abstract = {Today’s world is witnessing a shift from human-written software to machine-learned software, with the rise of systems that rely on machine learning. These systems typically operate in non-static environments, which are prone to unexpected changes, as is the case of self-driving cars and enterprise systems. In this context, machine-learned software can misbehave. Thus, it is paramount that these systems are capable of detecting problems with their machined-learned components and adapting themselves to maintain desired qualities. For instance, a fraud detection system that cannot adapt its machine-learned model to efficiently cope with emerging fraud patterns or changes in the volume of transactions is subject to losses of millions of dollars. In this paper, we take a first step towards the development of a framework for self-adaptation of systems that rely on machine-learned components. We describe: (i) a set of causes of machine-learned component misbehavior and a set of adaptation tactics inspired by the literature on machine learning, motivating them with the aid of two running examples from the enterprise systems and cyber-physical systems domains; (ii) the required changes to the MAPE-K loop, a popular control loop for self-adaptive systems; and (iii) the challenges associated with developing this framework. We conclude with a set of research questions to guide future work.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021 Tracks and Workshops; V\"{a}xj\"{o}, Sweden, September 13–17, 2021, Revised Selected Papers},
pages = {133–155},
numpages = {23},
keywords = {Self-adaptive systems, Machine learning, Model degradation, Learning-enabled systems, Learning-enabled components},
location = {V\"{a}xj\"{o}, Sweden}
}

@book{10.5555/3278388,
author = {Gori, Marco},
title = {Machine Learning: A Constraint-Based Approach},
year = {2017},
isbn = {0081006594},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Machine Learning: A Constraint-Based Approach provides readers with a refreshing look at the basic models and algorithms of machine learning, with an emphasis on current topics of interest that includes neural networks and kernel machines. The book presents the information in a truly unified manner that is based on the notion of learning from environmental constraints. While regarding symbolic knowledge bases as a collection of constraints, the book draws a path towards a deep integration with machine learning that relies on the idea of adopting multivalued logic formalisms, like in fuzzy systems. A special attention is reserved to deep learning, which nicely fits the constrained- based approach followed in this book. This book presents a simpler unified notion of regularization, which is strictly connected with the parsimony principle, and includes many solved exercises that are classified according to the Donald Knuth ranking of difficulty, which essentially consists of a mix of warm-up exercises that lead to deeper research problems. A software simulator is also included. Presents fundamental machine learning concepts, such as neural networks and kernel machines in a unified manner Provides in-depth coverage of unsupervised and semi-supervised learning Includes a software simulator for kernel machines and learning from constraints that also includes exercises to facilitate learning Contains 250 solved examples and exercises chosen particularly for their progression of difficulty from simple to complex}
}

@inproceedings{10.1145/3493649.3493654,
author = {Kanso, Ali and Palencia, Edi and Patra, Kinshuman and Shan, Jiaxin and Chao, Mengyuan and Wei, Xu and Cai, Tengwei and Chen, Kang and Qiao, Shuai},
title = {Designing a Kubernetes Operator for Machine Learning Applications},
year = {2021},
isbn = {9781450391719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493649.3493654},
doi = {10.1145/3493649.3493654},
abstract = {Machine Learning workloads such as deep learning and hyperparameter tuning are compute-intensive by nature. Parallel execution is key to reducing the learning time. The Ray Framework is a distributed middleware that provides primitives to seamlessly parallelize machine learning code execution across a cluster of compute node. Launching a Ray managed machine learning application requires a Ray cluster that is diligently configured, well connected and easily scalable. Kubernetes, the container management middleware, satisfies all the requirements to create and scale ray clusters. However, setting up a cluster within Kubernetes, is a tedious and error prone task when done manually. In this paper we present KubeRay, an Operator and suite of tools designed, and built to create Ray cluster in Kubernetes with minimum effort. We present our architectural choices, our open-source implementation, and we analyze the performance of our solution.},
booktitle = {Proceedings of the Seventh International Workshop on Container Technologies and Container Clouds},
pages = {7–12},
numpages = {6},
keywords = {Clustering, Linux Containers, Distributed Systems, Kubernetes, Machine Learning, Operators, Ray Framework},
location = {Virtual Event, Canada},
series = {WoC '21}
}

@phdthesis{10.5555/AAI28864695,
author = {Rahgooy, Taher and Jason, Harman, and Arash, Mahyari, and Nicholas, Mattei,},
advisor = {Brent, Venable, K.},
title = {Machine Learning Guided by Linguistic and Behavioral Knowledge},
year = {2021},
isbn = {9798762100731},
publisher = {The University of West Florida},
abstract = {The recent success of AI has been primarily driven by the extraordinary progress in the field of machine learning. The ultimate goal of machine learning is to develop algorithms capable of making accurate predictions in an explainable way by learning efficiently from a small amount of training data. Despite an exceptionally fast-paced growth, machine learning has been exceedingly successful in achieving accurate predictions, at the cost of sacrificing most of, if not all, explainability and by relying on huge amount of training data. Recent work has, on the other hand, shown that domain knowledge, when properly incorporated in learning algorithms, can facilitate learning from small data sets and provide various forms of explainability. In this dissertation, I propose novel ways of incorporating linguistic and behavioral knowledge into machine learning models for achieving different goals such as improving prediction accuracy, using less data, increase explainability, and evaluating cognitive biases.  We exemplify our novel approaches on some challenging tasks that require special treatment either due to lack of data and/or need for explainable predictions.  We first consider extracting spatial relations from language, which is a complex task due to the ambiguity of spatial relations and scarcity of available training data. To this end, we use linguistic knowledge to define various constraints imposed on classifiers to infer the correct classifications holistically.  Human choice prediction is the other domain that we consider because of the fundamental role it plays in the understanding of human behavior and in the design of intelligent systems. We propose novel methods to leverage procedural knowledge, in the form of psychological models of decision making, in combination with machine learning, to achieve better predictions, understand the underlying deliberation processes, and elicit user preferences.  Finally, we extend our work to the domain of sequential decision making by designing agents that learn constraints from demonstrations and then use cognitive models as orchestrators to exploit these learned constraints for making choices between conflicting goals.  We use various real world and synthetic data to evaluate our proposed methods throughout this dissertation. Our experimental results show the efficacy of our methods which significantly improves upon the state-of-the-art in all of the considered tasks.},
note = {AAI28864695}
}

@article{10.1007/s00500-021-05687-4,
author = {Wierzbi\'{n}ski, Micha\l{} and P\l{}awiak, Pawe\l{} and Hammad, Mohamed and Acharya, U. Rajendra},
title = {Development of accurate classification of heavenly bodies using novel machine learning techniques},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {10},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05687-4},
doi = {10.1007/s00500-021-05687-4},
abstract = {The heavenly bodies are objects that swim in the outer space. The classification of these objects is a challenging task for astronomers. This article presents a novel methodology that enables an efficient and accurate classification of cosmic objects (3 classes) based on evolutionary optimization of classifiers. This research collected the data from Sloan Digital Sky Survey database. In this work, we are proposing to develop a novel machine learning model to classify stellar spectra of stars, quasars and galaxies. First, the input data are normalized and then subjected to principal component analysis to reduce the dimensionality. Then, the genetic algorithm is implemented on the data which helps to find the optimal parameters for the classifiers. We have used 21 classifiers to develop an accurate and robust classification with fivefold cross-validation strategy. Our developed model has achieved an improvement in the accuracy using nineteen out of twenty-one models. We have obtained the highest classification accuracy of 99.16%, precision of 98.78%, recall of 98.08% and F1-score of 98.32% using evolutionary system based on voting classifier. The developed machine learning prototype can help the astronomers to make accurate classification of heavenly bodies in the sky. Proposed evolutionary system can be used in other areas where accurate classification of many classes is required.},
journal = {Soft Comput.},
month = may,
pages = {7213–7228},
numpages = {16},
keywords = {Celestial objects, Machine learning, Evolutionary systems, Genetic optimization, Heavenly bodies, Celestial bodies}
}

@article{10.1287/mnsc.2018.3255,
author = {Yoganarasimhan, Hema},
title = {Search Personalization Using Machine Learning},
year = {2020},
issue_date = {March 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {66},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2018.3255},
doi = {10.1287/mnsc.2018.3255},
abstract = {Firms typically use query-based search to help consumers find information/products on their websites. We consider the problem of optimally ranking a set of results shown in response to a query. We propose a personalized ranking mechanism based on a user’s search and click history. Our machine-learning framework consists of three modules: (a) feature generation, (b) normalized discounted cumulative gain–based LambdaMART algorithm, and (c) feature selection wrapper. We deploy our framework on large-scale data from a leading search engine using Amazon EC2 servers and present results from a series of counterfactual analyses. We find that personalization improves clicks to the top position by 3.5% and reduces the average error in rank of a click by 9.43% over the baseline. Personalization based on short-term history or within-session behavior is shown to be less valuable than long-term or across-session personalization. We find that there is significant heterogeneity in returns to personalization as a function of user history and query type. The quality of personalized results increases monotonically with the length of a user’s history. Queries can be classified based on user intent as transactional, informational, or navigational, and the former two benefit more from personalization. We also find that returns to personalization are negatively correlated with a query’s past average performance. Finally, we demonstrate the scalability of our framework and derive the set of optimal features that maximizes accuracy while minimizing computing time.This paper was accepted by Juanjuan Zhang, marketing.},
journal = {Manage. Sci.},
month = mar,
pages = {1045–1070},
numpages = {26},
keywords = {marketing, online search, personalization, machine learning, search engines}
}

@article{10.1016/j.neucom.2020.08.094,
author = {Pastor-L\'{o}pez, Iker and Sanz, Borja and Tellaeche, Alberto and Psaila, Giuseppe and de la Puerta, Jos\'{e} Gaviria and Bringas, Pablo G.},
title = {Quality assessment methodology based on machine learning with small datasets: Industrial castings defects},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {456},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2020.08.094},
doi = {10.1016/j.neucom.2020.08.094},
journal = {Neurocomput.},
month = oct,
pages = {622–628},
numpages = {7},
keywords = {Artificial vision, Machine-learning, Surface defect detection, Defect categorization}
}

@article{10.1007/s11042-020-09512-2,
author = {Khan, Ayaz H. and Zubair, Muhammad},
title = {Classification of multi-lingual tweets, into multi-class model using Na\"{\i}ve Bayes and semi-supervised learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09512-2},
doi = {10.1007/s11042-020-09512-2},
abstract = {Twitter is a social media platform which has been proven to be a great tool for insights of emotions about products, policies etc. through a 280-character message called tweet, containing direct and unfiltered emotions by a large amount of user population. Twitter has attracted the attention of many researchers owing to the fact that every tweet is by default, public in nature which is not the case with Facebook. This paper proposes a model for multi-lingual (English and Roman Urdu) classification of tweets over diversely ranged classes (non-hierarchical architecture). Previous work in tweet classification is narrowly focused either on single language or either on uniform set of classes at most (Positive, Extremely Positive, Negative and Extremely Negative). The proposed model is based on semi-supervised learning and proposed feature selection approach makes it less dependent and highly adaptive for grabbing trending terms. This makes it a strong contender of choice for streaming data. In the methodology, using Na\"{\i}ve Bayes learning algorithm for each phase, obtained remarkable accuracy of up to 87.16% leading from both KNN and SVM models which are popular for NLP and Text classification domains.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {32749–32767},
numpages = {19},
keywords = {Twitter, Sentiment analysis, Sentiment classification, Semi-supervised learning}
}

@article{10.1016/j.cosrev.2020.100341,
author = {Kotsiopoulos, Thanasis and Sarigiannidis, Panagiotis and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2020.100341},
doi = {10.1016/j.cosrev.2020.100341},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {25},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid}
}

@inproceedings{10.1109/CEC45853.2021.9504696,
author = {Vieira, Carlos and de Ara\'{u}jo, Adelson and Andrade, Jos\'{e} E. and Bezerra, Leonardo C. T.},
title = {iSklearn: Automated Machine Learning with irace},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC45853.2021.9504696},
doi = {10.1109/CEC45853.2021.9504696},
abstract = {Automated algorithm engineering has become an important asset for academia and industry. irace, for instance, is an algorithm configurator (AC) that has successfully designed effective algorithms for optimization problems. The major advantage of irace is combining learning and parallelization, but no fully-functional automated machine learning (AutoML) system powered by irace has yet been proposed. This is rather striking, as some of the most relevant existing AutoML tools are powered by ACs, of which irace is one of the most effective examples.In this work, we propose iSklearn, an irace-powered AutoML system. Our proposal improves existing work applying an AC to engineer a machine learning (ML) pipeline. First, our configuration space represents a minimalist pipeline template, demonstrating that simpler pipelines can be competitive with elaborate approaches (e.g. ensembles). Second, our configuration setup improves the application of AC-based AutoML to time series (TS) problems, and is more flexible to fit other applications.We evaluate iSklearn on three major ML domains, namely computer vision (CV), natural language processing (NLP), and TS. Results prove competitive to AUTOSKLEARN, a state-of-the-art AutoML system also built on scikit-learn. Furthermore, the compositions of the pipelines devised vary with the problem domain and dataset considered, providing further evidence for the need of AutoML tools. We conclude our investigation ablating through the proposed configuration space and setup to understand their impact on the performance of iSklearn.},
booktitle = {2021 IEEE Congress on Evolutionary Computation (CEC)},
pages = {2354–2361},
numpages = {8},
location = {Krak\'{o}w, Poland}
}

@inproceedings{10.1007/978-3-030-66096-3_28,
author = {Tokmakov, Pavel and Hebert, Martial and Schmid, Cordelia},
title = {Unsupervised Learning of Video Representations via Dense Trajectory Clustering},
year = {2020},
isbn = {978-3-030-66095-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66096-3_28},
doi = {10.1007/978-3-030-66096-3_28},
abstract = {This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part II},
pages = {404–421},
numpages = {18},
keywords = {Unsupervised representation learning, Action recognition},
location = {Glasgow, United Kingdom}
}

@inbook{10.5555/3454287.3455036,
author = {G\"{o}lz, Paul and Kahng, Anson and Procaccia, Ariel D.},
title = {Paradoxes in fair machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {749},
numpages = {11}
}

@article{10.14778/3450980.3450989,
author = {Liu, Tongyu and Fan, Ju and Luo, Yinqing and Tang, Nan and Li, Guoliang and Du, Xiaoyong},
title = {Adaptive data augmentation for supervised learning over missing data},
year = {2021},
issue_date = {March 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3450980.3450989},
doi = {10.14778/3450980.3450989},
abstract = {Real-world data is dirty, which causes serious problems in (supervised) machine learning (ML). The widely used practice in such scenario is to first repair the labeled source (a.k.a. train) data using rule-, statistical- or ML-based methods and then use the "repaired" source to train an ML model. During production, unlabeled target (a.k.a. test) data will also be repaired, and is then fed in the trained ML model for prediction. However, this process often causes a performance degradation when the source and target datasets are dirty with different noise patterns, which is common in practice.In this paper, we propose an adaptive data augmentation approach, for handling missing data in supervised ML. The approach extracts noise patterns from target data, and adapts the source data with the extracted target noise patterns while still preserving supervision signals in the source. Then, it patches the ML model by retraining it on the adapted data, in order to better serve the target. To effectively support adaptive data augmentation, we propose a novel generative adversarial network (GAN) based framework, called DAGAN, which works in an unsupervised fashion. DAGAN consists of two connected GAN networks. The first GAN learns the noise pattern from the target, for target mask generation. The second GAN uses the learned target mask to augment the source data, for source data adaptation. The augmented source data is used to retrain the ML model. Extensive experiments show that our method significantly improves the ML model performance and is more robust than the state-of-the-art missing data imputation solutions for handling datasets with different missing value patterns.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1202–1214},
numpages = {13}
}

@inproceedings{10.1007/978-3-030-44584-3_43,
author = {von Rueden, Laura and Mayer, Sebastian and Sifa, Rafet and Bauckhage, Christian and Garcke, Jochen},
title = {Combining Machine Learning and Simulation to a Hybrid Modelling Approach: Current and Future Directions},
year = {2020},
isbn = {978-3-030-44583-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-44584-3_43},
doi = {10.1007/978-3-030-44584-3_43},
abstract = {In this paper, we describe the combination of machine learning and simulation towards a hybrid modelling approach. Such a combination of data-based and knowledge-based modelling is motivated by applications that are partly based on causal relationships, while other effects result from hidden dependencies that are represented in huge amounts of data. Our aim is to bridge the knowledge gap between the two individual communities from machine learning and simulation to promote the development of hybrid systems. We present a conceptual framework that helps to identify potential combined approaches and employ it to give a structured overview of different types of combinations using exemplary approaches of simulation-assisted machine learning and machine-learning assisted simulation. We also discuss an advanced pairing in the context of Industry 4.0 where we see particular further potential for hybrid systems.},
booktitle = {Advances in Intelligent Data Analysis XVIII: 18th International Symposium on Intelligent Data Analysis, IDA 2020, Konstanz, Germany, April 27–29, 2020, Proceedings},
pages = {548–560},
numpages = {13},
keywords = {Machine learning, Simulation, Hybrid approaches},
location = {Konstanz, Germany}
}

@article{10.1007/s00521-021-06485-7,
author = {Phamtoan, Dinh and Vovan, Tai},
title = {Building fuzzy time series model from unsupervised learning technique and genetic algorithm},
year = {2021},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06485-7},
doi = {10.1007/s00521-021-06485-7},
abstract = {This paper proposes a new model to interpolate time series and forecast it effectively for the future. The important contribution of this study is the combination of optimal techniques for fuzzy clustering problem using genetic algorithm and forecasting model for fuzzy time series. Firstly, the proposed model finds the suitable number of clusters for a series and optimizes the clustering problem by the genetic algorithm using the improved Davies and Bouldin index as the objective function. Secondly, the study gives the method to establish the fuzzy relationship of each element to the established clusters. Finally, the developed model establishes the rule to forecast for the future. The steps of the proposed model are presented clearly and illustrated by the numerical example. Furthermore, it has been realized positively by the established MATLAB procedure. Performing for a lot of series (3007 series) with the differences about characteristics and areas, the new model has shown the significant performance in comparison with the existing models via some parameters to evaluate the built model. In addition, we also present an application of the proposed model in forecasting the COVID-19 victims in Vietnam that it can perform similarly for other countries. The numerical examples and application show potential in the forecasting area of this research.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {7235–7252},
numpages = {18},
keywords = {Cluster analysis, Forecast, Fuzzy time series, Interpolate}
}

@inproceedings{10.1007/978-3-030-30446-1_16,
author = {Kawamoto, Yusuke},
title = {Towards Logical Specification of Statistical Machine Learning},
year = {2019},
isbn = {978-3-030-30445-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30446-1_16},
doi = {10.1007/978-3-030-30446-1_16},
abstract = {We introduce a logical approach to formalizing statistical properties of machine learning. Specifically, we propose a formal model for statistical classification based on a Kripke model, and formalize various notions of classification performance, robustness, and fairness of classifiers by using epistemic logic. Then we show some relationships among properties of classifiers and those between classification performance and robustness, which suggests robustness-related properties that have not been formalized in the literature as far as we know. To formalize fairness properties, we define a notion of counterfactual knowledge and show techniques to formalize conditional indistinguishability by using counterfactual epistemic operators. As far as we know, this is the first work that uses logical formulas to express statistical properties of machine learning, and that provides epistemic (resp. counterfactually epistemic) views on robustness (resp. fairness) of classifiers.},
booktitle = {Software Engineering and Formal Methods: 17th International Conference, SEFM 2019, Oslo, Norway, September 18–20, 2019, Proceedings},
pages = {293–311},
numpages = {19},
keywords = {Epistemic logic, Possible world semantics, Divergence, Machine learning, Statistical classification, Robustness, Fairness},
location = {Oslo, Norway}
}

@inproceedings{10.1145/3318464.3386140,
author = {Higginson, Antony S. and Dediu, Mihaela and Arsene, Octavian and Paton, Norman W. and Embury, Suzanne M.},
title = {Database Workload Capacity Planning using Time Series Analysis and Machine Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386140},
doi = {10.1145/3318464.3386140},
abstract = {When procuring or administering any I.T. system or a component of an I.T. system, it is crucial to understand the computational resources required to run the critical business functions that are governed by any Service Level Agreements. Predicting the resources needed for future consumption is like looking into the proverbial crystal ball. In this paper we look at the forecasting techniques in use today and evaluate if those techniques are applicable to the deeper layers of the technological stack such as clustered database instances, applications and groups of transactions that make up the database workload. The approach has been implemented to use supervised machine learning to identify traits such as reoccurring patterns, shocks and trends that the workloads exhibit and account for those traits in the forecast. An experimental evaluation shows that the approach we propose reduces the complexity of performing a forecast, and accurate predictions have been produced for complex workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {769–783},
numpages = {15},
keywords = {DBAAS, database capacity planning, forecasting, machine learning, supervised learning, time series analysis},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3351095.3375624,
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter},
title = {Explainable machine learning in deployment},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375624},
doi = {10.1145/3351095.3375624},
abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {648–657},
numpages = {10},
keywords = {deployed systems, explainability, machine learning, qualitative study, transparency},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1007/s10664-021-09994-0,
author = {Vitui, Arthur and Chen, Tse-Hsun (Peter)},
title = {MLASP: Machine learning assisted capacity planning: An industrial experience report},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09994-0},
doi = {10.1007/s10664-021-09994-0},
abstract = {In industrial environments it is critical to find out the capacity of a system and plan for a deployment layout that meets the production traffic demands. The system capacity is influenced by both the performance of the system’s constituting components and the physical environment setup. In a large system, the configuration parameters of individual components give the flexibility to developers and load test engineers to tune system performance without changing the source code. However, due to the large search space, estimating the capacity of the system given different configuration values is a challenging and costly process. In this paper, we propose an approach, called MLASP, that uses machine learning models to predict the system key performance indicators (i.e., KPIs), such as throughput, given a set of features made off configuration parameter values, including server cluster setup, to help engineers in capacity planning for production environments. Under the same load, we evaluate MLASP on two large-scale mission-critical enterprise systems developed by Ericsson and on one open-source system. We find that: 1) MLASP can predict the system throughput with a very high accuracy. The difference between the predicted and the actual throughput is less than 1%; and 2) By using only a small subset of the training data (e.g., 3% of the entire data for the open-source system), MLASP can still predict the throughput accurately. We also document our experience of successfully integrating the approach into an industrial setting. In summary, this paper highlights the benefits and potential of using machine learning models to assist load test engineers in capacity planning.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {27},
keywords = {Load testing, Capacity testing, Performance testing, Machine learning, Deep learning}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@article{10.1007/s00521-021-05749-6,
author = {Karlos, Stamatis and Aridas, Christos and Kanas, Vasileios G. and Kotsiantis, Sotiris},
title = {Classification of acoustical signals by combining active learning strategies with semi-supervised learning schemes},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05749-6},
doi = {10.1007/s00521-021-05749-6},
abstract = {In real-world cases, handling both labeled and unlabeled data has raised the interest of several Data Scientists and Machine Learning engineers, leading to several demonstrations that apply data-augmenting approaches in order to obtain a robust and, at the same time, accurate enough learning behavior. The main reason is the existence of much unlabeled data that are ignored by conventional supervised approaches, reducing the chance of enriching the final formatted hypothesis. However, the majority of the proposed methods that operate using both kinds of these data are oriented toward exploiting only one category of these algorithms, without combining their strategies. Since the most popular of them regarding the classification task are Active and Semi-supervised Learning approaches, we aim to design a framework that combines both of them trying to fuse their advantages during the main core of the learning process. Thus, we conduct an empirical evaluation of such a combinatory approach over three problems, which stem from various fields but are all tackled through the use of acoustical signals, operating under the pool-based scenario: gender identification, emotion detection and automatic speaker recognition. Into the proposed combinatory framework, which operates under training sets with small cardinality, our results prove the benefits of adopting such kind of semi-automated approaches regarding both the achieved predictive correctness when reduced consumption of resources takes place, as well as the smoothness of the learning convergence. Several learners have been examined for reaching to more general conclusions, and a variant of self-training scheme has been also examined.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {3–20},
numpages = {18},
keywords = {Combined learning framework, Self-training scheme, Active learning queries, Acoustical signal classification, Data augmentation techniques, Semi-automated approaches}
}

@article{10.1016/j.cor.2021.105504,
author = {Xu, Xiang and Zhu, Daoli},
title = {New method for solving Ivanov regularization-based support vector machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {136},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2021.105504},
doi = {10.1016/j.cor.2021.105504},
journal = {Comput. Oper. Res.},
month = dec,
numpages = {10},
keywords = {Support vector machine, Ivanov regularization, Randomized primal–dual coordinate method}
}

@article{10.3233/JIFS-189520,
author = {Guangxu, Yu and Ramachandran, Varatharajan},
title = {Research on computer network information security based on improved machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189520},
doi = {10.3233/JIFS-189520},
abstract = {The 21st century is an era of rapid development of the Internet. Internet technology is widely used in various fields. With the rapid development of network, the importance of network information security is also highlighted. The traditional network information security technology has been difficult to ensure the security of network information. Therefore, we mainly study the application of machine learning feature extraction method in situational awareness system. A feature selection method based on machine learning is proposed to extract situational features.By analyzing whether the background of network information is safe or not, and according to the current research situation at home and abroad and the trend of Internet development, this paper tries out the practical application of machine learning feature extraction method in a certain perception system. Based on the above points, a selection method based on machine learning is proposed to extract situational features. The accuracy and timeliness of situational awareness system detection are seriously affected by the high dimension, noise and redundant features of massive network traffic data.Therefore, it is of great value to further study network intrusion detection technology on the basis of machine learning.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6889–6900},
numpages = {12},
keywords = {Network information security, security potential perception, intrusion security detection, machine learning}
}

@article{10.1016/j.knosys.2021.107358,
author = {Tang, Mengzi and P\'{e}rez-Fern\'{a}ndez, Ra\'{u}l and De Baets, Bernard},
title = {A comparative study of machine learning methods for ordinal classification with absolute and relative information},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {230},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107358},
doi = {10.1016/j.knosys.2021.107358},
journal = {Know.-Based Syst.},
month = oct,
numpages = {15},
keywords = {Machine learning, Absolute information, Relative information, Ordinal classification}
}

@phdthesis{10.5555/AAI28712992,
author = {Dick, Sebastian and Michael, Zingale, and Thomas, Allison, and J, Harrison, Robert},
advisor = {Marivi, Fern\'{a}ndez-Serra,},
title = {Improving Density Functional Theory with Machine Learning},
year = {2021},
isbn = {9798460433902},
publisher = {State University of New York at Stony Brook},
address = {USA},
abstract = {In times of increasing threats from climate change and pandemics, the need for efficient and accurate tools for the in silico design of new therapeutics and materials is more pressing than ever. Electronic structure calculations based on quantum mechanical methods provide a way to generate scientific insights from first principles and thus form a fundamental pillar of modern molecular sciences. In principle, any atomic system is described to an extremely high degree of accuracy by the equations of quantum mechanics. Although these equations are well understood, solving them for systems larger than a few atoms is still prohibitively expensive.Advanced methods from quantum chemistry offer reliable approximate solutions and are routinely used to simulate small to medium-sized molecules. Unfortunately, their computational cost and poor scaling rule out their use in simulating larger molecules, which are often relevant to understanding biochemical processes and the properties of materials. A different tool, density functional theory (DFT), in particular Kohn-Sham DFT, initially created to investigate properties of solids, has seen an increase in popularity due to its favorable cubic scaling. The core idea of DFT is as simple as it is appealing. Rather than treating interactions between any two electrons explicitly, electrons merely interact with a mean-field density. This approximation makes the efficient implementation of DFT possible, enabling researchers to simulate systems with up to thousands of atoms.Despite its attractiveness, DFT comes with its own set of problems. The theory resembles an incomplete jigsaw puzzle with all pieces known except for one: the so-called exchange-correlation (XC) functional. Finding the true XC functional would render DFT exact, and chemical processes could be accurately described. However, the true form of this elusive functional is so far unknown, and there is little hope that it can ever be written down in a closed-form expression. For practical applications, this missing piece to the puzzle has to be approximated. Many approximations, varying in complexity and accuracy, exist, and researchers have to decide on a case-by-case basis which functional to use. Doing so, however, is far from ideal, as the added degree of freedom can introduce hard-to-control systematic errors.In this work, we outline avenues for creating new XC functionals with the help of neural networks, a machine learning method. Neural networks are considered universal approximators, which means they can fit any function with arbitrary accuracy. For this reason, some people believe machine learning might hold the key to achieving something close to an exact functional.We introduce the concept of physically informed machine learning and propose two approaches to fitting density functionals. In one approach, prior physical knowledge is injected into the training procedure by learning to add small corrections to physically motivated calculations. Our second approach demonstrates how physical information can be directly incorporated into the optimization algorithm in the form of differential equations. We show that both approaches lead to machine learning models that are significantly more data-efficient and reliable than those without physical priors. Trained automatically, the thus created models routinely outperform carefully hand-designed functionals. However, we also find that caution needs to be exercised when using machine-learned models, as they lack some of the safety-nets that traditional functionals are designed with and therefore run the risk of failing in unexpected scenarios.We conclude that machine-learned models have become a valuable addition to a researcher's toolbox while not replacing hand-designed functionals. In particular, we conjecture that a future path towards an exact functional will depend on a strong symbiosis between the two seemingly divergent approaches. Physical insights will lead the way to new functionals by posing strong constraints on their form, while flexible machine-learning methods will realize these functionals within the given bounds. Our efforts are complemented by a collection of novel software libraries and toolkits that make machine-learned functionals available to the broader research community.},
note = {AAI28712992}
}

@inproceedings{10.1109/MILCOM52596.2021.9652909,
author = {Chin, Peter and Do, Emily and Doucette, Cody and Kalashian, Brandon and Last, David and Lenz, Nathan and Lu, Edward and Minor, Devon and Noyes, Elias and Rock, Colleenn and Soule, Nathaniel and Walczak, Nicholas and Canestrare, Dave},
title = {TAK-ML: Applying Machine Learning at the Tactical Edge},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM52596.2021.9652909},
doi = {10.1109/MILCOM52596.2021.9652909},
abstract = {The “Every Soldier is a Sensor” (ES2) concept employs warfighters' proximity to unfolding events in order to provide better situational awareness and decision-making capabilities. However, today's ES2 practices put the burden of data collection on warfighters themselves, and the burden of interpretation (across potentially many inputs) on commanders. This leads to a situation where data collection is limited by the capacity of the warfighter (who is busy executing their core objectives), and data fusion, interpretation, and analysis are limited by the cognitive constraints of the human commanders and analysts interpreting the potentially massive amounts of data. The TAK-ML framework transitions these burdens to machines, allowing collection, fusion, and learning to operate at machine speed and scale. To accomplish this, TAK-ML takes recent advancements in mobile device capabilities and machine learning techniques and applies them to the Tactical Assault Kit (TAK) ecosystem, e.g., ATAK mobile devices and TAK servers, to facilitate the easy application of ML to real mission sets. This paper describes the TAK-ML framework which supports data collection, model building, and model execution/employment in tactical environments, as well as a set of initial applications of this framework. The framework and applications are described and evaluated, showing the capabilities available, the ease of use of the system, and initial insights into the efficacy of the resulting models and applications.},
booktitle = {MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM)},
pages = {108–114},
numpages = {7},
location = {San Diego, CA, USA}
}

@inproceedings{10.5555/3172077.3172089,
author = {Bing, Lidong and Cohen, William W. and Dhingra, Bhuwan},
title = {Using graphs of classifiers to impose declarative constraints on semi-supervised learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1454–1460},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1007/978-3-030-88238-9_10,
author = {Yadav, Tarun and Kumar, Manoj},
title = {Differential-ML Distinguisher: Machine Learning Based Generic Extension for&nbsp;Differential Cryptanalysis},
year = {2021},
isbn = {978-3-030-88237-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88238-9_10},
doi = {10.1007/978-3-030-88238-9_10},
abstract = {The differential attack is a basic cryptanalytic technique for block ciphers. Application of machine learning shows promising results for the differential cryptanalysis. In this paper, we present a new technique to extend the classical differential distinguisher using machine learning (ML). We use r-round classical differential distinguisher to build an s-round ML based differential distinguisher. This s-round ML distinguisher is used to construct an (r+s)-round differential-ML distinguisher with the reduced data complexity. We demonstrate this technique on the lightweight block ciphers SPECK32, SIMON32, and GIFT64 by constructing the differential-ML distinguishers. The data complexities of distinguishers for 9-round SPECK32, 12-round SIMON32, and 8-round GIFT64 are reduced from 230 to 220, 234 to 222, and 238 to 220 respectively. Moreover, the differential-ML distinguisher for SIMON32 is the first 12-round distinguisher with the data complexity less than 232.},
booktitle = {Progress in Cryptology – LATINCRYPT 2021: 7th International Conference on Cryptology and Information Security in Latin America, Bogot\'{a}, Colombia, October 6–8, 2021, Proceedings},
pages = {191–212},
numpages = {22},
keywords = {Block cipher, Differential cryptanalysis, Machine learning},
location = {Bogot\'{a}, Colombia}
}

@inproceedings{10.1145/3394486.3406461,
author = {Ahmad, Muhammad Aurangzeb and Patel, Arpit and Eckert, Carly and Kumar, Vikas and Teredesai, Ankur},
title = {Fairness in Machine Learning for Healthcare},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406461},
doi = {10.1145/3394486.3406461},
abstract = {The issue of bias and fairness in healthcare has been around for centuries. With the integration of AI in healthcare the potential to discriminate and perpetuate unfair and biased practices in healthcare increases many folds The tutorial focuses on the challenges, requirements and opportunities in the area of fairness in healthcare AI and the various nuances associated with it. The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3529–3530},
numpages = {2},
keywords = {fate ml, fatml, healthcare ai, machine learning in healthcare, fairness},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1007/s10994-021-06052-0,
author = {Yang, Cong and Wang, Wenfeng and Zhang, Yunhui and Zhang, Zhikai and Shen, Lina and Li, Yipeng and See, John},
title = {MLife: a lite framework for machine learning lifecycle initialization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {11},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06052-0},
doi = {10.1007/s10994-021-06052-0},
abstract = {Machine learning (ML) lifecycle is a cyclic process to build an efficient ML system. Though a lot of commercial and community (non-commercial) frameworks have been proposed to streamline the major stages in the ML lifecycle, they are normally overqualified and insufficient for an ML system in its nascent phase. Driven by real-world experience in building and maintaining ML systems, we find that it is more efficient to initialize the major stages of ML lifecycle first for trial and error, followed by the extension of specific stages to acclimatize towards more complex scenarios. For this, we introduce a simple yet flexible framework, MLife, for fast ML lifecycle initialization. This is built on the fact that data flow in MLife is in a closed loop driven by bad cases, especially those which impact ML model performance the most but also provide the most value for further ML model development—a key factor towards enabling enterprises to fast track their ML capabilities. Better yet, MLife is also flexible enough to be easily extensible to more complex scenarios for future maintenance. For this, we introduce two real-world use cases to demonstrate that MLife is particularly suitable for ML systems in their early phases.},
journal = {Mach. Learn.},
month = dec,
pages = {2993–3013},
numpages = {21},
keywords = {Machine learning, Machine learning lifecycle, Machine learning system, Deep learning, Data flow}
}

@inproceedings{10.1145/3430665.3456326,
author = {Wunderlich, Linus and Higgins, Allen and Lichtenstein, Yossi},
title = {Machine Learning for Business Students: An Experiential Learning Approach},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456326},
doi = {10.1145/3430665.3456326},
abstract = {This paper reports on the design and teaching of a course on Machine Learning (ML) for business students, focusing on the interaction between content and teaching approach. We demonstrate that the nature of ML technology is amenable to experiential learning and is well suited for business students as users of the technology. The community-based tools, documentation and datasets enable non-programmers to use and adapt open, public-domain ML examples. Students learn how to select algorithms for specific data and tasks, experiment with hyper-parameters and neural-network structures, evaluate results and interpret their business implications. We conclude that business students, at least in our school, often have good abstract understanding of computing and may be ready for deeper learning of digital technology. The general-purpose nature of ML makes it suitable for real-world business problems, making it business-relevant technology that should be introduced into business schools.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {512–518},
numpages = {7},
keywords = {business analytics and big data, experiential learning, higher education, machine learning, non-CS majors},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{10.1145/3472749.3474734,
author = {Fran\c{c}oise, Jules and Caramiaux, Baptiste and Sanchez, T\'{e}o},
title = {Marcelle: Composing Interactive Machine Learning Workflows and Interfaces},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474734},
doi = {10.1145/3472749.3474734},
abstract = {Human-centered approaches to machine learning have established theoretical foundations, design principles and interaction techniques to facilitate end-user interaction with machine learning systems. Yet, general-purpose toolkits supporting the design of interactive machine learning systems are still missing, despite their potential to foster reuse, appropriation and collaboration between different stakeholders including developers, machine learning experts, designers and end users. In this paper, we present an architectural model for toolkits dedicated to the design of human interactions with machine learning. The architecture is built upon a modular collection of interactive components that can be composed to build interactive machine learning workflows, using reactive pipelines and composable user interfaces. We introduce Marcelle, a toolkit for the design of human interactions with machine learning that implements this model. We illustrate Marcelle with two implemented case studies: (1) a HCI researcher conducts user studies to understand novice interaction with machine learning, and (2) a machine learning expert and a clinician collaborate to develop a skin cancer diagnosis system. Finally, we discuss our experience with the toolkit, along with its limitation and perspectives.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–53},
numpages = {15},
keywords = {Architectural Model, Interactive Machine Learning, Machine Teaching, Toolkit.},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1504/ijaip.2021.117607,
author = {Sharma, Upasana and Khatri, Sunil Kumar and Patnaik, L.M.},
title = {A supervised learning approach for link prediction in complex social networks},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {1–2},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.117607},
doi = {10.1504/ijaip.2021.117607},
abstract = {In the current scenario, social networking is being used for social and business purpose such as Facebook, Twitter, and LinkedIn. Social networking websites are attracting the focus of many researchers. New links are being created in every fraction of a second. The major challenge in link prediction domain is to predict the future link. The social networks require effective and precise technique to predict the future connections in the network system. The focus of this work is on supervised machine learning approach for link prediction in complex social networks. Many researchers have been worked on supervised approach by using only unweighted networks in the past. Our aim is to assign weight to each connection in the network where the weight represents the strength of the connection. This paper introduces a new approach using closed triangle concept to recommend the future links in social networks. Extensive experiments have been performed on real YouTube data set and the proposed technique performs well and improves the accuracy of the link predictor.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {1–15},
numpages = {14},
keywords = {link prediction, social networks, artificial neural network, supervised learning approach, learning algorithms}
}

@inproceedings{10.1145/3318299.3318363,
author = {Zheng, Heci and Wu, Chunhua},
title = {Predicting Personality Using Facebook Status Based on Semi-supervised Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318363},
doi = {10.1145/3318299.3318363},
abstract = {Personality analysis on social media is a research hotspot due to the importance of personality research in psychology as well as the rapid development of social media. Many studies have used social media status to analyze user's personality, but most of them are conducted on inadequate label data and linguistic features. In this paper, to explore the usage of unlabeled data on personality analysis, a personality analysis framework based on semi-supervised learning is introduced. Besides, for making full use of the language information in social media status, the well-known n-gram model is adopted to extract linguistic features. The experimental results demonstrate the semi-supervised learning can take advantage of unlabeled data and improve the accuracy of prediction model.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {59–64},
numpages = {6},
keywords = {Personality, semi-surpervised learning, social media status},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3475992.3475998,
author = {Amelin, Vladislav and Romanov, Nikita and Vasilyev, Robert and Shvets, Rostyslav and Yanovich, Yury and Zhygulin, Viacheslav},
title = {Machine Learning View on Blockchain Parameter Adjustment},
year = {2021},
isbn = {9781450389518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475992.3475998},
doi = {10.1145/3475992.3475998},
abstract = {A fundamental problem in distributed computing is achieving agreement among many parties for a single data value in the presence of faulty processes–to get consensus. The consensus mechanism is an underlying part of blockchain design and commits new blocks and changes protocol itself. In addition to classic correctness requirements, blockchains need specific ones: high performance regarding transactions per second, fast transaction confirmation,&nbsp;etc. Blockchains control the requirements with parameters. But how to meet qualitative and optimize quantitative requirements? Typically we have the main blockchain network without access to try different parameters and the test network to do whatever we want. In the paper, we provide a machine learning view on the blockchain parameter adjustment. We list the blockchain parameters for Solana blockchain and apply feature importance to select the most significant parameters during the forthcoming optimization.},
booktitle = {Proceedings of the 2021 3rd Blockchain and Internet of Things Conference},
pages = {38–43},
numpages = {6},
keywords = {blockchain, consensus, machine learning, optimization, simulation},
location = {Ho Chi Minh City, Vietnam},
series = {BIOTC '21}
}

@article{10.1016/j.compeleceng.2021.107329,
author = {Oprea, Simona-Vasilica and B\^{a}ra, Adela},
title = {Machine learning classification algorithms and anomaly detection in conventional meters and Tunisian electricity consumption large datasets},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107329},
doi = {10.1016/j.compeleceng.2021.107329},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {17},
keywords = {Machine learning, Fraud detection, Feature engineering, Probability density function, Conventional meter}
}

@article{10.1016/j.inffus.2019.12.001,
author = {Meng, Tong and Jing, Xuyang and Yan, Zheng and Pedrycz, Witold},
title = {A survey on machine learning for data fusion},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2019.12.001},
doi = {10.1016/j.inffus.2019.12.001},
journal = {Inf. Fusion},
month = may,
pages = {115–129},
numpages = {15},
keywords = {Data fusion, Machine learning, Fusion methods, Fusion criteria}
}

@article{10.1137/20M1317992,
author = {Sim, Byeongsu and Oh, Gyutaek and Kim, Jeongsol and Jung, Chanyong and Ye, Jong Chul},
title = {Optimal Transport Driven CycleGAN for Unsupervised Learning in Inverse Problems},
year = {2020},
issue_date = {January 2020},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1137/20M1317992},
doi = {10.1137/20M1317992},
abstract = {To improve the performance of  classical generative adversarial networks (GANs), Wasserstein generative adversarial networks (WGANs) were developed as a Kantorovich dual formulation of the optimal transport (OT) problem using Wasserstein-1 distance. However, it was not clear how CycleGAN-type generative models can be derived from the OT theory. Here we show that a novel  CycleGAN architecture can be derived as a Kantorovich dual OT formulation if a penalized least squares (PLS) cost with deep learning--based inverse path penalty is used as a transportation cost. One of the most important advantages of this formulation is that depending on the knowledge of the forward problem, distinct variations of CycleGAN architecture can be derived: for example,  one with two pairs of generators and discriminators, and the other with only a single pair of generator and discriminator. Even for the two generator cases, we show that the structural knowledge of the forward operator can lead to a simpler generator architecture which significantly simplifies the neural network training. The new CycleGAN formulation, which we call the OT-CycleGAN, has been applied for various biomedical imaging problems,  such as
accelerated magnetic resonance imaging (MRI), super-resolution microscopy, and low-dose X-ray computed tomography (CT). Experimental results confirm the efficacy and flexibility of the theory.},
journal = {SIAM J. Img. Sci.},
month = jan,
pages = {2281–2306},
numpages = {26},
keywords = {unsupervised learning, optimal transport, CycleGAN, penalized least squares, inverse problems, 68Q32, 68T05, 68T45, 65L09, 68U10}
}

@inproceedings{10.1145/3411501.3419432,
author = {Haralampieva, Veneta and Rueckert, Daniel and Passerat-Palmbach, Jonathan},
title = {A Systematic Comparison of Encrypted Machine Learning Solutions for Image Classification},
year = {2020},
isbn = {9781450380881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411501.3419432},
doi = {10.1145/3411501.3419432},
abstract = {This work provides a comprehensive review of existing frameworks based on secure computing techniques in the context of private image classification. The in-depth analysis of these approaches is followed by careful examination of their performance costs, in particular runtime and communication overhead.To further illustrate the practical considerations when using different privacy-preserving technologies, experiments were conducted using four state-of-the-art libraries implementing secure computing at the heart of the data science stack: PySyft and CrypTen supporting private inference via Secure Multi-Party Computation, TF-Trusted utilising Trusted Execution Environments and HE-Transformer relying on Homomorphic encryption.Our work aims to evaluate the suitability of these frameworks from a usability, runtime requirements and accuracy point of view. In order to better understand the gap between state-of-the-art protocols and what is currently available in practice for a data scientist, we designed three neural network architecture to obtain secure predictions via each of the four aforementioned frameworks. Two networks were evaluated on the MNIST dataset and one on the Malaria Cell image dataset. We observed satisfying performances for TF-Trusted and CrypTen and noted that all frameworks perfectly preserved the accuracy of the corresponding plaintext model.},
booktitle = {Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice},
pages = {55–59},
numpages = {5},
keywords = {benchmark, image classification, machine learning, privacy-preserving inference},
location = {Virtual Event, USA},
series = {PPMLP'20}
}

@inproceedings{10.5555/3465085.3465119,
author = {Jabi, Wassim and Alymani, Abdulrahman},
title = {Graph machine learning using 3D topological models},
year = {2020},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Classification of urban and architectural works using machine learning techniques have typically focused on 2D pixel-based image recognition. In this paper we present a novel proof-of-concept workflow that enables a machine learning computer system to learn to classify 3D conceptual models based on topological graphs rather than 2D images. The system leverages two main technologies. The first is a custom designed software library that enhances the representation of 3D models through non-manifold topology and embedded semantic information. The second is an end-to-end deep graph convolutional network (DGCNN) that accepts graphs of an arbitrary structure without the need to first convert them into vectors. The experimental workflow consists of two stages. In the first stage, a generative parametric system was designed to create a large synthetic dataset of an urban block with several typological variations. The geometric models were then automatically labelled and converted into semantically rich topological dual graphs. In the second stage, the dual graphs were imported into the DGCNN for graph classification. Experiments demonstrate that the proposed workflow achieves accuracy results that are highly competitive with DGCNN's classification of benchmark graph datasets.},
booktitle = {Proceedings of the 11th Annual Symposium on Simulation for Architecture and Urban Design},
articleno = {34},
numpages = {8},
keywords = {graphs, machine learning, topological models},
location = {Virtual Event, Austria},
series = {SimAUD '20}
}

@inproceedings{10.1007/978-3-030-79379-1_1,
author = {Meinke, Karl and Khosrowjerdi, Hojat},
title = {Use Case Testing: A Constrained Active Machine Learning Approach},
year = {2021},
isbn = {978-3-030-79378-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79379-1_1},
doi = {10.1007/978-3-030-79379-1_1},
abstract = {As a methodology for system design and testing, use cases are well-known and widely used. While current active machine learning (ML) algorithms can effectively automate unit testing, they do not scale up&nbsp;to use case testing of complex systems in an efficient way.We present a new parallel distributed processing (PDP) architecture for a constrained active machine learning (CAML) approach to use case testing. To exploit CAML we introduce a use case modeling language with: (i) compile-time constraints on query generation, and (ii) run-time constraints using dynamic constraint checking. We evaluate this approach by applying a prototype implementation of CAML to use case testing of simulated multi-vehicle autonomous driving scenarios.},
booktitle = {Tests and Proofs: 15th International Conference, TAP 2021, Held as Part of STAF 2021, Virtual Event, June 21–22, 2021, Proceedings},
pages = {3–21},
numpages = {19},
keywords = {Autonomous driving, Constraint solving, Learning-based testing, Machine learning, Model checking, Requirements testing, Use case testing}
}

@article{10.1007/s42979-020-00374-x,
author = {Ugraiah, Purushotham and Shivabasave Gowda, Chethan Kanakapura},
title = {Semi-Supervised Learning to Enhance Speech Signal for Mobile Communication},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00374-x},
doi = {10.1007/s42979-020-00374-x},
abstract = {Machine learning algorithm to enhance the complex speech signal for mobile communication is one of the research problems in signal processing. The objective of this research paper is to develop a learning algorithm that improves the quality and intelligibility of voice signals that gets are corrupted by real world noise while they are transmitted through the channel. In this paper, we consider a semi-supervised machine learning algorithm for mobile phones that comes with system software to improve SNR of speech signal which is corrupted by manmade disturbance. Most of the disturbances are non-stationary where the effect of noise is non-uniform for all spectral components. In the projected algorithm training, the system is completed with a set of speech and noise data base. System parameters are derived during training process; these parameters are updated as per the disturbance present in the signal. These parameters are used to remove the noise present in speech signal. The obtained results show a substantial progress in SNR by 5–8% as compared to traditional methods.},
journal = {SN Comput. Sci.},
month = nov,
numpages = {10},
keywords = {Speech enhancement, Semi-supervised, Non-stationary, Machine learning, SNR}
}

@inproceedings{10.1145/3233547.3233667,
author = {Ahmad, Muhammad Aurangzeb and Eckert, Carly and Teredesai, Ankur},
title = {Interpretable Machine Learning in Healthcare},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233667},
doi = {10.1145/3233547.3233667},
abstract = {This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {559–560},
numpages = {2},
keywords = {explainable ai, explainable machine learning, interpretable machine learning, machine learning in healthcare},
location = {Washington, DC, USA},
series = {BCB '18}
}

@article{10.3103/S0146411621080198,
author = {Marshev, I. I. and Zhukovskii, E. V. and Aleksandrova, E. B.},
title = {Protection against Adversarial Attacks on Malware Detectors Using Machine Learning Algorithms},
year = {2021},
issue_date = {Dec 2021},
publisher = {Allerton Press, Inc.},
address = {USA},
volume = {55},
number = {8},
issn = {0146-4116},
url = {https://doi.org/10.3103/S0146411621080198},
doi = {10.3103/S0146411621080198},
journal = {Autom. Control Comput. Sci.},
month = dec,
pages = {1025–1028},
numpages = {4},
keywords = {malware detection, machine learning, adversarial attacks, neural networks, statistical analysis}
}

@inproceedings{10.1007/978-3-030-77870-5_28,
author = {Benamira, Adrien and Gerault, David and Peyrin, Thomas and Tan, Quan Quan},
title = {A Deeper Look at Machine Learning-Based Cryptanalysis},
year = {2021},
isbn = {978-3-030-77869-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77870-5_28},
doi = {10.1007/978-3-030-77870-5_28},
abstract = {At CRYPTO’19, Gohr proposed a new cryptanalysis strategy based on the utilisation of machine learning algorithms. Using deep neural networks, he managed to build a neural based distinguisher that surprisingly surpassed state-of-the-art cryptanalysis efforts on one of the versions of the well studied NSA block cipher SPECK (this distinguisher could in turn be placed in a larger key recovery attack). While this work opens new possibilities for machine learning-aided cryptanalysis, it remains unclear how this distinguisher actually works and what information is the machine learning algorithm deducing. The attacker is left with a black-box that does not tell much about the nature of the possible weaknesses of the algorithm tested, while hope is thin as interpretability of deep neural networks is a well-known difficult task.In this article, we propose a detailed analysis and thorough explanations of the inherent workings of this new neural distinguisher. First, we studied the classified sets and tried to find some patterns that could guide us to better understand Gohr’s results. We show with experiments that the neural distinguisher generally relies on the differential distribution on the ciphertext pairs, but also on the differential distribution in penultimate and antepenultimate rounds. In order to validate our findings, we construct a distinguisher for SPECK cipher based on pure cryptanalysis, without using any neural network, that achieves basically the same accuracy as Gohr’s neural distinguisher and with the same efficiency (therefore improving over previous non-neural based distinguishers).Moreover, as another approach, we provide a machine learning-based distinguisher that strips down Gohr’s deep neural network to a bare minimum. We are able to remain very close to Gohr’s distinguishers’ accuracy using simple standard machine learning tools. In particular, we show that Gohr’s neural distinguisher is in fact inherently building a very good approximation of the Differential Distribution Table (DDT) of the cipher during the learning phase, and using that information to directly classify ciphertext pairs. This result allows a full interpretability of the distinguisher and represents on its own an interesting contribution towards interpretability of deep neural networks.Finally, we propose some method to improve over Gohr’s work and possible new neural distinguishers settings. All our results are confirmed with experiments we have been conducted on SPECK block cipher (source code available online).},
booktitle = {Advances in Cryptology – EUROCRYPT 2021: 40th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, October 17–21, 2021, Proceedings, Part I},
pages = {805–835},
numpages = {31},
keywords = {Differential cryptanalysis, SPECK, Machine learning, Deep neural networks, Interpretability},
location = {Zagreb, Croatia}
}

@article{10.1016/j.asoc.2021.107115,
author = {Chan, Kit Yan and Yiu, Ka Fai Cedric and Lam, Hak-Keung and Wong, Bert Wei},
title = {Ball bonding inspections using a conjoint framework with machine learning and human judgement},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107115},
doi = {10.1016/j.asoc.2021.107115},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {13},
keywords = {Machine learning, Human judgement, Threshold detection, Manufacturing of electronic products, Manufacturing inspection, Ball bonding}
}

@inproceedings{10.1145/3394486.3403214,
author = {Guo, Lan-Zhe and Zhou, Zhi and Li, Yu-Feng},
title = {RECORD: Resource Constrained Semi-Supervised Learning under Distribution Shift},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403214},
doi = {10.1145/3394486.3403214},
abstract = {Semi-supervised learning (SSL) tries to improve performance with the use of massive unlabeled data, which typically works in an offline manner with two assumptions. i) Data distribution is static; ii) Data storage overhead is unlimited. In many online tasks, however, none of the above assumptions is valid. For example, in online image classification, a large amount of unlabeled images increases sharply, which makes it difficult to store them in full; meanwhile, the content of unlabeled images changes constantly, and it is no longer suitable to assume a fixed distribution. We call such a novel setting Resource Constrained SSL under Distribution Shift (or Record for short) and to our best knowledge, it has not been thoroughly studied yet. This paper presents a systemic solution Record consisting of three sub-steps, that is, distribution tracking, sample selection and model updating. Specifically, we propose an effective method to track the distribution changes and locate distribution shifted samples. A novel influence-based approach is used to select the most influential samples for the distribution change based on resource constraints. Finally, we free up memory to put the latest unlabeled data with its pseudo-label for the next distribution tracking. Extensive empirical results confirm the effectiveness of our scheme. In the case of diverse and unknown distribution shifts, our solution is consistently and clearly better than many baseline and SOTA methods along with the memory budget and in some cases it can even approximate the performance of oracle.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1636–1644},
numpages = {9},
keywords = {distribution shift, resource constraint, semi-supervised learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.5555/3367471.3367546,
author = {Verma, Vikas and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and Lopez-Paz, David},
title = {Interpolation consistency training for semi-supervised learning},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {3635–3641},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1007/978-3-030-64881-7_16,
author = {Sharma, Arnab and Wehrheim, Heike},
title = {Automatic Fairness Testing of Machine Learning Models},
year = {2020},
isbn = {978-3-030-64880-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64881-7_16},
doi = {10.1007/978-3-030-64881-7_16},
abstract = {In recent years, there has been an increased application of machine learning (ML) to decision making systems. This has prompted an urgent need for validating requirements on ML models. Fairness is one such requirement to be ensured in numerous application domains. It specifies a software as “learned” by an ML algorithm to not be biased in the sense of discriminating against some attributes (like gender or age), giving different decisions upon flipping the values of these attributes.In this work, we apply verification-based testing (VBT) to the fairness checking of ML models. Verification-based testing employs verification technology to generate test cases potentially violating the property under interest. For fairness testing, we additionally provide a specification language for the formalization of different fairness requirements. From the ML model under test and fairness specification VBT automatically generates test inputs specific to the specified fairness requirement. The empirical evaluation on several benchmark ML models shows verification-based testing to perform better than existing fairness testing techniques with respect to effectiveness.},
booktitle = {Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings},
pages = {255–271},
numpages = {17},
keywords = {Fairness, Machine learning testing, SMT solving},
location = {Naples, Italy}
}

@inproceedings{10.1145/3461702.3462556,
author = {Simons, Joshua and Adams Bhatti, Sophia and Weller, Adrian},
title = {Machine Learning and the Meaning of Equal Treatment},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462556},
doi = {10.1145/3461702.3462556},
abstract = {Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {956–966},
numpages = {11},
keywords = {equal treatment, fairness, machine learning, philosophy, politics},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1007/s11128-019-2470-8,
author = {Hou, S. C. and Yi, X. X.},
title = {Quantum Lyapunov control with machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1570-0755},
url = {https://doi.org/10.1007/s11128-019-2470-8},
doi = {10.1007/s11128-019-2470-8},
abstract = {Quantum state engineering is a central task in Lyapunov-based quantum control. Given different initial states, better performance may be achieved if the control parameters, such as the Lyapunov function, are individually optimized for each initial state, however, at the expense of computing resources. To tackle this issue, we propose an initial-state-adaptive Lyapunov control strategy with machine learning. Specifically, artificial neural networks are used to learn the relationship between the optimal control parameters and initial states through supervised learning with samples. Two designs are presented where the feedforward neural network and the general regression neural network are used to select control schemes and design Lyapunov functions, respectively. We demonstrate the performance of the designs with a three-level quantum system for an eigenstate control problem. Since the sample generation and the training of neural networks are carried out in advance, the initial-state-adaptive Lyapunov control can be implemented for new initial states without much increase of computational resources.},
journal = {Quantum Information Processing},
month = nov,
numpages = {20},
keywords = {Lyapunov control, Machine learning, Neural network, Quantum state preparation}
}

@article{10.1016/j.eswa.2021.114818,
author = {Lattanzi, Emanuele and Freschi, Valerio},
title = {Machine Learning Techniques to Identify Unsafe Driving Behavior by Means of In-Vehicle Sensor Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114818},
doi = {10.1016/j.eswa.2021.114818},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {10},
keywords = {Road safety, Driving behavior, Machine learning, Neural networks, Support vector machines}
}

@inproceedings{10.1007/978-3-030-90785-3_16,
author = {Wang, Baoping and Wang, Wennan and Zhu, Linkai and Liu, Wenjian},
title = {Research on Cross-Project Software Defect Prediction Based on Machine Learning},
year = {2021},
isbn = {978-3-030-90784-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90785-3_16},
doi = {10.1007/978-3-030-90785-3_16},
abstract = {In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution.},
booktitle = {Advances in Web-Based Learning – ICWL 2021: 20th International Conference, ICWL 2021, Macau, China, November 13–14, 2021, Proceedings},
pages = {160–165},
numpages = {6},
keywords = {Machine learning, Software defect prediction model, Metric},
location = {Macau, China}
}

@inproceedings{10.1109/WETSEB52558.2021.00012,
author = {Ibba, Giacomo and Pierro, Giuseppe Antonio and Francesco, Marco Di},
title = {Evaluating Machine-Learning Techniques for Detecting Smart Ponzi Schemes},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WETSEB52558.2021.00012},
doi = {10.1109/WETSEB52558.2021.00012},
abstract = {Ethereum is one of the most popular platforms for exchanging cryptocurrencies as well as the most established for peer to peer programming and smart contracts publishing [3]. The versatility of the Solidity language allows developers to program general-purpose smart contracts. Among the various smart contracts, there may be some fraudulent ones, whose purpose is to steal Ether from the network participants. A notorious example of such cases are Ponzi schemes, i.e. a financial frauds that require investors to be repaid through the investments of others who have just entered the scheme. Within the Ethereum blockchain, several contracts have been identified as being Ponzi schemes. The paper proposes a machine learning model that uses textual classification techniques to recognize contracts emulating the behavior of a Ponzi scheme. Starting from a contracts dataset containing exclusively Ponzi schemes uploaded between 2016 and 2018, we built models able to properly classify Ponzi schemes contracts. We tested several models, some of which returned an overall accuracy of 99% on classification. The best model turned out to be the linear Support Vector Machine and the Multinomial Naive Bayes model, which provides the best results in terms of metrics evaluation.},
booktitle = {2021 IEEE/ACM 4th International Workshop on Emerging Trends in Software Engineering for Blockchain (WETSEB)},
pages = {34–40},
numpages = {7},
location = {Madrid, Spain}
}

@article{10.1016/j.asoc.2021.107147,
author = {Mulfari, Davide and Meoni, Gabriele and Marini, Marco and Fanucci, Luca},
title = {Machine learning assistive application for users with speech disorders},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107147},
doi = {10.1016/j.asoc.2021.107147},
journal = {Appl. Soft Comput.},
month = may,
numpages = {11},
keywords = {Assistive technology, Automatic speech recognition, Dysarthria, Machine learning}
}

@article{10.1145/3424660,
author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua},
title = {From Server-Based to Client-Based Machine Learning: A Comprehensive Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3424660},
doi = {10.1145/3424660},
abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users’ local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users’ sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {6},
numpages = {36},
keywords = {Mobile intelligence, decentralized training, distributed system, federated learning, machine learning}
}

@article{10.1007/s10796-016-9724-0,
author = {Liu, Jun and Timsina, Prem and El-Gayar, Omar},
title = {A comparative analysis of semi-supervised learning: The case of article selection for medical systematic reviews},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-016-9724-0},
doi = {10.1007/s10796-016-9724-0},
abstract = {While systematic reviews are positioned as an essential element of modern evidence-based medical practice, the creation of these reviews is resource intensive. To mitigate this problem, there have been some attempts to leverage supervised machine learning to automate the article triage procedure. This approach has been proved to be helpful for updating existing systematic reviews. However, this technique holds very little promise for creating new reviews because training data is rarely available when it comes to systematic creation. In this research we assess and compare the applicability of semi-supervised learning to overcome this labeling bottleneck and support the creation of systematic reviews. The results indicated that semi-supervised learning could significantly reduce the human effort and is a viable technique for automating medical systematic review creation with a small-sized training dataset.},
journal = {Information Systems Frontiers},
month = apr,
pages = {195–207},
numpages = {13},
keywords = {Active learning, Medical systematic reviews, Self-training, Semi-supervised learning, Text analytics, Text mining}
}

@article{10.1016/j.compag.2019.104932,
author = {Amorim, Willian Paraguassu and Tetila, Everton Castel\~{a}o and Pistori, Hemerson and Papa, Jo\~{a}o Paulo},
title = {Semi-supervised learning with convolutional neural networks for UAV images automatic recognition},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.104932},
doi = {10.1016/j.compag.2019.104932},
journal = {Comput. Electron. Agric.},
month = sep,
numpages = {9},
keywords = {Semi-supervised learning, Convolutional Neural Networks, Fine tuning, Transfer learning}
}

@article{10.1145/3433987,
author = {Kristiansen, Stein and Nikolaidis, Konstantinos and Plagemann, Thomas and Goebel, Vera and Traaen, Gunn Marit and \O{}verland, Britt and Aaker\o{}y, Lars and Hunt, Tove-Elizabeth and Loennechen, Jan P\r{a}l and Steinshamn, Sigurd Loe and Bendz, Christina Holt and Anfinsen, Ole-Gunnar and Gullestad, Lars and Akre, Harriet},
title = {Machine Learning for Sleep Apnea Detection with Unattended Sleep Monitoring at Home},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3433987},
doi = {10.1145/3433987},
abstract = {Sleep apnea is a common and strongly under-diagnosed severe sleep-related respiratory disorder with periods of disrupted or reduced breathing during sleep. To diagnose sleep apnea, sleep data are collected with either polysomnography or polygraphy and scored by a sleep expert. We investigate in this work the use of supervised machine learning to automate the analysis of polygraphy data from the A3 study containing more than 7,400 hours of sleep monitoring data from 579 patients. We conduct a systematic comparative study of classification performance and resource use with different combinations of 27 classifiers and four sleep signals. The classifiers achieve up to 0.8941 accuracy (kappa: 0.7877) when using all four signal types simultaneously and up to 0.8543 accuracy (kappa: 0.7080) with only one signal, i.e., oxygen saturation. Methods based on deep learning outperform other methods by a large margin. All deep learning methods achieve nearly the same maximum classification performance even when they have very different architectures and sizes. When jointly accounting for classification performance, resource consumption and the ability to achieve with less training data high classification performance, we find that convolutional neural networks substantially outperform the other classifiers.},
journal = {ACM Trans. Comput. Healthcare},
month = feb,
articleno = {14},
numpages = {25},
keywords = {Sleep apnea, machine learning, polygraphy, portable sleep monitor, unattended sleep monitoring}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1007/978-3-030-58601-0_26,
author = {Ke, Zhanghan and Qiu, Di and Li, Kaican and Yan, Qiong and Lau, Rynson W. H.},
title = {Guided Collaborative Training for Pixel-Wise Semi-Supervised Learning},
year = {2020},
isbn = {978-3-030-58600-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58601-0_26},
doi = {10.1007/978-3-030-58601-0_26},
abstract = {We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collaborative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outperforms state-of-the-art SSL methods by a large margin.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII},
pages = {429–445},
numpages = {17},
keywords = {Semi-supervised learning, Pixel-wise vision tasks},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3524938.3525766,
author = {Sim, Rachael Hwee Ling and Zhang, Yehong and Chan, Mun Choon and Low, Bryan Kian Hsiang},
title = {Collaborative machine learning with incentive-aware model rewards},
year = {2020},
publisher = {JMLR.org},
abstract = {Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {828},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.procs.2021.09.132,
author = {Khang, Pham Quoc and Kaczmarczyk, Klaudia and Tutak, Piotr and Golec, Pawe\l{} and Kuziak, Katarzyna and Depczy\'{n}ski, Rados\l{}aw and Hernes, Marcin and Rot, Artur},
title = {Machine learning for liquidity prediction on Vietnamese stock market},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.132},
doi = {10.1016/j.procs.2021.09.132},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {3590–3597},
numpages = {8},
keywords = {stock market, liquidity, machine learning, prediction}
}

@article{10.1016/j.neucom.2015.11.042,
author = {Li, Jianqiang and Wang, Fei},
title = {Semi-supervised learning via mean field methods},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {177},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.11.042},
doi = {10.1016/j.neucom.2015.11.042},
abstract = {The recent years have witnessed a surge of interest in semi-supervised learning methods. Numerous methods have been proposed for learning from partially labeled data. In this paper, a novel semi-supervised learning approach based on statistical physics is proposed. We treat each data point as an Ising spin and the interaction between pairwise spins is captured by the similarity between the pairwise points. The labels of the data points are treated as the directions of the corresponding spins. In semi-supervised setting, some of the spins have fixed directions (which corresponds to the labeled data), and our task is to determine the directions of other spins. An approach based on the Mean Field theory is proposed to achieve this goal. Finally the experimental results on both toy and real world data sets are provided to show the effectiveness of our method.},
journal = {Neurocomput.},
month = feb,
pages = {385–393},
numpages = {9},
keywords = {Mean field, Semi-supervised learning}
}

@article{10.1613/jair.1.12228,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {245–317},
numpages = {73}
}

@inproceedings{10.1145/3382734.3405695,
author = {El-Mhamdi, El-Mahdi and Guerraoui, Rachid and Guirguis, Arsany and Hoang, L\^{e} Nguy\^{e}n and Rouault, S\'{e}bastien},
title = {Genuinely Distributed Byzantine Machine Learning},
year = {2020},
isbn = {9781450375825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382734.3405695},
doi = {10.1145/3382734.3405695},
abstract = {Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the "general" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes.We show that this problem can be solved in an asynchronous system, despite the presence of ⅓ Byzantine parameter servers and ⅓ Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony.We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24--176% in the synchronous case and 28--220% in the asynchronous case.},
booktitle = {Proceedings of the 39th Symposium on Principles of Distributed Computing},
pages = {355–364},
numpages = {10},
keywords = {byzantine fault tolerance, byzantine parameter servers, distributed machine learning},
location = {Virtual Event, Italy},
series = {PODC '20}
}

@phdthesis{10.5555/AAI28643572,
author = {Lu, Jie and Prateek, Mittal, and Mengdi, Wang, and Sun-Yuan, Kung,},
advisor = {K, Jha, Niraj and Naveen, Verma,},
title = {Energy-Efficient Implementation of Machine Learning Algorithms},
year = {2021},
isbn = {9798471106895},
publisher = {Princeton University},
address = {USA},
abstract = {Pattern-recognition algorithms from the domain of machine learning play a prominent role in embedded sensing systems, in order to derive inferences from sensor data. Very often, such systems face severe energy constraints. The focus of this thesis is on mitigating the energy required for computation, communication, and storage by exploiting various forms of computation algorithms. In the first part of our work, we focus on reducing the computations necessary during linear signal-processing. In order to achieve computation reduction, we consider random projection. Random projection is a form of compression that preserves a similarity metric widely used for pattern recognition is used for computational energy reduction. The form of compression is random projection, and the similarity metric is inner product between source vectors. Given the prominence of random projections within compressive sensing, previous research has explored this idea for application to compressively-sensed signals. We show that random projections can be exploited more generally without compressive sensing, enabling significant reduction in computational energy and avoiding a significant source of error. The approach is referred to as compressed signal processing (CSP). It applies to Nyquist-sampled signals.The second part of our work focuses on dealing with signal processing that may not be linear. We look into approximate computing and its potential as an algorithmic approach to reducing energy. Approximate computing is a broad approach that has recently received considerable attention in the context of inference systems. This stems from the observation that many inference systems exhibit various forms of tolerance to data noise. While some systems have demonstrated significant approximation-vs.-energy knobs to exploit this, they have been applicable to specific kernels and architectures; the more generally available knobs have been relatively weak, resulting in large data noise for relatively modest energy savings (e.g., voltage overscaling, bit precision scaling). In this work, we explore the use of genetic programming (GP) to compute approximate features. Further, we leverage a method that enhances tolerance to feature-data noise through directed retraining of the inference stage. Previous work in GP has shown that it generalizes well to enable approximation of a broad range of computations, raising the potential for broad applicability of the proposed approach. The focus on feature extraction is deliberate because it involves diverse, often highly nonlinear, operations, challenging general applicability of energy-reducing approaches.The third part of our work takes into consideration multi-task algorithms. By exploiting the concept of transfer learning and energy-efficient data show accelerators, we show that the use of convolutional autoencoders can enable various levels of reduction in computational energy and avoid a significant reduction in inference performance when multiple task categories are targeted for obtaining an inference. In order to minimize inference computational energy, a convolutional autoencoder is used for learning a generalized representation of inputs. We consider three scenarios: transferring layers using convolutional autoencoders, transferring layers using convolutional neural networks trained on different tasks, and no layer transfer. We also take into account the performance when transferring only convolutional layers versus when transferring convolutional layers and a fully connected layer.We study our methodologies through validations of their generalizability and through applications using clinical and image data.},
note = {AAI28643572}
}

@article{10.1016/j.asoc.2019.105871,
author = {Sarkar, Jnanendra Prasad and Saha, Indrajit and Chakraborty, Sinjan and Maulik, Ujjwal},
title = {Machine learning integrated credibilistic semi supervised clustering for categorical data},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105871},
doi = {10.1016/j.asoc.2019.105871},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {14},
keywords = {Categorical data, Credibilistic clustering, Friedman test, Fuzzy set, Machine learning, Possibilistic measure, Semi supervised clustering, Statistical significance}
}

@inproceedings{10.1007/978-3-030-61470-6_26,
author = {Bure\v{s}, Tom\'{a}\v{s} and Gerostathopoulos, Ilias and Hn\v{e}tynka, Petr and Pacovsk\'{y}, Jan},
title = {Forming Ensembles at Runtime: A Machine Learning Approach},
year = {2020},
isbn = {978-3-030-61469-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61470-6_26},
doi = {10.1007/978-3-030-61470-6_26},
abstract = {Smart system applications (SSAs) built on top of cyber-physical and socio-technical systems are increasingly composed of components that can work both autonomously and by cooperating with each other. Cooperating robots, fleets of cars and fleets of drones, emergency coordination systems are examples of SSAs. One approach to enable cooperation of SSAs is to form dynamic cooperation groups—ensembles—between components at runtime. Ensembles can be formed based on predefined rules that determine which components should be part of an ensemble based on their current state and the state of the environment (e.g., “group together 3 robots that are closer to the obstacle, their battery is sufficient and they would not be better used in another ensemble”). This is a computationally hard problem since all components are potential members of all possible ensembles at runtime. In our experience working with ensembles in several case studies the past years, using constraint programming to decide which ensembles should be formed does not scale for more than a limited number of components and ensembles. Also, the strict formulation in terms of hard/soft constraints does not easily permit for runtime self-adaptation via learning. This poses a serious limitation to the use of ensembles in large-scale and partially uncertain SSAs. To tackle this problem, in this paper we propose to recast the ensemble formation problem as a classification problem and use machine learning to efficiently form ensembles at scale.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Engineering Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part II},
pages = {440–456},
numpages = {17},
keywords = {Adaptation, Ensembles, Cooperative systems, Machine learning},
location = {Rhodes, Greece}
}

@article{10.1007/s11042-020-09412-5,
author = {Choi, Dong Yoon and Song, Byung Cheol},
title = {Semi-supervised learning for facial expression-based emotion recognition in the continuous domain},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {37–38},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09412-5},
doi = {10.1007/s11042-020-09412-5},
abstract = {Emotion recognition is a very important technique for effective interaction between human and artificial intelligence (AI) system. For a long time, facial expression-based methods have been actively studied, and they are showing high recognition performance thanks to powerful deep learning recently. On the other hand, the images of the datasets used in the conventional emotion recognition studies are usually short in length and often generated through intentional expression. Also, continuous domain annotation of emotional labels in dataset configuration requires high cost. In order to overcome such problems, this paper proposes an emotion recognition method based on semi-supervised learning that utilizes an appropriate amount of unlabeled dataset in parallel while minimizing the use of labeled dataset requiring high training cost. The proposed emotion recognition method is based on CNN-LSTM-based regressor for regressing arousal and valence in continuous domain. In addition, we present scenarios and design criteria in which semi-supervised learning can be effectively applied to emotion recognition tasks through experiments using well-known MAHNOB-HCI and AFEW-VA datasets.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {28169–28187},
numpages = {19},
keywords = {Emotion recognition, Semi-supervised learning, Convolutional neural network, Long shot-term memory}
}

@inproceedings{10.1007/978-3-319-29817-7_11,
author = {Saydali, Sajad and Parvin, Hamid and Safaei, Ali A.},
title = {Classifier Ensemble by Semi-supervised Learning: Local Aggregation Methodology},
year = {2015},
isbn = {9783319298160},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29817-7_11},
doi = {10.1007/978-3-319-29817-7_11},
abstract = {A novel approach for automatic mine detection using SONAR data is proposed in this paper relying on a probabilistic based fusion method to classify SONAR instances as mine or mine-like object. The proposed semi-supervised algorithm minimizes some target functions, which fuse context identification, multi-algorithm fusion criteria and a semi-supervised learning term. Our optimization purpose is to learn contexts as compact clusters in subspaces of the high-dimensional feature space through probabilistic feature discrimination and semi-supervised learning. The semi-supervised clustering component appoints degree of typicality to each data sample in order to identify and reduce the influence of noise points and outliers. Then, the approach yields optimal fusion parameters for each context. The experiments on synthetic datasets and standard SONAR dataset illustrate that our semi-supervised local fusion outperforms individual classifiers and unsupervised local fusion.},
booktitle = {Revised Selected Papers of the 10th International Doctoral Workshop on Mathematical and Engineering Methods in Computer Science - Volume 9548},
pages = {119–132},
numpages = {14},
keywords = {Classifier fusion, Ensemble learning, Supervised learning},
location = {Tel\u{a}\'{z}, Czech Republic},
series = {MEMICS 2015}
}

@article{10.1016/j.asoc.2017.06.017,
author = {Hang, Wenlong and Choi, Kup-Sze and Wang, Shitong and Qian, Pengjiang},
title = {Semi-supervised learning using hidden feature augmentation},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.06.017},
doi = {10.1016/j.asoc.2017.06.017},
abstract = {The novel feature augmentation method, which utilizes the hidden features, the raw features, and zero vectors, is proposed.The novel hidden feature transformation model is proposed based on the maximum joint probability principle.With hinge loss function and least square loss function, two semi-supervised classification formulations are proposed. Semi-supervised learning methods are conventionally conducted by simultaneously utilizing abundant unlabeled samples and a few labeled samples given. However, the unlabeled samples are usually adopted with assumptions, e.g., cluster and manifold assumptions, which degrade the performance when the assumptions become invalid. The reliable hidden features embedded in both the labeled and the unlabeled samples can potentially be used to tackle this issue. In this regard, we investigate the feature augmentation technique to improve the robustness of semi-supervised learning in this paper. By introducing an orthonormal projection matrix, we first transform both the unlabeled and labeled samples into a shared hidden subspace to determine the connections between the samples. Then we utilize the hidden features, the raw features, and zero vectors determined to develop a novel feature augmentation strategy. Finally, a hidden feature transformation (HTF) model is proposed to compute the desired projection matrix by applying the maximum joint probability distribution principle in the augmented feature space. The effectiveness of the proposed method is evaluated in terms of the hinge and square loss functions respectively, based on two types of semi-supervised classification formulations developed using only the labeled samples with their original features and hidden features. The experimental results have demonstrated the effectiveness of the proposed feature augmentation technique for semi-supervised learning.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {448–461},
numpages = {14},
keywords = {Cluster assumption, Hidden features, Joint probability distribution, Manifold assumption, Semi-supervised learning}
}

@inproceedings{10.1145/3313831.3376488,
author = {Cryan, Jenna and Tang, Shiliang and Zhang, Xinyi and Metzger, Miriam and Zheng, Haitao and Zhao, Ben Y.},
title = {Detecting Gender Stereotypes: Lexicon vs. Supervised Learning Methods},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376488},
doi = {10.1145/3313831.3376488},
abstract = {Biases in language influence how we interact with each other and society at large. Language affirming gender stereotypes is often observed in various contexts today, from recommendation letters and Wikipedia entries to fiction novels and movie dialogue. Yet to date, there is little agreement on the methodology to quantify gender stereotypes in natural language (specifically the English language). Common methodology (including those adopted by companies tasked with detecting gender bias) rely on a lexicon approach largely based on the original BSRI study from 1974.In this paper, we reexamine the role of gender stereotype detection in the context of modern tools, by comparatively analyzing efficacy of lexicon-based approaches and end-to-end, ML-based approaches prevalent in state-of-the-art natural language processing systems. Our efforts using a large dataset show that even compared to an updated lexicon-based approach, end-to-end classification approaches are significantly more robust and accurate, even when trained by moderately sized corpora.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {gender bias, gender stereotypes, lexicon, machine learning, natural language processing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3386263.3407599,
author = {Zhang, Jiliang and Li, Chen and Ye, Jing and Qu, Gang},
title = {Privacy Threats and Protection in Machine Learning},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3407599},
doi = {10.1145/3386263.3407599},
abstract = {With the improvement of computing power and storage level, Machine Learning (ML), especially Deep Learning (DL), has shown its capabilities beyond humans in areas such as image recognition, speech processing, and content recommendation. However, the data collected to build ML models often contains sensitive information, and models may have high commercial value. Compared with the security problem of model prediction errors caused by malicious external influences, privacy threats have not attracted widespread attention, and they have characteristics that are difficult to define and detect. This article reviews recent research progress on ML privacy. First, the privacy threats on data and models in different scenarios are described in detail. Then, typical privacy protection methods are introduced. Finally, the limitations and future development trends of ML privacy research are discussed.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {531–536},
numpages = {6},
keywords = {machine learning, privacy protection, privacy threats},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@book{10.5555/3278343,
author = {Ghatak, Abhijit},
title = {Machine Learning with R},
year = {2017},
isbn = {9789811068072},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book helps readers understand the mathematics of machine learning, and apply them in different situations. It is divided into two basic parts, the first of which introduces readers to the theory of linear algebra, probability, and data distributions and its applications to machine learning. It also includes a detailed introduction to the concepts and constraints of machine learning and what is involved in designing a learning algorithm. This part helps readers understand the mathematical and statistical aspects of machine learning. In turn, the second part discusses the algorithms used in supervised and unsupervised learning. It works out each learning algorithm mathematically and encodes it in R to produce customized learning applications. In the process, it touches upon the specifics of each algorithm and the science behind its formulation. The book includes a wealth of worked-out examples along with R codes. It explains the code for each algorithm, and readers can modify the code to suit their own needs. The book will be of interest to all researchers who intend to use R for machine learning, and those who are interested in the practical aspects of implementing learning algorithms for data analysis. Further, it will be particularly useful and informative for anyone who has struggled to relate the concepts of mathematics and statistics to machine learning.}
}

@article{10.1016/j.compbiomed.2021.104838,
author = {Khandakar, Amith and Chowdhury, Muhammad E.H. and Ibne Reaz, Mamun Bin and Md Ali, Sawal Hamid and Hasan, Md Anwarul and Kiranyaz, Serkan and Rahman, Tawsifur and Alfkey, Rashad and Bakar, Ahmad Ashrif A. and Malik, Rayaz A.},
title = {A machine learning model for early detection of diabetic foot using thermogram images},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104838},
doi = {10.1016/j.compbiomed.2021.104838},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {10},
keywords = {Thermogram, Diabetes mellitus, Diabetic foot, Convolutional neural network, Machine learning algorithms, Image enhancement techniques, Diagnostic utility}
}

@inproceedings{10.1007/978-3-030-58820-5_41,
author = {Perri, Damiano and Simonetti, Marco and Lombardi, Andrea and Faginas-Lago, Noelia and Gervasi, Osvaldo},
title = {Binary Classification of Proteins by a Machine Learning Approach},
year = {2020},
isbn = {978-3-030-58819-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58820-5_41},
doi = {10.1007/978-3-030-58820-5_41},
abstract = {In this work we present a system based on a Deep Learning approach, by using a Convolutional Neural Network, capable of classifying protein chains of amino acids based on the protein description contained in the Protein Data Bank. Each protein is fully described in its chemical-physical-geometric properties in a file in XML format. The aim of the work is to design a prototypical Deep Learning machinery for the collection and management of a huge amount of data and to validate it through its application to the classification of a sequences of amino acids. We envisage applying the described approach to more general classification problems in biomolecules, related to structural properties and similarities.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VII},
pages = {549–558},
numpages = {10},
keywords = {Machine Learning, Computational chemistry, Protein Data Bank},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-030-64583-0_44,
author = {Carta, Salvatore and Recupero, Diego Reforgiato and Saia, Roberto and Stanciu, Maria Madalina},
title = {A General Approach for Risk Controlled Trading Based on Machine Learning and Statistical Arbitrage},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_44},
doi = {10.1007/978-3-030-64583-0_44},
abstract = {Nowadays, machine learning usage has gained significant interest in financial time series prediction, hence being a promise land for financial applications such as algorithmic trading. In this setting, this paper proposes a general approach based on an ensemble of regression algorithms and dynamic asset selection applied to the well-known statistical arbitrage trading strategy. Several extremely heterogeneous state-of-the-art machine learning algorithms, exploiting different feature selection processes in input, are used as base components of the ensemble, which is in charge to forecast the return of each of the considered stocks. Before being used as an input to the arbitrage mechanism, the final ranking of the assets takes also into account a quality assurance mechanism that prunes the stocks with poor forecasting accuracy in the previous periods. The approach has a general application for any risk balanced trading strategy aiming to exploit different financial assets. It was evaluated implementing an intra-day trading statistical arbitrage on the stocks of the S&amp;P500 index. Our approach outperforms each single base regressor we adopted, which we considered as baselines. More important, it also outperforms Buy-and-hold of S&amp;P500 Index, both during financial turmoil such as the global financial crisis, and also during the massive market growth in the recent years.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {489–503},
numpages = {15},
keywords = {Stock market forecast, Machine learning, Statistical arbitrage, Ensemble learning},
location = {Siena, Italy}
}

@article{10.1109/TASLP.2021.3133189,
author = {Wu, Haibin and Li, Xu and Liu, Andy T. and Wu, Zhiyong and Meng, Helen and Lee, Hung-Yi},
title = {Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3133189},
doi = {10.1109/TASLP.2021.3133189},
abstract = {Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. The purification module aims at alleviating the adversarial perturbations in the samples and pulling the contaminated adversarial inputs back towards the decision boundary. Experimental results show that our proposed purification module effectively counters adversarial attacks and outperforms traditional filters from both alleviating the adversarial noise and maintaining the performance of genuine samples. The detection module aims at detecting adversarial samples from genuine ones based on the statistical properties of ASV scores derived by a unique ASV integrating with different number of SSLMs. Experimental results show that our detection module helps shield the ASV by detecting adversarial samples. Both purification and detection methods are helpful for defending against different kinds of attack algorithms. Moreover, since there is no common metric for evaluating the ASV performance under adversarial attacks, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {202–217},
numpages = {16}
}

@inproceedings{10.1145/3394486.3406697,
author = {Umamahesan, Aniththa and Babu, Deepak Mukunthu Iyappan},
title = {From Zero to AI Hero with Automated Machine Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406697},
doi = {10.1145/3394486.3406697},
abstract = {Automated ML is an emerging field in Machine Learning that helps developers and new data scientists with little data science knowledge build Machine Learning models and solutions without understanding the complexity of Learning Algorithm selection, and Hyper parameter tuning. With Azure Machine Learning's automated machine learning capability, given a dataset and a few configuration parameters, you will get a trained high quality machine learning model for the dataset that you can use for predictions. In this session, you will learn how to use Automated ML for productivity gains, empowering domain experts to build ML based solutions and scale to build several models with Azure Machine Learning's Automated ML.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3495},
numpages = {1},
keywords = {ai, automl, azure machine learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3473856.3474027,
author = {Foerste, Markus and Nadj, Mario and Knaeble, Merlin and Maedche, Alexander and Gehrmann, Leonie and Stahl, Florian},
title = {An Interactive Machine Learning System for Image Advertisements},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3474027},
doi = {10.1145/3473856.3474027},
abstract = {Advertising is omnipresent in all countries around the world and has a strong influence on consumer behavior. Given that advertisements aim to be memorable, attract attention and convey the intended information in a limited space, it seems striking that previous research in economics and management has mostly neglected the content and style of actual advertisements and their evolution over time. With this in mind, we collected more than one million print advertisements from the English-language weekly news magazine “The Economist” from 1843 to 2014. However, there is a lack of interactive intelligent systems capable of processing such a vast amount of image data and allowing users to automatically and manually add metadata, explore images, find and test assertions, and use machine learning techniques they did not have access to before. Inspired by the research field of interactive machine learning, we propose such a system that enables domain experts like marketing scholars to process and analyze this huge collection of image advertisements.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {574–577},
numpages = {4},
keywords = {advertising, image ads, interactive machine learning},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1007/978-3-030-86993-9_44,
author = {Nahar, Nazmun and Ara, Ferdous and Neloy, Md. Arif Istiek and Biswas, Anik and Hossain, Mohammad Shahadat and Andersson, Karl},
title = {Feature Selection Based Machine Learning to Improve Prediction of&nbsp;Parkinson Disease},
year = {2021},
isbn = {978-3-030-86992-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86993-9_44},
doi = {10.1007/978-3-030-86993-9_44},
abstract = {Parkinson’s disease (PD) is a kind of neurodegenerative disorder characterized by the loss of dopamine-producing cells in the brain. The disruption of brain cells that create dopamine, a chemical that allows brain cells to connect with one another, causes Parkinson’s disease. Control, adaptability, and rapidity of movement are all controlled by dopamine-producing cells in the brain. Researchers have been investigating for techniques to identify non-motor symptoms that show early in the disease as soon as possible, slowing the disease’s progression. A machine learning-based detection of Parkinson’s disease is proposed in this research. Feature selection and classification techniques are used in the proposed detection technique. Boruta, Recursive Feature Elimination (RFE) and Random Forest (RF) Classifier have been used for the feature selection process. Four classification algorithms are considered to detect Parkinson disease which are gradient boosting, extreme gradient boosting, bagging and Extra Tree Classifier. Bagging with recursive feature elimination was found to outperform the other methods. The lowest number of voice characteristics for the diagnosis in Parkinson attained 82.35% accuracy.},
booktitle = {Brain Informatics: 14th International Conference, BI 2021, Virtual Event, September 17–19, 2021, Proceedings},
pages = {496–508},
numpages = {13},
keywords = {Parkinson disease, Boruta, RFE, RF, Feature selection}
}

@inproceedings{10.1007/978-3-031-21671-8_11,
author = {Rahgooy, Taher and Venable, K. Brent and Trueblood, Jennifer S.},
title = {Integrating Machine Learning and Cognitive Modeling of Decision Making},
year = {2021},
isbn = {978-3-031-21670-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21671-8_11},
doi = {10.1007/978-3-031-21671-8_11},
abstract = {Modeling human decision making plays a fundamental role in the design of intelligent systems capable of rich interactions and effective teamwork. In this paper we consider the task of choice prediction in settings with multiple alternatives. Cognitive models of decision making can successfully replicate and explain behavioral effects involving uncertainty and interactions among alternatives but are computationally intensive to train. ML approaches excel in terms of choice prediction accuracy, but fail to provide insights on the underlying preference reasoning. We study different degrees of integration of ML and cognitive models for this task. We show, via testing on behavioral data, that our hybrid approach, based on the integration of a neural network and the Multi-alternative Linear Ballistic Accumulator cognitive model, requires significantly less time to train, and allows to capture important cognitive parameters while maintaining similar accuracy to the pure ML approach.},
booktitle = {Computational Theory of Mind for Human-Machine Teams: First International Symposium, ToM for Teams 2021, Virtual Event, November 4–6, 2021, Revised Selected Papers},
pages = {173–193},
numpages = {21},
keywords = {Cognitive models, Decision making, Machine learning, Preferential choice prediction, Artificial neural networks, Behavioral effects}
}

@inproceedings{10.1145/3313831.3376275,
author = {Dove, Graham and Fayard, Anne-Laure},
title = {Monsters, Metaphors, and Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376275},
doi = {10.1145/3313831.3376275},
abstract = {Machine learning (ML) poses complex challenges for user experience (UX) designers. Typically unpredictable and opaque, it may produce unforeseen outcomes detrimental to particular groups or individuals, yet simultaneously promise amazing breakthroughs in areas as diverse as medical diagnosis and universal translation. This results in a polarized view of ML, which is often manifested through a technology-as-monster metaphor. In this paper, we acknowledge the power and potential of this metaphor by resurfacing historic complexities in human-monster relations. We (re)introduce these liminal and ambiguous creatures, and discuss their relation to ML. We offer a background to designers' use of metaphor, and show how the technology-as-monster metaphor can generatively probe and (re)frame the questions ML poses. We illustrate the effectiveness of this approach through a detailed discussion of an early-stage generative design workshop inquiring into ML approaches to supporting student mental health and well-being.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {generative metaphor, machine learning, monster theory, ux design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1007/s00521-019-04547-5,
author = {Yadav, Anjali and Singh, Anushikha and Dutta, Malay Kishore and Travieso, Carlos M.},
title = {Machine learning-based classification of cardiac diseases from PCG recorded heart sounds},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04547-5},
doi = {10.1007/s00521-019-04547-5},
abstract = {Cardiovascular diseases are one of the most fatal diseases across the globe. Clinically, conventional stethoscope is used to check the medical condition of a human heart. Only a trained medical professional can understand and interpret the heart auscultations clinically. This paper presents a machine learning-based automatic classification system based on heart sounds to diagnose cardiac disorders. The proposed framework involves strategic processing and framing of heart sound to extract discriminatory features for machine learning. The most prominent features are selected and used to train a supervised classifier for automatic detection of cardiac diseases. The biological abnormalities disturbing the physical functioning of the heart cause variations in the auscultations, which is strategically used in terms of some discriminatory features for machine learning-based automatic classification. The proposed method achieved 97.78% accuracy with the equal error rate of 2.22% for abnormal and normal heart sound classification. The experimental results exhibit that the performance of the proposed method in proper diagnosis of the cardiac diseases is high in terms of accuracy and has low error rate which makes the proposed algorithm suitable for real-time applications.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {17843–17856},
numpages = {14},
keywords = {Body auscultation, Cardiac disease, Feature extraction, p value, Machine learning, Automatic classification}
}

@article{10.1145/3474121,
author = {Slijepcevic, Djordje and Horst, Fabian and Lapuschkin, Sebastian and Horsak, Brian and Raberger, Anna-Maria and Kranzl, Andreas and Samek, Wojciech and Breiteneder, Christian and Sch\"{o}llhorn, Wolfgang Immanuel and Zeppelzauer, Matthias},
title = {Explaining Machine Learning Models for Clinical Gait Analysis},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3474121},
doi = {10.1145/3474121},
abstract = {Machine Learning (ML) is increasingly used to support decision-making in the healthcare sector. While ML approaches provide promising results with regard to their classification performance, most share a central limitation, their black-box character. This article investigates the usefulness of Explainable Artificial Intelligence (XAI) methods to increase transparency in automated clinical gait classification based on time series. For this purpose, predictions of state-of-the-art classification methods are explained with a XAI method called Layer-wise Relevance Propagation (LRP). Our main contribution is an approach that explains class-specific characteristics learned by ML models that are trained for gait classification. We investigate several gait classification tasks and employ different classification methods, i.e.,&nbsp;Convolutional Neural Network, Support Vector Machine, and Multi-layer Perceptron. We propose to evaluate the obtained explanations with two complementary approaches: a statistical analysis of the underlying data using Statistical Parametric Mapping and a qualitative evaluation by two clinical experts. A gait dataset comprising ground reaction force measurements from 132 patients with different lower-body gait disorders and 62 healthy controls is utilized. Our experiments show that explanations obtained by LRP exhibit promising statistical properties concerning inter-class discriminativity and are also in line with clinically relevant biomechanical gait characteristics.},
journal = {ACM Trans. Comput. Healthcare},
month = dec,
articleno = {14},
numpages = {27},
keywords = {Explainable artificial intelligence, clinical gait analysis, human gait classification, layer-wise relevance propagation, statistical parametric mapping, ground reaction forces, convolutional neural networks}
}

@article{10.1016/j.procs.2021.08.057,
author = {Mosqueira-Rey, Eduardo and Alonso-R\'{\i}os, David and Baamonde-Lozano, Andr\'{e}s},
title = {Integrating Iterative Machine Teaching and Active Learning into the Machine Learning Loop},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.08.057},
doi = {10.1016/j.procs.2021.08.057},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {553–562},
numpages = {10},
keywords = {Iterative Machine Teaching, Active Learning, Machine Learning, Human-in-the-Loop Machine Learning}
}

@article{10.1504/ijahuc.2021.119091,
author = {Kavadi, Durga Prasad and Al-Turjman, Fadi and Reddy, K. Adi Narayana and Patan, Rizwan},
title = {A machine learning approach for celebrity profiling},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {38},
number = {1–3},
issn = {1743-8225},
url = {https://doi.org/10.1504/ijahuc.2021.119091},
doi = {10.1504/ijahuc.2021.119091},
abstract = {The celebrity profiling is used to predict the sub-profiles like gender, fame, birth-year and occupation of a celebrity for a given textual content. The task of celebrity profiling is introduced in PAN Competition 2019. Most of the researchers in the competition have shown interest on stylistic features to differentiate the writing styles of the celebrities. In this work, a sub-profile based weighted approach is proposed to improve the accuracy of celebrity profiling. In this approach, most frequent terms are used to compute the document weight. The document weights were used to represent the document vectors instead of weights of features. The document vectors forwarded to machine learning algorithms to build the training model. The proposed method achieved competitive accuracies of 77.13% for gender prediction, 87.76% for fame prediction and 91.54% for occupation prediction. The accuracies of the proposed approach for sub-profiles prediction outperform several existing approaches for celebrity profiling.},
journal = {Int. J. Ad Hoc Ubiquitous Comput.},
month = jan,
pages = {111–126},
numpages = {15},
keywords = {celebrity profiling, author profiling, gender prediction, fame prediction, occupation prediction, machine learning algorithms, accuracy}
}

@inproceedings{10.1007/978-3-030-59851-8_18,
author = {Ozer, Gence and Netti, Alessio and Tafani, Daniele and Schulz, Martin},
title = {Characterizing HPC Performance Variation with Monitoring and Unsupervised Learning},
year = {2020},
isbn = {978-3-030-59850-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59851-8_18},
doi = {10.1007/978-3-030-59851-8_18},
abstract = {As HPC systems grow larger and more complex, characterizing the relationships between their different components and gaining insight on their behavior becomes difficult. In turn, this puts a burden on both system administrators and developers who aim at improving the efficiency and reliability of systems, algorithms and applications. Automated approaches capable of extracting a system’s behavior, as well as identifying anomalies and outliers, are necessary more than ever.In this work we discuss our exploratory study of Bayesian Gaussian mixture models, an unsupervised machine learning technique, to characterize the performance of an HPC system’s components, as well as to identify anomalies, based on sensor data. We propose an algorithmic framework for this purpose, implement it within the DCDB monitoring and operational data analytics system, and present several case studies carried out using data from a production HPC system.},
booktitle = {High Performance Computing: ISC High Performance 2020 International Workshops, Frankfurt, Germany, June 21–25, 2020, Revised Selected Papers},
pages = {280–292},
numpages = {13},
keywords = {HPC systems, Monitoring, Operational data analytics, Clustering, Anomaly detection},
location = {Frankfurt am Main, Germany}
}

@inproceedings{10.1145/3433210.3437513,
author = {Li, Jiangnan and Yang, Yingyuan and Sun, Jinyuan Stella and Tomsovic, Kevin and Qi, Hairong},
title = {ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3437513},
doi = {10.1145/3433210.3437513},
abstract = {Recent research demonstrated that the superficially well-trained machine learning (ML) models are highly vulnerable to adversarial examples. As ML techniques are becoming a popular solution for cyber-physical systems (CPSs) applications in research literatures, the security of these applications is of concern. However, current studies on adversarial machine learning (AML) mainly focus on pure cyberspace domains. The risks the adversarial examples can bring to the CPS applications have not been well investigated. In particular, due to the distributed property of data sources and the inherent physical constraints imposed by CPSs, the widely-used threat models and the state-of-the-art AML algorithms in previous cyberspace research become infeasible.We study the potential vulnerabilities of ML applied in CPSs by proposing Constrained Adversarial Machine Learning (ConAML), which generates adversarial examples that satisfy the intrinsic constraints of the physical systems. We first summarize the difference between AML in CPSs and AML in existing cyberspace systems and propose a general threat model for ConAML. We then design a best-effort search algorithm to iteratively generate adversarial examples with linear physical constraints. We evaluate our algorithms with simulations of two typical CPSs, the power grids and the water treatment system. The results show that our ConAML algorithms can effectively generate adversarial examples which significantly decrease the performance of the ML models even under practical constraints.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {52–66},
numpages = {15},
keywords = {adversarial machine learning, cyber-physical system, intrusion detection},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1007/978-3-031-21517-9_12,
author = {Sujatha, G. and Sankareswari, K.},
title = {A Comparative Study on Machine Learning Based Classifier Model for Wheat Seed Classification},
year = {2021},
isbn = {978-3-031-21516-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21517-9_12},
doi = {10.1007/978-3-031-21517-9_12},
abstract = {Seed classification is a process of categorizing different varieties of seeds into different classes on the basis of their morphological features. Seed identification is further complicated due to common object recognition constraints such as light, pose and orientation. Wheat has always been one of the globally common consumed foods in India. A large number of wheat varieties have been cultivated, exported and imported all around the world. Enormous studies have been done on identifying crop diseases and classifying the crop types. In the present work, wheat seed classification is performed to distinguish the three different Indian wheat varieties by their collected morphological features and applied machine learning models to develop wheat variety classification system. The seed features used here are length of kernel, compactness, asymmetry coefficient, width of kernel, length of kernel groove, area and perimeter. The present work carried out with different classifiers such as Decision Tree, Random Forest, Neural Net, Nearest Neighbors, Gaussian Process, AdaBoost, Naive Bayes, Support Vector Machine(SVM) Linear, SVM RBF(SVM with the Radial Basis Function) and SVM Sigmoid with 2&nbsp;K-fold cross validation. Also obtained the results using 5 fold and 10-fold Cross Validation.},
booktitle = {Mining Intelligence and Knowledge Exploration: 9th International Conference, MIKE 2021, Hammamet, Tunisia, November 1–3, 2021, Proceedings},
pages = {120–127},
numpages = {8},
keywords = {Agriculture, Seed classification, SVM, Neural net, Gaussian, K-fold cross validation, Random forest, Decision tree, Classifiers},
location = {Hammamet, Tunisia}
}

@article{10.1007/s00521-020-05462-w,
author = {Dornaika, Fadi},
title = {Flexible data representation with graph convolution for semi-supervised learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05462-w},
doi = {10.1007/s00521-020-05462-w},
abstract = {This paper introduces a scheme for semi-supervised data representation. It proposes a flexible nonlinear embedding model that imitates the principle of spectral graph convolutions. Structured data are exploited in order to determine nonlinear and linear models. The introduced scheme takes advantage of data graphs at two different levels. First, it incorporates manifold regularization that is naturally encoded by the graph itself. Second, the regression model is built on the convolved data samples that are obtained by the joint use of the data and their associated graph. The proposed semi-supervised embedding can tackle challenges related to over-fitting in image data spaces. The proposed graph convolution-based semi-supervised embedding paves the way to new theoretical and application perspectives related to the nonlinear embedding. Indeed, building flexible models that adopt convolved data samples can enhance both the data representation and the final performance of the learning system. Several experiments are conducted on six image datasets for comparing the introduced scheme with many state-of-art semi-supervised approaches. These experimental results show the effectiveness of the introduced data representation scheme.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6851–6863},
numpages = {13},
keywords = {Graph-based embedding, Semi-supervised learning, Graph convolutions, Discriminant embedding, Pattern recognition}
}

@inproceedings{10.1145/3373509.3373558,
author = {Zhao, Xuan and Li, Yali and Wang, Shengjin},
title = {Face Quality Assessment via Semi-supervised Learning},
year = {2020},
isbn = {9781450376570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373509.3373558},
doi = {10.1145/3373509.3373558},
abstract = {Face quality assessment, used for selecting a "good" subset from face images captured over multiple frames in uncontrolled conditions, plays a significant role in video-based face recognition. By removing the poor quality images, it can not only improve recognition performance but also reduce the computation cost. This paper proposes an end-to-end face quality assessment algorithm based on a semi-supervised learning framework. The contributions of the proposed method are threefold. (i) Making use of unlabeled data from target domain to fine-tune a neural network by a strategy of automatically updating labels. (ii) Combining prior knowledge with feature learning by using a set of characteristics as binary constraints. (iii) Proposing a light neural network model for training and predicting. Experiments demonstrate that our model can get much higher accuracy in face quality assessment task than the models trained with the same amount of labeled faces, meanwhile the complexity is lower. Experimental results also show that our method can improve the performance of face recognition by face selection.},
booktitle = {Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition},
pages = {288–293},
numpages = {6},
keywords = {Face quality assessment, light CNN, prior knowledge, semi-supervised learning},
location = {Beijing, China},
series = {ICCPR '19}
}

@article{10.1016/j.compag.2021.106423,
author = {Hu, Chengsong and Thomasson, J. Alex and Bagavathiannan, Muthukumar V.},
title = {A powerful image synthesis and semi-supervised learning pipeline for site-specific weed detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106423},
doi = {10.1016/j.compag.2021.106423},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {9},
keywords = {Convolutional neural networks, Object detection, Precision agriculture, Digital weeds, Deep learning}
}

@inproceedings{10.1109/ISLPED52811.2021.9502472,
author = {Marculescu, Diana},
title = {When climate meets machine learning: edge to cloud ML energy efficiency},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISLPED52811.2021.9502472},
doi = {10.1109/ISLPED52811.2021.9502472},
abstract = {A large portion of current cloud and edge workloads feature Machine Learning (ML) tasks, thereby requiring a deep understanding of their energy efficiency. While the holy grail for judging the quality of a ML model has largely been testing accuracy, and only recently its resource usage, neither of these metrics translate directly to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers the need for building accurate, platform-specific power and latency models for ML and efficient hardware-aware ML design methodologies, thus allowing machine learners and hardware designers to identify not just the best accuracy ML model configuration, but also those that satisfy given hardware constraints.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {37},
numpages = {1},
keywords = {hardware-aware ML, model compression, neural architecture search, quantization},
location = {Boston, Massachusetts},
series = {ISLPED '21}
}

@phdthesis{10.5555/AAI28768362,
author = {Hilgard, Anna Sophia and E., Glassman, and N., Rosenfeld, and S, Mullainathan,},
advisor = {David, Parkes,},
title = {Machine Learning for Humans: Building Models that Adapt to Behavior},
year = {2021},
isbn = {9798496526142},
publisher = {Harvard University},
address = {USA},
abstract = {As machine learning continues to exhibit remarkable performance across a wide range of experimental tasks, there is an increasing enthusiasm to deploy these models in the real world. However, the traditional supervised learning framework optimizes performance without consideration to the use of these models by humans. In nearly all applications, human interaction affects the generation of input data, outcomes, or both. For example, a doctor may choose to either incorporate or override a machine-generated medical risk score. This judgment influences outcomes and invalidates the predicted level of performance in isolation. In the case of movie recommendation, digital records of human viewing behavior are guided by a recommendation engine, such that the distribution of input data is a function of the recommender itself. Human behavior is dynamic and responsive, and failing to account for this leads to suboptimal and even harmful results when machine learning models trained in isolation begin to interact with human stakeholders. In this thesis, I consider humans in three different roles relative to the machine learning system: humans as model users, humans as model subjects, and humans as model auditors. For the first two configurations, I develop new frameworks that are capable of considering and adapting to relevant human behavior. For the last configuration, I reveal an important vulnerability in popular tools intended to assist human auditors. Specifically, when humans are model users, I design a new model architecture and training procedure that allows machine learning decision aids to directly adapt to how humans use them, optimizing for performance of the entire machine-human pipeline rather than solely machine accuracy. This system is validated in experiments with real human users, confirming its ability to adapt productively to different human behaviors. For humans as model subjects, I introduce a new form of model regularization that considers the motivations of to adopt new behaviors when regarding predictive models as accurate proxies for causal phenomena. This look-ahead regularizer balances model accuracy against ensuring that behavior change motivated in users results in positive outcomes with high probability. Finally, I construct an adversarial model capable of causing popular explainability tools to lead human auditors to incorrect inferences about model behavior. I show that on a variety of real world datasets, predictive models can exhibit discriminatory behavior (e.g. racial or gender disparity of outcomes) while passing proposed tests for such behavior.},
note = {AAI28768362}
}

@article{10.1007/s10270-020-00825-2,
author = {Kawamoto, Yusuke},
title = {An epistemic approach to the formal specification of statistical machine learning},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00825-2},
doi = {10.1007/s10270-020-00825-2},
abstract = {We propose an epistemic approach to formalizing statistical properties of machine learning. Specifically, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then, we formalize various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic. In this formalization, we show relationships among properties of classifiers, and relevance between classification performance and robustness. As far as we know, this is the first work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {293–310},
numpages = {18},
keywords = {Modal logic, Possible world semantics, Machine learning, Classification performance, Robustness, Fairness}
}

@inproceedings{10.1145/3211346.3211349,
author = {Dolby, Julian and Shinnar, Avraham and Allain, Allison and Reinen, Jenna},
title = {Ariadne: analysis for machine learning programs},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211349},
doi = {10.1145/3211346.3211349},
abstract = {Machine learning has transformed domains like vision and translation, and is now increasingly used in science, where the correctness of such code is vital. Python is popular for machine learning, in part because of its wealth of machine learning libraries, and is felt to make development faster; however, this dynamic language has less support for error detection at code creation time than tools like Eclipse. This is especially problematic for machine learning: given its statistical nature, code with subtle errors may run and produce results that look plausible but are meaningless. This can vitiate scientific results. We report on : applying a static framework, WALA, to machine learning code that uses TensorFlow. We have created static analysis for Python, a type system for tracking tensors—Tensorflow’s core data structures—and a data flow analysis to track their usage. We report on how it was built and present some early results.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {1–10},
numpages = {10},
keywords = {machine learning, program analysis},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@article{10.1007/s11390-019-1954-4,
author = {Chen, Wei and Zhou, Jia-Hong and Zhu, Jia-Xin and Wu, Guo-Quan and Wei, Jun},
title = {Semi-Supervised Learning Based Tag Recommendation for Docker Repositories},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1954-4},
doi = {10.1007/s11390-019-1954-4},
abstract = {Docker has been the mainstream technology of providing reusable software artifacts recently. Developers can easily build and deploy their applications using Docker. Currently, a large number of reusable Docker images are publicly shared in online communities, and semantic tags can be created to help developers effectively reuse the images. However, the communities do not provide tagging services, and manually tagging is exhausting and time-consuming. This paper addresses the problem through a semi-supervised learning-based approach, named SemiTagRec. SemiTagRec contains four components: (1) the predictor, which calculates the probability of assigning a specific tag to a given Docker repository; (2) the extender, which introduces new tags as the candidates based on tag correlation analysis; (3) the evaluator, which measures the candidate tags based on a logistic regression model; (4) the integrator, which calculates a final score by combining the results of the predictor and the evaluator, and then assigns the tags with high scores to the given Docker repositories. SemiTagRec includes the newly tagged repositories into the training data for the next round of training. In this way, SemiTagRec iteratively trains the predictor with the cumulative tagged repositories and the extended tag vocabulary, to achieve a high accuracy of tag recommendation. Finally, the experimental results show that SemiTagRec outperforms the other approaches and SemiTagRec’s accuracy, in terms of Recall@5 and Recall@10, is 0.688 and 0.781 respectively.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {957–971},
numpages = {15},
keywords = {tag recommendation, Docker repository, Dockerfile, semi-supervised learning}
}

@inproceedings{10.1007/978-3-031-08421-8_34,
author = {Gaglio, Salvatore and Giammanco, Andrea and Lo Re, Giuseppe and Morana, Marco},
title = {Adversarial Machine Learning in&nbsp;e-Health: Attacking a&nbsp;Smart Prescription System},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_34},
doi = {10.1007/978-3-031-08421-8_34},
abstract = {Machine learning (ML) algorithms are the basis of many services we rely on in our everyday life. For this reason, a new research line has recently emerged with the aim of investigating how ML can be misled by adversarial examples. In this paper we address an e-health scenario in which an automatic system for prescriptions can be deceived by inputs forged to subvert the model’s prediction. In particular, we present an algorithm capable of generating a precise sequence of moves that the adversary has to take in order to elude the automatic prescription service. Experimental analyses performed on a real dataset of patients’ clinical records show that a minimal alteration of the clinical records can subvert predictions with high probability.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {490–502},
numpages = {13},
keywords = {Adversarial Machine Learning, Healthcare, Evasion attacks}
}

@article{10.1016/j.eswa.2021.114774,
author = {Rasheed, Fareeha and Wahid, Abdul},
title = {Learning style detection in E-learning systems using machine learning techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114774},
doi = {10.1016/j.eswa.2021.114774},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Machine learning, Classification, Learning style, E-learning}
}

@article{10.1007/s11042-020-09079-y,
author = {Umer, Saiyed and Mohanta, Partha Pratim and Rout, Ranjeet Kumar and Pandey, Hari Mohan},
title = {Machine learning method for cosmetic product recognition: a visual searching approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {28–29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09079-y},
doi = {10.1007/s11042-020-09079-y},
abstract = {A cosmetic product recognition system is proposed in this paper. For this recognition system, we have proposed a cosmetic product database that contains image samples of forty different cosmetic items. The purpose of this recognition system is to recognize Cosmetic products with there types, brands and retailers such that to analyze a customer experience what kind of products and brands they need. This system has various applications in such as brand recognition, product recognition and also the availability of the products to the vendors. The implementation of the proposed system is divided into three components: preprocessing, feature extraction and classification. During preprocessing we have scaled and transformed the color images into gray-scaled images to speed up the process. During feature extraction, several different feature representation schemes: transformed, structural and statistical texture analysis approaches have been employed and investigated by employing the global and local feature representation schemes. Various machine learning supervised classification methods such as Logistic Regression, Linear Support Vector Machine, Adaptive k-Nearest Neighbor, Artificial Neural Network and Decision Tree classifiers have been employed to perform the classification tasks. Apart from this, we have also performed some data analytic tasks for Brand Recognition as well as Retailer Recognition and for these experimentation, we have employed some datasets from the ‘Kaggle’ website and have obtained the performance due to the above-mentioned classifiers. Finally, the performance of the cosmetic product recognition system, Brand Recognition and Retailer Recognition have been aggregated for the customer decision process in the form of the state-of-the-art for the proposed system.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {34997–35023},
numpages = {27},
keywords = {Cosmetic products, E-commerce application, Feature extraction, Machine learning, Visual search}
}

@article{10.1007/s11063-017-9724-1,
author = {Kim, Jonghong and Bukhari, Waqas and Lee, Minho},
title = {Feature Analysis of Unsupervised Learning for Multi-task Classification Using Convolutional Neural Network},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-017-9724-1},
doi = {10.1007/s11063-017-9724-1},
abstract = {This study analyzes the characteristics of unsupervised feature learning using a convolutional neural network (CNN) to investigate its efficiency for multi-task classification and compare it to supervised learning features. We keep the conventional CNN structure and introduce modifications into the convolutional auto-encoder design to accommodate a subsampling layer and make a fair comparison. Moreover, we introduce non-maximum suppression and dropout for a better feature extraction and to impose sparsity constraints. The experimental results indicate the effectiveness of our sparsity constraints. We also analyze the efficiency of unsupervised learning features using the t-SNE and variance ratio. The experimental results show that the feature representation obtained in unsupervised learning is more advantageous for multi-task learning than that obtained in supervised learning.},
journal = {Neural Process. Lett.},
month = jun,
pages = {783–797},
numpages = {15},
keywords = {Auto-encoder, Convolutional neural networks, Deep learning, Multi-task learning, Unsupervised learning}
}

@article{10.1145/3447556.3447567,
author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
title = {Techniques for Automated Machine Learning},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3447556.3447567},
doi = {10.1145/3447556.3447567},
abstract = {Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a problem description, its task type, and datasets. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we portray AutoML as a bi-level optimization problem, where one problem is nested within another to search the optimum in the search space, and review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter tuning (AutoMHT), and automated deep learning (AutoDL). Stateof- the-art techniques in the three categories are presented. The iterative solver is proposed to generalize AutoML techniques. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {35–50},
numpages = {16}
}

@article{10.1016/j.compbiomed.2021.104450,
author = {Sharma, Samriti and Singh, Gurvinder and Sharma, Manik},
title = {A comprehensive review and analysis of supervised-learning and soft computing techniques for stress diagnosis in humans},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104450},
doi = {10.1016/j.compbiomed.2021.104450},
journal = {Comput. Biol. Med.},
month = jul,
numpages = {19},
keywords = {Stress, Supervised learning, Soft computing, Nature-inspired methods, Fuzzy logic, Deep learning techniques}
}

@article{10.1007/s00521-020-05109-w,
author = {Jirak, Doreen and Biertimpel, David and Kerzel, Matthias and Wermter, Stefan},
title = {Solving visual object ambiguities when pointing: an unsupervised learning approach},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {7},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05109-w},
doi = {10.1007/s00521-020-05109-w},
abstract = {Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Particularly, pointing gestures are necessary to dissolve ambiguities in a scene and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domestic domains, the development of gesture interfaces complementing human–robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the corresponding object have to be correctly recognized in real time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be captured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects and use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised learning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define the so-called ambiguity classes. Our implementation for the hand and object detection is independent of any markers or skeleton models; thus, it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities resulting from close object proximity.},
journal = {Neural Comput. Appl.},
month = apr,
pages = {2297–2319},
numpages = {23},
keywords = {Pointing gestures, Pointing intention, Object ambiguities, Grow-when-required networks, Human–robot interaction}
}

@inproceedings{10.1145/3425577.3425581,
author = {Chen, Bin and Zhao, Congcong},
title = {Weakly Supervised Learning with Discrimination Mechanism for Object Detection},
year = {2021},
isbn = {9781450388023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425577.3425581},
doi = {10.1145/3425577.3425581},
abstract = {In order to reduce the time consuming and expensive process of manually annotating data, and achieve the purpose of lightweight deployment. In this paper, an object detection method for weakly supervised learning with discrimination mechanism is proposed. We introduce the classification branch and the location branch based on the Darknet-53 backbone network of YOLO model, utilize Global Average Pooling (GAP) and Softmax to complete classification on selected areas, and adopt classification activation map for location. In addition, we use a model compression mechanism for model pruning operations, which reduces the size of the model and achieves the lightweight goal. These can effectively solve the problems of object detection to a certain extent. The results show that the improved model achieves good performance in terms of robustness and stability while maintaining the accuracy and efficiency of object detection, further improving the effectiveness of object detection tasks in practical application scenarios.},
booktitle = {Proceedings of the 3rd International Conference on Control and Computer Vision},
pages = {17–21},
numpages = {5},
keywords = {discrimination mechanism, model compression mechanism, object detection, weakly supervised learning},
location = {Macau, China},
series = {ICCCV '20}
}

@inproceedings{10.1145/3465332.3470875,
author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Shaikh, Aadil and Velikov, Lukas and Zadok, Erez},
title = {A Machine Learning Framework to Improve Storage System Performance},
year = {2021},
isbn = {9781450385503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465332.3470875},
doi = {10.1145/3465332.3470875},
abstract = {Storage systems and their OS components are designed to accommodate a wide variety of applications and dynamic workloads. Storage components inside the OS contain various heuristic algorithms to provide high performance and adaptability for different workloads. These heuristics may be tunable via parameters, and some system calls allow users to optimize their system performance. These parameters are often predetermined based on experiments with limited applications and hardware. Thus, storage systems often run with these predetermined and possibly suboptimal values. Tuning these parameters manually is impractical: one needs an adaptive, intelligent system to handle dynamic and complex workloads. Machine learning (ML) techniques are capable of recognizing patterns, abstracting them, and making predictions on new data. ML can be a key component to optimize and adapt storage systems. In this position paper, we propose KML, an ML framework for storage systems. We implemented a prototype and demonstrated its capabilities on the well-known problem of tuning optimal readahead values. Our results show that KML has a small memory footprint, introduces negligible overhead, and yet enhances throughput by as much as 2.3x.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {94–102},
numpages = {9},
keywords = {machine learning, operating systems, storage performance optimization, storage systems},
location = {Virtual, USA},
series = {HotStorage '21}
}

@inproceedings{10.1007/978-3-030-63924-2_22,
author = {Dang, Quang-Vinh},
title = {Understanding the Decision of Machine Learning Based Intrusion Detection Systems},
year = {2020},
isbn = {978-3-030-63923-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63924-2_22},
doi = {10.1007/978-3-030-63924-2_22},
abstract = {Intrusion Detection Systems (IDSs) is an important research topic in security engineering. The role of an IDS is to detect the malicious incoming network flows, hence it can protect a computer system from attack. Recent research studies in IDS focus in using different machine learning techniques to build an IDS. However, due to the black-box nature of the machine learning algorithms, it is difficult to understand and get insights of the system. In this work, we extend the recent studies by providing the explanation of the decisions of the IDSs built in the previous studies. Given a deeper understanding of the IDS, the users will have more trust to use the system while the engineers can rely on the explanation to tweet the system. The experimental results show that we can significantly reduce the computational power requirement of the IDS based on the explanation of the model.},
booktitle = {Future Data and Security Engineering: 7th International Conference, FDSE 2020, Quy Nhon, Vietnam, November 25–27, 2020, Proceedings},
pages = {379–396},
numpages = {18},
keywords = {Intrusion detection system, Machine learning, Classification.},
location = {Quy Nhon, Vietnam}
}

@article{10.1007/s10994-020-05941-0,
author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, C\'{e}line and Gromowski, Mark and Schmid, Ute},
title = {Beneficial and harmful explanatory machine learning},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {4},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-020-05941-0},
doi = {10.1007/s10994-020-05941-0},
abstract = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.},
journal = {Mach. Learn.},
month = apr,
pages = {695–721},
numpages = {27},
keywords = {Inductive logic programming, Comprehensibility, Ultra-strong machine learning, Explainable AI}
}

@inproceedings{10.1007/978-3-030-58548-8_30,
author = {Taherkhani, Fariborz and Dabouei, Ali and Soleymani, Sobhan and Dawson, Jeremy and Nasrabadi, Nasser M.},
title = {Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning},
year = {2020},
isbn = {978-3-030-58547-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58548-8_30},
doi = {10.1007/978-3-030-58548-8_30},
abstract = {Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {509–526},
numpages = {18},
keywords = {Semi-Supervised Learning, Hierarchical optimal transport},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.compeleceng.2021.107574,
author = {Rajpoot, Vikram and Garg, Lalit and Alam, M. Zahid and Sangeeta and Parashar, Vivek and Tapashetti, Pratibhadevi and Arjariya, Tripti},
title = {Analysis of machine learning based LEACH robust routing in the Edge Computing systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {PB},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107574},
doi = {10.1016/j.compeleceng.2021.107574},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {17},
keywords = {Wireless sensor networks, Edge Computing, Machine learning, Data Fusion Method, LEACH routing protocol, Independent RNN}
}

@inproceedings{10.1007/978-3-030-77385-4_37,
author = {Bloem, Peter and Wilcke, Xander and van Berkel, Lucas and de Boer, Victor},
title = {kgbench: A Collection of Knowledge Graph Datasets for Evaluating Relational and Multimodal Machine Learning},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_37},
doi = {10.1007/978-3-030-77385-4_37},
abstract = {Graph neural networks and other machine learning models offer a promising direction for machine learning on relational and multimodal data. Until now, however, progress in this area is difficult to gauge. This is primarily due to a limited number of datasets with (a) a high enough number of labeled nodes in the test set for precise measurement of performance, and (b) a rich enough variety of multimodal information to learn from. We introduce a set of new benchmark tasks for node classification on RDF-encoded knowledge graphs. We focus primarily on node classification, since this setting cannot be solved purely by node embedding models. For each dataset, we provide test and validation sets of at least 1000 instances, with some over 10000. Each task can be performed in a purely relational manner, or with multimodal information. All datasets are packaged in a CSV format that is easily consumable in any machine learning environment, together with the original source data in RDF and pre-processing code for full provenance. We provide code for loading the data into numpy and pytorch. We compute performance for several baseline models.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {614–630},
numpages = {17},
keywords = {Knowledge graphs, Machine learning, Message passing models, Multimodal learning}
}

@article{10.1016/j.compeleceng.2021.107362,
author = {P, Gouthaman and Sankaranarayanan, Suresh},
title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107362},
doi = {10.1016/j.compeleceng.2021.107362},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {9},
keywords = {Software model, Agile, Waterfall, Evolutionary, Incremental, Machine learning, Risk prediction}
}

@article{10.1145/3445812,
author = {Jesus, Gon\c{c}alo and Casimiro, Ant\'{o}nio and Oliveira, Anabela},
title = {Using Machine Learning for Dependable Outlier Detection in Environmental Monitoring Systems},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3445812},
doi = {10.1145/3445812},
abstract = {Sensor platforms used in environmental monitoring applications are often subject to harsh environmental conditions while monitoring complex phenomena. Therefore, designing dependable monitoring systems is challenging given the external disturbances affecting sensor measurements. Even the apparently simple task of outlier detection in sensor data becomes a hard problem, amplified by the difficulty in distinguishing true data errors due to sensor faults from deviations due to natural phenomenon, which look like data errors. Existing solutions for runtime outlier detection typically assume that the physical processes can be accurately modeled, or that outliers consist in large deviations that are easily detected and filtered by appropriate thresholds. Other solutions assume that it is possible to deploy multiple sensors providing redundant data to support voting-based techniques. In this article, we propose a new methodology for dependable runtime detection of outliers in environmental monitoring systems, aiming to increase data quality by treating them. We propose the use of machine learning techniques to model each sensor behavior, exploiting the existence of correlated data provided by other related sensors. Using these models, along with knowledge of processed past measurements, it is possible to obtain accurate estimations of the observed environment parameters and build failure detectors that use these estimations. When a failure is detected, these estimations also allow one to correct the erroneous measurements and hence improve the overall data quality. Our methodology not only allows one to distinguish truly abnormal measurements from deviations due to complex natural phenomena, but also allows the quantification of each measurement quality, which is relevant from a dependability perspective.We apply the methodology to real datasets from a complex aquatic monitoring system, measuring temperature and salinity parameters, through which we illustrate the process for building the machine learning prediction models using a technique based on Artificial Neural Networks, denoted ANNODE (ANN Outlier Detection). From this application, we also observe the effectiveness of our ANNODE approach for accurate outlier detection in harsh environments. Then we validate these positive results by comparing ANNODE with state-of-the-art solutions for outlier detection. The results show that ANNODE improves existing solutions regarding accuracy of outlier detection.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jul,
articleno = {29},
numpages = {30},
keywords = {Dependability, aquatic monitoring, data quality, machine learning, neural networks, outlier detection}
}

@inproceedings{10.1145/3404835.3462814,
author = {Li, Yunqi and Ge, Yingqiang and Zhang, Yongfeng},
title = {Tutorial on Fairness of Machine Learning in Recommender Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462814},
doi = {10.1145/3404835.3462814},
abstract = {Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users' or providers' satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2654–2657},
numpages = {4},
keywords = {AI ethics, fairness, machine learning, recommender systems},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.compbiomed.2021.104985,
author = {Ali, Md Mamun and Ahmed, Kawsar and Bui, Francis M. and Paul, Bikash Kumar and Ibrahim, Sobhy M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
title = {Machine learning-based statistical analysis for early stage detection of cervical cancer},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104985},
doi = {10.1016/j.compbiomed.2021.104985},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {17},
keywords = {Cervical cancer, Biopsy, Cytology, Hinselmann, Schiller, Random tree}
}

@inproceedings{10.1145/3394486.3403290,
author = {Karla\v{s}, Bojan and Interlandi, Matteo and Renggli, Cedric and Wu, Wentao and Zhang, Ce and Mukunthu Iyappan Babu, Deepak and Edwards, Jordan and Lauren, Chris and Xu, Andy and Weimer, Markus},
title = {Building Continuous Integration Services for Machine Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403290},
doi = {10.1145/3394486.3403290},
abstract = {Continuous integration (CI) has been a de facto standard for building industrial-strength software. Yet, there is little attention towards applying CI to the development of machine learning (ML) applications until the very recent effort on the theoretical side. In this paper, we take a step forward to bring the theory into practice.We develop the first CI system for ML, to the best of our knowledge, that integrates seamlessly with existing ML development tools. We present its design and implementation details.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2407–2415},
numpages = {9},
keywords = {continuous integration, data management, machine learning, overfitting prevention, testing},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.4018/IJEHMC.20220701.oa1,
author = {Gupta, Charu and Gaur, Dev and Agrawal, Prateek and Virmani, Deepali},
title = {HuDA_COVID Human Disposition Analysis During COVID-19 Using Machine Learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {13},
number = {2},
issn = {1947-315X},
url = {https://doi.org/10.4018/IJEHMC.20220701.oa1},
doi = {10.4018/IJEHMC.20220701.oa1},
abstract = {Coronavirus has greatly impacted various aspects of human life, including human psychology &amp; human disposition. In this paper, we attempted to analyze the impact of the COVID-19 pandemic on human health. We propose Human Disposition Analysis during COVID-19 using machine learning (HuDA_COVID), where factors such as age, employment, addiction, stress level are studied for human disposition analysis. A mass survey is conducted on individuals of various age groups, regions &amp; professions, and the methodology achieved varied accuracy ranges of 87.5% to 98%. The study shows people are worried about lockdown, work &amp; relationships. Furthermore, 23% of the respondents have not had any effect. 45% and 32% have had positive and negative effects, respectively. It is a novel study in human disposition analysis in COVID-19 where a novel weighted assignment indicating the health status is also proposed. HuDA_COVID clearly indicates a need for a methodical approach towards the human psychological needs to help the social organizations formulating holistic interventions for affected individuals.},
journal = {Int. J. E-Health Med. Commun.},
month = sep,
pages = {1–15},
numpages = {15},
keywords = {ANN, Coronavirus, Data Classification, Decision Tree, Human Psychology, Machine Learning, Random Forest, SVM}
}

@article{10.1016/j.sysarc.2021.102295,
author = {Ren, Jinting and Chen, Xianzhang and Liu, Duo and Tan, Yujuan and Duan, Moming and Li, Ruolan and Liang, Liang},
title = {A machine learning assisted data placement mechanism for hybrid storage systems},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2021.102295},
doi = {10.1016/j.sysarc.2021.102295},
journal = {J. Syst. Archit.},
month = nov,
numpages = {12},
keywords = {Machine learning, Hybrid storage, Data placement}
}

@article{10.1613/jair.1.11854,
author = {Z\"{o}ller, Marc-Andr\'{e} and Huber, Marco F.},
title = {Benchmark and Survey of Automated Machine Learning Frameworks},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11854},
doi = {10.1613/jair.1.11854},
abstract = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {409–472},
numpages = {64}
}

@inproceedings{10.1007/978-3-030-67670-4_39,
author = {Gijsbers, Pieter and Vanschoren, Joaquin},
title = {GAMA: A General Automated Machine Learning Assistant},
year = {2020},
isbn = {978-3-030-67669-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67670-4_39},
doi = {10.1007/978-3-030-67670-4_39},
abstract = {The General Automated Machine learning Assistant (GAMA) is a modular AutoML system developed to empower users to track and control how AutoML algorithms search for optimal machine learning pipelines, and facilitate AutoML research itself. In contrast to current, often black-box systems, GAMA allows users to plug in different AutoML and post-processing techniques, logs and visualizes the search process, and supports easy benchmarking. It currently features three AutoML search algorithms, two model post-processing steps, and is designed to allow for more components to be added.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part V},
pages = {560–564},
numpages = {5},
location = {Ghent, Belgium}
}

@article{10.1016/j.ipm.2021.102555,
author = {\.{Z}bikowski, Kamil and Antosiuk, Piotr},
title = {A machine learning, bias-free approach for predicting business success using Crunchbase data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {4},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102555},
doi = {10.1016/j.ipm.2021.102555},
journal = {Inf. Process. Manage.},
month = jul,
numpages = {18},
keywords = {Look-ahead bias, Crunchbase, XGBoost, Supervised learning, Startups}
}

@article{10.1007/s42979-021-00967-0,
author = {Kilaskar, Mohini and Saindane, Neha and Ansari, Nabeel and Doshi, Dhaval and Kulkarni, Mayuri},
title = {Machine Learning Algorithms for Analysis and Prediction of Depression},
year = {2021},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {2},
url = {https://doi.org/10.1007/s42979-021-00967-0},
doi = {10.1007/s42979-021-00967-0},
abstract = {Today, depression is one of the critical mental health problems faced by humans of all ages and gender. In this era of increasing technology, it causes a life of less physical work, continuous pressure on one's life, which creates a risk of intellectual disturbance. The work culture, peer pressure, stressful life, emotional imbalance, family disturbances, and social life are resulting in depression. Depression may also sometimes lead to a heart attack. Depression causes adverse effects and becomes a serious medical problem in how individuals feel and act in everyday life. This psychological state causes feelings of sadness, anxiety, loss of interest in things and jobs, and could barely result in suicide. In this paper, the analysis of different Machine Learning Algorithms has been done and compared them by selecting various parameters and then showing which algorithm is more accurate for predicting depression.},
journal = {SN Comput. Sci.},
month = dec,
numpages = {6},
keywords = {Random forest, Logistic regression, SVM, XGBoost, Depression, Machine learning}
}

@inproceedings{10.1007/978-3-030-29726-8_25,
author = {Siebert, Sophie and Schon, Claudia and Stolzenburg, Frieder},
title = {Commonsense Reasoning Using Theorem Proving and Machine Learning},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_25},
doi = {10.1007/978-3-030-29726-8_25},
abstract = {Commonsense reasoning is a difficult task for a computer to handle. Current algorithms score around 80% on benchmarks. Usually these approaches use machine learning which lacks explainability, however. Therefore, we propose a combination with automated theorem proving here. Automated theorem proving allows us to derive new knowledge in an explainable way, but suffers from the inevitable incompleteness of existing background knowledge. We alleviate this problem by using machine learning. In this paper, we present our approach which uses an automatic theorem prover, large existing ontologies with background knowledge, and machine learning. We present first experimental results and identify an insufficient amount of training data and lack of background knowledge as causes for our system not to stand out much from the baseline.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {395–413},
numpages = {19},
keywords = {Commonsense reasoning, Causal reasoning, Machine learning, Theorem proving, Large background knowledge},
location = {Canterbury, United Kingdom}
}

@article{10.1007/s00521-020-05457-7,
author = {Liu, Dunnan and Xu, Xiaofeng and Liu, Mingguang and Liu, Yaling},
title = {Dynamic traffic classification algorithm and simulation of energy Internet of things based on machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05457-7},
doi = {10.1007/s00521-020-05457-7},
abstract = {With the rapid development of information technology, a large amount of traffic generated by various Internet applications occupies a large amount of network resources. It poses a huge challenge to service quality and has a negative impact on Internet security. In order to utilize network resources effectively and provide effective management and control measures for network administrators, network traffic classification technologies is a hot topic for scientists to identify application layer protocols. Today, there are more and more applications based on TCP/IP. With the emergence of various anti-surveillance applications, traditional port and application-based identification methods are difficult to meet current or future traffic identification requirements. It has become a very challenging problem to require more efficient, accurate, intelligent and real-time Internet traffic identification. The Internet of Things is a new network concept proposed by people who based on Internet prototypes. It enables the end user of the system can carry out communication and exchange of information and data between any project. In recent years, with the continuous advancement of Internet of Things technology, the coverage of the Internet of Things has become very wide, and the number of different types of networks that make up the Internet of Things is also increasing. This paper aims to find the dynamic network traffic classification problem of hybrid fixed in dynamic network and dynamic network in mobile network, and gives a reasonable mapping scheme. The dynamics of network traffic for Internet of Things are reflected fully and will not cause route flapping. The simulation results show that the decision tree classification algorithm in machine learning has higher efficiency, and improves the utilization of network resources.},
journal = {Neural Comput. Appl.},
month = may,
pages = {3967–3976},
numpages = {10},
keywords = {Traffic Classification, Network Traffic, Internet of Things, Machine Learning}
}

@inproceedings{10.1145/3368089.3417043,
author = {Ahmed, Md Sohel and Ishikawa, Fuyuki and Sugiyama, Mahito},
title = {Testing machine learning code using polyhedral region},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417043},
doi = {10.1145/3368089.3417043},
abstract = {To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1533–1536},
numpages = {4},
keywords = {Testing, Polyhedral region, Mutation Analysis, Machine learning code, Lasso},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@phdthesis{10.5555/AAI28763987,
author = {Wang, Yuyang and de, Veciana, Gustavo and Nuria, Gonz\'{a}lez Prelcic, and Aldebaro, Klautau, and Lili, Qiu,},
advisor = {Jr, Heath, Robert W.,},
title = {Millimeter Wave Vehicular Link Configuration Using Machine Learning},
year = {2020},
isbn = {9798538144792},
publisher = {The University of Texas at Austin},
abstract = {Millimeter-wave (MmWave) vehicular communication enables massive sensor data sharing and various emerging applications related to safety, traffic efficiency and infotainment. Estimating and tracking beams in mmWave vehicular communication, however, is challenging due to the use of large antenna arrays and high mobility in the vehicular context. Fortunately, wireless cellular communication systems have access to vast data resources, which can make beam training more efficient. Data-driven approaches are able to leverage side information and underlying channel statistics to optimize link configuration in mmWave vehicular communication with negligible overhead. indentIn the first part of this dissertation, we develop a situational awareness-aided beam alignment solution using machine learning. Situational awareness, defined as the locations and shapes of the receiver and its surrounding vehicles, can be obtained from sensors to extract environment information and retrieve good beam directions. We formulate mmWave beam selection as a multi-class classification problem, based on hand-crafted features that capture the situational awareness in different coordinates. We provide a comprehensive comparison among the different classification models and various levels of situational awareness. To demonstrate the scalability of the proposed beam selection solution in the large antenna array regime, we propose two solutions to recommend multiple beams and exploit an extra phase of beam sweeping among the recommended beams.In the second part of this dissertation, we develop mmWave vehicular beam alignment solutions with relaxed requirements of connected vehicles and sensor information sharing. The proposed model focuses on designing compressive sensing techniques that leverage the underlying channel angular statistics in site-specific areas using fewer channel measurements. We investigate the problem from an online learning-based approach that optimizes the sensing matrix on the fly and an offline approach that designs the compressive sensing framework using a convolutional neural network. We incorporate hardware constraints of the phased array in the sensing matrix optimization. We investigate structures in frequency-domain channels and propose solutions to optimize power allocated for different subcarriers.  Numerical results show that data-driven approaches can achieve accurate link configuration for mmWave vehicular communication with negligible training overhead.},
note = {AAI28763987}
}

@article{10.1007/s00521-020-05058-4,
author = {Borg, Anton and Boldt, Martin and Rosander, Oliver and Ahlstrand, Jim},
title = {E-mail classification with machine learning and word embeddings for improved customer support},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05058-4},
doi = {10.1007/s00521-020-05058-4},
abstract = {Classifying e-mails into distinct labels can have a great impact on customer support. By using machine learning to label e-mails, the system can set up queues containing e-mails of a specific category. This enables support personnel to handle request quicker and more easily by selecting a queue that match their expertise. This study aims to improve a manually defined rule-based algorithm, currently implemented at a large telecom company, by using machine learning. The proposed model should have higher F1-score and classification rate. Integrating or migrating from a manually defined rule-based model to a machine learning model should also reduce the administrative and maintenance work. It should also make the model more flexible. By using the frameworks, TensorFlow, Scikit-learn and Gensim, the authors conduct a number of experiments to test the performance of several common machine learning algorithms, text-representations, word embeddings to investigate how they work together. A long short-term memory network showed best classification performance with an F1-score of 0.91. The authors conclude that long short-term memory networks outperform other non-sequential models such as support vector machines and AdaBoost when predicting labels for e-mails. Further, the study also presents a Web-based interface that were implemented around the LSTM network, which can classify e-mails into 33 different labels.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1881–1902},
numpages = {22},
keywords = {Natural language processing, Long short-term memory, Machine learning, E-mail classification}
}

@inproceedings{10.1145/3486001.3486243,
author = {Ahmed, Shakkeel and Bisht, Prakash and Mula, Ravi and Dhavala, Soma S},
title = {A Deep Learning framework for Interoperable Machine Learning},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486243},
doi = {10.1145/3486001.3486243},
abstract = {In this paper, we introduce an opinionated, extensible, Python framework that transpiles variety of classical Statistical and Machine Learning models onto Open Neural Network Exchange (ONNX) format via an underlying Deep Learning model. We achieve this by exploiting the compositionality of Deep Learning technology. By appropriately choosing the features, architecture, loss functions, and regularizers, among others, the fidelity between the source model and the target model can be specified. Depending on the model being transpiled, the fidelity can be exact or approximate. We present the design details, APIs of the framework, reference implementations, road map for development, and guidelines for contributions. A reference implementation is available for the popular scikit-learn APIs.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {23},
numpages = {7},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@article{10.1007/s00607-021-00902-4,
author = {Le Thi, Thuy and Phan Thi, Tuoi and Quan Thanh, Tho},
title = {Machine learning using context vectors for object coreference resolution},
year = {2021},
issue_date = {Mar 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {105},
number = {3},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-00902-4},
doi = {10.1007/s00607-021-00902-4},
abstract = {Object coreference resolution is used in sentiment analysis to identify sentiment words referring to an aspect of an object in a document. However, this poses a challenge in natural language processing and is consequently an area of ongoing research. Further, to the best of our knowledge, object coreference resolution with more than one object has not been given much attention. To effectively address object coreference resolution, this paper proposes a method in which machine learning is applied to a large volume of textual data represented by context vectors, constituting a new form of language representation. The proposed machine learning model uses these vectors to achieve state-of-the-art performance in object coreference resolution. In addition, a combination of dependency grammar, sentiment ontology, and coreference graphs is used to obtain triplets of object, aspect, and sentiment. In experiments conducted on sentiment textual data obtained from Amazon.com, the proposed method achieved an average coreference resolution of object, aspect, and sentiment precision value of approximately 90%. This result suggests that the proposed method can contribute considerably to the field of object coreference resolution, and further research is therefore warranted.},
journal = {Computing},
month = jan,
pages = {539–558},
numpages = {20},
keywords = {68T50, CROAS, Sentiment ontology, Sentiment analysis, Object aspect, Object coreference resolution}
}

@inproceedings{10.1145/3334480.3382937,
author = {Ferrario, Andrea and Weibel, Raphael and Feuerriegel, Stefan},
title = {ALEEDSA: Augmented Reality for Interactive Machine Learning},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382937},
doi = {10.1145/3334480.3382937},
abstract = {In this work, we present ALEEDSA: the first system for performing interactive machine learning with augmented reality. The system is characterized by the following three distinctive features: First, immersion is used for visualizing machine learning models in terms of their outcomes. The outcomes can then be compared against domain knowledge (e.g., via counterfactual explanations) so that users can better understand the behavior of machine learning models. Second, interactivity with augmented reality along the complete machine learning pipeline fosters rapid modeling. Third, collaboration enables a multi-user setting, wherein machine learning engineers and domain experts can jointly discuss the behavior of machine learning models. The effectiveness of our proof-of-concept is demonstrated in an experimental study involving both students and business professionals. Altogether, ALEEDSA provides a more straightforward utilization of machine learning in organizational and educational practice.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {augmented reality, collaborative interaction, machine learning, mixed reality},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3447555.3466566,
author = {Herzog, Benedict and Reif, Stefan and H\"{u}gel, Fabian and H\"{o}nig, Timo and Schr\"{o}der-Preikschat, Wolfgang},
title = {Towards Automated System-Level Energy-Efficiency Optimisation using Machine Learning: Poster},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3466566},
doi = {10.1145/3447555.3466566},
abstract = {Modern computing systems need to execute applications in an energy-efficient manner. To this end, operating systems, middleware, and run-time systems offer plenty of parameters that support fine-tuning their behaviour. However, their individual and combined impact on performance and power draw is so complex that this optimisation potential is often ignored in practice. This paper therefore discusses a cross-layer system design that uses machine learning internally to enable fine-tuning run-time systems to their current workload. Our approach includes all layers, from the hardware to the application, considering both performance and power draw.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {274–275},
numpages = {2},
keywords = {System Configuration, Machine Learning, Energy Efficiency},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@article{10.1109/MCOM.001.2000367,
author = {V\'{a}zquez, Miguel \'{A}ngel and Henarejos, Pol and Pappalardo, Irene and Grechi, Elena and Fort, Joan and Gil, Juan Carlos and Lancellotti, Rocco Michele},
title = {Machine Learning for Satellite Communications Operations},
year = {2021},
issue_date = {February 2021},
publisher = {IEEE Press},
volume = {59},
number = {2},
issn = {0163-6804},
url = {https://doi.org/10.1109/MCOM.001.2000367},
doi = {10.1109/MCOM.001.2000367},
abstract = {This article introduces the application of machine learning (ML)-based procedures in real-world satellite communication operations. While the application of ML in image processing has led to unprecedented advantages in new services and products, the application of ML in wireless systems is still in its infancy. In particular, this article focuses on the introduction of ML-based mechanisms in satellite network operation centers such as interference detection, flexible payload configuration, and congestion prediction. Three different use cases are described, and the proposed ML models are introduced. All the models have been constructed using real data and considering current operations. As reported in the numerical results, the proposed ML-based techniques show good numerical performance: the interference detector presents a false detection probability decrease of 44 percent, the flexible payload optimizer reduces the unmet capacity by 32 percent, and the traffic predictor reduces the prediction error by 10 percent compared to other approaches. In light of these results, the proposed techniques are useful in the process of automating satellite communication systems.},
journal = {Comm. Mag.},
month = feb,
pages = {22–27},
numpages = {6}
}

@inproceedings{10.1007/978-3-030-60450-9_30,
author = {Lin, Pingping and Luo, Xudong},
title = {A Survey of Sentiment Analysis Based on Machine Learning},
year = {2020},
isbn = {978-3-030-60449-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60450-9_30},
doi = {10.1007/978-3-030-60450-9_30},
abstract = {Every day, Facebook, Twitter, Weibo and other social network sites and major e-commerce sites generate a large number of online reviews with emotions. The analysing people’s opinions from these reviews can assist a variety of decision-making processes in organisations, products, and administrations. Therefore, it is practically and theoretically important to study how to analyse online reviews with emotions. To help researchers study sentiment analysis, in this paper, we survey the machine learning based method for sentiment analysis of online reviews. These methods are main based on Support Vector Machine, Neural Networks, Na\"{\i}ve Bayes, Bayesian network, Maximum entropy, and some hybrid methods. In particular, we point out the main problems in the machine learning based methods for sentiment analysis and the problems to be solved in the future.},
booktitle = {Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part I},
pages = {372–387},
numpages = {16},
keywords = {Transfer learning, Integrated learning, Machine learning, Sentiment analysis},
location = {Zhengzhou, China}
}

@inproceedings{10.1007/978-3-030-71158-0_14,
author = {Ferreira, Lu\'{\i}s and Pilastri, Andr\'{e} and Martins, Carlos and Santos, Pedro and Cortez, Paulo},
title = {A Scalable and Automated Machine Learning Framework to Support Risk Management},
year = {2020},
isbn = {978-3-030-71157-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71158-0_14},
doi = {10.1007/978-3-030-71158-0_14},
abstract = {Due to the growth of data and widespread usage of Machine Learning (ML) by non-experts, automation and scalability are becoming key issues for ML. This paper presents an automated and scalable framework for ML that requires minimum human input. We designed the framework for the domain of telecommunications risk management. This domain often requires non-ML-experts to continuously update supervised learning models that are trained on huge amounts of data. Thus, the framework uses Automated Machine Learning (AutoML), to select and tune the ML models, and distributed ML, to deal with Big Data. The modules included in the framework are task detection (to detect classification or regression), data preprocessing, feature selection, model training, and deployment. In this paper, we focus the experiments on the model training module. We first analyze the capabilities of eight AutoML tools: Auto-Gluon, Auto-Keras, Auto-Sklearn, Auto-Weka, H2O AutoML, Rminer, TPOT, and TransmogrifAI. Then, to select the tool for model training, we performed a benchmark with the only two tools that address a distributed ML (H2O AutoML and TransmogrifAI). The experiments used three real-world datasets from the telecommunications domain (churn, event forecasting, and fraud detection), as provided by an analytics company. The experiments allowed us to measure the computational effort and predictive capability of the AutoML tools. Both tools obtained high-quality results and did not present substantial predictive differences. Nevertheless, H2O AutoML was selected by the analytics company for the model training module, since it was considered a more mature technology that presented a more interesting set of features (e.g., integration with more platforms). After choosing H2O AutoML for the ML training, we selected the technologies for the remaining components of the architecture (e.g., data preprocessing and web interface).},
booktitle = {Agents and Artificial Intelligence: 12th International Conference, ICAART 2020, Valletta, Malta, February 22–24, 2020, Revised Selected Papers},
pages = {291–307},
numpages = {17},
keywords = {Risk management, Supervised learning, Distributed machine learning, Automated machine learning},
location = {Valletta, Malta}
}

@inproceedings{10.1007/978-3-030-80599-9_20,
author = {Sazzed, Salim},
title = {Improving Sentiment Classification in Low-Resource Bengali Language Utilizing Cross-Lingual Self-supervised Learning},
year = {2021},
isbn = {978-3-030-80598-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-80599-9_20},
doi = {10.1007/978-3-030-80599-9_20},
abstract = {One of the barriers of sentiment analysis research in low-resource languages such as Bengali is the lack of annotated data. Manual annotation requires resources, which are scarcely available in low-resource languages. We present a cross-lingual hybrid methodology that utilizes machine translation and prior sentiment information to generate accurate pseudo-labels. By leveraging the pseudo-labels, a supervised ML classifier is trained for sentiment classification. We contrast the performance of the proposed self-supervised methodology with the Bengali and English sentiment classification methods (i.e., methods which do not require labeled data). We observe that the self-supervised hybrid methodology improves the macro F1 scores by 15%–25%. The results infer that the proposed framework can improve the performance of sentiment classification in low-resource languages that lack labeled data.},
booktitle = {Natural Language Processing and Information Systems: 26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021, Saarbr\"{u}cken, Germany, June 23–25, 2021, Proceedings},
pages = {218–230},
numpages = {13},
keywords = {Cross-lingual sentiment analysis, Pseudo-label generation, Bangla sentiment analysis},
location = {Saarbr\"{u}cken, Germany}
}

@inproceedings{10.1145/2957276.2957280,
author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
title = {Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957280},
doi = {10.1145/2957276.2957280},
abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
booktitle = {Proceedings of the 2016 ACM International Conference on Supporting Group Work},
pages = {3–8},
numpages = {6},
keywords = {unsupervised learning, supervised learning, machine learning, grounded theory, coding families, axial coding},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@article{10.1134/S0005117919090078,
author = {Popkov, Yu. S.},
title = {Randomized Machine Learning Procedures},
year = {2019},
issue_date = {Sep 2019},
publisher = {Plenum Press},
address = {USA},
volume = {80},
number = {9},
issn = {0005-1179},
url = {https://doi.org/10.1134/S0005117919090078},
doi = {10.1134/S0005117919090078},
abstract = {A new concept of machine learning based on the computer simulation of entropy-optimal randomized models is proposed. The procedures of randomized machine learning (RML) with “hard” and “soft” randomization are considered; the former imply the exact reproduction of empirical balances while the latter their rough reproduction with an accepted approximation criterion. RML algorithms are formulated as functional entropy-linear programming problems. Applications of RML procedures to text classification and the randomized forecasting of migratory interaction of regional systems are presented.},
journal = {Autom. Remote Control},
month = sep,
pages = {1653–1670},
numpages = {18},
keywords = {dynamic regression, text classification, empirical balances, matrix norms, entropy, uncertainty, hard and soft randomization procedures, randomization}
}

@inproceedings{10.1109/SIBGRAPI.2013.13,
author = {Escalante, Diego Alonso Ch\'{a}vez and Taubin, Gabriel and Nonato, Luis Gustavo and Goldenstein, Siome Klein},
title = {Using Unsupervised Learning for Graph Construction in Semi-supervised Learning with Graphs},
year = {2013},
isbn = {9780769550992},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SIBGRAPI.2013.13},
doi = {10.1109/SIBGRAPI.2013.13},
abstract = {Semi-supervised Learning with Graphs can achieve good results in classification tasks even in difficult conditions. Unfortunately, it can be slow and use a lot of memory. The first important step of the graph-based semi-supervised learning approaches is the construction of the graph from the data, where each data-point usually becomes a vertex in the graph - a potential problem with large amounts of data. In this paper, we present a graph construction method that uses an unsupervised neural network called growing neural gas (GNG). The GNG instance presents a intelligent stopping criteria that determines when the final network configuration maps correctly the input-data points. With that in mind, we use the final trained network as a reduced input graph for the semi-supervised classification algorithm, associating original data-points to the neurons they have activated in the unsupervised training process.},
booktitle = {Proceedings of the 2013 XXVI Conference on Graphics, Patterns and Images},
pages = {24–30},
numpages = {7},
series = {SIBGRAPI '13}
}

@inproceedings{10.1007/978-3-030-81242-3_10,
author = {Heaps, John and Krishnan, Ram and Huang, Yufei and Niu, Jianwei and Sandhu, Ravi},
title = {Access Control Policy Generation from&nbsp;User Stories Using Machine Learning},
year = {2021},
isbn = {978-3-030-81241-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-81242-3_10},
doi = {10.1007/978-3-030-81242-3_10},
abstract = {Agile software development methodology involves developing code incrementally and iteratively from a set of evolving user stories. Since software developers use user stories to write code, these user stories are better representations of the actual code than that of the high-level product documentation. In this paper, we develop an automated approach using machine learning to generate access control information from a set of user stories that describe the behavior of the software product in question. This is an initial step to automatically produce access control specifications and perform automated security review of a system with minimal human involvement. Our approach takes a set of user stories as input to a transformers-based deep learning model, which classifies if each user story contains access control information. It then identifies the actors, data objects, and operations the user story contains in a named entity recognition task. Finally, it determines the type of access between the identified actors, data objects, and operations through a classification prediction. This information can then be used to construct access control documentation and information useful to stakeholders for assistance during access control engineering, development, and review.},
booktitle = {Data and Applications Security and Privacy XXXV: 35th Annual IFIP WG 11.3 Conference, DBSec 2021, Calgary, Canada, July 19–20, 2021, Proceedings},
pages = {171–188},
numpages = {18},
keywords = {Deep learning, Machine learning, User stories, Agile development, Software engineering, Access control},
location = {Calgary, AB, Canada}
}

@inproceedings{10.1145/3472673.3473966,
author = {Chakraborty, Suranjan and Deng, Lin and Dehlinger, Josh},
title = {Towards authentic undergraduate research experiences in software engineering and machine learning},
year = {2021},
isbn = {9781450386241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472673.3473966},
doi = {10.1145/3472673.3473966},
abstract = {Authentic undergraduate research experiences have been shown to be very effective at sustaining students’ learning motivation and enhancing students’ theoretical knowledge and practical skills. However, there still exists some common challenges in undergraduate research. In this paper, we describe an approach that offers undergraduate students authentic and immersive research experience focusing on applied machine learning for software engineering and discuss our experiences with example undergraduate research projects and outcomes. A survey was designed to assess students’ overall experience of participating in authentic undergraduate research projects in machine learning for software engineering. Preliminary results from this survey are provided.},
booktitle = {Proceedings of the 3rd International Workshop on Education through Advanced Software Engineering and Artificial Intelligence},
pages = {54–57},
numpages = {4},
keywords = {undergraduate research, software engineering, computing education, Machine learning},
location = {Athens, Greece},
series = {EASEAI 2021}
}

@inproceedings{10.1145/3338906.3341466,
author = {Sonnekalb, Tim},
title = {Machine-learning supported vulnerability detection in source code},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341466},
doi = {10.1145/3338906.3341466},
abstract = {The awareness of writing secure code rises with the increasing number of attacks and their resultant damage. But often, software developers are no security experts and vulnerabilities arise unconsciously during the development process. They use static analysis tools for bug detection, which often come with a high false positive rate. The developers, therefore, need a lot of resources to mind about all alarms, if they want to consistently take care of the security of their software project. We want to investigate, if machine learning techniques could point the user to the position of a security weak point in the source code with a higher accuracy than ordinary methods with static analysis. For this purpose, we focus on current machine learning on code approaches for our initial studies to evolve an efficient way for finding security-related software bugs. We will create a configuration interface to discover certain vulnerabilities, categorized in CWEs. We want to create a benchmark tool to compare existing source code representations and machine learning architectures for vulnerability detection and develop a customizable feature model. At the end of this PhD project, we want to have an easy-to-use vulnerability detection tool based on machine learning on code.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1180–1183},
numpages = {4},
keywords = {vulnerability detection, vulnerabilities, source code analysis, software security, machine learning on code},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3387940.3391486,
author = {Wijaya, Herman and Aniche, Maur\'{\i}cio and Mathur, Aditya},
title = {Domain-Based Fuzzing for Supervised Learning of Anomaly Detection in Cyber-Physical Systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391486},
doi = {10.1145/3387940.3391486},
abstract = {A novel approach is proposed for constructing models of anomaly detectors using supervised learning from the traces of normal and abnormal operations of an Industrial Control System (ICS). Such detectors are of value in detecting process anomalies in complex critical infrastructure such as power generation and water treatment systems. The traces are obtained by systematically "fuzzing", i.e., manipulating the sensor readings and actuator actions in accordance with the boundaries/partitions that define the system's state. The proposed approach is tested in a Secure Water Treatment (SWaT) testbed -- a replica of a real-world water purification plant, located at the Singapore University of Technology and Design. Multiple supervised classifiers are trained using the traces obtained from SWaT. The efficacy of the proposed approach is demonstrated through empirical evaluation of the supervised classifiers under various performance metrics. Lastly, it is shown that the supervised approach results in significantly lower false positive rates as compared to the unsupervised ones.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {237–244},
numpages = {8},
keywords = {supervised learning, security, fuzzing, domain testing, cyber physical system, anomaly detection},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.5555/3540261.3541475,
author = {Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Chee-Kong},
title = {Motif-based graph self-supervised learning for molecular property prediction},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being finetuned for specific tasks. However, most existing self-supervised pre-training frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1214},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-61078-4_1,
author = {Takahashi, Junko and Okabe, Keiichi and Itoh, Hiroki and Ngo, Xuan-Thuy and Guilley, Sylvain and Shrivastwa, Ritu-Ranjan and Ahmed, Mushir and Lejoly, Patrick},
title = {Machine Learning Based Hardware Trojan Detection Using Electromagnetic Emanation},
year = {2020},
isbn = {978-3-030-61077-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61078-4_1},
doi = {10.1007/978-3-030-61078-4_1},
abstract = {The complexity and outsourcing trend of modern System-on-Chips (SoC) has made Hardware Trojan (HT) a real threat for the SoC security. In the state-of-the-art, many techniques have been proposed in order to detect the HT insertion. Side-channel based methods emerge as a good approach used for the HT detection. They can extract any difference in the power consumption, electromagnetic (EM) emanation, delay propagation, etc. caused by the HT insertion/modification in the genuine design. Therefore, they can be applied to detect the HT even when it is not activated. However, these methods are evaluated on overly simple design prototypes such as AES coprocessors. Moreover, the analytical approach used for these methods is limited by some statistical metrics such as the direct comparison of EM traces or the T-test coefficients. In this paper, we propose two new detection methodologies based on Machine Learning algorithms. The first method consists in applying the supervised Machine Learning (ML) algorithms on raw EM traces for the classification and detection of HT. It offers a detection rate close to 90% and false negative smaller than 5%. For the second method, we propose a method based on the Outlier/Novelty algorithms. This method combined with the T-test based signal processing technique, when compared with state-of-the-art, offers a better performance with a detection rate close to 100% and a false positive smaller than 1%. We have evaluated the performance of our method on a complex target design: RISC-V generic processors. The three HTs with the corresponding sizes of 0.53%, 0.27% and 0.1% of the RISC-V processors are inserted for the experimentation. The experimental results show that the inserted HTs, though minimalist, can be detected using our new methodology.},
booktitle = {Information and Communications Security: 22nd International Conference, ICICS 2020, Copenhagen, Denmark, August 24–26, 2020, Proceedings},
pages = {3–19},
numpages = {17},
keywords = {Outliers detection, Machine learning, Side-channel analysis, Electromagnetic, Hardware trojan},
location = {Copenhagen, Denmark}
}

@article{10.1016/j.advengsoft.2021.103029,
author = {Nagy, Enik\H{o} and Lovas, R\'{o}bert and Pintye, Istv\'{a}n and Hajnal, \'{A}kos and Kacsuk, P\'{e}ter},
title = {Cloud-agnostic architectures for machine learning based on Apache Spark},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {159},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2021.103029},
doi = {10.1016/j.advengsoft.2021.103029},
journal = {Adv. Eng. Softw.},
month = sep,
numpages = {9},
keywords = {Spark, Stream processing, Distributed computing, Orchestration, Cloud computing, Machine learning, Artificial intelligence, Big data, Reference architectures}
}

@inproceedings{10.1007/978-3-030-67731-2_37,
author = {Bayram, Firas and Garbarino, Davide and Barla, Annalisa},
title = {Predicting Tennis Match Outcomes with Network Analysis and Machine Learning},
year = {2021},
isbn = {978-3-030-67730-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67731-2_37},
doi = {10.1007/978-3-030-67731-2_37},
abstract = {Singles tennis is one of the most popular individual sports in the world. Many researchers have embarked on a wide range of approaches to model a tennis match, using probabilistic modeling, or applying machine learning models to predict the outcome of matches. In this paper, we propose a novel approach based on network analysis to infer a surface-specific and time-varying score for professional tennis players and use it in addition to players’ statistics of previous matches to represent tennis match data. Using the resulting features, we apply advanced machine learning paradigms such as Multi-Output Regression and Learning Using Privileged Information, and compare the results with standard machine learning approaches. The models are trained and tested on more than 83,000 men’s singles tennis matches between the years 1991 and 2020. Evaluating the results shows the proposed methods provide more accurate predictions of tennis match outcome than classical approaches and outperform the existing methods in the literature and the current state-of-the-art models in tennis.},
booktitle = {SOFSEM 2021: Theory and Practice of Computer Science: 47th International Conference on Current Trends in Theory and Practice of Computer Science, SOFSEM 2021, Bolzano-Bozen, Italy, January 25–29, 2021, Proceedings},
pages = {505–518},
numpages = {14},
keywords = {Tennis outcome prediction, Multi-Output Regression, Learning Using Privileged Information, Network analysis, Machine learning},
location = {Bolzano-Bozen, Italy}
}

@article{10.1007/s11042-020-10067-5,
author = {\"{O}zer, \c{C}a\u{g}da\c{s} and \c{C}evik, Taner and G\"{u}rhanl\i{}, Ahmet},
title = {A machine learning-based framework for predicting game server load},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10067-5},
doi = {10.1007/s11042-020-10067-5},
abstract = {Server load prediction can be utilized for load-balancing and load-sharing in distributed systems. The use of machine learning (ML) algorithms for load estimation in distributed system applications can increase the availability and performance of servers. Hence, a number of machine learning algorithms have been applied thus far for server load estimation. This study focuses on increasing the performance of game servers by accurately predicting the workload of game servers in short, medium and long term prediction situations. While doing this, various machine learning techniques have been applied and the algorithms that give the best results are presented. In terms of implementation, companies using their servers and data centers can try to increase their level of satisfaction by using these algorithms. A prediction model is developed and the estimation performances of a number of fundamental ML methods i.e., Na\"{\i}ve Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), Support Vector Machine (SVM), Fast Large Margin (FLM), Convolutional Neural Network CNN are analyzed. The data used during the training stage is obtained by listening to the TCP/IP packet traffic and the real-data is extracted by performing an extensive analysis of the total transferred-data that includes also the payload. In the analysis phase, the goodput is considered in order to reveal exact resource requirements. Comprehensive simulations are performed under various conditions for high accuracy performance analysis. Experimental results indicate that the proposed ML-based prediction shows promising performance in terms of load prediction when compared to the common approaches present in the literature.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {9527–9546},
numpages = {20},
keywords = {Game server, Load prediction, Machine learning}
}

@article{10.1007/s00607-021-00958-2,
author = {Khalid, Yasir Noman and Aleem, Muhammad and Ahmed, Usman and Prodan, Radu and Islam, Muhammad Arshad and Iqbal, Muhammad Azhar},
title = {FusionCL: a machine-learning based approach for OpenCL kernel fusion to increase system performance},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {10},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-00958-2},
doi = {10.1007/s00607-021-00958-2},
abstract = {Employing general-purpose graphics processing units (GPGPU) with the help of OpenCL has resulted in greatly reducing the execution time of data-parallel applications by taking advantage of the massive available parallelism. However, when a small data size application is executed on GPU there is a wastage of GPU resources as the application cannot fully utilize GPU compute-cores. There is no mechanism to share a GPU between two kernels due to the lack of operating system support on GPU. In this paper, we propose the provision of a GPU sharing mechanism between two kernels that will lead to increasing GPU occupancy, and as a result, reduce execution time of a job pool. However, if a pair of the kernel is competing for the same set of resources (i.e., both applications are compute-intensive or memory-intensive), kernel fusion may also result in a significant increase in execution time of fused kernels. Therefore, it is pertinent to select an optimal pair of kernels for fusion that will result in significant speedup over their serial execution. This research presents FusionCL, a machine learning-based GPU sharing mechanism between a pair of OpenCL kernels. FusionCL identifies each pair of kernels (from the job pool), which are suitable candidates for fusion using a machine learning-based fusion suitability classifier. Thereafter, from all the candidates, it selects a pair of candidate kernels that will produce maximum speedup after fusion over their serial execution using a fusion speedup predictor. The experimental evaluation shows that the proposed kernel fusion mechanism reduces execution time by 2.83\texttimes{} when compared to a baseline scheduling scheme. When compared to state-of-the-art, the reduction in execution time is up to 8%.},
journal = {Computing},
month = oct,
pages = {2171–2202},
numpages = {32},
keywords = {68M20, Machine learning, High-performance computing, Kernel fusion, Scheduling}
}

@article{10.3233/JIFS-189609,
author = {Lee, Bor-Hon and Yang, Albert Jing-Fuh and Chen, Yenming J. and Hsieh, Wen-Hsiang},
title = {Machine learning of stochastic automata and evolutionary games},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189609},
doi = {10.3233/JIFS-189609},
abstract = {A large categories of time series fluctuate dramatically, for example, prices of agriculture produce. Traditional methods in time series and stochastic prediction may not capture such dynamics. This paper tries to use machine learning to tune the model for a real situation by establishing a price determination mechanism on the model of stochastic automata (SA) and evolutionary game (EG). Time series volatility attributed to the chaotic process can be obtained through the learning algorithm of Markov Chain Monte Carlo (MCMC). Using machine learning through the chaotic analysis of stochastic automata and evolutionary games, we find that a more spatially aggregated distribution (smaller entropy) leads to larger time series fluctuations, regardless of the initial distribution of crops. By integrating the factors discovered in this study, we can develop a better learning algorithm in a highly volatile time series in agriculture prices.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7875–7881},
numpages = {7},
keywords = {Distribution entropy, spatial diffusion, stochastic automata (SA), evolutionary game (EG), machine learning}
}

@inproceedings{10.1145/3318464.3389715,
author = {Derakhshan, Behrouz and Rezaei Mahdiraji, Alireza and Abedjan, Ziawasch and Rabl, Tilmann and Markl, Volker},
title = {Optimizing Machine Learning Workloads in Collaborative Environments},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389715},
doi = {10.1145/3318464.3389715},
abstract = {Effective collaboration among data scientists results in high-quality and efficient machine learning (ML) workloads. In a collaborative environment, such as Kaggle or Google Colabratory, users typically re-execute or modify published scripts to recreate or improve the result. This introduces many redundant data processing and model training operations. Reusing the data generated by the redundant operations leads to the more efficient execution of future workloads. However, existing collaborative environments lack a data management component for storing and reusing the result of previously executed operations. In this paper, we present a system to optimize the execution of ML workloads in collaborative environments by reusing previously performed operations and their results. We utilize a so-called Experiment Graph (EG) to store the artifacts, i.e., raw and intermediate data or ML models, as vertices and operations of ML workloads as edges. In theory, the size of EG can become unnecessarily large, while the storage budget might be limited. At the same time, for some artifacts, the overall storage and retrieval cost might outweigh the recomputation cost. To address this issue, we propose two algorithms for materializing artifacts based on their likelihood of future reuse. Given the materialized artifacts inside EG, we devise a linear-time reuse algorithm to find the optimal execution plan for incoming ML workloads. Our reuse algorithm only incurs a negligible overhead and scales for the high number of incoming ML workloads in collaborative environments. Our experiments show that we improve the run-time by one order of magnitude for repeated execution of the workloads and 50% for the execution of modified workloads in collaborative environments.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1701–1716},
numpages = {16},
keywords = {materialization and reuse, machine learning, collaborative ML},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.5555/3305381.3305435,
author = {Bojanowski, Piotr and Joulin, Armand},
title = {Unsupervised learning by predicting Noise},
year = {2017},
publisher = {JMLR.org},
abstract = {Convolutional neural networks provide visual features that perform well in many computer vision applications. However, training these networks requires large amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {517–526},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.patcog.2016.04.020,
author = {Amorim, Willian P. and Falc\~{a}o, Alexandre X. and Papa, Jo\~{a}o P. and Carvalho, Marcelo H.},
title = {Improving semi-supervised learning through optimum connectivity},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {60},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.04.020},
doi = {10.1016/j.patcog.2016.04.020},
abstract = {The annotation of large data sets by a classifier is a problem whose challenge increases as the number of labeled samples used to train the classifier reduces in comparison to the number of unlabeled samples. In this context, semi-supervised learning methods aim at discovering and labeling informative samples among the unlabeled ones, such that their addition to the correct class in the training set can improve classification performance. We present a semi-supervised learning approach that connects unlabeled and labeled samples as nodes of a minimum-spanning tree and partitions the tree into an optimum-path forest rooted at the labeled nodes. It is suitable when most samples from a same class are more closely connected through sequences of nearby samples than samples from distinct classes, which is usually the case in data sets with a reasonable relation between number of samples and feature space dimension. The proposed solution is validated by using several data sets and state-of-the-art methods as baselines. HighlightsA new algorithm for semi-supervised learning based on optimum-path forest.The algorithm provides significant improvements in accuracy and efficiency.Labels are propagated from labeled to unlabeled training samples with less errors.The novel classifier can be more accurate than other state-of-the-art methods.A fast and effective algorithm suitable for developing active learning methods.},
journal = {Pattern Recogn.},
month = dec,
pages = {72–85},
numpages = {14},
keywords = {Semi-supervised learning, Optimum-path forest classifiers}
}

@inproceedings{10.1007/978-3-030-58475-7_49,
author = {Ignatiev, Alexey and Cooper, Martin C. and Siala, Mohamed and Hebrard, Emmanuel and Marques-Silva, Joao},
title = {Towards Formal Fairness in Machine Learning},
year = {2020},
isbn = {978-3-030-58474-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58475-7_49},
doi = {10.1007/978-3-030-58475-7_49},
abstract = {One of the challenges of deploying machine learning (ML) systems is fairness. Datasets often include sensitive features, which ML algorithms may unwittingly use to create models that exhibit unfairness. Past work on fairness offers no formal guarantees in their results. This paper proposes to exploit formal reasoning methods to tackle fairness. Starting from an intuitive criterion for fairness of an ML model, the paper formalises it, and shows how fairness can be represented as a decision problem, given some logic representation of an ML model. The same criterion can also be applied to assessing bias in training data. Moreover, we propose a reasonable set of axiomatic properties which no other definition of dataset bias can satisfy. The paper also investigates the relationship between fairness and explainability, and shows that approaches for computing explanations can serve to assess fairness of particular predictions. Finally, the paper proposes SAT-based approaches for learning fair ML models, even when the training data exhibits bias, and reports experimental trials.},
booktitle = {Principles and Practice of Constraint Programming: 26th International Conference, CP 2020, Louvain-La-Neuve, Belgium, September 7–11, 2020, Proceedings},
pages = {846–867},
numpages = {22},
location = {Louvain-la-Neuve, Belgium}
}

@article{10.1016/j.jbi.2021.103762,
author = {Jia, Yan and Lawton, Tom and Burden, John and McDermid, John and Habli, Ibrahim},
title = {Safety-driven design of machine learning for sepsis treatment},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103762},
doi = {10.1016/j.jbi.2021.103762},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {16},
keywords = {Safety assurance, Sepsis treatment, Machine learning}
}

@inproceedings{10.1145/3460319.3464844,
author = {Dutta, Saikat and Selvam, Jeeva and Jain, Aryaman and Misailovic, Sasa},
title = {TERA: optimizing stochastic regression tests in machine learning projects},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464844},
doi = {10.1145/3460319.3464844},
abstract = {The stochastic nature of many Machine Learning (ML) algorithms makes testing of ML tools and libraries challenging. ML algorithms allow a developer to control their accuracy and run-time through a set of hyper-parameters, which are typically manually selected in tests. This choice is often too conservative and leads to slow test executions, thereby increasing the cost of regression testing.  We propose TERA, the first automated technique for reducing the cost of regression testing in Machine Learning tools and libraries(jointly referred to as projects) without making the tests more flaky. TERA solves the problem of exploring the trade-off space between execution time of the test and its flakiness as an instance of Stochastic Optimization over the space of algorithm hyper-parameters. TERA presents how to leverage statistical convergence-testing techniques to estimate the level of flakiness of the test for a specific choice of hyper-parameters during optimization.  We evaluate TERA on a corpus of 160 tests selected from 15 popular machine learning projects. Overall, TERA obtains a geo-mean speedup of 2.23x over the original tests, for the minimum passing probability threshold of 99%. We also show that the new tests did not reduce fault detection ability through a mutation study and a study on a set of 12 historical build failures in studied projects.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {413–426},
numpages = {14},
keywords = {Test Optimization, Software Testing, Machine Learning, Bayesian Optimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.5555/3540261.3541639,
author = {Richards, Dominic and Negahban, Sahand N and Rebeschini, Patrick},
title = {Distributed machine learning with sparse heterogeneous data},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Motivated by distributed machine learning settings such as Federated Learning, we consider the problem of fitting a statistical model across a distributed collection of heterogeneous data sets whose similarity structure is encoded by a graph topology. Precisely, we analyse the case where each node is associated with fitting a sparse linear model, and edges join two nodes if the difference of their solutions is also sparse. We propose a method based on Basis Pursuit Denoising with a total variation penalty, and provide finite sample guarantees for sub-Gaussian design matrices. Taking the root of the tree as a reference node, we show that if the sparsity of the differences across nodes is smaller than the sparsity at the root, then recovery is successful with fewer samples than by solving the problems independently, or by using methods that rely on a large overlap in the signal supports, such as the group Lasso. We consider both the noiseless and noisy setting, and numerically investigate the performance of distributed methods based on Distributed Alternating Direction Methods of Multipliers (ADMM) and hyperspectral unmixing.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1378},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.patrec.2021.07.004,
author = {Briguglio, William and Moghaddam, Parisa and Yousef, Waleed A. and Traor\'{e}, Issa and Mamun, Mohammad},
title = {Machine learning in precision medicine to preserve privacy via encryption},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.07.004},
doi = {10.1016/j.patrec.2021.07.004},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {148–154},
numpages = {7},
keywords = {Privacy, Precision medicine, Homomorphic encryption, Encryption, Machine learning}
}

@inproceedings{10.1145/3338498.3358646,
author = {Reith, Robert Nikolai and Schneider, Thomas and Tkachenko, Oleksandr},
title = {Efficiently Stealing your Machine Learning Models},
year = {2019},
isbn = {9781450368308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338498.3358646},
doi = {10.1145/3338498.3358646},
abstract = {Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.},
booktitle = {Proceedings of the 18th ACM Workshop on Privacy in the Electronic Society},
pages = {198–210},
numpages = {13},
keywords = {support vector regression machine, support vector machine, model extraction, machine learning, ideal leakage},
location = {London, United Kingdom},
series = {WPES'19}
}

@inproceedings{10.1007/978-3-030-62008-0_37,
author = {Abburi, Harika and Parikh, Pulkit and Chhaya, Niyati and Varma, Vasudeva},
title = {Fine-grained Multi-label Sexism Classification Using Semi-supervised Learning},
year = {2020},
isbn = {978-3-030-62007-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62008-0_37},
doi = {10.1007/978-3-030-62008-0_37},
abstract = {Sexism, a pervasive form of oppression, causes profound suffering through various manifestations. Given the rising number of experiences of sexism reported online, categorizing these recollections automatically can aid the fight against sexism, as it can facilitate effective analyses by gender studies researchers and government officials involved in policy making. In this paper, we explore the fine-grained, multi-label classification of accounts (reports) of sexism. To the best of our knowledge, we consider substantially more categories of sexism than any related prior work through our 23-class problem formulation. Moreover, we present the first semi-supervised work for the multi-label classification of accounts describing any type(s) of sexism wherein the approach goes beyond merely fine-tuning pre-trained models using unlabeled data. We devise self-training based techniques tailor-made for the multi-label nature of the problem to utilize unlabeled samples for augmenting the labeled set. We identify high textual diversity with respect to the existing labeled set as a desirable quality for candidate unlabeled instances and develop methods for incorporating it into our approach. We also explore ways of infusing class imbalance alleviation for multi-label classification into our semi-supervised learning, independently and in conjunction with the method involving diversity. Several proposed methods outperform a variety of baselines on a recently released dataset for multi-label sexism categorization across several standard metrics.},
booktitle = {Web Information Systems Engineering – WISE 2020: 21st International Conference, Amsterdam, The Netherlands, October 20–24, 2020, Proceedings, Part II},
pages = {531–547},
numpages = {17},
keywords = {Fine-grained categorization, Multi-label classification, Semi-supervised learning, Sexism classification},
location = {Amsterdam, The Netherlands}
}

@article{10.1016/j.comcom.2019.05.005,
author = {Khangura, Sukhpreet Kaur and Fidler, Markus and Rosenhahn, Bodo},
title = {Machine learning for measurement-based bandwidth estimation},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {144},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.05.005},
doi = {10.1016/j.comcom.2019.05.005},
journal = {Comput. Commun.},
month = aug,
pages = {18–30},
numpages = {13},
keywords = {Machine learning, Available bandwidth estimation}
}

@phdthesis{10.5555/AAI28495393,
author = {Nguyen, Anthony T. and Shiqian, Ma, and Luis, Rademacher,},
advisor = {Krishnakumar, Balasubramanian,},
title = {Advances in Stochastic Optimization for Machine Learning},
year = {2021},
isbn = {9798538100583},
publisher = {University of California, Davis},
abstract = {We discuss two advances made in Stochastic Optimization where they arise out of a general problem, namely minimizing an objective function of the form $f(x) = mathbb{E}_{xi}[F(x, xi)]$ for $x in X subseteq mathbb{R}.n$, where $F(x, xi)$ is a stochastic function with some random variable $xi$.  ewlineindent The first project, in extbf{Chapter} $2$, deals with minimizing an objective function of the form $f_1 circ cdots circ f_T(x)$ where $f_i(x) = mathbb{E}_{xi_i}[G_i(x,xi_i)]$. In this setting, we assume that each component $f_i$ is smooth, and in addition, we assume the access of a first-order oracle that outputs noisy estimates of the components and their derivatives. We introduce two algorithms that utilize moving average updates, and we prove that they converge to an $epsilon$-stationary point. The difference between these two algorithms is the first uses a mini-batch of samples in each iteration while the second uses linearized stochastic estimates of the function values. The sample complexities of the mini-batches and the stochastic linearized approaches for obtaining an $epsilon$-stationary point are $mathcal{O}(frac{1}{epsilon.6})$ and $mathcal{O}(frac{1}{epsilon.4})$, respectively. indent The second project, in extbf{Chapter} $3$, discusses minimizing a convex function $f_0(x) = mathbb{E}_{xi_0}[F_0(x, xi_0)]$ with functional inequality constraints $f_i(x) = mathbb{E}_{xi_i}[F_i(x, xi_i)] leqslant 0$ ($i in {1, dots, m}$) using a zeroth-order oracle. We assume that we have access to noisy function value evaluations.  The algorithm performs an extrapolation and numerically solves the dual optimization problem by performing a gradient ascent and descent at each iteration. Finally, the numerical solution is the weighted average of the iterates from the gradient descents. The number of calls to the oracle to find an $epsilon$-approximate optimal solution is $mathcal{O}(frac{(m+1)n}{epsilon.2})$. Next, we present an algorithm in the non-convex setting based on cite{boob2019proximal}; utilizing our algorithm for the convex setting, the non-convex algorithm has sample complexity $mathcal{O}(frac{(m+1)n}{epsilon.3})$.},
note = {AAI28495393}
}

@inproceedings{10.1145/3306618.3314293,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {machine learning, interpretability, explainable artificial intelligence, active learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@article{10.1016/j.neucom.2019.01.083,
author = {Zhang, Xiao-Yu and Shi, Haichao and Zhu, Xiaobin and Li, Peng},
title = {Active semi-supervised learning based on self-expressive correlation with generative adversarial networks},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {345},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.01.083},
doi = {10.1016/j.neucom.2019.01.083},
journal = {Neurocomput.},
month = jun,
pages = {103–113},
numpages = {11},
keywords = {Representation learning, Batch selection, Generative adversarial networks, Semi-supervised learning, Active learning}
}

@inproceedings{10.5555/3507788.3507808,
author = {Tan, Waikeat and Alhamid, Mohammed and Kalil, Mohamad and Yang, Ronghao and Corvinelli, Vincent and Zuzarte, Calisto and Finnie, Liam},
title = {Query predicate selectivity using machine learning in Db2®},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The accuracy of cardinality estimation or the number of rows flowing through the query execution plan operators plays an important role in SQL query optimization. Cost-based optimizers depend on cardinality estimation to evaluate execution costs to select an optimal access plan. Achieving accurate cardinality estimation is difficult or expensive on tables that have correlated or skewed columns. Inaccurate cardinality estimation can lead to slow or unstable query performance. Although collecting statistics on multiple column combinations can minimize estimation errors with multiple predicates, it is hard to cover all column combinations. This paper presents a novel integrated approach using Machine Learning (ML) to learn and approximate the multivariate Cumulative Frequency Function (CFF) of column values, which is used to estimate cardinality for predicates with various relational operators. The key idea is that a model can learn the distribution of the data in the relation and can be used to predict cardinality for the query predicates accurately. The CFF model is also extended to estimate join cardinalities between tables. Experimental results demonstrate a significant improvement of cardinality estimation accuracy, computation efficiency, and amount of input required to train the model. Integration with the traditional optimizer is key to a smooth transition towards use in a production environment. This paper covers earlier technology previews shipped with Db2®.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {143–152},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1007/978-3-030-79457-6_42,
author = {Quang, Do Nguyet and Selamat, Ali and Krejcar, Ondrej},
title = {Recent Research on Phishing Detection Through Machine Learning Algorithm},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_42},
doi = {10.1007/978-3-030-79457-6_42},
abstract = {The rapid growth of emerging technologies, smart devices, 5G communication, etc. have contributed to the accumulation of data, hence introducing the big data era. Big data imposes a variety of challenges associated with machine learning, especially in phishing detection. Therefore, this paper aims to provide an analysis and summary of current research in phishing detection through machine learning for big data. To achieve this goal, this study adopted a systematic literature review (SLR) technique and critically analyzed a total of 30 papers from various journals and conference proceedings. These papers were selected from previous studies in five different databases on content published between 2018 and January 2021. The results obtained from this study reveal a limited number of research works that comprehensively reviewed the feasibility of applying both machine learning and big data technologies in the context of phishing detection.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {495–508},
numpages = {14},
keywords = {Big data, Machine learning (ML), Phishing detection, Cybersecurity},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1007/s10922-020-09583-4,
author = {Awad, Mohamad Khattar and Ahmed, Marwa Hassan Hafez and Almutairi, Ali F. and Ahmad, Imtiaz},
title = {Machine Learning-Based Multipath Routing for Software Defined Networks},
year = {2021},
issue_date = {Apr 2021},
publisher = {Plenum Press},
address = {USA},
volume = {29},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-020-09583-4},
doi = {10.1007/s10922-020-09583-4},
abstract = {Network softwarization has recently been enabled via the software-defined networking (SDN) paradigm, which separates the data plane from control plane allowing for a flexible and centralized control of networks. This separation facilitates implementation of machine learning techniques for network management and optimization. In this work, a machine learning-based multipath routing (MLMR) framework is proposed for software-defined networks with quality-of-service (QoS) constraints and flow rules space constraints. The QoS-aware multipath routing problem in SDN is modeled as multicommodity network flow problem with side constraints, that is known to be NP-hard. The proposed framework utilizes network status estimates, and their corresponding routing configurations available at the network central controller to learn a mapping function between them. Once the mapping function is learned, it is applied on live-inputs of network status and routing requests to predict a multipath routing solutions in real-time. Performance evaluations of the MLMR framework on real traces of network traffic verify its accuracy and resilience to noise in training data. Furthermore, the MLMR framework demonstrates more than 98.99% improvement in computational efficiency.},
journal = {J. Netw. Syst. Manage.},
month = apr,
numpages = {30},
keywords = {Routing, Software defined networking, Software defined networks, Machine learning}
}

@article{10.1016/j.compbiomed.2021.104798,
author = {Torres, Noelia and Trujillo, Leonardo and Maldonado, Yazmin and Vera, Carlos},
title = {Correction of the travel time estimation for ambulances of the red cross Tijuana using machine learning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104798},
doi = {10.1016/j.compbiomed.2021.104798},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {13},
keywords = {Machine learning, Ambulance travel time, Mapping systems, Emergency medical services}
}

@article{10.1504/ijaip.2021.113782,
author = {Dey, Barnali and Hossain, Ashraf and Bera, Rabindranath},
title = {Possible adoption of various machine learning techniques in cognitive radio - a survey},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.113782},
doi = {10.1504/ijaip.2021.113782},
abstract = {The concept of cognitive radio (CR) system is the need for next generation wireless communication technology in terms of providing intelligence and superior performance to a wireless device. The CR is mainly an intelligent system which is aware of its environment and is well capable to adapt in accordance with the changing environment and user needs. The concept of adaptation of the communication system can be realised well with machine learning capability inculcated within the system. It is a well-known fact that, the key strengths of any machine learning paradigm is its ability to adapt with respect to the dynamic changing system parameters. In this paper, an attempt has been made to compile various applications of machine learning techniques for different activities of CR cycle. Further, this note reviews the work on development of machine learning techniques for spectrum sensing of CR in order to make the CR system as a whole practically feasible and robust, thus mitigating its existing computational limitations due to the use of conventional techniques.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {439–463},
numpages = {24},
keywords = {energy detection, spectrum sensing, machine learning, cognitive radio}
}

@article{10.1007/s10618-021-00748-6,
author = {Hu, Yifan and Hu, Changwei and Tran, Thanh and Kasturi, Tejaswi and Joseph, Elizabeth and Gillingham, Matt},
title = {What’s in a name? – gender classification of names with character based machine learning models},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00748-6},
doi = {10.1007/s10618-021-00748-6},
abstract = {Gender information is no longer a mandatory input when registering for an account at many leading Internet companies. However, prediction of demographic information such as gender and age remains an important task, especially in intervention of unintentional gender/age bias in recommender systems. Therefore it is necessary to infer the gender of those users who did not to provide this information during registration. We consider the problem of predicting the gender of registered users based on their declared name. By analyzing the first names of 100M+ users, we found that genders can be very effectively classified using the composition of the name strings. We propose a number of character based machine learning models, and demonstrate that our models are able to infer the gender of users with much higher accuracy than baseline models. Moreover, we show that using the last names in addition to the first names improves classification performance further.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1537–1563},
numpages = {27},
keywords = {Gender, Demography, Character-based machine learning model, Neural network, Natural language processing}
}

@inproceedings{10.1145/3318299.3318385,
author = {Imran, Ali Shariq and Shahrebabaki, Abdolreza Sabzi and Olfati, Negar and Svendsen, Torbj\o{}rn},
title = {A Study on the Performance Evaluation of Machine Learning Models for Phoneme Classification},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318385},
doi = {10.1145/3318299.3318385},
abstract = {This paper provides a comparative performance analysis of both shallow and deep machine learning classifiers for speech recognition task using frame-level phoneme classification. Phoneme recognition is still a fundamental and equally crucial initial step toward automatic speech recognition (ASR) systems. Often conventional classifiers perform exceptionally well on domain-specific ASR systems having a limited set of vocabulary and training data in contrast to deep learning approaches. It is thus imperative to evaluate performance of a system using deep artificial networks in terms of correctly recognizing atomic speech units, i.e., phonemes in this case with conventional state-of-the-art machine learning classifiers. Two deep learning models - DNN and LSTM with multiple configuration architectures by varying the number of layers and the number of neurons in each layer on the OLLO speech corpora along with six shallow machine learning classifiers for Filterbank acoustic features are thoroughly studied.Additionally, features with three and ten frames temporal context are computed and compared with no-context features for different models. The classifier's performance is evaluated in terms of precision, recall, and F1 score for 14 consonants and 10 vowels classes for 10 speakers with 4 different dialects. High classification accuracy of 93% and 95% F1 score is obtained with DNN and LSTM networks respectively on context-dependent features for 3-hidden layers containing 1024 nodes each. SVM surprisingly obtained even a higher classification score of 96.13% and a misclassification error of less than 5% for consonants and 4% for vowels.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {52–58},
numpages = {7},
keywords = {SVM, Phoneme Classification, Machine Learning, LSTM, Filterbank, DNN, Acoustic Features},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1007/s00357-019-09352-2,
author = {Lu, Chengbo and Mei, Ying},
title = {An Optimal Weight Semi-Supervised Learning Machine for Neural Networks with Time Delay},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {3},
issn = {0176-4268},
url = {https://doi.org/10.1007/s00357-019-09352-2},
doi = {10.1007/s00357-019-09352-2},
abstract = {In this paper, an optimal weight semi-supervised learning machine for a single-hidden layer feedforward network (SLFN) with time delay is developed. Both input weights and output weights of the SLFN are globally optimized with manifold regularization. By feature mapping, input vectors can be placed at the prescribed positions in the feature space in the sense that the separability of all nonlinearly separable patterns can be maximized, unlabeled data can be leveraged to improve the classification accuracy when labeled data are scarce, and a high degree of recognition accuracy can be achieved with a small number of hidden nodes in the SLFN. Some simulation examples are presented to show the excellent performance of the proposed algorithm.},
journal = {J. Classif.},
month = oct,
pages = {656–670},
numpages = {15},
keywords = {Time delay, Manifold regularization, Semi-supervised learning, Optimal weight learning, Neural networks}
}

@article{10.1007/s00500-020-05226-7,
author = {Khuat, Thanh Tung and Ruta, Dymitr and Gabrys, Bogdan},
title = {Hyperbox-based machine learning algorithms: a comprehensive survey},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05226-7},
doi = {10.1007/s00500-020-05226-7},
abstract = {With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.},
journal = {Soft Comput.},
month = jan,
pages = {1325–1363},
numpages = {39},
keywords = {Online learning, Clustering, Data classification, Hybrid classifiers, Fuzzy min–max neural network, Membership function, Hyperboxes}
}

@article{10.1016/j.asoc.2021.107269,
author = {Bayliss, Christopher},
title = {Machine learning based simulation optimisation for urban routing problems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107269},
doi = {10.1016/j.asoc.2021.107269},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {17},
keywords = {Metaheuristics, Machine learning, Traffic simulation, Learnheuristic, Team orienteering problem}
}

@inproceedings{10.5555/3524938.3525346,
author = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip B.},
title = {The non-IID data quagmire of decentralized machine learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {408},
numpages = {12},
series = {ICML'20}
}

@phdthesis{10.5555/AAI30013420,
author = {Nandi, Anupama and Atanas, Rountev, and Wei-Lun, Chao,},
advisor = {Raef, Bassily,},
title = {Addressing Fundamental Limitations in Differentially Private Machine Learning},
year = {2021},
isbn = {9798351442815},
publisher = {The Ohio State University},
abstract = {Differential privacy has become a widely accepted data privacy model because of the strong formal guarantee it offers, namely no individual's data has a significant impact on the outcome of analyses on the data set. Unfortunately, this strict guarantee leads to fundamental limitations pertaining to the utility of different machine learning algorithms. These limitations manifest in various problem settings.For instance, learning simple classes of functions (e.g. one-dimensional thresholds over R) is impossible under differential privacy even though it can be easily learned without privacy constraints. As another example, the resulting error in privately solving fundamental problems such as stochastic convex optimization (SCO) or empirical risk minimization (ERM) is known to incur a necessary dependence on the dimension of the problem, which limits the applicability of such private algorithms in practical scenarios where the dimension is often very large.The goal of this dissertation is to tackle some of these fundamental limitations. To circumvent these challenges, we pursue two directions: (1) relaxing the model of differentially private learning, and (2) exploiting the geometry of the learning problem.In the first direction, the goal is to exploit a limited amount of public data to achieve substantial gains in accuracy (or, equivalently savings in sample complexity) of differentially private algorithms. In particular,We study the problem of differentially private release of classification queries. In this problem, the algorithm is given a private training dataset drawn from some unknown distribution and a stream of classification queries given by a sequence unlabeled feature vectors. Here, the feature-vectors defining the set of queries are assumed to be public (i.e., they do not involve any privacy constraints), and drawn from the same distribution as the feature vectors of the private dataset. We formally study this problem in the agnostic probably approximately correct (PAC) learning model and give a construction with formal guarantees on the sample complexity. In particular, given any hypothesis class with VC-dimension d, we show that our construction can privately answer up to m classification queries with average excess error α using a private sample of size [formula omitted] (assuming the privacy parameter [formula omitted]). We also extend our construction to show that one can privately answer any number of classification queries with average excess error α using a private sample of size [formula omitted]. When [formula omitted] and the privacy parameter [formula omitted], our private sample complexity bound is essentially optimal.Next, we examine a new model of supervised learning under privacy constraints in which a learning algorithm has access to a mixed dataset of private and public examples that arise from possibly different populations (distributions). In particular, the target population D is a mixture of two sub-populations: a private sub-population Dpriv of private and sensitive data, and a public sub-population Dpub of data with no privacy concerns. Each example drawn from D is assumed to contain a privacy-status bit that indicates whether the example is private or public. Here the goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. Prior works that studied utilizing public data in differentially private learning assumed that the private and public data come from the same distribution. We give a construction for learning linear classifiers in Rd and show that such an assumption can be circumvented. We show that in the case where the privacy status is correlated with the target label, linear classifiers in Rd can be learned, in the agnostic as well as the realizable setting, with sample complexity which is comparable to that of the classical (non-private) PAC-learning.The second direction we explore aims at exploiting the geometry of the learning problem to provide better accuracy guarantees for differentially private stochastic convex optimization (DP-SCO). In DP-SCO, the goal is to minimize the population risk [formula omitted] for convex loss functions of x over some feasible set given access to i.i.d. samples z1,...,zn from a data distribution D, under the constraint of differential privacy. Prior works in DP-SCO have focused only on the Euclidean setting (ℓ setting), where both the feasible set and the subgradients of the loss are bounded in the ℓ2 norm. For this setting, existing results show that the optimal rate for the population risk of DP-SCO has a necessary dependence on √d, where d is the dimensionality of the problem. In this dissertation, we initiate a systematic study of DP-SCO in non-Euclidean settings, where the geometry of the feasible set and the space of the sub-gradients of the loss are described in non-Euclidean norms (e.g. ℓp norm). Particularly for the polyhedral setup, where the feasible set is polyhedral, and the losses are convex and smooth w.r.t. a polyhedral norm (e.g. the ℓ1 norm), we give the first linear-time DP-SCO algorithm. Our excess risk guarantee is nearly optimal and nearly independent of the dimension. We extend our study to DP-SCO over non-Euclidean settings, particularly, over ℓp-normed spaces for [formula omitted]. For p ∈ (1,∞), we derive a lower bound on the excess risk for this range of p showing a necessary dependence on √d, where d is the dimensionality of the ℓp space. For p ∈ (1,2), under standard smoothness assumption, we give the first linear-time algorithm with nearly optimal excess risk. For p ∈ (2, ∞), existing noisy stochastic gradient methods attain optimal excess risk in the low-dimensional regime.},
note = {AAI30013420}
}

@inproceedings{10.1145/3340482.3342743,
author = {Foidl, Harald and Felderer, Michael},
title = {Risk-based data validation in machine learning-based software systems},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342743},
doi = {10.1145/3340482.3342743},
abstract = {Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Risk-based Testing, Machine Learning, Data Validation},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1145/3359786,
author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
title = {Techniques for interpretable machine learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3359786},
doi = {10.1145/3359786},
abstract = {Uncovering the mysterious ways machine learning models make decisions.},
journal = {Commun. ACM},
month = dec,
pages = {68–77},
numpages = {10}
}

@article{10.1007/s00521-020-05497-z,
author = {Kasinathan, Thenmozhi and Uyyala, Srinivasulu Reddy},
title = {Machine learning ensemble with image processing for pest identification and classification in field crops},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {13},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05497-z},
doi = {10.1007/s00521-020-05497-z},
abstract = {In agriculture field, yield loss is a major problem due to attack of various insects in field crops. Traditional insect identification and classification methods are time-consuming and require entomologist experts. Early information about the attack of insects helps farmers to control the crop damage to improve the productivity and reduce the use of pesticides. This research work focuses on the classification of crop insects by applying machine vision and knowledge-based techniques with image processing by using different feature descriptors including texture, color, shape, histogram of oriented gradients&nbsp;(HOG) and global image descriptor&nbsp;(GIST). A combination of all these features was used in the classification of insects. In this research, several machine learning algorithms including both base classifiers and ensemble classifiers were applied for three different insect datasets and the performances of classification results were evaluated by majority voting. Naive bayes&nbsp;(NB), support vector machine&nbsp;(SVM), K-nearest-neighbor&nbsp;(KNN) and multi-layer perceptron&nbsp;(MLP) were used as base classifiers. Ensemble classifiers include random forest&nbsp;(RF), bagging and XGBoost were utilized; 10-fold cross-validation test was conducted to achieve a better classification and identification of insects. The experimental results showed that the classification accuracy is improved by majority voting with ensemble classifiers in the combination of texture, color, shape, HOG and GIST features.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {7491–7504},
numpages = {14},
keywords = {Majority voting, Machine learning algorithm, Insect classification, Image processing, Ensemble classification, Crops}
}

@inproceedings{10.1145/3278721.3278722,
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
title = {Non-Discriminatory Machine Learning through Convex Fairness Criteria},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278722},
doi = {10.1145/3278721.3278722},
abstract = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {116},
numpages = {1},
keywords = {proportional fairness, non-discrimination, machine learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3486609.3487199,
author = {Atouani, Abdallah and Kirchhof, J\"{o}rg Christian and Kusmenko, Evgeny and Rumpe, Bernhard},
title = {Artifact and reference models for generative machine learning frameworks and build systems},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487199},
doi = {10.1145/3486609.3487199},
abstract = {Machine learning is a discipline which has become ubiquitous in the last few years. While the research of machine learning algorithms is very active and continues to reveal astonishing possibilities on a regular basis, the wide usage of these algorithms is shifting the research focus to the integration, maintenance, and evolution of AI-driven systems. Although there is a variety of machine learning frameworks on the market, there is little support for process automation and DevOps in machine learning-driven projects. In this paper, we discuss how metamodels can support the development of deep learning frameworks and help deal with the steadily increasing variety of learning algorithms. In particular, we present a deep learning-oriented artifact model which serves as a foundation for build automation and data management in iterative, machine learning-driven development processes. Furthermore, we show how schema and reference models can be used to structure and maintain a versatile deep learning framework. Feasibility is demonstrated on several state-of-the-art examples from the domains of image and natural language processing as well as decision making and autonomous driving.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {55–68},
numpages = {14},
keywords = {training, reference models, metamodeling, machine learning, compiler, build systems, artificial intelligence, artifact models},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@article{10.3233/IP-200264,
author = {Keen, Justin and Ruddle, Roy and Palczewski, Jan and Aivaliotis, Georgios and Palczewska, Anna and Megone, Christopher and Macnish, Kevin},
title = {Machine learning, materiality and governance: A health and social care case study},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {1},
issn = {1570-1255},
url = {https://doi.org/10.3233/IP-200264},
doi = {10.3233/IP-200264},
abstract = {There is a widespread belief that machine learning tools can be used to improve decision-making in health and social care. At the same time, there are concerns that they pose threats to privacy and confidentiality. Policy makers therefore need to develop governance arrangements that balance benefits and risks associated with the new tools. This article traces the history of developments of information infrastructures for secondary uses of personal datasets, including routine reporting of activity and service planning, in health and social care. The developments provide broad context for a study of the governance implications of new tools for the analysis of health and social care datasets. We find that machine learning tools can increase the capacity to make inferences about the people represented in datasets, although the potential is limited by the poor quality of routine data, and the methods and results are difficult to explain to other stakeholders. We argue that current local governance arrangements are piecemeal, but at the same time reinforce centralisation of the capacity to make inferences about individuals and populations. They do not provide adequate oversight, or accountability to the patients and clients represented in datasets.},
journal = {Info. Pol.},
month = jan,
pages = {57–69},
numpages = {13},
keywords = {social care, health care, information infrastructure, accountability, governance, Machine learning}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Software product lines, Longitudinal study, Fault proneness prediction, Cross-release reuse, Cross-product reuse, Cross-product prediction, Assessment}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous integration, DevOps, Machine learning, Continuous defect prediction}
}

@article{10.1016/j.ins.2016.04.040,
author = {Forestier, Germain and Wemmert, C\'{e}dric},
title = {Semi-supervised learning using multiple clusterings with limited labeled data},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {361},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.04.040},
doi = {10.1016/j.ins.2016.04.040},
abstract = {Supervised classification consists in learning a predictive model using a set of labeled samples. It is accepted that predictive models accuracy usually increases as more labeled samples are available. Labeled samples are generally difficult to obtain as the labeling step if often performed manually. On the contrary, unlabeled samples are easily available. As the labeling task is tedious and time consuming, users generally provide a very limited number of labeled objects. However, designing approaches able to work efficiently with a very limited number of labeled samples is highly challenging. In this context, semi-supervised approaches have been proposed to leverage from both labeled and unlabeled data.In this paper, we focus on cases where the number of labeled samples is very limited. We review and formalize eight semi-supervised learning algorithms and introduce a new method that combine supervised and unsupervised learning in order to use both labeled and unlabeled data. The main idea of this method is to produce new features derived from a first step of data clustering. These features are then used to enrich the description of the input data leading to a better use of the data distribution. The efficiency of all the methods is compared on various artificial, UCI datasets, and on the classification of a very high resolution remote sensing image. The experiments reveal that our method shows good results, especially when the number of labeled sample is very limited. It also confirms that combining labeled and unlabeled data is very useful in pattern recognition.},
journal = {Inf. Sci.},
month = sep,
pages = {48–65},
numpages = {18},
keywords = {Semi-supervised learning, Remote sensing, Pattern recognition, Classification}
}

@inproceedings{10.1145/3468218.3469051,
author = {Luo, Zhengping and Zhao, Shangqing and Duan, Rui and Lu, Zhuo and Sagduyu, Yalin E. and Xu, Jie},
title = {Low-cost Influence-Limiting Defense against Adversarial Machine Learning Attacks in Cooperative Spectrum Sensing},
year = {2021},
isbn = {9781450385619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468218.3469051},
doi = {10.1145/3468218.3469051},
abstract = {Cooperative spectrum sensing aims to improve the reliability of spectrum sensing by individual sensors for better utilization of the scarce spectrum bands, which gives the feasibility for secondary spectrum users to transmit their signals when primary users remain idle. However, there are various vulnerabilities experienced in cooperative spectrum sensing, especially when machine learning techniques are applied. The influence-limiting defense is proposed as a method to defend the data fusion center when a small number of spectrum sensing devices is controlled by an intelligent attacker to send erroneous sensing results. Nonetheless, this defense suffers from a computational complexity problem. In this paper, we propose a low-cost version of the influence-limiting defense and demonstrate that it can decrease the computation cost significantly (the time cost is reduced to less than 20% of the original defense) while still maintaining the same level of defense performance.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning},
pages = {55–60},
numpages = {6},
keywords = {machine learning, defense, data fusion, attack, adversarial machine learning, Cooperative spectrum sensing},
location = {Abu Dhabi, United Arab Emirates},
series = {WiseML '21}
}

@inproceedings{10.1145/3409334.3452059,
author = {Qiu, Jiabao and Moh, Melody and Moh, Teng-Sheng},
title = {Fast streaming translation using machine learning with transformer},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452059},
doi = {10.1145/3409334.3452059},
abstract = {Machine Translation is the usage of machine learning techniques in translation from one language to another. It has recently been applied to streaming translation, also known as automatic subtitling. The most common challenge in this area is the trade-off between correctness and speed. Due to its real-time feature, streaming translation needs high speed as it has strict playtime constraints. This paper proposes an enhanced Transformer model for fast streaming translation. The proposed machine-learning method is described, implemented, and evaluated based on a common German-English bilingual dataset. The evaluation results have shown that the proposed system successfully achieved a good speed in the training phase, and a high speed in the actual translating phrase that is fast enough for real-time applications, while also maintaining robust correctness. We believe the proposed Transformer model is a significant contribution to natural-language processing, and would be useful for other real-time translation applications.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {9–16},
numpages = {8},
keywords = {machine learning, machine translation, natural language processing, neural networks},
location = {Virtual Event, USA},
series = {ACMSE '21}
}

@article{10.3233/JIFS-189498,
author = {Wang, Lejie and Ramachandran, Varatharajan},
title = {Improving the performance of precision poverty alleviation based on big data mining and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189498},
doi = {10.3233/JIFS-189498},
abstract = {Since the reform began in our country, with the rapid economic growth in recent years, the income level has grown extremely unequal, and it is difficult for the low-income poor to benefit from the rapid economic growth. The most important prerequisite for the fight against poverty is the accurate identification of the causes of poverty. To date, our country has not reached the level of maturity required to accurately study the causes of poverty in various households. However, with the rapid development of Internet technology and big data technology in recent years, the application of large-scale data technology and data extraction algorithms to poverty reduction can identify truly poor households faster and more accurately. Compared with traditional machine learning algorithms, there are no machine storage and technical constraints, can use a large amount of data and rely on multiple data samples.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6617–6628},
numpages = {12},
keywords = {Big data mining, machine learning, precision poverty alleviation, performance improvement}
}

@article{10.1016/j.tcs.2015.06.048,
author = {Anselmi, Fabio and Leibo, Joel Z. and Rosasco, Lorenzo and Mutch, Jim and Tacchetti, Andrea and Poggio, Tomaso},
title = {Unsupervised learning of invariant representations},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {633},
number = {C},
issn = {0304-3975},
url = {https://doi.org/10.1016/j.tcs.2015.06.048},
doi = {10.1016/j.tcs.2015.06.048},
abstract = {The present phase of Machine Learning is characterized by supervised learning algorithms relying on large sets of labeled examples ( n \'{z} ∞ ). The next phase is likely to focus on algorithms capable of learning from very few labeled examples ( n \'{z} 1 ), like humans seem able to do. We propose an approach to this problem and describe the underlying theory, based on the unsupervised, automatic learning of a "good" representation for supervised learning, characterized by small sample complexity. We consider the case of visual object recognition, though the theory also applies to other domains like speech. The starting point is the conjecture, proved in specific cases, that image representations which are invariant to translation, scaling and other transformations can considerably reduce the sample complexity of learning. We prove that an invariant and selective signature can be computed for each image or image patch: the invariance can be exact in the case of group transformations and approximate under non-group transformations. A module performing filtering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such signature. The theory offers novel unsupervised learning algorithms for "deep" architectures for image and speech recognition. We conjecture that the main computational goal of the ventral stream of visual cortex is to provide a hierarchical representation of new objects/images which is invariant to transformations, stable, and selective for recognition-and show how this representation may be continuously learned in an unsupervised way during development and visual experience.},
journal = {Theor. Comput. Sci.},
month = jun,
pages = {112–121},
numpages = {10},
keywords = {Invariance, Hierarchy, Cortex, Convolutional networks}
}

@inproceedings{10.1007/978-3-030-58586-0_11,
author = {Im, Woobin and Kim, Tae-Kyun and Yoon, Sung-Eui},
title = {Unsupervised Learning of Optical Flow with Deep Feature Similarity},
year = {2020},
isbn = {978-3-030-58585-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58586-0_11},
doi = {10.1007/978-3-030-58586-0_11},
abstract = {Deep unsupervised learning for optical flow has been proposed, where the loss measures image similarity with the warping function parameterized by estimated flow. The census transform, instead of image pixel values, is often used for the image similarity. In this work, rather than the handcrafted features i.e. census or pixel values, we propose to use deep self-supervised features with a novel similarity measure, which fuses multi-layer similarities. With the fused similarity, our network better learns flow by minimizing our proposed feature separation loss. The proposed method is a polarizing scheme, resulting in a more discriminative similarity map. In the process, the features are also updated to get high similarity for matching pairs and low for uncertain pairs, given estimated flow. We evaluate our method on FlyingChairs, MPI Sintel, and KITTI benchmarks. In quantitative and qualitative comparisons, our method effectively improves the state-of-the-art techniques.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV},
pages = {172–188},
numpages = {17},
keywords = {Unsupervised, Self-supervised, Optical flow, Deep feature, Similarity},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/GLOBECOM46510.2021.9685681,
author = {Nacef, Abdelhakim and Bagaa, Miloud and Aklouf, Youcef and Kaci, Abdellah and Dutra, Diego Leonel Cadette and Ksentini, Adlen},
title = {Self-optimized network: When Machine Learning Meets Optimization},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM46510.2021.9685681},
doi = {10.1109/GLOBECOM46510.2021.9685681},
abstract = {The fifth generation of the mobile network aims to revolutionize mobile communication by offering both unparalleled performance and broader service offerings. 5G technology responds to the growing demand for higher bandwidth and lower latency, caused by a significant increase of connected resources. Leveraging on Software-Defined Networking (SDN) and artificial intelligence (AI) technologies, the 6G system can autonomously adapt to user requirements. This paper proposes a framework, named intelligent optimization framework (IoF), that leverages both network optimization and machine learning techniques for achieving the best performance results. The IoF framework allows for finding an exemplary resource allocation configuration of the mobile network by leveraging SDN technology. This work aims to configure the SDN-enabled switches and Open vSwitchs (OVSs) to enable an optimized data plane that reduces the overall operational expense (OPEX) cost and capital expense (CAPEX) cost within an optimized execution time. The evaluation results show our proposed framework's efficiency for delivering optimal configurations by reducing the number of allocated OVSs in a reasonable execution time.},
booktitle = {2021 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Madrid, Spain}
}

@article{10.1016/j.jpdc.2019.07.007,
author = {Garc\'{\i}a-Mart\'{\i}n, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, H\r{a}kan},
title = {Estimation of energy consumption in machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.07.007},
doi = {10.1016/j.jpdc.2019.07.007},
journal = {J. Parallel Distrib. Comput.},
month = dec,
pages = {75–88},
numpages = {14},
keywords = {High performance computing, Deep learning, Energy consumption, GreenAI, Machine learning}
}

@inproceedings{10.1145/3368089.3418538,
author = {\v{C}egi\v{n}, J\'{a}n},
title = {Machine learning based test data generation for safety-critical software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418538},
doi = {10.1145/3368089.3418538},
abstract = {Unit testing focused on Modified Condition/Decision Coverage (MC/DC) criterion is essential in development safety-critical systems. However, design of test data that meets the MC/DC criterion currently needs detailed manual analysis of branching conditions in units under test by test engineers. Multiple state-of-art approaches exist with proven usage even in industrial projects. However, these approaches have multiple shortcomings, one of them being the Path explosion problem which has not been fully solved yet. Machine learning methods as meta-heuristic approximations can model behaviour of programs that are hard to test using traditional approaches, where the Path explosion problem does occur and thus could solve the limitations of the current state-of-art approaches. I believe, motivated by an ongoing collaboration with an industrial partner, that the machine learning methods could be combined with existing approaches to produce an approach suitable for testing of safety-critical projects.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1678–1681},
numpages = {4},
keywords = {unit testing, test data generation, machine learning, MC/DC criterion},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3469968.3469973,
author = {Li, Haomiao and Jin, Zian and Krishnamoorthy, Sujatha},
title = {E-Waste Management Using Machine Learning},
year = {2021},
isbn = {9781450389808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469968.3469973},
doi = {10.1145/3469968.3469973},
abstract = {In the realm of innovation, as individuals begin adjusting to new development, advancement begins clearing its way into individuals' lives. Each adjustment in innovation gets better than ever, with old gadgets being supplanted and deserted. Such Electronic and Electrical Equipments (EEEs) that are disposed of by clients are named e-waste. EEEs are comprised of a large number of segments, some containing poisonous substances that affect human wellbeing and climate, if not handled appropriately. The volume of EEEs that is delivered all through the world, has driven governments in different nations to make severe arrangements, to guarantee effective removal of the created e-waste. This paper presents the utilization of ML (Machine Learning) for E-Waste Management framework with a center to build up a prescient model by contrasting the presentation of gradient boosting regression tree (GBRT) and Neural Network (NN) ML calculations to gauge week by week e-wastage for every Urban sub-segments. Various information-driven strategies incorporate the current designed model and its alteration alongside regular machine learning calculations. The utilization of Machine Learning calculation gives improved arrangement exactness of 99.1 % when utilizing the best performing algorithm. Notwithstanding the prominent quantitative upgrades, the proposed plan can likewise help in enhancing long-haul e-waste management in smart-city ambient utilizing the recorded insights.},
booktitle = {Proceedings of the 6th International Conference on Big Data and Computing},
pages = {30–35},
numpages = {6},
keywords = {data mining, Random Forest algorithm, E-waste management},
location = {Shenzhen, China},
series = {ICBDC '21}
}

@inproceedings{10.1007/978-3-030-60884-2_7,
author = {Espinosa, Ricardo and Ponce, Hiram and Guti\'{e}rrez, Sebasti\'{a}n and Hern\'{a}ndez, Eluney},
title = {Click Event Sound Detection Using Machine Learning in Automotive Industry},
year = {2020},
isbn = {978-3-030-60883-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60884-2_7},
doi = {10.1007/978-3-030-60884-2_7},
abstract = {Artificial intelligence has been playing an important role when it comes to the automotive industry and its quality of assemblies in the production line, this is because since the arrival of the industry 4.0 it has been subject to change and continuous improvement. In the past, we’ve observed how many machine learning architectures have been used to create environmental sound classification systems in order to improve traditional systems, thus overcoming efficiency issues with great results. In this work, we present a machine learning solution/approach for click event sound detection using audio sensors that are used in the assembly of electric harnesses for engines, this being done on an automotive production line, where we divided our workflow into: data collection, pre-processing, feature extraction, training and inference and finally the detection of the click event sounds. We created a dataset that is composed by 25,000 audio files that have an average duration of 0.025 seconds per click sound with the purpose of training a Multi-layer Perceptron and bring it into the inference phase. In order to test this approach, we’ve performed various implementations in a laboratory and in the real automotive industry. We obtained 95.23% in F1-Score Metric in a laboratory, while in real conditions, we obtained less reliable results, as 84.00% as the best results.},
booktitle = {Advances in Soft Computing: 19th Mexican International Conference on Artificial Intelligence, MICAI 2020, Mexico City, Mexico, October 12–17, 2020, Proceedings, Part I},
pages = {88–103},
numpages = {16},
keywords = {Supervised learning, Signal spectral characteristics, Neural network, MLP, Machine learning, Feature extraction, Events sound recognition, Audio signal processing},
location = {Mexico City, Mexico}
}

@article{10.1007/s11633-020-1233-4,
author = {Aljanabi, Maryam and Shkoukani, Mohammad and Hijjawi, Mohammad},
title = {Ground-level Ozone Prediction Using Machine Learning Techniques: A Case Study in Amman, Jordan},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-020-1233-4},
doi = {10.1007/s11633-020-1233-4},
abstract = {Air pollution is one of the most serious hazards to humans' health nowadays, it is an invisible killer that takes many human lives every year. There are many pollutants existing in the atmosphere today, ozone being one of the most threatening pollutants. It can cause serious health damage such as wheezing, asthma, inflammation, and early mortality rates. Although air pollution could be forecasted using chemical and physical models, machine learning techniques showed promising results in this area, especially artificial neural networks. Despite its importance, there has not been any research on predicting ground-level ozone in Jordan. In this paper, we build a model for predicting ozone concentration for the next day in Amman, Jordan using a mixture of meteorological and seasonal variables of the previous day. We compare a multi-layer perceptron neural network (MLP), support vector regression (SVR), decision tree regression (DTR), and extreme gradient boosting (XGBoost) algorithms. We also explore the effect of applying various smoothing filters on the time-series data such as moving average, Holt-Winters smoothing and Savitzky-Golay filters. We find that MLP outperformed the other algorithms and that using Savitzky-Golay improved the results by 50% for coefficient of determination (R2) and 80% for root mean square error (RMSE) and mean absolute error (MAE). Another point we focus on is the variables required to predict ozone concentration. In order to reduce the time required for prediction, we perform feature selection which greatly reduces the time by 91% as well as shrinking the number of features required for prediction to the previous day values of ozone, humidity, and temperature. The final model scored 98.653% for R2, 1.016 ppb for RMSE and 0.800 ppb for MAE.},
journal = {Int. J. Autom. Comput.},
month = oct,
pages = {667–677},
numpages = {11},
keywords = {regression, supervised learning, neural networks, machine learning, Ozone prediction}
}

@article{10.1145/3422993,
author = {Mesanza-Moraza, Amaia and Garc\'{\i}a-G\'{o}mez, Ismael and Azkarate, Agust\'{\i}n},
title = {Machine Learning for the Built Heritage Archaeological Study},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3422993},
doi = {10.1145/3422993},
abstract = {The presence of artificial intelligence in our lives is increasing and being applied to fields such as medicine, engineering, telecommunications, remote sensing and 3D visualization. Nevertheless, it has never been used for the stratigraphic study of historical buildings. Thus far, archaeologists and architects, the experts in archaeology of architecture, have led this research. The method consisted of visually—and, consequently, subjectively—identifying certain evidence regarding the elevations of such buildings that could be a consequence of the passage of time. In this article, we would like to present the results from one of the research projects pursued by our group, in which we automated the stratigraphic study of some historic buildings using multivariate statistic techniques. To this end, we first measured the building using surveying techniques to create a 3D model, and then, we broke down every stone into qualitative and quantitative variables. To identify the stratigraphic features on the walls, we applied machine learning by conducting different predictive and descriptive analyses. The predictive analyses were used to rule out any blocks of stone with different characteristics, such as rough stones, joint ashlars, and voussoirs of arches; these are irregularities that probably show building processes and whose identification is crucial in ascertaining the structural evolution of the building. In supervised learning, we experimented with decision trees and random forest—and although the results were good in all cases, we ultimately opted to implement the predictive model obtained using the last one. While identifying the evidence on the walls, it was also very important to identify different continuity solutions or interfaces present on them, because although these are elements without materiality, they are of great value in terms of timescale, because they delimit different strata and allow us to deduce the relationship between them.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {stratigraphic analysis, multivariate analysis, machine learning, data mining, built heritage, building archaeology, Archaeology of Architecture}
}

@inproceedings{10.1145/3318464.3386126,
author = {Liberty, Edo and Karnin, Zohar and Xiang, Bing and Rouesnel, Laurence and Coskun, Baris and Nallapati, Ramesh and Delgado, Julio and Sadoughi, Amir and Astashonok, Yury and Das, Piali and Balioglu, Can and Chakravarty, Saswata and Jha, Madhav and Gautier, Philip and Arpin, David and Januschowski, Tim and Flunkert, Valentin and Wang, Yuyang and Gasthaus, Jan and Stella, Lorenzo and Rangapuram, Syama and Salinas, David and Schelter, Sebastian and Smola, Alex},
title = {Elastic Machine Learning Algorithms in Amazon SageMaker},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386126},
doi = {10.1145/3318464.3386126},
abstract = {There is a large body of research on scalable machine learning (ML). Nevertheless, training ML models on large, continuously evolving datasets is still a difficult and costly undertaking for many companies and institutions. We discuss such challenges and derive requirements for an industrial-scale ML platform. Next, we describe the computational model behind Amazon SageMaker, which is designed to meet such challenges. SageMaker is an ML platform provided as part of Amazon Web Services (AWS), and supports incremental training, resumable and elastic learning as well as automatic hyperparameter optimization. We detail how to adapt several popular ML algorithms to its computational model. Finally, we present an experimental evaluation on large datasets, comparing SageMaker to several scalable, JVM-based implementations of ML algorithms, which we significantly outperform with regard to computation time and cost.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {731–737},
numpages = {7},
keywords = {scalable machine learning, elastic machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1007/978-3-030-86044-8_5,
author = {Caporuscio, Mauro and De Toma, Marco and Muccini, Henry and Vaidhyanathan, Karthik},
title = {A Machine Learning Approach to Service Discovery for Microservice Architectures},
year = {2021},
isbn = {978-3-030-86043-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86044-8_5},
doi = {10.1007/978-3-030-86044-8_5},
abstract = {Service discovery mechanisms have continuously evolved during the last years to support the effective and efficient service composition in large-scale microservice applications. Still, the dynamic nature of services (and of their contexts) are being rarely taken into account for maximizing the desired quality of service. This paper proposes using machine learning techniques, as part of the service discovery process, to select microservice instances in a given context, maximize QoS, and take into account the continuous changes in the execution environment. Both deep neural networks and reinforcement learning techniques are used. Experimental results show how the proposed approach outperforms traditional service discovery mechanisms.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021, Virtual Event, Sweden, September 13-17, 2021, Proceedings},
pages = {66–82},
numpages = {17},
keywords = {Microservices architecture, Machine learning, Service discovery}
}

@article{10.1016/j.jnca.2021.103186,
author = {Cheng, Qiumei and Wu, Chunming and Zhou, Haifeng and Kong, Dezhang and Zhang, Dong and Xing, Junchi and Ruan, Wei},
title = {Machine learning based malicious payload identification in software-defined networking},
year = {2021},
issue_date = {Oct 2021},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {192},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2021.103186},
doi = {10.1016/j.jnca.2021.103186},
journal = {J. Netw. Comput. Appl.},
month = oct,
numpages = {12},
keywords = {Linear prediction, Machine learning, Deep packet inspection, Software-defined networking}
}

@inproceedings{10.1145/3460112.3471966,
author = {Beery, Sara and Cole, Elijah and Parker, Joseph and Perona, Pietro and Winner, Kevin},
title = {Species Distribution Modeling for Machine Learning Practitioners: A Review},
year = {2021},
isbn = {9781450384537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460112.3471966},
doi = {10.1145/3460112.3471966},
abstract = {Conservation science depends on an accurate understanding of what’s happening in a given ecosystem. How many species live there? What is the makeup of the population? How is that changing over time? Species Distribution Modeling (SDM) seeks to predict the spatial (and sometimes temporal) patterns of species occurrence, i.e. where a species is likely to be found. The last few years have seen a surge of interest in applying powerful machine learning tools to challenging problems in ecology [2, 5, 8]. Despite its considerable importance, SDM has received relatively little attention from the computer science community. Our goal in this work is to provide computer scientists with the necessary background to read the SDM literature and develop ecologically useful ML-based SDM algorithms. In particular, we introduce key SDM concepts and terminology, review standard models, discuss data availability, and highlight technical challenges and pitfalls.},
booktitle = {Proceedings of the 4th ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {329–348},
numpages = {20},
keywords = {species distribution modeling, machine learning, ecological niche modeling},
location = {Virtual Event, Australia},
series = {COMPASS '21}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {test prioritization, machine learning, continuous integration, CI},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3318299.3318300,
author = {Wu, Qingyang},
title = {A Review of Methods Used in Machine Learning and Data Analysis},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318300},
doi = {10.1145/3318299.3318300},
abstract = {This report provides an overview of machine learning and data analysis with explanation of the advantages and disadvantages of different methods. I also demonstrate a practical implementation of the described methods on a dataset of real estate prices.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {43–51},
numpages = {9},
keywords = {principal component analysis, machine learning, Data exploration},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1007/s10915-019-00908-3,
author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier},
title = {Machine Learning for Semi Linear PDEs},
year = {2019},
issue_date = {June      2019},
publisher = {Plenum Press},
address = {USA},
volume = {79},
number = {3},
issn = {0885-7474},
url = {https://doi.org/10.1007/s10915-019-00908-3},
doi = {10.1007/s10915-019-00908-3},
abstract = {Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.},
journal = {J. Sci. Comput.},
month = jun,
pages = {1667–1712},
numpages = {46},
keywords = {Semilinear PDEs, Monte-Carlo methods, Machine learning, Deep learning}
}

@inproceedings{10.5555/3540261.3541519,
author = {von K\"{u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch\"{o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
title = {Self-supervised learning with data augmentations provably isolates content from style},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1258},
numpages = {17},
series = {NIPS '21}
}

@inproceedings{10.1145/3480433.3480444,
author = {Phiasai, Tejtasin and Chinpanthana, Nutchanun},
title = {Deep Belief Network based Machine Learning for Daily Activities Classification},
year = {2021},
isbn = {9781450384148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480433.3480444},
doi = {10.1145/3480433.3480444},
abstract = {Human activity recognition has been a very active topic in pervasive computing for several years for its important applications in assisted living, healthcare, and security surveillance. Many researchers are finding and representing the details of human body gestures to determine human activity. While simple activities can be easily recognized only by acceleration data, our research has focused on the recognition and understanding the various activities in daily living. In this work, we address this problem by proposing approach theory of deep learning with the Deep belief network. Deep belief network comprises a series of Restricted Boltzmann Machines will be formed by superimposed multiple Restricted Boltzmann Machines and training the model parameters for data reconstruction, feature construction and classification. We tested our approach on PASCAL VOC datasets. The experimental results indicate that our proposed approach offers significant performance improvements with the maximum of 79.8%.},
booktitle = {2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR)},
pages = {83–88},
numpages = {6},
keywords = {human activity classification, deep learning, classification, Image processing},
location = {Kumamoto, Japan},
series = {AIVR 2021}
}

@article{10.14778/3476311.3476408,
author = {Jindal, Alekh and Interlandi, Matteo},
title = {Machine learning for cloud data systems: the progress so far and the path forward},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476408},
doi = {10.14778/3476311.3476408},
abstract = {The goal of this tutorial is to educate the audience about the state of the art in ML for cloud data systems, both in research and in practice. The tutorial is divided in two parts: the progress, and the path forward.Part I covers the recent successes in deploying machine learning solutions for cloud data systems. We will discuss the practical considerations taken into account and the progress made at various levels. The goal is to compare and contrast the promise of ML for systems with the ground actually covered in industry.Finally, Part II discusses practical issues of machine learning in the enterprise covering the generation of explanations, model debugging, model deployment, model management, constraints on eyes-on data usage and anonymization, and a discussion of the technical debt that can accrue through machine learning and models in the enterprise.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3202–3205},
numpages = {4}
}

@inproceedings{10.1145/3329486.3329489,
author = {Louren\c{c}o, Raoni and Freire, Juliana and Shasha, Dennis},
title = {Debugging Machine Learning Pipelines},
year = {2019},
isbn = {9781450367974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329486.3329489},
doi = {10.1145/3329486.3329489},
abstract = {Machine learning tasks entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous or uninformative outputs, the pipeline may fail or produce incorrect results. Inferring the root cause of failures and unexpected behavior is challenging, usually requiring much human thought, and is both time consuming and error prone. We propose a new approach that makes use of iteration and provenance to automatically infer the root causes and derive succinct explanations of failures. Through a detailed experimental evaluation, we assess the cost, precision, and recall of our approach compared to the state of the art. Our source code and experimental data will be available for reproducibility and enhancement.},
booktitle = {Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning},
articleno = {3},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {DEEM'19}
}

@article{10.3233/SW-200388,
author = {Hitzler, Pascal and Janowicz, Krzysztof and d’Amato, Claudia},
title = {Machine Learning for the Semantic Web: Lessons learnt and next research directions},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {11},
number = {1},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-200388},
doi = {10.3233/SW-200388},
abstract = {Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed, jointly with the analysis of their main peculiarities and drawbacks. Afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.},
journal = {Semant. Web},
month = jan,
pages = {195–203},
numpages = {9},
keywords = {numeric-based methods, symbol-based methods, Machine Learning}
}

@inproceedings{10.1145/3341162.3344854,
author = {Balabka, Dmitrijs},
title = {Semi-supervised learning for human activity recognition using adversarial autoencoders},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3344854},
doi = {10.1145/3341162.3344854},
abstract = {SHL recognition challenge 2019 goal is to recognize eight locomotion and transportation (activities) from the inertial sensor data of a smartphone. The dataset contains information from different mobile-phones placement (torso, bag, hips, hand). Participants must provide their predictions based on test data that contains Hand phone sensors information. Only a small amount of Hand phone labeled data exists in the validation data. Train data consists only of torso, bag and hips placed mobile devices. Team DB proposes to apply deep semi-supervised learning. As the base for our model, we have chosen Adversarial Autoencoder (AAE) and employ Convolutional Networks for feature extraction. We prove that semi-supervised learning gives possibility to utilize test unlabeled data during AAE training with small amount of validation labeled data and achieve high model accuracy for Human Activity Recognition task.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {685–688},
numpages = {4},
keywords = {semi-supervised learning, human activity recognition, adversarial autoencoder, CNN},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3315508.3329972,
author = {Baudart, Guillaume and Hirzel, Martin and Kate, Kiran and Mandel, Louis and Shinnar, Avraham},
title = {Machine learning in Python with no strings attached},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315508.3329972},
doi = {10.1145/3315508.3329972},
abstract = {Machine-learning frameworks in Python, such as scikit-learn, Keras, Spark, or Pyro, use embedded domain specific languages (EDSLs) to assemble a computational graph. Unfortunately, these EDSLs make heavy use of strings as names for computational graph nodes and other entities, leading to repetitive and hard-to-maintain code that does not benefit from standard Python tooling. This paper proposes eliminating strings where possible, reusing Python variable names instead. We demonstrate this on two examples from opposite ends of the design space: Keras.na, a light-weight wrapper around the Keras library, and , a new embedding of Stan into Python. Our techniques do not require modifications to the underlying library. Avoiding strings removes redundancy, simplifies maintenance, and enables Python tooling to better reason about the code and assist users.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {1–9},
numpages = {9},
keywords = {Programming Languages, Machine Learning},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}

@article{10.1016/j.eswa.2021.114645,
author = {Adegboye, Adesola and Kampouridis, Michael},
title = {Machine learning classification and regression models for predicting directional changes trend reversal in FX markets},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114645},
doi = {10.1016/j.eswa.2021.114645},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {15},
keywords = {Machine learning, Forex/FX, Genetic programming, Classification, Regression, Directional changes}
}

@inproceedings{10.1145/3457682.3457690,
author = {Lasri, Imane and RiadSolh, Anouar and El Belkacemi, Mourad},
title = {Toward an Effective Analysis of COVID-19 Moroccan Business Survey Data using Machine Learning Techniques},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457690},
doi = {10.1145/3457682.3457690},
abstract = {COVID-19 pandemic has gravely affected our societies and economies with severe consequences. To contain the spread of the disease, most governments around the world authorized unprecedented measures, including Morocco, which has closed the borders and adopted full lockdown between March and June 2020. However, these measures have resulted in economic loss and have led to dramatic changes in how businesses act and consumers behave. The main focus of this study was to examine the impact of the full lockdown on Moroccan enterprises based on the COVID-19 Moroccan business survey carried out by the High Commission for Planning (HCP). A three-stage analysis method was employed. First, multiple correspondence analysis (MCA) was used to reduce the dimensionality of the categorical variables, and k-means clustering algorithm was used to cluster the data, then decision tree algorithm was performed in order to interpret each cluster and the maximum accuracy achieved is 84.45%. Compared with the decision tree algorithm, an artificial neural network (ANN) with stratified 10-fold cross-validation was applied to the dataset and has reached an accuracy of 83.4%. The simulation results confirm the effectiveness of the proposed techniques for analyzing survey data.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {50–58},
numpages = {9},
keywords = {Machine learning, K-Means, Factor analysis, Decision tree, Categorical surveys, ANN},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.1609/aaai.v33i01.33018868,
author = {Sam, Deepak Babu and Sajjan, Neeraj N. and Maurya, Himanshu and Babu, R. Venkatesh},
title = {Almost unsupervised learning for dense crowd counting},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33018868},
doi = {10.1609/aaai.v33i01.33018868},
abstract = {We present an unsupervised learning method for dense crowd count estimation. Marred by large variability in appearance of people and extreme overlap in crowds, enumerating people proves to be a difficult task even for humans. This implies creating large-scale annotated crowd data is expensive and directly takes a toll on the performance of existing CNN based counting models on account of small datasets. Motivated by these challenges, we develop Grid Winner-Take-All (GWTA) autoencoder to learn several layers of useful filters from unlabeled crowd images. Our GWTA approach divides a convolution layer spatially into a grid of cells. Within each cell, only the maximally activated neuron is allowed to update the filter. Almost 99.9% of the parameters of the proposed model are trained without any labeled data while the rest 0.1% are tuned with supervision. The model achieves superior results compared to other unsupervised methods and stays reasonably close to the accuracy of supervised baseline. Furthermore, we present comparisons and analyses regarding the quality of learned features across various models.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1088},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3292500.3330667,
author = {Ahmed, Zeeshan and Amizadeh, Saeed and Bilenko, Mikhail and Carr, Rogan and Chin, Wei-Sheng and Dekel, Yael and Dupre, Xavier and Eksarevskiy, Vadim and Filipi, Senja and Finley, Tom and Goswami, Abhishek and Hoover, Monte and Inglis, Scott and Interlandi, Matteo and Kazmi, Najeeb and Krivosheev, Gleb and Luferenko, Pete and Matantsev, Ivan and Matusevych, Sergiy and Moradi, Shahab and Nazirov, Gani and Ormont, Justin and Oshri, Gal and Pagnoni, Artidoro and Parmar, Jignesh and Roy, Prabhat and Siddiqui, Mohammad Zeeshan and Weimer, Markus and Zahirazami, Shauheen and Zhu, Yiwen},
title = {Machine Learning at Microsoft with ML.NET},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330667},
doi = {10.1145/3292500.3330667},
abstract = {Machine Learning is transitioning from an art and science into a technology available to every developer. In the near future, every application on every platform will incorporate trained models to encode data-based decisions that would be impossible for developers to author. This presents a significant engineering challenge, since currently data science and modeling are largely decoupled from standard software development processes. This separation makes incorporating machine learning capabilities inside applications unnecessarily costly and difficult, and furthermore discourage developers from embracing ML in first place. In this paper we present ML.NET, a framework developed at Microsoft over the last decade in response to the challenge of making it easy to ship machine learning models in large software applications. We present its architecture, and illuminate the application demands that shaped it. Specifically, we introduce DataView, the core data abstraction of ML.NET which allows it to capture full predictive pipelines efficiently and consistently across training and inference lifecycles. We close the paper with a surprisingly favorable performance study of ML.NET compared to more recent entrants, and a discussion of some lessons learned.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2448–2458},
numpages = {11},
keywords = {pipelines, model deployment, machine learning, data view},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3184558.3186235,
author = {B\"{u}hmann, Lorenz and Lehmann, Jens and Westphal, Patrick and Bin, Simon},
title = {DL-Learner Structured Machine Learning on Semantic Web Data},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186235},
doi = {10.1145/3184558.3186235},
abstract = {The following paper is an extended summary of the journal paper "DL-Learner A framework for inductive learning on the Semantic Web". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, e.g. life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {467–471},
numpages = {5},
keywords = {machine learning, owl, rdf, semantic web, supervised learning, system description},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3459637.3483280,
author = {Li, Yunqi and Ge, Yingqiang and Zhang, Yongfeng},
title = {CIKM 2021 Tutorial on Fairness of Machine Learning in Recommender Systems},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3483280},
doi = {10.1145/3459637.3483280},
abstract = {Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users' or providers' satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4857–4860},
numpages = {4},
keywords = {recommender systems, machine learning, fairness, AI ethics},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1007/978-3-030-79457-6_52,
author = {Dedabrishvili, Mariam and Dundua, Besik and Mamaiashvili, Natia},
title = {Smartphone Sensor-Based Fall Detection Using Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_52},
doi = {10.1007/978-3-030-79457-6_52},
abstract = {Human Activity Recognition and particularly detection of abnormal activities such as falls have become a point of interest to many researchers worldwide since falls are considered to be one of the leading causes of injury and death, especially in the elderly population. The prompt intervention of caregivers in critical situations can significantly improve the autonomy and well-being of individuals living alone and those who require remote monitoring. This paper presents a study of accelerometer and gyroscope data retrieved from smartphone embedded sensors, using iOS-based devices. In the project framework there was developed a mobile application for data collection with the following fall type and fall-like activities: Falling Right, Falling Left, Falling Forward, Falling Backward, Sitting Fast, and Jumping. The collected dataset has passed the preprocessing phase and afterward was classified using different Machine Learning algorithms, namely, by Decision Trees, Random Forest, Logistic Regression, k-Nearest Neighbour, XGBoost, LightGBM, and Pytorch Neural Network. Unlike other similar studies, during the experimental setting, volunteers were asked to have smartphones freely in their pockets without tightening and fixing them on the body. This natural way of keeping a mobile device is quite challenging in terms of noisiness however it is more comfortable to wearers and causes fewer constraints. The obtained results are promising that encourages us to continue working with the aim to reach sufficient accuracy along with building a real-time application for potential users.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {609–620},
numpages = {12},
keywords = {Mobile applications, Smartphone embedded sensors, Data classification, Data preprocessing, Fall detection},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1002/smr.2238,
author = {Naeem, Muhammad Rashid and Lin, Tao and Naeem, Hamad and Liu, Hailu},
title = {A machine learning approach for classification of equivalent mutants},
year = {2020},
issue_date = {May 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {5},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2238},
doi = {10.1002/smr.2238},
abstract = {Mutation testing is a fault‐based technique to test the quality of test suites by inducing artificial syntactic faults or mutants in a source program. However, some mutants have the same semantics as original program and cannot be detected by any test suite input known as equivalent mutants. Equivalent mutant problem (EMP) is undecidable as it requires manual human effort to identify a mutant as equivalent or killable. The constraint‐based testing (CBT) theory suggests the use of mathematical constraints which can help reveal some equivalent mutants using mutant features. In this paper, we consider three metrics of CBT theory, ie, reachability, necessity, and sufficiency to extract feature constraints from mutant programs. Constraints are extracted using program dependency graphs. Other features such as degree of significance, semantic distance, and information entropy of mutants are also extracted to build a binary classification model. Machine learning algorithms such as Random Forest, GBT, and SVM are applied under two application scenarios (split‐project and cross‐project) on ten Java programs to predict equivalent mutants. The analysis of the study demonstrates that that the proposed techniques not only improves the efficiency of the equivalent mutant detection but also reduces the effort required to perform it with small accuracy loss.},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {32},
keywords = {static analysis, program semantics, mutation testing, machine learning, equivalent mutants}
}

@article{10.1016/j.ijcip.2021.100446,
author = {V\'{a}vra, Jan and Hromada, Martin and Luk\'{a}\v{s}, Lud\v{e}k and Dworzecki, Jacek},
title = {Adaptive anomaly detection system based on machine learning algorithms in an industrial control environment},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {34},
number = {C},
issn = {1874-5482},
url = {https://doi.org/10.1016/j.ijcip.2021.100446},
doi = {10.1016/j.ijcip.2021.100446},
journal = {Int. J. Crit. Infrastruct. Prot.},
month = sep,
numpages = {11},
keywords = {Industrial Control System, Anomaly Detection, Critical Information Infrastructure, Machine Learning, Cyber Security}
}

@inproceedings{10.1145/3404555.3404565,
author = {Lee, Chun-Hsiang and Li, Zhaofeng and Lu, Xu and Chen, Tiyun and Yang, Saisai and Wu, Chao},
title = {Multi-Tenant Machine Learning Platform Based on Kubernetes},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404565},
doi = {10.1145/3404555.3404565},
abstract = {In this paper, we propose a flexible and scalable machine learning architecture based on Kubernetes that can support simultaneous use by huge numbers of users. Its utilization of computing resources is superior to virtual-machine-based architectures because of its container-level resource isolation and highperformance orchestration mechanism. We also describe the implementation of several important features that are designed to simplify the entire modeling lifecycle for machine learning developers. Real case studies for machine learning model development are presented that demonstrates the effectiveness of the platform in reducing the barriers to machine learning.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {5–12},
numpages = {8},
keywords = {machine learning platform, kubernetes, Cloud-native},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1007/978-3-030-29959-0_2,
author = {Zheng, Yifeng and Duan, Huayi and Wang, Cong},
title = {Towards Secure and Efficient Outsourcing of Machine Learning Classification},
year = {2019},
isbn = {978-3-030-29958-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29959-0_2},
doi = {10.1007/978-3-030-29959-0_2},
abstract = {Machine learning classification has been successfully applied in numerous applications, such as healthcare, finance, and more. Outsourcing classification services to the cloud has become an intriguing practice as this brings many prominent benefits like ease of management and scalability. Such outsourcing, however, raises critical privacy concerns to both the machine learning model provider and the client interested in using the classification service. In this paper, we focus on classification outsourcing with decision trees, one of the most popular classifiers. We propose for the first time a secure framework allowing decision tree based classification outsourcing while maintaining the confidentiality of the provider’s model (parameters) and the client’s input feature vector. Our framework requires no interaction from the provider and the client—they can go offline after the initial submission of their respective encrypted inputs to the cloud. This is a distinct advantage over prior art for practical deployment, as they all work under the client-provider setting where synchronous online interactions between the provider and client is required. Leveraging the lightweight additive secret sharing technique, we build our protocol from the ground up&nbsp;to enable secure and efficient outsourcing of decision tree evaluation, tailored to address the challenges posed by secure in-the-cloud dealing with versatile components including input feature selection, decision node evaluation, path evaluation, and classification generation. Through evaluation we show the practical performance of our design, and the substantial client-side savings over prior art, say up&nbsp;to four orders of magnitude in computation and 163\texttimes{} in communication.},
booktitle = {Computer Security – ESORICS 2019: 24th European Symposium on Research in Computer Security, Luxembourg, September 23–27, 2019, Proceedings, Part I},
pages = {22–40},
numpages = {19},
keywords = {Cloud security, Machine learning, Secure outsourcing},
location = {Luxembourg, Luxembourg}
}

@phdthesis{10.5555/AAI28764024,
author = {Shi, Zhan and Don, Fussell, and Qiang, Liu, and Milad, Hashemi,},
advisor = {Calvin, Lin,},
title = {Machine Learning for Prediction Problems in Computer Architecture},
year = {2020},
isbn = {9798538144778},
publisher = {The University of Texas at Austin},
abstract = {The solutions to many problems in computer architecture involve predictions, which are often based on heuristics. Given the success of machine learning in solving prediction problems, it is natural to wonder if machine learning can better solve architectural prediction problems. Unfortunately, despite vastly outperforming traditional heuristics in various fields, machine learning has seen limited impact on prediction problems in computer architecture. The main challenge is that each architectural prediction problem exhibits unique constraints that prevent off-the-shelf machine learning algorithms from being more effective than heuristics. For example, hardware prediction problems, such as branch prediction and cache replacement, impose severe latency and area constraints that make multi-layer neural networks largely infeasible.In this thesis, we propose machine learning solutions to three important problems in computer architecture, namely cache replacement, data prefetching, and the automatic design of neural network accelerators. In our solutions, we focus on not only the design of learning algorithms, but also the use of learning algorithms under the unique constraints of each problem. In particular, to deal with the extremely tight area and latency constraints of replacement policies and data prefetchers, we propose to first design powerful yet impractical neural network models, from which we derive important insights that can be used to design practical predictors. To deal with the highly constrained search space in the automated design of neural network accelerators, we propose a new constrained Bayesian optimization framework to effectively explore the search space where over 90% of designs are infeasible.},
note = {AAI28764024}
}

@inproceedings{10.1007/978-3-030-78713-4_24,
author = {Wood, Chad and Georgakoudis, Giorgis and Beckingsale, David and Poliakoff, David and Gimenez, Alfredo and Huck, Kevin and Malony, Allen and Gamblin, Todd},
title = {Artemis: Automatic Runtime Tuning of&nbsp;Parallel Execution Parameters Using&nbsp;Machine Learning},
year = {2021},
isbn = {978-3-030-78712-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78713-4_24},
doi = {10.1007/978-3-030-78713-4_24},
abstract = {Portable parallel programming models provide the potential for high performance and productivity, however they come with a multitude of runtime parameters that can have significant impact on execution performance. Selecting the optimal set of those parameters is non-trivial, so that HPC applications perform well in different system environments and on different input data sets, without the need of time consuming parameter exploration or major algorithmic adjustments.We present Artemis, a method for online, feedback-driven, automatic parameter tuning using machine learning that is generalizable and suitable for integration into high-performance codes. Artemis monitors execution at runtime and creates adaptive models for tuning execution parameters, while being minimally invasive in application development and runtime overhead. We demonstrate the effectiveness of Artemis by optimizing the execution times of three HPC proxy applications: Cleverleaf, LULESH, and Kokkos Kernels SpMV. Evaluation shows that Artemis selects the optimal execution policy with over 85% accuracy, has modest monitoring overhead of less than 9%, and increases execution speed by up&nbsp;to 47% despite its runtime overhead.},
booktitle = {High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24 – July 2, 2021, Proceedings},
pages = {453–472},
numpages = {20},
keywords = {Machine learning, In situ, Performance, HPC, Artemis}
}

@inproceedings{10.1145/3318299.3318340,
author = {Zhenning, Guo},
title = {Distributed Machine Learning over Directed Network with Fixed Communication Delays},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318340},
doi = {10.1145/3318299.3318340},
abstract = {In this paper, we present a distributed machine learning algorithm over a network with fixed-delay tolerance. The network is directed and strongly connected. The training dataset is distributed to all agents in the network. We combine the distributed convex optimization (which utilizes double linear iterations) and corresponding machine learning algorithm. Each agent can only access its own local dataset. Suppose the delay between any pair of agents is time-invariant. The simulation shows that our algorithm is able to work under delayed transmission, in the sense that over time at each agent t the ratio of the estimate value xi(t) and scaling variable yi(t) can converge to the optimal point of the global cost function corresponding to the machine learning problem.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {22–26},
numpages = {5},
keywords = {network, machine learning, convex optimization, Distributed system},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1007/s11276-021-02781-1,
author = {Kamboj, Anil Kumar and Jindal, Poonam and Verma, Pankaj},
title = {Machine learning-based physical layer security: techniques, open challenges, and applications},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02781-1},
doi = {10.1007/s11276-021-02781-1},
abstract = {Wireless physical layer security (WPLS) is a powerful technology for current and emerging mobile networks. Physical layer authentication (PLA), antenna selection (AS), and relay node selection are the main elements that add diversity and strength to the paradigm of WPLS. However, heterogeneity, ultra-density, and high mobility requirements make the work difficult for the security of wireless networks. Recently, machine learning has emerged as a promising tool to alleviate the increasing complexity of wireless networks. Hence, this paper introduces intelligent WPLS by concentration on PLA, AS, and relay node selection. First, it presents the background and types of WPLS and ML. Then, revisit the three basic methods of WPLS enhancement, i.e., relay node selection, AS, and authentication, and their integration with ML. Furthermore, several key challenges faced by intelligent WPLS were&nbsp;discussed along with the comprehensive investigation of its different applications in the wireless networks such as the internet of things, device-to-device communication, cognitive radio, non-orthogonal multiple access, and unmanned aerial vehicles. Finally, the appendix includes a detailed survey of ML techniques for WPLS. This article proposes to motivate and help interested readers to easily and rapidly understand the state-of-the-art of WPLS and intelligent WPLS.},
journal = {Wirel. Netw.},
month = nov,
pages = {5351–5383},
numpages = {33},
keywords = {Machine learning, Wireless networks, Relay selection, Antenna selection, Physical layer authentication, Cooperative communication, Physical layer security}
}

@article{10.1145/3418207,
author = {Rodi\'{c}, Lea Duji\'{c} and \v{Z}upanovi\'{c}, Tomislav and Perkovi\'{c}, Toni and \v{S}oli\'{c}, Petar and Rodrigues, Joel J. P. C.},
title = {Machine Learning and Soil Humidity Sensing: Signal Strength Approach},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418207},
doi = {10.1145/3418207},
abstract = {The Internet-of-Things vision of ubiquitous and pervasive computing gives rise to future smart irrigation systems comprising the physical and digital worlds. A smart irrigation ecosystem combined with Machine Learning can provide solutions that successfully solve the soil humidity sensing task in order to ensure optimal water usage. Existing solutions are based on data received from the power hungry/expensive sensors that are transmitting the sensed data over the wireless channel. Over time, the systems become difficult to maintain, especially in remote areas due to the battery replacement issues with a large number of devices. Therefore, a novel solution must provide an alternative, cost- and energy-effective device that has unique advantage over the existing solutions. This work explores the concept of a novel, low-power, LoRa-based, cost-effective system that achieves humidity sensing using Deep Learning techniques that can be employed to sense soil humidity with high accuracy simply by measuring the signal strength of the given underground beacon device.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {39},
numpages = {21},
keywords = {LSTM, SVR, Deep learning, LoRa, RSSI, Soil humidity}
}

@inproceedings{10.1145/3394486.3403300,
author = {Lei, Jing and Akhter, Nasrin and Qiao, Wanli and Shehu, Amarda},
title = {Reconstruction and Decomposition of High-Dimensional Landscapes via Unsupervised Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403300},
doi = {10.1145/3394486.3403300},
abstract = {Uncovering the organization of a landscape that encapsulates all states of a dynamic system is a central task in many domains, as it promises to reveal, in an unsupervised manner, a system's inner working. One domain where this task is crucial is in bioinformatics, where the energy landscape that organizes three-dimensional structures of a molecule by their energetics is a powerful construct. The landscape can be leveraged, among other things, to reveal macrostates where a molecule is biologically-active. This is a daunting task, as landscapes of complex actuated systems, such as molecules, are inherently high-dimensional. Nonetheless, our laboratories have made some progress via topological and statistical analysis of spatial data over the recent years. We have proposed what is essentially a dichotomy, methods that are more pertinent for visualization-driven discovery, and methods that are more pertinent for discovery of the biologically-active macrostates but not amenable to visualization. In this paper, we present a novel, hybrid method that combines strengths of these methods, allowing both visualization of the landscape and discovery of macrostates. We demonstrate what the method is capable of uncovering in comparison with existing methods over structure spaces sampled with conformational sampling algorithms. Though the direct evaluation in this paper is on protein energy landscapes, the proposed method is of broad interest in cross-cutting problems that necessitate characterization of fitness and optimization landscapes.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2505–2513},
numpages = {9},
keywords = {unsupervised learning, topological analysis, statistical analysis, protein energy landscape, landscape decomposition, high-dimensional landscape, decoy selection},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1016/j.inffus.2018.09.013,
author = {Praveen Kumar, D. and Amgoth, Tarachand and Annavarapu, Chandra Sekhara Rao},
title = {Machine learning algorithms for wireless sensor networks: A survey},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.09.013},
doi = {10.1016/j.inffus.2018.09.013},
journal = {Inf. Fusion},
month = sep,
pages = {1–25},
numpages = {25},
keywords = {Data aggregation, Network lifetime, Energy efficiency, Machine learning, Wireless sensor networks}
}

@inproceedings{10.1145/3475738.3480943,
author = {Mosaner, Raphael and Leopoldseder, David and Stadler, Lukas and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Using machine learning to predict the code size impact of duplication heuristics in a dynamic compiler},
year = {2021},
isbn = {9781450386753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475738.3480943},
doi = {10.1145/3475738.3480943},
abstract = {Code duplication is a major opportunity to enable optimizations in subsequent compiler phases. However, duplicating code prematurely or too liberally can result in tremendous code size increases. Thus, modern compilers use trade-offs between estimated costs in terms of code size increase and benefits in terms of performance increase. In the context of this ongoing research project, we propose the use of machine learning to provide trade-off functions with accurate predictions for code size impact. To evaluate our approach, we implemented a neural network predictor in the GraalVM compiler and compared its performance against a human-crafted, highly tuned heuristic. First results show promising performance improvements, leading to code size reductions of more than 10% for several benchmarks. Additionally, we present an assistance mode for finding flaws in the human-crafted heuristic, leading to improvements for the duplication optimization itself.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {127–135},
numpages = {9},
keywords = {Regression, Optimization, Neural Networks, Machine Learning, Heuristics, Dynamic Compiler, Code Duplication},
location = {M\"{u}nster, Germany},
series = {MPLR 2021}
}

@article{10.1016/j.eswa.2021.114942,
author = {Li, Hao and Misra, Siddharth},
title = {Robust machine-learning workflow for subsurface geomechanical characterization and comparison against popular empirical correlations},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {177},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114942},
doi = {10.1016/j.eswa.2021.114942},
journal = {Expert Syst. Appl.},
month = sep,
numpages = {16},
keywords = {Neural Network, Oil and Gas, Sonic, Geomechanical, Machine Learning}
}

@article{10.1016/j.asoc.2021.107948,
author = {Min, Liangyu and Dong, Jiawei and Liu, Jiangwei and Gong, Xiaomin},
title = {Robust mean-risk portfolio optimization using machine learning-based trade-off parameter},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107948},
doi = {10.1016/j.asoc.2021.107948},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {22},
keywords = {Risk measures, Machine learning, Hybrid models, Robust optimization, Portfolio selection}
}

@article{10.5555/3322706.3361994,
author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
title = {Tunability: importance of hyperparameters of machine learning algorithms},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define databased defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1934–1965},
numpages = {32},
keywords = {tuning, supervised learning, meta-learning, machine learning, hyperparameters, classification}
}

@inproceedings{10.1007/978-3-030-64793-3_25,
author = {Belavadi, Vibha and Zhou, Yan and Kantarcioglu, Murat and Thuriasingham, Bhavani},
title = {Attacking Machine Learning Models for Social Good},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_25},
doi = {10.1007/978-3-030-64793-3_25},
abstract = {As machine learning (ML) techniques are becoming widely used, awareness of the harmful effect of automation is growing. Especially, in problem domains where critical decisions are made, machine learning-based applications may raise ethical issues with respect to fairness and privacy. Existing research on fairness and privacy in the ML community mainly focuses on providing remedies during the ML model training phase. Unfortunately, such remedies may not be voluntarily adopted by the industry that is concerned about the profits. In this paper, we propose to apply, from the user’s end, a fair and legitimate technique to “game” the ML system to ameliorate its social accountability issues. We show that although adversarial attacks can be exploited to tamper with ML systems, they can also be used for social good. We demonstrate the effectiveness of our proposed technique on real world image and credit data.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {457–471},
numpages = {15},
keywords = {Data privacy, Artificial intelligence fairness, Adversarial attacks, Adversarial machine learning},
location = {College Park, MD, USA}
}

@article{10.1145/3328932,
author = {Saeed, Aaqib and Ozcelebi, Tanir and Lukkien, Johan},
title = {Multi-task Self-Supervised Learning for Human Activity Detection},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3328932},
doi = {10.1145/3328932},
abstract = {Deep learning methods are successfully used in applications pertaining to ubiquitous computing, pervasive intelligence, health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily transformed by the convolutional and recurrent neural networks, thanks to their ability to learn semantic representations directly from raw input. However, in order to extract generalizable features massive amounts of well-curated data are required, which is a notoriously challenging task; hindered by privacy issues and annotation costs. Therefore, unsupervised representation learning (i.e., learning without manually labeling the instances) is of prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require access to any form of semantic labels, i.e., activity classes. We learn a multi-task temporal convolutional network to recognize transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory signal for extracting useful features for the down-stream task. We extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised and transfer learning settings. Our method achieves performance levels superior to or comparable with fully-supervised networks trained directly with activity labels, and it performs significantly better than unsupervised learning through autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate by attaining a kappa score between 0.7 - 0.8 with only 10 labeled examples per class. We get similar impressive performance even if the features are transferred from a different data source. Self-supervision drastically reduces the requirement of labeled activity data, effectively narrowing the gap between supervised and unsupervised techniques for learning meaningful representations. While this paper focuses on HAR as the application domain, the proposed approach is general and could be applied to a wide variety of problems in other areas.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {61},
numpages = {30},
keywords = {transfer learning, temporal convolutional neural networks, semi-supervised learning, representation learning, multi-task learning, human activity recognition, deep learning, Self-supervised learning}
}

@article{10.1007/s42979-021-00856-6,
author = {El Mezouari, Asmae and El Fazziki, Abdelaziz and Sadgal, Mohammed},
title = {Hadoop–Spark Framework for Machine Learning-Based Smart Irrigation Planning},
year = {2021},
issue_date = {Jan 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {1},
url = {https://doi.org/10.1007/s42979-021-00856-6},
doi = {10.1007/s42979-021-00856-6},
abstract = {Up-to-date, given the expanding increase of the population and the development of human daily lifestyles, the expenditure of freshwater resources increments progressively. It appears that there is a need to optimize at least the consumption of fresh water in agriculture. For this reason, novel various irrigation technologies have been deployed in this context like drip irrigation, flood irrigation, and decision support systems to come up with the constraints of climate changes that decrease the water availability but it is still limited. Therefore, the majority of researchers are working until today on automating the irrigation systems. These smart systems rely mainly on the advances of information technologies like the internet of things, big data, and machine learning for aligning irrigations with climatic changes. Besides, integrating the predictive process helps in anticipating and adapting to the climatic constraints in agriculture, using meticulous soil and environment dependencies analysis based on features’ prediction. In this paper, we enriched our proposed flexible online learning (OL) framework designed for promoting irrigation decisions based on soil characteristics analysis and prediction. We shed the light on a comparative study of four predictive methods, in particular, the auto-regressive moving average, the eXtreme Gradient Boosting, the random forest, and the deep artificial neural networks implemented inside the Hadoop/Spark environment to predict the humidity of the soil, relying on soil temperature and time in several depths. In the end, we discussed the precision of these models in various conditions.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {10},
keywords = {Big data, Machine learning, Spark, Hadoop, Time series, Irrigation planning}
}

@article{10.1016/j.infsof.2021.106573,
author = {Zhang, Fanlong and Khoo, Siau-cheng},
title = {An empirical study on clone consistency prediction based on machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106573},
doi = {10.1016/j.infsof.2021.106573},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {16},
keywords = {Machine learning, Software maintenance, Clone consistency prediction, Clone consistent change, Code clones}
}

@article{10.1007/s00521-020-04912-9,
author = {Li, Daming and Deng, Lianbing and Cai, Zhiming},
title = {RETRACTED ARTICLE: Design of traffic object recognition system based on machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {14},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04912-9},
doi = {10.1007/s00521-020-04912-9},
abstract = {In recent years, researchers have proposed many methods to solve the problem of obstacle detection. However, computer vision-based vehicle detection and recognition technology is still not mature enough. This research combines machine learning technology to construct a traffic object recognition system and applies innovative technology to the computer vision recognition system to construct an automatic identification system suitable for current traffic demand and improve the stability of the traffic system. Moreover, this study uses a combination of a monocular camera and a binocular camera to sense the traffic environment and obtain vehicle position and velocity information. In addition, this study is based on the binocular stereo camera to find the obstacle space and obtain the obstacle relative to the position and speed of the vehicle and combine the obstacle space information to optimize the obstacle frame of the target vehicle. Through experimental research and analysis, it can be seen that the algorithm proposed in this study has certain recognition effect and can be applied to traffic object recognition.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {8143–8156},
numpages = {14},
keywords = {Vehicle identification system, Traffic, Image recognition, Machine learning}
}

@inproceedings{10.1145/3394171.3413547,
author = {Yin, Shi and Wang, Shangfei and Chen, Xiaoping and Chen, Enhong},
title = {Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413547},
doi = {10.1145/3394171.3413547},
abstract = {Current work of facial landmark tracking usually requires large amounts of fully annotated facial videos to train a landmark tracker. To relieve the burden of manual annotations, we propose a novel facial landmark tracking method that makes full use of unlabeled facial videos by exploiting both self-supervised and semi-supervised learning mechanisms. First, self-supervised learning is adopted for representation learning from unlabeled facial videos. Specifically, a facial video and its shuffled version are fed into a feature encoder and a classifier. The feature encoder is used to learn visual representations, and the classifier distinguishes the input videos as the original or the shuffled ones. The feature encoder and the classifier are trained jointly. Through self-supervised learning, the spatial and temporal patterns of a facial video are captured at representation level. After that, the facial landmark tracker, consisting of the pre-trained feature encoder and a regressor, is trained semi-supervisedly. The consistencies among the tracking results of the original, the inverse and the disturbed facial sequences are exploited as the constraints on the unlabeled facial videos, and the supervised loss is adopted for the labeled videos. Through semi-supervised end-to-end training, the tracker captures sequential patterns inherent in facial videos despite small amount of manual annotations. Experiments on two benchmark datasets show that the proposed framework outperforms state-of-the-art semi-supervised facial landmark tracking methods, and also achieves advanced performance compared to fully supervised facial landmark tracking methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2991–2998},
numpages = {8},
keywords = {semi-supervised learning, self-supervised learning, facial landmark tracking},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1007/s10639-021-10570-8,
author = {Gresse von Wangenheim, Christiane and Hauck, Jean C. R. and Pacheco, Fernando S. and Bertonceli Bueno, Matheus F.},
title = {Visual tools for teaching machine learning in K-12: A ten-year systematic mapping},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-021-10570-8},
doi = {10.1007/s10639-021-10570-8},
abstract = {Teaching Machine Learning in school helps students to be better prepared for a society rapidly changing due to the impact of Artificial Intelligence. This requires age-appropriate tools that allow students to develop a comprehensive understanding of Machine Learning in order to become creators of smart solutions. Following the trend of visual languages for introducing algorithms and programming in K-12, we present a ten-year systematic mapping of emerging visual tools that support the teaching of Machine Learning at this educational stage and analyze the tools concerning their educational characteristics, support for the development of ML models as well as their deployment and how the tools have been developed and evaluated. As a result, we encountered 16 tools targeting students mostly as part of short duration extracurricular activities. Tools mainly support the interactive development of ML models for image recognition tasks using supervised learning covering basic steps of the ML process. Being integrated into popular block-based programming languages (primarily Scratch and App Inventor), they also support the deployment of the created ML models as part of games or mobile applications. Findings indicate that the tools can effectively leverage students’ understanding of Machine Learning, however, further studies regarding the design of the tools concerning educational aspects are required to better guide their effective adoption in schools and their enhancement to support the learning process more comprehensively.},
journal = {Education and Information Technologies},
month = sep,
pages = {5733–5778},
numpages = {46},
keywords = {Visual tool, Machine learning, Computing education, K-12}
}

@inproceedings{10.1145/3457682.3457686,
author = {Wang, Yunzhuo and Sun, Hao and Sun, Guangzhong},
title = {DSP-PIGAN: A Precision-Consistency Machine Learning Algorithm for Solving Partial Differential Equations},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457686},
doi = {10.1145/3457682.3457686},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {21–26},
numpages = {6},
keywords = {Precision-Consistency, Physics-informed Machine Learning, Generative Adversarial Network},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1177/02783649211045736,
author = {Nanayakkara, Thrishantha and Barfoot, Tim and Howard, Thomas and Tang, Tim Y. and De Martini, Daniele and Wu, Shangzhe and Newman, Paul},
title = {Self-supervised learning for using overhead imagery as maps in outdoor range sensor localization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {40},
number = {12–14},
issn = {0278-3649},
url = {https://doi.org/10.1177/02783649211045736},
doi = {10.1177/02783649211045736},
abstract = {Traditional approaches to outdoor vehicle localization assume a reliable, prior map is available, typically built using the same sensor suite as the on-board sensors used during localization. This work makes a different assumption. It assumes that an overhead image of the workspace is available and utilizes that as a map for use for range-based sensor localization by a vehicle. Here, range-based sensors are radars and lidars. Our motivation is simple, off-the-shelf, publicly available overhead imagery such as Google satellite images can be a ubiquitous, cheap, and powerful tool for vehicle localization when a usable prior sensor map is unavailable, inconvenient, or expensive. The challenge to be addressed is that overhead images are clearly not directly comparable to data from ground range sensors because of their starkly different modalities. We present a learned metric localization method that not only handles the modality difference, but is also cheap to train, learning in a self-supervised fashion without requiring metrically accurate ground truth. By evaluating across multiple real-world datasets, we demonstrate the robustness and versatility of our method for various sensor configurations in cross-modality localization, achieving localization errors on-par with a prior supervised approach while requiring no pixel-wise aligned ground truth for supervision at training. We pay particular attention to the use of millimeter-wave radar, which, owing to its complex interaction with the scene and its immunity to weather and lighting conditions, makes for a compelling and valuable use case.},
journal = {Int. J. Rob. Res.},
month = dec,
pages = {1488–1509},
numpages = {22},
keywords = {Localization, cross-modality localization, deep learning, self-supervised learning}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Software product line scoping, Requirements engineering, Agile methods}
}

@inproceedings{10.5555/3432601.3432605,
author = {Krishnakumar, Sanjena and Abdou, Tamer},
title = {Towards interpretable and maintainable supervised learning using shapley values in arrhythmia},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\"{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {23–32},
numpages = {10},
keywords = {shapley value, multi-class classification, machine learning, local model-agnostic interpretation, healthcare, bellwether, arrhythmia, SHAP, LIME},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1016/j.comnet.2021.108474,
author = {Ara\'{u}jo, Samuel M.A. and de Souza, Fernanda S.H. and Mateus, Geraldo R.},
title = {A hybrid optimization-Machine Learning approach for the VNF placement and chaining problem},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {199},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108474},
doi = {10.1016/j.comnet.2021.108474},
journal = {Comput. Netw.},
month = nov,
numpages = {18},
keywords = {Online, Chaining, Placement, Machine Learning, Optimization, NFV, VNF}
}

@article{10.1007/s00607-020-00845-2,
author = {Gupta, Manjari and Bhargava, Lava and Indu, S.},
title = {Dynamic workload-aware DVFS for multicore systems using machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-020-00845-2},
doi = {10.1007/s00607-020-00845-2},
abstract = {With growing heterogeneity and complexity in applications, demand to design an energy-efficient and fast computing system in multi-core architecture has heightened. This paper presents a regression-based dynamic voltage frequency scaling model which studies and utilizes workload characteristics to obtain optimal voltage–frequency (v–f) settings. The proposed framework leverages the workload profile information together with power constraints to compute the best-suited voltage–frequency (v–f) settings to (a) maintain global power budget at chip-level, (b) maximize performance while enforcing power constraints at the per-core level. The presented algorithm works in conjunction with the workload characterizer and senses change in application requirements and apply the knowledge to select the next setting for the core. Our results when compared with two state-of-the-art algorithms MaxBIPS and TPEq achieve the average power reduction of 33% and 25% respectively across 32-core architecture for PARSEC benchmarks.},
journal = {Computing},
month = aug,
pages = {1747–1769},
numpages = {23},
keywords = {68M20, 62J05, Machine learning, Energy-performance tradeoff, Multicore processors, Workload decomposition, Dynamic voltage frequency scaling}
}

@article{10.1007/s00521-020-05476-4,
author = {Jankovi\'{c}, Radmila and Mihajlovi\'{c}, Ivan and \v{S}trbac, Nada and Amelio, Alessia},
title = {Machine learning models for ecological footprint prediction based on energy parameters},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05476-4},
doi = {10.1007/s00521-020-05476-4},
abstract = {The ecological footprint is an excellent tool to better understand the consequences of the human behavior on the environment. The growing need for natural resources emphasizes the necessity of their accurate observation, calculation, and prediction. This paper develops and compares four hybrid machine learning models for predicting the total ecological footprint of consumption based on a set of hyper-parameters predefined by the Bayesian optimization algorithm. In particular, K-nearest neighbor regression (KNNReg), random forest regression (RFR) with 93 trees, and two artificial neural networks (ANNs) with two hidden layers were developed and later compared in terms of their performance. As energy inputs, the primary energy consumption from (1) natural gas sources, (2) coal sources, (3) oil sources, (4) wind sources, (5) solar photovoltaic sources, (6) hydropower sources, (7) nuclear sources, and (8) other renewable sources was used. Additionally, population number has also been used as an input. The models were developed using a set of data that include 1804 instances. The ANNs were modeled using two different activation functions in the hidden layers: ReLU and SPOCU. The performance was evaluated using the mean absolute percentage error (MAPE), mean absolute scaled error (MASE), normalized root-mean-squared error (NRMSE), and symmetric mean absolute percentage error (SMAPE). The results show that KNNReg performs the best with MASE of 0.029, followed by the RFR (0.032), ReLU ANN (0.064), and SPOCU ANN (0.089). Moreover, SMOGN was utilized to produce a synthetic test set which was used to additionally test the best performed model. The performance on the SMOGN set demonstrates good performance (MASE=0.022). Lastly, the best performed model was implemented into a GUI that calculates the ecological footprint based on user inputs.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7073–7087},
numpages = {15},
keywords = {Machine learning, Modeling, Energy, Prediction, Ecological footprint}
}

@inproceedings{10.1007/978-3-030-58465-8_3,
author = {Je\v{z}ek, Bruno and Ouhrabka, Adam and Slab\'{y}, Antonin},
title = {Procedural Content Generation via Machine Learning in 2D Indoor Scene},
year = {2020},
isbn = {978-3-030-58464-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58465-8_3},
doi = {10.1007/978-3-030-58465-8_3},
abstract = {The article proposes a method of combining multiple deep forward neural networks to generate a distribution of objects in a 2D scene. The main concepts of machine learning, neural networks and procedural content generation concerning this intention are presented here. Additionally, these concepts are put into the context of computer graphics and used in a practical example of generating an indoor 2D scene. A method of vectorization of input datasets for training forward neural networks is proposed. Scene generation is based on the consequent placement of objects of different classes into the free space defining a room of a certain shape. Several evaluate methods have been proposed for testing the correctness of generation.},
booktitle = {Augmented Reality, Virtual Reality, and Computer Graphics: 7th International Conference, AVR 2020, Lecce, Italy, September 7–10, 2020, Proceedings, Part I},
pages = {34–49},
numpages = {16},
keywords = {Procedural content generation via machine learning (PCGML), Procedural content generation (PCG), Machine learning, Computer graphics},
location = {Lecce, Italy}
}

@article{10.1016/j.ins.2019.07.096,
author = {Zhang, Yun and Kwong, Sam and Wang, Shiqi},
title = {Machine learning based video coding optimizations: A survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {506},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.07.096},
doi = {10.1016/j.ins.2019.07.096},
journal = {Inf. Sci.},
month = jan,
pages = {395–423},
numpages = {29},
keywords = {Versatile video coding, Deep learning, Convolutional neural network, Visual quality assessment, Mode decision, Machine learning, High efficiency video coding, Video coding}
}

@article{10.1007/s13748-020-00217-z,
author = {Sawaqed, Laith S. and Alrayes, Ayman M.},
title = {Bearing fault diagnostic using machine learning algorithms},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {4},
url = {https://doi.org/10.1007/s13748-020-00217-z},
doi = {10.1007/s13748-020-00217-z},
abstract = {This study aims to enhance the condition monitoring of external ball bearings using the raw data provided by Paderborn University which provided sufficient data for motor current signal MCS. Three classes of bearings have been used: healthy bearings, bearings with an inner race defect, and bearings with outer race defect. Online data at different operating conditions, bearings, and faults extent of artificial and real damages have been chosen to provide the generalization and robustness of the model. After proper preprocessing to the raw data of vibration and MCS, time, frequency, and time–frequency domain features have been extracted. Then, optimal features have been selected using genetic algorithm. Artificial neural network with optimized structure using genetic algorithm has been implemented. A comparison between the performance of vibration and motor current signal has been presented. Moreover, our results are compared to previous work by using the same raw data. Results showed the potential of motor current signal in bearing fault diagnosis with high classification accuracy. Moreover, the results showed the possibility to provide a promised diagnostic model that can diagnose bearings of real faults with different fault severities using MCS.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {341–350},
numpages = {10},
keywords = {Genetic algorithm, Neural networks, Machine learning algorithm, Motor current signal, Vibration, Machine fault diagnostic, Bearing damage detection}
}

@article{10.1155/2021/9486949,
author = {Wang, Zhendong and Li, Zeyu and Wang, Junling and Li, Dahai and Feng, Wei},
title = {Network Intrusion Detection Model Based on Improved BYOL Self-Supervised Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/9486949},
doi = {10.1155/2021/9486949},
abstract = {The combination of deep learning and intrusion detection has become a hot topic in today’s network security. In the face of massive, high-dimensional network traffic with uneven sample distribution, how to be able to accurately detect anomalous traffic is the primary task of intrusion detection. Most research on intrusion detection systems based on network anomalous traffic detection has focused on supervised learning; however, the process of obtaining labeled data often requires a lot of time and effort, as well as the support of network experts. Therefore, it is worthwhile investigating the development of label-free self-supervised learning-based approaches called BYOL which is a simple and elegant framework with sufficiently powerful feature extraction capabilities for intrusion detection systems. In this paper, we propose a new data augmentation strategy for intrusion detection data and an intrusion detection model based on label-free self-supervised learning, using a new data augmentation strategy to introduce a perturbation enhancement model to learn invariant feature representation capability and an improved BYOL self-supervised learning method to train the UNSW-NB15 intrusion detection dataset without labels to extract network traffic feature representations. Linear evaluation on UNSW-NB15 and transfer learning on NSK-KDD, KDD CUP99, CIC IDS2017, and CIDDS_001 achieve excellent performance in all metrics.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {23}
}

@article{10.1007/s11276-020-02398-w,
author = {Sarmah, Rupam and Taggu, Amar and Marchang, Ningrinla},
title = {Detecting Byzantine attack in cognitive radio networks using machine learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02398-w},
doi = {10.1007/s11276-020-02398-w},
abstract = {One primary function in a cognitive radio network (CRN) is spectrum sensing. In an infrastructure-based CRN, instead of individual nodes independently sensing the presence of the incumbent signal and taking decisions thereon, a fusion center (FC) aggregates the sensing reports from the individual nodes and makes the final decision. Such collaborative spectrum sensing (CSS) is known to result in better sensing accuracy. On the other hand, CSS is vulnerable to Spectrum Sensing Data Falsification (SSDF) attack (a.k.a. Byzantine attack) wherein a node maliciously falsifies the sensing report prior to sending it to the FC, with the intention of disrupting the spectrum sensing process. This paper investigates the use of machine learning techniques, viz., SVM, Neural Network, Naive Bayes and Ensemble classifiers for detection of SSDF attacks in a CRN where the sensing reports are binary (i.e., either 1 or 0). The learning techniques are studied under two experimental scenarios: (a) when the training and test data are drawn from the same data-set, and (b) when separate data-sets are used for training and testing. Under the first scenario, of all the techniques, NN and Ensemble are the most robust showing consistently very good performance across varying presence of attackers in the system. Moreover performance comparison with an existing non-machine learning technique shows that the learning techniques are generally more robust than the existing algorithm under high presence of attackers. Under the second scenario, in a limited environment, Ensemble is the most robust method showing good overall performance.},
journal = {Wirel. Netw.},
month = nov,
pages = {5939–5950},
numpages = {12},
keywords = {Data science, Frequency property, Cognitive radio network, Machine learning, Spectrum sensing, Vulnerability detection}
}

@article{10.1109/TCBB.2020.2992605,
author = {Qiu, Yushan and Jiang, Hao and Ching, Wai-Ki},
title = {Unsupervised Learning Framework With Multidimensional Scaling in Predicting Epithelial-Mesenchymal Transitions},
year = {2020},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.2992605},
doi = {10.1109/TCBB.2020.2992605},
abstract = {Clustering tumor metastasis samples from gene expression data at the whole genome level remains an arduous challenge, in particular, when the number of experimental samples is small and the number of genes is huge. We focus on the prediction of the epithelial-mesenchymal transition (EMT), which is an underlying mechanism of tumor metastasis, here, rather than tumor metastasis itself, to avoid confounding effects of uncertainties derived from various factors. In this paper, we propose a novel model in predicting EMT based on multidimensional scaling (MDS) strategies and integrating entropy and random matrix detection strategies to determine the optimal reduced number of dimension in low dimensional space. We verified our proposed model with the gene expression data for EMT samples of breast cancer and the experimental results demonstrated the superiority over state-of-the-art clustering methods. Furthermore, we developed a novel feature extraction method for selecting the significant genes and predicting the tumor metastasis. The source code is available at “&lt;uri&gt;https://github.com/yushanqiu/yushan.qiu-szu.edu.cn&lt;/uri&gt;”.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {2714–2723},
numpages = {10}
}

@inproceedings{10.5555/3408352.3408705,
author = {Xun, Lei and Tran-Thanh, Long and Al-Hashimi, Bashir M and Merrett, Geoff V.},
title = {Optimising resource management for embedded machine learning},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Machine learning inference is increasingly being executed locally on mobile and embedded platforms, due to the clear advantages in latency, privacy and connectivity. In this paper, we present approaches for online resource management in heterogeneous multi-core systems and show how they can be applied to optimise the performance of machine learning workloads. Performance can be defined using platform-dependent (e.g. speed, energy) and platform-independent (accuracy, confidence) metrics. In particular, we show how a Deep Neural Network (DNN) can be dynamically scalable to trade-off these various performance metrics. Achieving consistent performance when executing on different platforms is necessary yet challenging, due to the different resources provided and their capability, and their time-varying availability when executing alongside other workloads. Managing the interface between available hardware resources (often numerous and heterogeneous in nature), software requirements, and user experience is increasingly complex.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1556–1561},
numpages = {6},
keywords = {dynamic deep neural network, embedded machine learning, runtime resource management},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.1007/978-3-030-32245-8_66,
author = {Chen, Jun and Zhang, Heye and Zhang, Yanping and Zhao, Shu and Mohiaddin, Raad and Wong, Tom and Firmin, David and Yang, Guang and Keegan, Jennifer},
title = {Discriminative Consistent Domain Generation for Semi-supervised Learning},
year = {2019},
isbn = {978-3-030-32244-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32245-8_66},
doi = {10.1007/978-3-030-32245-8_66},
abstract = {Deep learning based task systems normally rely on a large amount of manually labeled training data, which is expensive to obtain and subject to operator variations. Moreover, it does not always hold that the manually labeled data and the unlabeled data are sitting in the same distribution. In this paper, we alleviate these problems by proposing a discriminative consistent domain generation (DCDG) approach to achieve a semi-supervised learning. The discriminative consistent domain is achieved by a double-sided domain adaptation. The double-sided domain adaptation aims to make a fusion of the feature spaces of labeled data and unlabeled data. In this way, we can fit the differences of various distributions between labeled data and unlabeled data. In order to keep the discriminativeness of generated consistent domain for the task learning, we apply an indirect learning for the double-sided domain adaptation. Based on the generated discriminative consistent domain, we can use the unlabeled data to learn the task model along with the labeled data via a consistent image generation. We demonstrate the performance of our proposed DCDG on the late gadolinium enhancement cardiac MRI (LGE-CMRI) images acquired from patients with atrial fibrillation in two clinical centers for the segmentation of the left atrium anatomy (LA) and proximal pulmonary veins (PVs). The experiments show that our semi-supervised approach achieves compelling segmentation results, which can prove the robustness of DCDG for the semi-supervised learning using the unlabeled data along with labeled data acquired from a single center or multicenter studies.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part II},
pages = {595–604},
numpages = {10},
location = {Shenzhen, China}
}

@article{10.1016/j.ijar.2019.07.009,
author = {Denundefinedux, Thierry and Kanjanatarakul, Orakanya and Sriboonchitta, Songsak},
title = {A new evidential K-nearest neighbor rule based on contextual discounting with partially supervised learning},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2019.07.009},
doi = {10.1016/j.ijar.2019.07.009},
journal = {Int. J. Approx. Reasoning},
month = oct,
pages = {287–302},
numpages = {16},
keywords = {Uncertain data, Soft labels, Machine learning, Classification, Dempster-Shafer theory, Belief functions}
}

@article{10.1016/j.procs.2021.10.078,
author = {Alamri, Hassan A. and Thayananthan, Vijey},
title = {Analysis of Machine Learning for Securing Software-Defined Networking},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {194},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.10.078},
doi = {10.1016/j.procs.2021.10.078},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {229–236},
numpages = {8},
keywords = {Software-Defined Networking (SDN), Security, Machine Learning Algorithm, Distributed Denial of Service (DDoS) Attack}
}

@inproceedings{10.1145/3440094.3440389,
author = {Kabanda, Gabriel},
title = {A bayesian network model for machine learning and cyber security},
year = {2021},
isbn = {9781450387675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440094.3440389},
doi = {10.1145/3440094.3440389},
abstract = {The phenomenal growth in the use of internet-based technologies has resulted in complexities in cyber security subjecting organizations to cyber-attacks. This research is purposed to develop a cyber-security system that uses the Bayesian Network structure and Machine Learning. The research determined the cyber-security framework appropriate for a developing nation; evaluated network detection and prevention systems that use Artificial Intelligence paradigms such as finite automata, neural networks, genetic algorithms, fuzzy logic, support vector machines, or diverse data-mining-based approaches; analyzed Bayesian Networks that can be represented as graphical models and are directional to represent cause-effect relationships; and developed a Bayesian Network model that can handle complexity in cybersecurity. The Pragmatism paradigm used in this research, as a philosophy is intricately related to the mixed-method approach, which is largely quantitative with the research design being a survey and an experiment, but supported by qualitative approaches where Focus Group discussions were held. The Artificial Intelligence paradigms evaluated include machine learning methods, autonomous robotic vehicles, artificial neural networks, and fuzzy logic. Alternative improved solutions discussed include the use of machine learning algorithms specifically Artificial Neural Networks (ANN), Decision Tree C4.5, Random Forests, and Support Vector Machines (SVM).},
booktitle = {Proceedings of the 2nd Africa-Asia Dialogue Network (AADN) International Conference on Advances in Business Management and Electronic Commerce Research},
articleno = {9},
numpages = {7},
keywords = {machine learning (ML), cybersecurity, artificial neural networks (ANN) and decision tree, artificial intelligence (AI), Bayesian network model},
location = {Ganzhou, China},
series = {AADNIC-ABMECR '20}
}

@phdthesis{10.5555/AAI28770522,
author = {Sun, Mengzhen},
advisor = {A, Friesner, Richard},
title = {Machine Learning Applications in Proteins: Interaction Prediction and Structure Prediction},
year = {2021},
isbn = {9798460487943},
publisher = {Columbia University},
address = {USA},
abstract = {This thesis focuses on the two research projects which have applied machine learning techniques to the protein-related topics. The first project is to use protein sequences and the interaction graph to address the protein-protein interaction prediction problem. The second project is to leverage the sequences of protein loops within and beyond homologs to predict the protein loop structures.In the protein-protein interaction prediction project, we applied the pretrained language models, which were trained on large sets of protein sequences, as one of the protein feature extraction methods. Another feature extraction method is the graph learning on the protein interaction graph. The graph learning embeddings and the language model embeddings were fed into classification models to predict if two proteins are interacting or not. We trained and tested our methods on the S. cerevisiae dataset and the human dataset. Our results are comparable to or better than other state-of-art methods, with the advantages that our method is faster at the sample preparation step and has a larger application scope for requiring only protein sequences. We also did experiments with datasets from different similarity cutoffs between the train and test set of the human dataset, and our method has shown an effective prediction ability even with a strict similarity cutoff.In the protein loop prediction project, we utilized the attention-based encoder-decoder language models to predict the protein loop inter-residue distances from the protein loop sequences. We fed the model with the loop sequences and received arrays of numbers representing the distances between each Cα pair in the loops. We utilized two different strategies to reconstruct the loops from the predicted distances. One was firstly to calculate the Cα coordinates from the predicted distances, and then apply a fast full-atom reconstruction method starting from Cα coordinates to build the local loop structures. Our local loop structure prediction results of this method are very competitive with low local RMSDs, especially with the lowest standard deviations. The second method was to integrate the predicted inter-residue distances as constraints to the de novo loop prediction method PLOP (Jacobson et al. 2004). We tested the loop reconstruction process on the 8-res and 12-res loop benchmark sets. This method has the best performance compared to other state-of-art methods, and the incorporation of such machine learning step decreased the computing time of the standalone PLOP program.},
note = {AAI28770522}
}

@article{10.1007/s00180-020-00970-8,
author = {Sambasivan, Rajiv and Das, Sourish and Sahu, Sujit K.},
title = {A Bayesian perspective of statistical machine learning for big data},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {3},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-020-00970-8},
doi = {10.1007/s00180-020-00970-8},
abstract = {Statistical Machine Learning (SML) refers to a body of algorithms and methods by which computers are allowed to discover important features of input data sets which are often very large in size. The very task of feature discovery from data is essentially the meaning of the keyword ‘learning’ in SML. Theoretical justifications for the effectiveness of the SML algorithms are underpinned by sound principles from different disciplines, such as Computer Science and Statistics. The theoretical underpinnings particularly justified by statistical inference methods are together termed as statistical learning theory. This paper provides a review of SML from a Bayesian decision theoretic point of view—where we argue that many SML techniques are closely connected to making inference by using the so called Bayesian paradigm. We discuss many important SML techniques such as supervised and unsupervised learning, deep learning, online learning and Gaussian processes especially in the context of very large data sets where these are often employed. We present a dictionary which maps the key concepts of SML from Computer Science and Statistics. We illustrate the SML techniques with three moderately large data sets where we also discuss many practical implementation issues. Thus the review is especially targeted at statisticians and computer scientists who are aspiring to understand and apply SML for moderately large to big data sets.},
journal = {Comput. Stat.},
month = sep,
pages = {893–930},
numpages = {38},
keywords = {Statistical learning, Machine learning, Big data, Bayesian methods}
}

@inproceedings{10.1007/978-3-030-89817-5_7,
author = {Valdez-Valenzuela, Eric and Kuri-Morales, Angel and Gomez-Adorno, Helena},
title = {Measuring the Effect of Categorical Encoders in Machine Learning Tasks Using Synthetic Data},
year = {2021},
isbn = {978-3-030-89816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89817-5_7},
doi = {10.1007/978-3-030-89817-5_7},
abstract = {Most of the datasets used in Machine Learning (ML) tasks contain categorical attributes. In practice, these attributes must be numerically encoded for their use in supervised learning algorithms. Although there are several encoding techniques, the most commonly used ones do not necessarily preserve possible patterns embedded in the data when they are applied inappropriately. This potential loss of information affects the performance of ML algorithms in automated learning tasks. In this paper, a comparative study is presented to measure how the different encoding techniques affect the performance of machine learning models. We test 10 encoding methods, using 5 ML algorithms on real and synthetic data. Furthermore, we propose a novel approach that uses synthetically created datasets that allows us to know a priori the relationship between the independent and the dependent variables, which implies a more precise measurement of the encoding techniques’ impact. We show that some ML models are affected negatively or positively depending on the encoding technique used. We also show that the proposed approach is more easily controlled and faster when performing experiments on categorical encoders.},
booktitle = {Advances in Computational Intelligence: 20th Mexican International Conference on Artificial Intelligence, MICAI 2021, Mexico City, Mexico, October 25–30, 2021, Proceedings, Part I},
pages = {92–107},
numpages = {16},
keywords = {Supervised machine learning, Data preprocessing, Categorical encoding, Synthetic data}
}

@inproceedings{10.1145/3447548.3467177,
author = {Hardt, Michaela and Chen, Xiaoguang and Cheng, Xiaoyi and Donini, Michele and Gelman, Jason and Gollaprolu, Satish and He, John and Larroy, Pedro and Liu, Xinyu and McCarthy, Nick and Rathi, Ashish and Rees, Scott and Siva, Ankit and Tsai, ErhYuan and Vasist, Keerthan and Yilmaz, Pinar and Zafar, Muhammad Bilal and Das, Sanjiv and Haas, Kevin and Hill, Tyler and Kenthapadi, Krishnaram},
title = {Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467177},
doi = {10.1145/3447548.3467177},
abstract = {Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2974–2983},
numpages = {10},
keywords = {machine learning, fairness, explainability},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3472456.3472499,
author = {Yu, Bowen and Cao, Huanqi and Shan, Tianyi and Wang, Haojie and Tang, Xiongchao and Chen, Wenguang},
title = {Sparker: Efficient Reduction for More Scalable Machine Learning with Spark},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472499},
doi = {10.1145/3472456.3472499},
abstract = {Machine learning applications on Spark suffers from poor scalability. In this paper, we reveal that the key reasons is the non-scalable reduction, which is restricted by the non-splittable object programming interface in Spark. This insight guides us to propose Sparker, Spark with Efficient Reduction. By providing a split aggregation interface, Sparker is able to perform split aggregation with scalable reduction while being backward compatible with existing applications. We implemented Sparker in 2,534 lines of code. Sparker can improve the aggregation performance by up to 6.47 \texttimes{} and can improve the end-to-end performance of MLlib model training by up to 3.69 \texttimes{} with a geometric mean of 1.81 \texttimes{} .},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {58},
numpages = {11},
keywords = {Spark, Reduction, Machine Learning, Aggregation},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@article{10.5555/3122009.3122047,
author = {Popovici, Elena},
title = {Bridging supervised learning and test-based co-optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1255–1293},
numpages = {39},
keywords = {supervised learning, optimal algorithms, free lunch, co-optimization, active learning}
}

@inproceedings{10.1007/978-3-030-58666-9_2,
author = {Akkiraju, Rama and Sinha, Vibha and Xu, Anbang and Mahmud, Jalal and Gundecha, Pritam and Liu, Zhe and Liu, Xiaotong and Schumacher, John},
title = {Characterizing Machine Learning Processes: A Maturity Framework},
year = {2020},
isbn = {978-3-030-58665-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58666-9_2},
doi = {10.1007/978-3-030-58666-9_2},
abstract = {Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from product managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from authors’ personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.},
booktitle = {Business Process Management: 18th International Conference, BPM 2020, Seville, Spain, September 13–18, 2020, Proceedings},
pages = {17–31},
numpages = {15},
keywords = {Machine learning models, Maturity model, Maturity framework, AI model life cycle management},
location = {Seville, Spain}
}

@inproceedings{10.1007/978-3-030-80599-9_1,
author = {Veres, Csaba and Sampson, Jennifer},
title = {You Can’t Learn What’s Not There: Self Supervised Learning and the Poverty of the Stimulus},
year = {2021},
isbn = {978-3-030-80598-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-80599-9_1},
doi = {10.1007/978-3-030-80599-9_1},
abstract = {Diathesis alternation describes the property of language that individual verbs can be used in different subcategorization frames. However, seemingly similar verbs such as drizzle and spray can behave differently in terms of the alternations they can participate in (drizzle/spray water on the plant; *drizzle/spray the plant with water). By hypothesis, primary linguistic data is not sufficient to learn which verbs alternate and which do not. We tested two state-of-the-art machine learning models trained by self supervision, and found little evidence that they could learn the correct pattern of acceptability judgement in the locative alternation. This is consistent with a poverty of stimulus argument that primary linguistic data does not provide sufficient information to learn aspects of linguistic knowledge. The finding has important consequences for machine learning models trained by self supervision, since they depend on the evidence present in the raw training input.},
booktitle = {Natural Language Processing and Information Systems: 26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021, Saarbr\"{u}cken, Germany, June 23–25, 2021, Proceedings},
pages = {3–14},
numpages = {12},
location = {Saarbr\"{u}cken, Germany}
}

@article{10.15388/21-INFOR468,
author = {Kovalev, Maxim and Utkin, Lev and Coolen, Frank and Konstantinov, Andrei},
title = {Counterfactual Explanation of Machine Learning Survival Models},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {4},
issn = {0868-4952},
url = {https://doi.org/10.15388/21-INFOR468},
doi = {10.15388/21-INFOR468},
abstract = {A method for counterfactual explanation of machine learning survival models is proposed. One of the difficulties of solving the counterfactual explanation problem is that the classes of examples are implicitly defined through outcomes of a machine learning survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the counterfactual is introduced. This condition is based on using a distance between mean times to event. It is shown that the counterfactual explanation problem can be reduced to a standard convex optimization problem with linear constraints when the explained black-box model is the Cox model. For other black-box models, it is proposed to apply the well-known Particle Swarm Optimization algorithm. Numerical experiments with real and synthetic data demonstrate the proposed method.},
journal = {Informatica},
month = jan,
pages = {817–847},
numpages = {31},
keywords = {Particle Swarm Optimization, Cox model, counterfactual explanation, convex optimization, censored data, survival analysis, explainable AI, interpretable model}
}

@article{10.1007/s10618-021-00781-5,
author = {Ottervanger, Gilles and Baratchi, Mitra and Hoos, Holger H.},
title = {MultiETSC: automated machine learning for early time series classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {6},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00781-5},
doi = {10.1007/s10618-021-00781-5},
abstract = {Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, “na\"{\i}ve” fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).},
journal = {Data Min. Knowl. Discov.},
month = nov,
pages = {2602–2654},
numpages = {53},
keywords = {Automated machine learning, Time series classification, Early classification}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Systematic literature review, Deep learning, Machine learning, Software process, Software development, Software engineering}
}

@article{10.1016/j.jbi.2021.103751,
author = {Shahid, Osama and Nasajpour, Mohammad and Pouriyeh, Seyedamin and Parizi, Reza M. and Han, Meng and Valero, Maria and Li, Fangyu and Aledhari, Mohammed and Sheng, Quan Z.},
title = {Machine learning research towards combating COVID-19: Virus detection, spread prevention, and medical assistance},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103751},
doi = {10.1016/j.jbi.2021.103751},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {16},
keywords = {Predictive analysis, Drug development, Healthcare, Artificial intelligence, Machine learning, COVID-19}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Stacking, Hybrid feature selection, Ensemble machine learning, Machine learning, Code smell}
}

@article{10.1007/s00778-021-00665-6,
author = {Wang, Jin and Wu, Jiacheng and Li, Mingda and Gu, Jiaqi and Das, Ariyam and Zaniolo, Carlo},
title = {Formal semantics and high performance in declarative machine learning using Datalog},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00665-6},
doi = {10.1007/s00778-021-00665-6},
abstract = {With an escalating arms race to adopt machine learning (ML) in diverse application domains, there is an urgent need to support declarative machine learning over distributed data platforms. Toward this goal, a new framework is needed where users can specify ML tasks in a manner where programming is decoupled from the underlying algorithmic and system concerns. In this paper, we argue that declarative abstractions based on Datalog are natural fits for machine learning and propose a purely declarative ML framework with a Datalog query interface. We show that using aggregates in recursive Datalog programs entails a concise expression of ML applications, while providing a strictly declarative formal semantics. This is achieved by introducing simple conditions under which the semantics of recursive programs is guaranteed to be equivalent to that of aggregate-stratified ones. We further provide specialized compilation and planning techniques for semi-naive fixpoint computation in the presence of aggregates and optimization strategies that are effective on diverse recursive programs and distributed data platforms. To test and demonstrate these research advances, we have developed a powerful and user-friendly system on top of Apache Spark. Extensive evaluations on large-scale datasets illustrate that this approach will achieve promising performance gains while improving both programming flexibility and ease of development and deployment for ML applications.},
journal = {The VLDB Journal},
month = may,
pages = {859–881},
numpages = {23},
keywords = {Scalability, Apache spark, Declarative machine learning, Datalog}
}

@article{10.1007/s10044-014-0401-y,
author = {Kara\c{c}al\i{}, Bilge},
title = {An efficient algorithm for large-scale quasi-supervised learning},
year = {2016},
issue_date = {May       2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-014-0401-y},
doi = {10.1007/s10044-014-0401-y},
abstract = {We present a novel formulation for quasi-supervised learning that extends the learning paradigm to large datasets. Quasi-supervised learning computes the posterior probabilities of overlapping datasets at each sample and labels those that are highly specific to their respective datasets. The proposed formulation partitions the data into sample groups to compute the dataset posterior probabilities in a smaller computational complexity. In experiments on synthetic as well as real datasets, the proposed algorithm attained significant reduction in the computation time for similar recognition performances compared to the original algorithm, effectively generalizing the quasi-supervised learning paradigm to applications characterized by very large datasets.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {311–323},
numpages = {13},
keywords = {Transductive inference, Quasi-supervised learning, Posterior probability estimation, Nearest neighbor rule, Large-scale pattern recognition}
}

@article{10.1007/s10462-020-09814-9,
author = {Gangavarapu, Tushaar and Jaidhar, C. D. and Chanduka, Bhabesh},
title = {Applicability of machine learning in spam and phishing email filtering: review and approaches},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {7},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09814-9},
doi = {10.1007/s10462-020-09814-9},
abstract = {With the influx of technological advancements and the increased simplicity in communication, especially through emails, the upsurge in the volume of unsolicited bulk emails (UBEs) has become a severe threat to global security and economy. Spam emails not only waste users’ time, but also consume a lot of network bandwidth, and may also include malware as executable files. Alternatively, phishing emails falsely claim users’ personal information to facilitate identity theft and are comparatively more dangerous. Thus, there is an intrinsic need for the development of more robust and dependable UBE filters that facilitate automatic detection of such emails. There are several countermeasures to spam and phishing, including blacklisting and content-based filtering. However, in addition to content-based features, behavior-based features are well-suited in the detection of UBEs. Machine learning models are being extensively used by leading internet service providers like Yahoo, Gmail, and Outlook, to filter and classify UBEs successfully. There are far too many options to consider, owing to the need to facilitate UBE detection and the recent advances in this domain. In this paper, we aim at elucidating on the way of extracting email content and behavior-based features, what features are appropriate in the detection of UBEs, and the selection of the most discriminating feature set. Furthermore, to accurately handle the menace of UBEs, we facilitate an exhaustive comparative study using several state-of-the-art machine learning algorithms. Our proposed models resulted in an overall accuracy of 99% in the classification of UBEs. The text is accompanied by snippets of Python code, to enable the reader to implement the approaches elucidated in this paper.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {5019–5081},
numpages = {63},
keywords = {Spam, Python, Phishing, Machine learning, Feature engineering}
}

@article{10.1007/s00521-019-04222-9,
author = {Faigl, Jan},
title = {Unsupervised learning-based solution of the Close Enough Dubins Orienteering Problem},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04222-9},
doi = {10.1007/s00521-019-04222-9},
abstract = {This paper reports on the application of novel unsupervised learning-based method called the Growing Self-Organizing Array (GSOA) to data collection planning with curvature-constrained paths that is motivated by surveillance missions with aerial vehicles. The planning problem is formulated as the Close Enough Dubins Orienteering Problem which combines combinatorial optimization with continuous optimization to determine the most rewarding data collection path that does not exceed the given travel budget and satisfies the motion constraints of the vehicle. The combinatorial optimization consists of selecting a subset of the most rewarding data to be collected and the schedule of data collection. On the other hand, the continuous optimization stands to determine the most suitable waypoint locations from which selected data can be collected together with the determination of the headings at the waypoints for the used Dubins vehicle model. The existing purely combinatorial approaches need to discretize the possible waypoint locations and headings into some finite sets, and the solution is computationally very demanding because the problem size is quickly increased. On the contrary, the employed GSOA performs online sampling of the waypoints and headings during the adaptation of the growing structure that represents the requested curvature-constrained data collection path. Regarding the presented results, the proposed approach provides solutions to orienteering problems with competitive quality, but it is significantly less computationally demanding.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {18193–18211},
numpages = {19},
keywords = {GSOA, Growing Self-Organizing Array, Surveillance missions and aerial vehicles, Data collection planning}
}

@inproceedings{10.1109/INFOCOM42981.2021.9488920,
author = {Li, Weiting and Xiang, Liyao and Zhou, Zhou and Peng, Feng},
title = {Privacy Budgeting for Growing Machine Learning Datasets},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM42981.2021.9488920},
doi = {10.1109/INFOCOM42981.2021.9488920},
abstract = {The wide deployment of machine learning (ML) models and service APIs exposes the sensitive training data to untrusted and unknown parties, such as end-users and corporations. It is important to preserve data privacy in the released ML models. An essential issue with today’s privacy-preserving ML platforms is a lack of concern on the tradeoff between data privacy and model utility: a private datablock can only be accessed a finite number of times as each access is privacy-leaking. However, it has never been interrogated whether such privacy leaked in the training brings good utility. We propose a differentially-private access control mechanism on the ML platform to assign datablocks to queries. Each datablock arrives at the platform with a privacy budget, which would be consumed at each query access. We aim to make the most use of the data under the privacy budget constraints. In practice, both datablocks and queries arrive continuously so that each access decision has to be made without knowledge about the future. Hence we propose online algorithms with a worst-case performance guarantee. Experiments on a variety of settings show our privacy budgeting scheme yields high utility on ML platforms.},
booktitle = {IEEE INFOCOM 2021 - IEEE Conference on Computer Communications},
pages = {1–10},
numpages = {10},
location = {Vancouver, BC, Canada}
}

@article{10.1016/j.cam.2019.112395,
author = {Chen, Zheshi and Li, Chunhong and Sun, Wenjun},
title = {Bitcoin price prediction using machine learning: An approach to sample dimension engineering},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {365},
number = {C},
issn = {0377-0427},
url = {https://doi.org/10.1016/j.cam.2019.112395},
doi = {10.1016/j.cam.2019.112395},
journal = {J. Comput. Appl. Math.},
month = feb,
numpages = {13},
keywords = {Machine learning algorithms, Bitcoin price prediction, Occam’s Razor principle, Sample dimension engineering}
}

@inproceedings{10.1007/978-3-030-82136-4_25,
author = {Kerestely, Arpad and Baicoianu, Alexandra and Bocu, Razvan},
title = {A Research Study on Running Machine Learning Algorithms on Big Data with Spark},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_25},
doi = {10.1007/978-3-030-82136-4_25},
abstract = {The design and implementation of proactive fault diagnosis systems concerning the bearings during their manufacturing process requires the selection of robust representation learning techniques, which belong to the broader scope of the machine learning techniques. Particular systems, such as those that are based on machine learning libraries like Scikit-learn, favor the actual processing of the data, while essentially disregarding relevant computational parameters, such as the speed of the data processing, or the consideration of scalability as an important design and implementation feature. This paper describes an integrated machine learning-based data analytics system, which processes the large amounts of data that are generated by the bearings manufacturing processes using a multinode cluster infrastructure. The data analytics system uses an optimally configured and deployed Spark environment. The proposed data analytics system is thoroughly assessed using a large dataset that stores real manufacturing data, which is generated by the respective bearings manufacturing processes. The performance assessment demonstrates that the described approach ensures the timely and scalable processing of the data. This achievement is relevant, as it exceeds the processing capabilities of significant existing data analytics systems.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {307–318},
numpages = {12},
keywords = {Machine learning, Representation techniques, Fault detection, Spark, High performance computing, Big data},
location = {Tokyo, Japan}
}

@inproceedings{10.1109/ISIT45174.2021.9517751,
author = {Wang, Ye and Aeron, Shuchin and Rakin, Adnan Siraj and Koike-Akino, Toshiaki and Moulin, Pierre},
title = {Robust Machine Learning via Privacy/ Rate-Distortion Theory},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISIT45174.2021.9517751},
doi = {10.1109/ISIT45174.2021.9517751},
abstract = {Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Our work draws the connection between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.},
booktitle = {2021 IEEE International Symposium on Information Theory (ISIT)},
pages = {1320–1325},
numpages = {6},
location = {Melbourne, Australia}
}

@article{10.1007/s11276-021-02565-7,
author = {Sharshembiev, Kumar and Yoo, Seong-Moo and Elmahdi, Elbasher},
title = {Protocol misbehavior detection framework using machine learning classification in vehicular Ad Hoc networks},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {3},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02565-7},
doi = {10.1007/s11276-021-02565-7},
abstract = {A novel approach is proposed to detect protocol misbehavior using state-of-the-art machine learning frameworks and entropy. Nodes in Vehicular Ad Hoc Networks (VANETs) use broadcast protocols to efficiently disseminate safety information, but nodes do not always behave according to the routing protocols. Misbehavior can be caused by a targeted attack, where an attacking vehicle can intentionally send or route malicious packets to harm. Due to the dynamic nature of nodes in VANETs and routing complexity, unintentional misbehavior can also happen due to hardware or software failures in the vehicle. We are not concerned with the targeted attacks, but rather explore how the unintentional misbehavior, which can cause statistical multi-hop routing protocols to operate as basic flooding protocols, can be detected and accurately classified. These methods and detection techniques are based on the IEEE 802.11p MAC layer and weighted p-persistence multi-hop routing protocol. The linear classification was done using the TensorFlow framework and evaluations were performed using the VEINS simulator using the p-persistence broadcast protocol in a US city area.},
journal = {Wirel. Netw.},
month = apr,
pages = {2103–2118},
numpages = {16},
keywords = {Vehicular ad hoc networks, Probabilistic broadcast protocols, Misbehaving node, Logistic regression, Linear classification, IEEE 802.11p}
}

@article{10.1016/j.jnca.2020.102577,
author = {Rovetta, Stefano and Suchacka, Gra\.{z}yna and Masulli, Francesco},
title = {Bot recognition in a Web store: An approach based on unsupervised learning},
year = {2020},
issue_date = {May 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {157},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2020.102577},
doi = {10.1016/j.jnca.2020.102577},
journal = {J. Netw. Comput. Appl.},
month = may,
numpages = {15},
keywords = {Web server, Machine learning, Unsupervised classification, Supervised classification, Web bot detection, Internet robot, Web bot}
}

@article{10.1016/j.cviu.2019.102879,
author = {Ahmad, Touqeer and Bebis, George and Nicolescu, Monica and Nefian, Ara and Fong, Terry},
title = {Horizon line detection using supervised learning and edge cues},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2019.102879},
doi = {10.1016/j.cviu.2019.102879},
journal = {Comput. Vis. Image Underst.},
month = feb,
numpages = {16},
keywords = {65D17, 65D05, 41A10, 41A05}
}

@inproceedings{10.1145/3400302.3415770,
author = {Klemme, Florian and Prinz, Jannik and van Santen, Victor M. and Henkel, J\"{o}rg and Amrouch, Hussam},
title = {Modeling emerging technologies using machine learning: challenges and opportunities},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415770},
doi = {10.1145/3400302.3415770},
abstract = {Compact models of transistors act as the link between semiconductor technology and circuit design via circuit simulations. Unfortunately, compact model development and calibration is a challenging and time-intensive task, hindering rapid prototyping of a circuit (via circuit simulations) in emerging technologies. Moreover, foundries want to protect their confidential technology details to prevent reverse engineering. Hence, they limit access to compact transistor models of commercial technologies (e.g., with Non-Disclosure-Agreements). In this work, we propose Machine Learning (ML) to bridge the gap between early device measurements and later occurring compact model development. Our approach employs a Neural Network (NN) that captures the electrical response of a conventional FinFET transistor without knowledge of semiconductor physics. Additionally, our approach can be applied to emerging technologies, using Negative Capacitance FinFET (NC-FinFET) as an example for a (challenging to model) emerging technology. Inherently, the black-box nature of ML approaches keeps technology manufacturing details confidential. Furthermore, we show how using solely R2 score as our fitness function is insufficient and instead propose fitness based on key electrical characteristics or transistors like threshold voltage. Our NN-based transistor modeling can infer FinFET and NC-FinFET with an R2 score larger than 0.99 and transistor characteristics within 5% of experimental data.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {15},
numpages = {9},
keywords = {transistor model, neural network, negative capacitance FinFET, machine learning, compact model, FinFET},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3298981},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {19},
keywords = {transfer learning, GDPR, Federated learning}
}

@article{10.1007/s10845-020-01706-7,
author = {Cui, Lu-jun and Sun, Man-ying and Cao, Yan-long and Zhao, Qi-jian and Zeng, Wen-han and Guo, Shi-rui},
title = {A novel tolerance geometric method based on machine learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01706-7},
doi = {10.1007/s10845-020-01706-7},
abstract = {In most cases, designers must manually specify geometric tolerance types and values when designing mechanical products. For the same nominal geometry, different designers may specify different types and values of geometric tolerances. To reduce the uncertainty and realize the tolerance specification automatically, a tolerance specification method based on machine learning is proposed. The innovation of this paper is to find out the information that affects geometric tolerances selection and use machine learning methods to generate tolerance specifications. The realization of tolerance specifications is changed from rule-driven to data-driven. In this paper, feature engineering is performed on the data for the application scenarios of tolerance specifications, which improves the performance of the machine learning model. This approach firstly considers the past tolerance specification schemes as cases and sets up the cases to the tolerance specification database which contains information such as datum reference frame, positional relationship, spatial relationship, and product cost. Then perform feature engineering on the data and established machine learning algorithm to convert the tolerance specification problem into an optimization problem. Finally, a gear reducer as a case study is given to verify the method. The results are evaluated with three different machine learning evaluation indicators and made a comparison with the tolerance specification method in the industry. The final results show that the machine learning algorithm can automatically generate tolerance specifications, and after feature engineering, the accuracy of the tolerance specification results is improved.},
journal = {J. Intell. Manuf.},
month = mar,
pages = {799–821},
numpages = {23},
keywords = {Optimization problem, Feature engineering, Machine learning, Tolerance specification, Computer-aided tolerancing (CAT)}
}

@article{10.1007/s00521-021-05838-6,
author = {Chandra, Subhash and Singh, Koushlendra Kumar and Kumar, Sanjay and Ganesh, K. V. K. S. and Sravya, Lavu and Kumar, B. Phani},
title = {A novel approach to validate online signature using machine learning based on dynamic features},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05838-6},
doi = {10.1007/s00521-021-05838-6},
abstract = {This paper presents a new and mathematical method for online signature validation based on machine learning. In this way, the average values of the factors are taken into account to ensure validity. Here, seven different types of features used are x coordinates, y coordinates, time stamp, pen up and down, azimuth, height and pressure. Three new features are extracted from it, i.e., (displacement, velocity and acceleration) using the correlated extraction process to obtain dynamic feature of signature. These features are extracted from the popular dataset SVC2004. The extracted feature is then passed to various classifiers named as Naive Bayes, random forest, J48, MLP, logistic regression and PART. The result of genuine and forge signatures is obtained in terms of precision, true positive rate, false positive rate, F-score, etc. The obtained result is then compared with the existing method with respect to false acceptance rate and false rejection rate.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12347–12366},
numpages = {20},
keywords = {Precision, Fallout, Threshold value, False rejection rate (FRR), False acceptance rate (FAR), Classifiers}
}

@article{10.1016/j.cageo.2019.02.002,
author = {Sudakov, Oleg and Burnaev, Evgeny and Koroteev, Dmitry},
title = {Driving digital rock towards machine learning: Predicting permeability with gradient boosting and deep neural networks},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2019.02.002},
doi = {10.1016/j.cageo.2019.02.002},
journal = {Comput. Geosci.},
month = jun,
pages = {91–98},
numpages = {8},
keywords = {Gradient boosting, Permeability prediction, Artificial neural networks, Machine learning, Digital rock}
}

@inproceedings{10.5555/3171837.3172033,
author = {Steeg, Greg Ver},
title = {Unsupervised learning via total correlation explanation},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Learning by children and animals occurs effortlessly and largely without obvious supervision. Successes in automating supervised learning have not translated to the more ambiguous realm of unsupervised learning where goals and labels are not provided. Barlow (1961) suggested that the signal that brains leverage for unsupervised learning is dependence, or redundancy, in the sensory environment. Dependence can be characterized using the information-theoretic multivariate mutual information measure called total correlation. The principle of Total Correlation Ex-planation (CorEx) is to learn representations of data that "explain" as much dependence in the data as possible. We review some manifestations of this principle along with successes in unsupervised learning problems across diverse domains including human behavior, biology, and language.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {5151–5155},
numpages = {5},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.future.2019.04.017,
author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J.P.C. and Hassan, Suhaidi and Korotaev, Valery V.},
title = {Machine learning in the Internet of Things: Designed techniques for smart cities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.017},
doi = {10.1016/j.future.2019.04.017},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {826–843},
numpages = {18},
keywords = {VANET, Smart grid, Medical, Machine learning, Internet of Things}
}

@inproceedings{10.1145/3318464.3386146,
author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
title = {The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386146},
doi = {10.1145/3318464.3386146},
abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of "pipeline jungles" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {785–800},
numpages = {16},
keywords = {software development, machine learning, ML primitives, ML pipelines, AutoML},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3338840.3355680,
author = {Park, Ki Sun and Hwang, Kyoung Soon and Lee, Keon Myung and Han, Chan Sik and Han, Jin},
title = {Machine learning modeling assistance for non-expert developers},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355680},
doi = {10.1145/3338840.3355680},
abstract = {Machine learning is one of crucial components for intelligent service development. This paper is concerned with assisting nonexpert developers to develop machine learning models. It introduces a modelling assistant tool that guides developers to make design choices in the course of machine learning model development process. The tool presents questions along with candidate answers about what to do at each phase and asks the developer to make choice of a candidate with additional associated information editing. For intelligent assistance, we have integrated and organized the knowledge for machine learning model development into the tool. The tool has been designed so as to support incremental integration of machine learning development knowledge. It supports such machine learning tasks as classification, regression, clustering, and so on for some well-known machine learning algorithms. It also partially generates program codes into which developers edit additional code, if needed.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {117–122},
numpages = {6},
keywords = {machine learning, automated machine learning, and machine learning model generation},
location = {Chongqing, China},
series = {RACS '19}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1145/3469890,
author = {Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin},
title = {Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3469890},
doi = {10.1145/3469890},
abstract = {Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {7},
numpages = {21},
keywords = {lightGBM, accuracy rate, intelligent support information system, big data analysis, Machine learning}
}

@article{10.1007/s00500-020-05209-8,
author = {Janani, R. and Vijayarani, S.},
title = {RETRACTED ARTICLE: Automatic text classification using machine learning and optimization algorithms},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05209-8},
doi = {10.1007/s00500-020-05209-8},
abstract = {In the recent years, the volume of text documents in the form of digital way has grown up extremely in size. As significance, there is a need to be competent to automatically bring together and classify the documents based on their content. The main goal of text classification is to partition the unstructured set of documents into their respective categories based on its content. The main aim of this research work is to automatically classify the documents which are stored in the personal computer into their relevant categories. This work has two significant phases. In the first phase, the important features are selected for classification and the second phase is the classification of text documents. For selecting the optimal features, this research work proposes a new algorithm, optimization technique for feature selection (OTFS) algorithm. To estimate the proficiency of proposed feature selection algorithm, the OTFS algorithm was compared with the existing approaches artificial bee colony, firefly algorithm, ant colony optimization and particle swarm optimization. In the second phase, this research work proposed machine learning-based automatic text classification (MLearn-ATC) algorithm for text classification. In classification, the MLearn-ATC algorithm was compared with widely used classification techniques probabilistic neural network, support vector machine, K-nearest neighbor and Na\"{\i}ve Bayes. From this, the output of first phase is used as the input for classification phase. The decisive results establish that the proposed algorithms achieve the better accuracy for optimizing the features and classifying the text documents based on their content.},
journal = {Soft Comput.},
month = jan,
pages = {1129–1145},
numpages = {17},
keywords = {Text mining, Information retrieval, Document classification, Content analysis, Feature selection, Bio-inspired algorithms, PSO, ACO, ABC, FA, OTFS algorithm, Machine learning algorithms, NB, KNN, SVM, PNN, MLearn-ATC}
}

@inproceedings{10.1145/3351095.3372836,
author = {Hancox-Li, Leif},
title = {Robustness in machine learning explanations: does it matter?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372836},
doi = {10.1145/3351095.3372836},
abstract = {The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {640–647},
numpages = {8},
keywords = {artificial intelligence, epistemology, ethics, explanation, machine learning, methodology, objectivity, philosophy, robustness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.1007/s00521-020-05035-x,
author = {Hamdia, Khader M. and Zhuang, Xiaoying and Rabczuk, Timon},
title = {An efficient optimization approach for designing machine learning models based on genetic algorithm},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05035-x},
doi = {10.1007/s00521-020-05035-x},
abstract = {Machine learning (ML) methods have shown powerful performance in different application. Nonetheless, designing ML models remains a challenge and requires further research as most procedures adopt a trial and error strategy. In this study, we present a methodology to optimize the architecture and the feature configurations of ML models considering a supervised learning process. The proposed approach employs genetic algorithm (GA)-based integer-valued optimization for two ML models, namely deep neural networks (DNN) and adaptive neuro-fuzzy inference system (ANFIS). The selected variables in the DNN optimization problems are the number of hidden layers, their number of neurons and their activation function, while the type and the number of membership functions are the design variables in the ANFIS optimization problem. The mean squared error (MSE) between the predictions and the target outputs is minimized as the optimization fitness function. The proposed scheme is validated through a case study of computational material design. We apply the method to predict the fracture energy of polymer/nanoparticles composites (PNCs) with a database gathered from the literature. The optimized DNN model shows superior prediction accuracy compared to the classical one-hidden layer network. Also, it outperforms ANFIS with significantly lower number of generations in GA. The proposed method can be easily extended to optimize similar architecture properties of ML models in various complex systems.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1923–1933},
numpages = {11},
keywords = {Fracture energy., Polymer nanocomposites, Genetic algorithm, Optimization, Deep neural networks, Machine learning}
}

@article{10.1007/s11704-020-9441-1,
author = {Sun, Xiaobing and Zhou, Tianchi and Wang, Rongcun and Duan, Yucong and Bo, Lili and Chang, Jianming},
title = {Experience report: investigating bug fixes in machine learning frameworks/libraries},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9441-1},
doi = {10.1007/s11704-020-9441-1},
abstract = {Machine learning (ML) techniques and algorithms have been successfully and widely used in various areas including software engineering tasks. Like other software projects, bugs are also common in ML projects and libraries. In order to more deeply understand the features related to bug fixing in ML projects, we conduct an empirical study with 939 bugs from five ML projects by manually examining the bug categories, fixing patterns, fixing scale, fixing duration, and types of maintenance. The results show that (1) there are commonly seven types of bugs in ML programs; (2) twelve fixing patterns are typically used to fix the bugs in ML programs; (3) 68.80% of the patches belong to micro-scale-fix and small-scale-fix; (4) 66.77% of the bugs in ML programs can be fixed within one month; (5) 45.90% of the bug fixes belong to corrective activity from the perspective of software maintenance. Moreover, we perform a questionnaire survey and send them to developers or users of ML projects to validate the results in our empirical study. The results of our empirical study are basically consistent with the feedback from developers. The findings from the empirical study provide useful guidance and insights for developers and users to effectively detect and fix bugs in ML projects.},
journal = {Front. Comput. Sci.},
month = dec,
numpages = {16},
keywords = {questionnaire survey, empirical study, machine learning project, bug fixing}
}

@article{10.1016/j.patcog.2017.10.022,
author = {Chang, Jianlong and Wang, Lingfeng and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
title = {Deep unsupervised learning with consistent inference of latent representations},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {77},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.022},
doi = {10.1016/j.patcog.2017.10.022},
abstract = {An end-to-end unsupervised method is proposed to pre-train deep neural networks.The presented model is optimized under the EM algorithm framework.A series of variants of the proposed method are obtained.The convergence of our approach and its variants is theoretically verified. Utilizing unlabeled data to train deep neural networks (DNNs) is a crucial but challenging task. In this paper, we propose an end-to-end approach to tackle this problem with consistent inference of latent representations. Specifically, each unlabeled data point is considered as a seed to generate a set of latent labeled data points by adding various random disturbances or transformations. Under the expectation maximization framework, DNNs can be trained in an unsupervised way by minimizing the distances between the data points with the same latent representations. Furthermore, several variants of our approach can be derived by applying regularized and sparse constraints during optimization. Theoretically, the convergence of the proposed method and its variants are fully analyzed. Experimental results show that the proposed approach can significantly improve the performance on various tasks, including image classification and clustering. Such results also indicate that our method can guide DNNs to learn more invariant feature representations in comparison with traditional unsupervised methods.},
journal = {Pattern Recogn.},
month = may,
pages = {438–453},
numpages = {16},
keywords = {Deep unsupervised learning, Consistent inference of latent representations}
}

@inproceedings{10.1145/3442442.3452301,
author = {Sun, Haipei and Yang, Yiding and Li, Yanying and Liu, Huihui and Wang, Xinchao and Wang, Wendy Hui},
title = {Automating Fairness Configurations for Machine Learning},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452301},
doi = {10.1145/3442442.3452301},
abstract = {Recent years have witnessed substantial efforts devoted to ensuring algorithmic fairness for machine learning (ML), spanning from formalizing fairness metrics to designing fairness-enhancing methods. These efforts lead to numerous possible choices in terms of fairness definitions and fairness-enhancing algorithms. However, finding the best fairness configuration (including both fairness definition and fairness-enhancing algorithms) for a specific ML task is extremely challenging in practice. The large design space of fairness configurations combined with the tremendous cost required for fairness deployment poses a major obstacle to this endeavor. This raises an important issue: can we enable automated fairness configurations for a new ML task on a potentially unseen dataset? To this point, we design Auto-Fair, a system that provides recommendations of fairness configurations by ranking all fairness configuration candidates based on their evaluations on prior ML tasks. At the core of Auto-Fair lies a meta-learning model that ranks all fairness configuration candidates by utilizing: (1) a set of meta-features that are derived from both datasets and fairness configurations that were used in prior evaluations; and (2) the knowledge accumulated from previous evaluations of fairness configurations on related ML tasks and datasets. The experimental results on 350 different fairness configurations and 1,500 data samples demonstrate the effectiveness of Auto-Fair.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {193–201},
numpages = {9},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3470974,
author = {Park, Jurn-Gyu and Dutt, Nikil and Lim, Sung-Soo},
title = {An Interpretable Machine Learning Model Enhanced Integrated CPU-GPU DVFS Governor},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3470974},
doi = {10.1145/3470974},
abstract = {Modern heterogeneous CPU-GPU-based mobile architectures, which execute intensive mobile gaming/graphics applications, use software governors to achieve high performance with energy-efficiency. However, existing governors typically utilize simple statistical or heuristic models, assuming linear relationships using a small unbalanced dataset of mobile games; and the limitations result in high prediction errors for dynamic and diverse gaming workloads on heterogeneous platforms. To overcome these limitations, we propose an interpretable machine learning (ML) model enhanced integrated CPU-GPU governor: (1) It builds tree-based piecewise linear models (i.e., model trees) offline considering both high accuracy (low error) and interpretable ML models based on mathematical formulas using a simulatability operation counts quantitative metric. And then (2) it deploys the selected models for online estimation into an integrated CPU-GPU Dynamic Voltage Frequency Scaling governor. Our experiments on a test set of 20 mobile games exhibiting diverse characteristics show that our governor achieved significant energy efficiency gains of over 10% (up to 38%) improvements on average in energy-per-frame with a surprising-but-modest 3% improvement in Frames-per-Second performance, compared to a typical state-of-the-art governor that employs simple linear regression models.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {108},
numpages = {28},
keywords = {interpretable machine learning models, model-based design, integrated GPU, dynamic voltage and frequency scaling (DVFS), power management policies, Machine learning techniques}
}

@inproceedings{10.1145/3386723.3387887,
author = {Daanouni, Othmane and Cherradi, Bouchaib and Tmiri, Amal},
title = {Diabetes Diseases Prediction Using Supervised Machine Learning and Neighbourhood Components Analysis},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387887},
doi = {10.1145/3386723.3387887},
abstract = {Diabetes mellitus (DM) is a chronic disease, which can affect the entire body system. Early Diagnosis of patient's diabetics can help improve their health quality or reducing the risk factors. The main objective of this study is to evaluate the performance of some Machine Learning algorithms, used to predict diabetes diseases, for this purpose we apply and evaluate four Machine Learning algorithms (Decision Tree, K-Nearest Neighbours, Artificial Neural Network and Deep Neural Network) to predict diabetes mellitus. These techniques have been trained and tested on Pima Indian dataset. The performances of the experimented algorithms have been evaluated after removing noisy data and using features selection with Neighbourhood components Analysis in order to reduce the number of features and mitigate the complexity of dimensionality in favour of speeds up the learning process, enhances data understanding. Different similarity metrics used to compare model performance like Accuracy, Sensitivity, and Specificity.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {68},
numpages = {5},
keywords = {prediction systems, machine learning, features Selection, deep learning, Diabetes diseases, Computer aided diagnosis (CAD)},
location = {Marrakech, Morocco},
series = {NISS '20}
}

@inproceedings{10.1145/3338840.3355691,
author = {Kim, Heejin and Kim, Younggwan and Hong, Jiman},
title = {Cluster management framework for autonomic machine learning platform},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355691},
doi = {10.1145/3338840.3355691},
abstract = {Autonomic machine learning platforms must provide the necessary management tasks while monitoring the execution status of remotely running machine learning tasks and the performance of the model being trained. In this paper, we design a cluster management framework. The proposed cluster management framework monitors distributed computing resources so that it helps the autonomic machine learning platform to select the proper machine learning algorithm and to execute the proper machine learning model.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {128–130},
numpages = {3},
keywords = {machine learning, cluster management framework, autonomic machine learning platform},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1007/978-3-030-61705-9_16,
author = {DeCastro-Garc\'{\i}a, Noem\'{\i} and Casta\~{n}eda, \'{A}ngel Luis Mu\~{n}oz and Fern\'{a}ndez-Rodr\'{\i}guez, Mario},
title = {RADSSo: An Automated Tool for the multi-CASH Machine Learning Problem},
year = {2020},
isbn = {978-3-030-61704-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61705-9_16},
doi = {10.1007/978-3-030-61705-9_16},
abstract = {The increasing application of machine learning techniques to different disciplines has driven the research in the field towards the creation of algorithms able to construct the best model with the optimal hyperparameter configuration for a particular problem, without the need of user’s expert knowledge. This is well-known as the Combined Algorithms Selection and Hyperparameter Optimization problem.In this work, we develop the open-source tool RADSSo in order to solve a multi-scenario of combined algorithms selection and hyperparameter optimization in which only one datastore is available containing many different machine learning problems. Then, several models need to be computed, at the same time, in an automated way. The tool is deployed in a modular form that allows to modify it and to customize the configuration files to adapt the tool to any context.The underlying model is a mathematical formula that scores each machine learning model providing the best one for each subsample of the datastore. This score is based on the suitability of the model, different metrics from the confusion matrix and the capability of the generalization by the learning curves. In addition, RADSSo provides intuitively reports with all the essential information.},
booktitle = {Hybrid Artificial Intelligent Systems: 15th International Conference, HAIS 2020, Gij\'{o}n, Spain, November 11-13, 2020, Proceedings},
pages = {183–194},
numpages = {12},
keywords = {Hyperparameter optimization, Model selection, Machine learning},
location = {Gij\'{o}n, Spain}
}

@inproceedings{10.1007/978-3-030-91100-3_16,
author = {Kamaleson, Nishanthan and Chu, Dominique and Otero, Fernando E. B.},
title = {Automatic Information Extraction from Electronic Documents Using Machine Learning},
year = {2021},
isbn = {978-3-030-91099-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91100-3_16},
doi = {10.1007/978-3-030-91100-3_16},
abstract = {The digital processing of electronic documents is widely exploited across many domains to improve the efficiency of information extraction. However, paper documents are still largely being used in practice. In order to process such documents, a manual procedure is used to inspect them and extract the values of interest. As this task is monotonous and time consuming, it is prone to introduce human errors during the process. In this paper, we present an efficient and robust system that automates the aforementioned task by using a combination of machine learning techniques: optical character recognition, object detection and image processing techniques. This not only speeds up the process but also improves the accuracy of extracted information compared to a manual procedure.},
booktitle = {Artificial Intelligence XXXVIII: 41st SGAI International Conference on Artificial Intelligence, AI 2021, Cambridge, UK, December 14–16, 2021, Proceedings},
pages = {183–194},
numpages = {12},
keywords = {Information extraction, Image detection, Layout analysis, OCR},
location = {Cambridge, United Kingdom}
}

@inproceedings{10.1145/3340531.3411860,
author = {Ding, Jiahao and Wang, Jingyi and Liang, Guannan and Bi, Jinbo and Pan, Miao},
title = {Towards Plausible Differentially Private ADMM Based Distributed Machine Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411860},
doi = {10.1145/3340531.3411860},
abstract = {The Alternating Direction Method of Multipliers (ADMM) and its distributed version have been widely used in machine learning. In the iterations of ADMM, model updates using local private data and model exchanges among agents impose critical privacy concerns. Despite some pioneering works to relieve such concerns, differentially private ADMM still confronts many research challenges. For example, the guarantee of differential privacy (DP) relies on the premise that the optimality of each local problem can be perfectly attained in each ADMM iteration, which may never happen in practice. The model trained by DP ADMM may have low prediction accuracy. In this paper, we address these concerns by proposing a novel (Improved) Plausible differentially Private ADMM algorithm, called PP-ADMM and IPP-ADMM. In PP-ADMM, each agent approximately solves a perturbed optimization problem that is formulated from its local private data in an iteration, and then perturbs the approximate solution with Gaussian noise to provide the DP guarantee. To further improve the model accuracy and convergence, an improved version IPP-ADMM adopts sparse vector technique (SVT) to determine if an agent should update its neighbors with the current perturbed solution. The agent calculates the difference of the current solution from that in the last iteration, and if the difference is larger than a threshold, it passes the solution to neighbors; or otherwise the solution will be discarded. Moreover, we propose to track the total privacy loss under the zero-concentrated DP (zCDP) and provide a generalization performance analysis. Experiments on real-world datasets demonstrate that under the same privacy guarantee, the proposed algorithms are superior to the state of the art in terms of model accuracy and convergence rate.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {285–294},
numpages = {10},
keywords = {distributed machine learning, differential privacy, decentralized optimization, ADMM},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1007/978-3-030-61380-8_35,
author = {Ortiz-D\'{\i}az, Agust\'{\i}n Alejandro and Bayer, Flavio Roberto and Baldo, Fabiano},
title = {SSL-C4.5: Implementation of a Classification Algorithm for Semi-supervised Learning Based on C4.5},
year = {2020},
isbn = {978-3-030-61379-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61380-8_35},
doi = {10.1007/978-3-030-61380-8_35},
abstract = {Classification algorithms have been extensively studied in many of the major scientific investigations in recent decades. Many of these algorithms are designed for supervised learning, which requires labeled instances to achieve effective learning models. However, in many of the real human processes, data labeling is expensive and time-consuming. Because of this, alternative learning paradigms have been proposed to reduce the cost of the labeling process without a significant loss of model performance. This paper presents the Semi-Supervised Learning C4.5 algorithm (SSL-C4.5) designed to work in scenarios where only a small part of the data is labeled. SSL-C4.5 was implemented over the J48 implementation of the C4.5 algorithm available at the WEKA platform. The J48 was modified incorporating a metric for semi-supervised learning. This metric aims at inducing decision tree models able to analyze and extract information from the entire training dataset, including instances of unlabeled data in scenarios where they are the majority. The assessment performed using eight different benchmark datasets showed that the new proposal has achieved promising results compared to the supervised version of C4.5.},
booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part II},
pages = {513–525},
numpages = {13},
keywords = {Decision tree, Semi-supervised-learning, Classification-algorithms},
location = {Rio Grande, Brazil}
}

@inproceedings{10.1145/3468218.3469042,
author = {Jiao, Long and Sun, Guohua and Le, Junqing and Zeng, Kai},
title = {Machine Learning-Assisted Wireless PHY Key Generation with Reconfigurable Intelligent Surfaces},
year = {2021},
isbn = {9781450385619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468218.3469042},
doi = {10.1145/3468218.3469042},
abstract = {The key generation rate (KGR) performance of wireless physical layer (PHY) key generation can be limited by the quasi-static slow fading environment. In this work, we aim to exploit the radio environment reconfiguration ability enabled by reconfigurable intelligent surface (RIS) to improve KGR of PHY key generation. By rapidly changing the RIS configurations, the randomness or entropy rate of the wireless channel can be significantly increased, thus improving the KGR. To achieve high KGR while keeping low bit disagreement ratio (BDR), for the first time, we propose a machine learning (ML) based adaptive quantization level prediction scheme to decide an optimal quantization level based on channel state information (CSI). Simulation results show that with a prediction accuracy as high as 98.2%, the proposed ML-based prediction model tends to assign high quantization levels in the high SNR regime to reduce BDR, while adopting low quantization levels under low SNRs to maintain a low BDR.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning},
pages = {61–66},
numpages = {6},
keywords = {Smart Environment Reconfiguration, Reconfigurable Intelligent Surface, Physical Layer Security, Physical Layer Key Generation},
location = {Abu Dhabi, United Arab Emirates},
series = {WiseML '21}
}

@article{10.3233/SW-180314,
author = {Maleshkova, Maria and Verborgh, Ruben and Ruta, Michele and Scioscia, Floriano and Loseto, Giuseppe and Pinto, Agnese and Di Sciascio, Eugenio and Ruta, Michele},
title = {Machine learning in the Internet of Things: A&nbsp;semantic-enhanced approach},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {10},
number = {1},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-180314},
doi = {10.3233/SW-180314},
abstract = {Novel Internet of Things (IoT) applications and services rely on an intelligent understanding of the environment leveraging data gathered via heterogeneous sensors and micro-devices. Though increasingly effective, Machine Learning (ML) techniques generally do not go beyond classification of events with opaque labels, lacking machine-understandable representation and explanation of taxonomies. This paper proposes a framework for semantic-enhanced data mining on sensor streams, amenable to resource-constrained pervasive contexts. It merges an ontology-based characterization of data distributions with non-standard reasoning for a fine-grained event detection. The typical classification problem of ML is treated as a resource discovery by exploiting semantic matchmaking. Outputs of classification are endowed with computer-processable descriptions in standard Semantic Web languages, while explanation of matchmaking outcomes motivates confidence on results. A&nbsp;case study on road and traffic analysis has allowed to validate the proposal and achieve an assessment with respect to state-of-the-art ML algorithms.},
journal = {Semant. Web},
month = jan,
pages = {183–204},
numpages = {22},
keywords = {Internet of Things, non-standard reasoning, machine learning, Semantic Web}
}

@article{10.1007/s11265-019-01505-1,
author = {Asgari, Bahar and Mukhopadhyay, Saibal and Yalamanchili, Sudhakar},
title = {MAHASIM: Machine-Learning Hardware Acceleration Using a Software-Defined Intelligent Memory System},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {6},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-019-01505-1},
doi = {10.1007/s11265-019-01505-1},
abstract = {As computations in machine-learning applications are increasing simultaneously along the size of datasets, the energy and performance costs of data movement dominate that of compute. This issue is more pronounced in embedded systems with limited resources and energy. Although near-data-processing (NDP) is pursued as an architectural solution, comparatively less attention has been focused on how to scale NDP for larger-scale embedded machine learning applications (e.g., speech and motion processing). We propose machine-learning hardware acceleration using a software-defined intelligent memory system (Mahasim). Mahasim is a scalable NDP-based memory system, in which application performance scales with the size of data. The building blocks of Mahasim are the programable memory slices, supported by data partitioning, compute-aware memory allocation, and an independent in-memory execution model. For recurrent neural networks, Mahasim shows up to 537.95 GFLOPS/W energy efficiency and 3.9x speedup, when the size of the system increases from 2 to 256 memory slices, which indicates that Mahasim favors larger problems.},
journal = {J. Signal Process. Syst.},
month = jun,
pages = {659–675},
numpages = {17},
keywords = {Memory system, Near-data-processing, Neural networks, Machine learning}
}

@inproceedings{10.1007/978-3-030-27520-4_17,
author = {Prasad, Bakshi Rohit and Agarwal, Sonali},
title = {Scalable Least Square Twin Support Vector Machine Learning},
year = {2019},
isbn = {978-3-030-27519-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27520-4_17},
doi = {10.1007/978-3-030-27520-4_17},
abstract = {Machine Learning (ML) on massive scale datasets, called Big Data, has become a challenge for traditional computing and storage technologies. Henceforth, massive scale ML is an emerging domain of research. Least Square Twin Support Vector Machine (LSTSVM) is a faster variant of Support Vector Machine (SVM). However, it suffers from scalability issues and shows computational and/or storage bottlenecks on massive datasets. Proposed work designs a scalable solution to LSTSVM called Distributed LSTSVM (DLSTSVM). DLSTSVM is designed using distributed parallel computing on top of cluster of multiple machines. After applying horizontal partitioning on massive datasets, DLSTSVM trains it in distributed parallel fashion and finds two non-parallel hyper-planes as decision boundaries for two different classes. MapReduce paradigm is utilized to execute parallel computation on partitioned data in a way that averts memory constraints. Proposed technique achieves computational and storage scalability without losing prediction accuracy.},
booktitle = {Big Data Analytics and Knowledge Discovery: 21st International Conference, DaWaK 2019, Linz, Austria, August 26–29, 2019, Proceedings},
pages = {239–249},
numpages = {11},
keywords = {Big Data, MapReduce, Cluster computing, Distributed machine learning, Supervised learning, LSTSVM, Parallel processing},
location = {Linz, Austria}
}

@article{10.1016/j.cose.2020.102092,
author = {Nowroozi, Ehsan and Dehghantanha, Ali and Parizi, Reza M. and Choo, Kim-Kwang Raymond},
title = {A survey of machine learning techniques in adversarial image forensics},
year = {2021},
issue_date = {Jan 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {100},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102092},
doi = {10.1016/j.cose.2020.102092},
journal = {Comput. Secur.},
month = jan,
numpages = {25},
keywords = {Cyber security, Image manipulation detection, Adversarial setting, Adversarial learning, Adversarial machine learning, Image forensics}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {self-training, open-source software, machine learning, classifiers ensemble, application security},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@phdthesis{10.5555/AAI28319582,
author = {Garcia, Joseph and Faryar, Jabbari, and Fadi, Kurdahi,},
advisor = {Gregory, Washington,},
title = {PHEV Power Management Optimization Using Trajectory Forecasting Based Machine Learning},
year = {2021},
isbn = {9798522946401},
publisher = {University of California, Irvine},
abstract = {In hopes of lessening the reliance on fossil fuels, Plug-in Hybrid Electric Vehicles (PHEVs) have become an attractive option as an alternative fuel vehicle due to their larger electric motors and energy storage systems (ESS). PHEVs can propel themself relying solely on their internal combustion engine (ICE), electric motor (EM), and or a hybrid of both. To improve their fuel efficiency, many studies have been done to investigate the use of a priori route information to optimize the use of a PHEV's ICE and EM. This study introduces a real-time machine learning application of a control strategy known as Trajectory Forecasting (TF). TF takes a priori knowledge of a PHEV's pre-planned route to determine when the vehicle will use its different forms of propulsion in the form of propulsion mode scheduling. However, it assumes constant route data such as traffic and resulting driving speed for its scheduling to be applicable. To automatically account for changing traffic as well as choose better alternative routes, this study looks at the use of a Convolutional Neural Network (CNN) to simulate a PHEV's operation along available routes beforehand according to the rules of TF to choose a route that best satisfies a driver's want, better fuel efficiency and possibly lower emissions. This new real-time TF-based machine learning control strategy is evaluated and compared to common PHEV control strategies such as Charge Sustaining (CS) and Charge Depletion (CD) using National Renewable Energy Laboratory's vehicle simulator ADVISOR. Results show possible increases in Mpgge from 1.72%-130%, decreases in emitted hydrocarbons (HC), carbon monoxide (CO), and nitrous oxides (NOx) from 0.05%-70%, and 0.05%-90.35% reduction in gasoline consumption depending on overall route length and PHEV configuration.},
note = {AAI28319582}
}

@article{10.1109/TCBB.2021.3091972,
author = {Bui, Lien A. and Yeboah, Dacosta and Steinmeister, Louis and Azizi, Sima and Hier, Daniel B. and Wunsch, Donald C. and Olbricht, Gayla R. and Obafemi-Ajayi, Tayo},
title = {Heterogeneity in Blood Biomarker Trajectories After Mild TBI Revealed by Unsupervised Learning},
year = {2021},
issue_date = {May-June 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3091972},
doi = {10.1109/TCBB.2021.3091972},
abstract = {Concussions, also known as mild traumatic brain injury (mTBI), are a growing health challenge. Approximately four million concussions are diagnosed annually in the United States. Concussion is a heterogeneous disorder in causation, symptoms, and outcome making precision medicine approaches to this disorder important. Persistent disabling symptoms sometimes delay recovery in a difficult to predict subset of mTBI patients. Despite abundant data, clinicians need better tools to assess and predict recovery. Data-driven decision support holds promise for accurate clinical prediction tools for mTBI due to its ability to identify hidden correlations in complex datasets. We apply a Locality-Sensitive Hashing model enhanced by varied statistical methods to cluster blood biomarker level trajectories acquired over multiple time points. Additional features derived from demographics, injury context, neurocognitive assessment, and postural stability assessment are extracted using an autoencoder to augment the model. The data, obtained from FITBIR, consisted of 301 concussed subjects (athletes and cadets). Clustering identified 11 different biomarker trajectories. Two of the trajectories (rising GFAP and rising NF-L) were associated with a greater risk of loss of consciousness or post-traumatic amnesia at onset. The ability to cluster blood biomarker trajectories enhances the possibilities for precision medicine approaches to mTBI.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {1365–1378},
numpages = {14}
}

@inproceedings{10.1145/3474085.3475435,
author = {Hu, Bingyu and Zha, Zheng-Jun and Liu, Jiawei and Zhu, Xierong and Xie, Hongtao},
title = {Cluster and Scatter: A Multi-grained Active Semi-supervised Learning Framework for Scalable Person Re-identification},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475435},
doi = {10.1145/3474085.3475435},
abstract = {Active learning has recently attracted increasing attention in the task of person re-identification, due to its unique scalability that not only maximally reduces the annotation cost but also retains the satisfying performance. Although some preliminary active learning methods have been explored in scalable person re-identification task, they have the following two problems: 1) the inefficiency in the selection process of image pairs due to the huge search space, and 2) the ineffectiveness caused by ignoring the impact of unlabeled data in model training. Considering that, we propose a Multi-grained Active Semi-Supervised learning framework, named MASS, to address the scalable person re-identification problem existing in the practical scenarios. Specifically, we firstly design a cluster-scatter procedure to alleviate the inefficiency problem, which consists of two components: cluster step and scatter step. The cluster step shrinks the search space into individual small clusters by a coarse-grained clustering method, and the subsequent scatter step further mines the hard distinguished image pairs from unlabelled set to purify the learned clusters by a novel centrality-based adaptive purification strategy. Afterward, we introduce a customized purification loss for the purified clustering, which utilizes the complementary information in both labeled and unlabeled data to optimize the model for solving the ineffectiveness problem. The cluster-scatter procedure and the model optimization are performed in an iterative fashion to achieve the promising performance while greatly reducing the annotation cost. Extensive experimental results have demonstrated that MASS can even achieve a competitive performance with fully supervised methods in the case of extremely less annotation requirements.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2605–2614},
numpages = {10},
keywords = {semi-supervised learning, person re-identification, active learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.compag.2021.106539,
author = {Pham, Vung and Weindorf, David C. and Dang, Tommy},
title = {Soil profile analysis using interactive visualizations, machine learning, and deep learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {191},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106539},
doi = {10.1016/j.compag.2021.106539},
journal = {Comput. Electron. Agric.},
month = dec,
numpages = {9},
keywords = {Machine learning and deep learning, Vis–NIR spectra, Soil property predictions, pXRF Data visualization, Intelligent visual analytics, Chemical measurement data analysis}
}

@article{10.3233/JIFS-189490,
author = {Qing, Yang and Zejun, Wang and Ramachandran, Varatharajan},
title = {Research on the impact of entrepreneurship policy on employment based on improved machine learning algorithms},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189490},
doi = {10.3233/JIFS-189490},
abstract = {After my country’s economy has entered a new normal, in terms of employment, which has led to the coexistence of the old and new contradictions in employment in our country and the coexistence of employment expansion and stabilization of employment. In this context, it is impossible to achieve full employment and completely eliminate unemployment by relying solely on economic growth. This paper improves traditional machine learning algorithms and builds an entrepreneurial policy analysis model based on improved machine learning to analyze the impact of entrepreneurial policies on employment. Moreover, this paper uses a projection pursuit comprehensive evaluation model optimized by genetic algorithm to conduct empirical research on entrepreneurial environment conditions. In addition, this paper verifies its rationality by regression analysis of empirical results and TEA (Entrepreneurial Activity of All Employees) index, and deeply explores the inherent laws and development characteristics of entrepreneurial environmental conditions from multiple perspectives such as time series and spatial distribution. The research results show that the method proposed in this paper is effective.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6517–6528},
numpages = {12},
keywords = {Machine learning, improved algorithm, entrepreneurial policy, employment impact}
}

@article{10.1016/j.asoc.2020.106071,
author = {Liu, Minjie and Zhou, Mingming and Zhang, Tao and Xiong, Naixue},
title = {Semi-supervised learning quantization algorithm with deep features for motor imagery EEG Recognition in smart healthcare application},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106071},
doi = {10.1016/j.asoc.2020.106071},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {13},
keywords = {Cartesian K-means, Smart healthcare, EEG Recognition, Semi-supervised classification, Convolutional neural networks}
}

@article{10.1007/s10817-020-09576-7,
author = {F\"{a}rber, Michael and Kaliszyk, Cezary and Urban, Josef},
title = {Machine Learning Guidance for Connection Tableaux},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {65},
number = {2},
issn = {0168-7433},
url = {https://doi.org/10.1007/s10817-020-09576-7},
doi = {10.1007/s10817-020-09576-7},
abstract = {Connection calculi allow for very compact implementations of goal-directed proof search. We give an overview of our work related to connection tableaux calculi: first, we show optimised functional implementations of connection tableaux proof search, including a consistent Skolemisation procedure for machine learning. Then, we show two guidance methods based on machine learning, namely reordering of proof steps with Naive Bayesian probabilities, and expansion of a proof search tree with Monte Carlo Tree Search.},
journal = {J. Autom. Reason.},
month = feb,
pages = {287–320},
numpages = {34},
keywords = {Monte Carlo, Internal guidance, Connection tableaux}
}

@inproceedings{10.1007/978-3-030-63061-4_21,
author = {Cascianelli, Silvia and Cristovao, Francisco and Canakoglu, Arif and Carman, Mark and Nanni, Luca and Pinoli, Pietro and Masseroli, Marco},
title = {Evaluating Deep Semi-supervised Learning for Whole-Transcriptome Breast Cancer Subtyping},
year = {2019},
isbn = {978-3-030-63060-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63061-4_21},
doi = {10.1007/978-3-030-63061-4_21},
abstract = {We investigate the important clinical problem of predicting prognosis-related breast cancer molecular subtypes using whole-transcriptome information present in The Cancer Genome Atlas Project (TCGA) dataset. From a Machine Learning perspective, the data is both high-dimensional with over nineteen thousand features, and extremely small with only about one thousand labeled instances in&nbsp;total. To deal with the dearth of information we compare classical, deep and semi-supervised learning approaches on the subtyping task. Specifically, we compare a L1-regularized Logistic Regression, a 2-hidden layer Feed Forward Neural Network and a Variational Autoencoder based semi-supervised learner that makes use of pan-cancer TCGA data as well as normal breast tissue data from a second source. We find that the classical supervised technique performs at least as well as the deep and semi-supervised learning approaches, although learning curve analysis suggests that insufficient unlabeled data may be being provided for the chosen semi-supervised learning technique to be effective.},
booktitle = {Computational Intelligence Methods for Bioinformatics and Biostatistics: 16th International Meeting, CIBB 2019, Bergamo, Italy, September 4–6, 2019, Revised Selected Papers},
pages = {232–244},
numpages = {13},
keywords = {Variational Autoencoder, Semi-supervised learning, Gene expression, Breast cancer, Deep learning},
location = {Bergamo, Italy}
}

@article{10.1007/s00500-015-1892-1,
author = {Chen, Zhenxiang and Liu, Zhusong and Peng, Lizhi and Wang, Lin and Zhang, Lei},
title = {A novel semi-supervised learning method for Internet application identification},
year = {2017},
issue_date = {April     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {8},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1892-1},
doi = {10.1007/s00500-015-1892-1},
abstract = {Several methods based on port, payload, and transport layer features have been proposed to detect, identify, and manage Internet traffic. The diminished effectiveness of port-based identification and overheads of deep packet inspection methods motivated us to identify Internet traffic by combining distinctive flow characteristics with the machine learning method. However, the abundant ground truth Internet traffic, which is important for building a supervised classifier, is difficult to be obtained in real conditions. In this study, we propose a semi-supervised learning method that combines further division of recognition space technique with data gravitation theory. The further division of recognition space classifier is a powerful multi-classification tool that can be helpful for multi-application identification. The data gravitation may reveal the underlying data space structure from unlabeled data, and thus, it is integrated into the classification to develop a better classifier. The experimental results on the real Internet application traffic datasets demonstrate the advantages of our proposed work.},
journal = {Soft Comput.},
month = apr,
pages = {1963–1975},
numpages = {13},
keywords = {Semi-supervised learning, Recognition space, Internet traffic classification, Data gravitation}
}

@article{10.1145/3459666,
author = {Pl\"{O}tz, Thomas},
title = {Applying Machine Learning for Sensor Data Analysis in Interactive Systems: Common Pitfalls of Pragmatic Use and Ways to Avoid Them},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459666},
doi = {10.1145/3459666},
abstract = {With the widespread proliferation of (miniaturized) sensing facilities and the massive growth and popularity of the field of machine learning (ML) research, new frontiers in automated sensor data analysis have been explored that lead to paradigm shifts in many application domains. In fact, many practitioners now employ and rely more and more on ML methods as integral part of their sensor data analysis workflows—thereby not necessarily being ML experts or having an interest in becoming one. The availability of toolkits that can readily be used by practitioners has led to immense popularity and widespread adoption and, in essence, pragmatic use of ML methods. ML having become mainstream helps pushing the core agenda of practitioners, yet it comes with the danger of misusing methods and as such running the risk of leading to misguiding if not flawed results.Based on years of observations in the ubiquitous and interactive computing domain that extensively relies on sensors and automated sensor data analysis, and on having taught and worked with numerous students in the field, in this article I advocate a considerate use of ML methods by practitioners, i.e., non-ML experts, and elaborate on pitfalls of an overly pragmatic use of ML techniques. The article not only identifies and illustrates the most common issues, it also offers ways and practical guidelines to avoid these, which shall help practitioners to benefit from employing ML in their core research domains and applications.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {134},
numpages = {25},
keywords = {machine learning applications, Sensor data analysis}
}

@article{10.1016/j.future.2019.09.009,
author = {Lopes, F\'{a}bio and Agnelo, Jo\~{a}o and Teixeira, C\'{e}sar A. and Laranjeiro, Nuno and Bernardino, Jorge},
title = {Automating orthogonal defect classification using machine learning algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.09.009},
doi = {10.1016/j.future.2019.09.009},
journal = {Future Gener. Comput. Syst.},
month = jan,
pages = {932–947},
numpages = {16},
keywords = {Text classification, Machine learning, Orthogonal defect classification, Bug reports, Software defects}
}

@article{10.1145/3457607,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {115},
numpages = {35},
keywords = {representation learning, natural language processing, machine learning, deep learning, Fairness and bias in artificial intelligence}
}

@inproceedings{10.1145/3412841.3442067,
author = {de Oliveira, Gabriel Bianchin and Pedrini, Helio and Dias, Zanoni},
title = {Protein secondary structure prediction based on fusion of machine learning classifiers},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442067},
doi = {10.1145/3412841.3442067},
abstract = {Protein secondary structure prediction plays an important role in protein folding and function classification. Although the works available in the literature present good results, protein secondary structure prediction is still an open problem. In this work, we present and discuss a fusion strategy using four different classifiers. The fusion is composed of bidirectional recurrent networks, random forests, Inception-v4 blocks and Inception recurrent networks. In order to evaluate our model, we used CB6133 dataset as training and testing. The fusion achieved 76.4% of Q8 accuracy using the amino acid sequence and similarity information on CB6133, surpassing state-of-the-art approaches.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {26–29},
numpages = {4},
keywords = {fusion classifiers, machine learning, neural networks, protein secondary structure prediction},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1007/978-3-030-60334-2_33,
author = {Cheng, Jieyu and Dalca, Adrian V. and Z\"{o}llei, Lilla},
title = {Unbiased Atlas Construction for Neonatal Cortical Surfaces via Unsupervised Learning},
year = {2020},
isbn = {978-3-030-60333-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60334-2_33},
doi = {10.1007/978-3-030-60334-2_33},
abstract = {Due to the dynamic cortical development of neonates after birth, existing cortical surface atlases for adults are not suitable for representing neonatal brains. It has been proposed that pediatric spatio-temporal atlases are more appropriate to characterize the neural development. We present a novel network comprised of an atlas inference module and a non-linear surface registration module, SphereMorph, to construct a continuous neonatal cortical surface atlas with respect to post-menstrual age. We explicitly aim to diminish bias in the constructed atlas by regularizing the mean displacement field. We trained the network on 445 neonatal cortical surfaces from the developing Human Connectome Project (dHCP). We assessed the quality of the constructed atlas by evaluating the accuracy of the spatial normalization of another 100 dHCP surfaces as well as the parcellation accuracy of 10 subjects from an independent dataset that included manual parcellations. We also compared the network’s performance to that of existing spatio-temporal cortical surface atlases, i.e. the 4D University of North Carolina (UNC) neonatal atlases. The proposed network provides continuous spatial-temporal atlases rather than other 4D atlases at discrete time points and we demonstrate that our representation preserves better alignment in cortical folding patterns across subjects than the 4D UNC neonatal atlases.},
booktitle = {Medical Ultrasound, and Preterm, Perinatal and Paediatric Image Analysis: First International Workshop, ASMUS 2020, and 5th International Workshop, PIPPI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4-8, 2020, Proceedings},
pages = {334–342},
numpages = {9},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-16145-3_27,
author = {He, Congqing and Peng, Li and Le, Yuquan and He, Jiawei},
title = {Dynamically Weighted Multi-View Semi-Supervised Learning for CAPTCHA},
year = {2019},
isbn = {978-3-030-16144-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-16145-3_27},
doi = {10.1007/978-3-030-16145-3_27},
abstract = {With the development of Optical Character Recognition and artificial intelligence technologies, the security of Behavioral Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA) has become an increasingly difficult task. In order to prevent malicious attacks and maintain network security, most existing works on CAPTCHA are to construct a fine binary classifier model but are not yet capable of detecting new attack means during confrontation. This motivates us to propose a Dynamically Weighted Multi-View Semi-Supervised Learning, dubbed as DWMVSSL method, to relieve this problem. More specifically, our proposed method extracts hidden patterns from multiple perspectives and updates the view weighting dynamically which can constantly detect new attack means. In addition, due to existing some redundant feature in views, we design a Filter Artificial Bee Colony method, named as FABC for feature selection which can efficiently reduce the impact of high dimensional features. The experimental results show that, compared the existing representative baseline methods, our DWMVSSL method can effectively detecting new attacks on confrontation.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part II},
pages = {343–354},
numpages = {12},
keywords = {CAPTCHA, Semi-supervised learning, Multi-view, Feature selection},
location = {Macau, China}
}

@inproceedings{10.1109/WCNC.2019.8885524,
author = {Santos, R. and Sousa, M. and Vieira, P. and Queluz, M. P. and Rodrigues, A.},
title = {An Unsupervised Learning Approach for Performance and Configuration Optimization of 4G Networks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2019.8885524},
doi = {10.1109/WCNC.2019.8885524},
abstract = {Performing an efficient management of mobile networks has become an increasingly difficult task, as these networks are becoming more complex, which results in more data to evaluate and optimize. This paper is focused on the performance evaluation of a Long Term Evolution (LTE) network using unsupervised learning techniques. The main objective is to detect groups of cells that show similar performances and, consequently, to identify the groups that perform below the desired threshold. Additionally, this work also aims to identify which cell configurations are associated with the best performance. In order to fulfill the first objective, a methodology based on the application of clustering algorithms to features extracted from Key Performance Indicators (KPIs) was developed. Regarding the second objective, statistical testing was applied to evaluate the dependence between the cells configuration and the respective cells performance. It was verified that there is not a significant difference in the obtained results using different clustering algorithms. In the majority of the cases, only two groups of cells were identified: one group consisting essentially on the cells with the best performance and the other group containing the worst performing cells. As far as the connection between configuration data and performance data is concerned, only one influencing case, referring to a parameter associated with the cell subscription capacity, was detected.},
booktitle = {2019 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Marrakesh, Morocco}
}

@inproceedings{10.1145/3477543.3477551,
author = {Yao, Honglei and Zhu, Guangjie and Yang, Yijie},
title = {Primary User Emulation Detection in Wireless Networks with Machine Learning Approach},
year = {2021},
isbn = {9781450390101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477543.3477551},
doi = {10.1145/3477543.3477551},
abstract = {A novel method based on machine learning approach to detect primary user emulation in cognitive radio networks is proposed. The states of wireless channels are collected. And using the locally weighted linear regression algorithm (LWLR), the number of the available channels in next cycle is predicted in advance. In the paper, the error distribution is estimated and calculated. With a given error, the primary user emulation can be detected in the system. Simulation results demonstrate the prediction results performance with the different thresholds of the prediction error.},
booktitle = {2021 8th International Conference on Automation and Logistics (ICAL)},
pages = {49–53},
numpages = {5},
keywords = {primary user emulation detection, prediction, machine learning, available channels, LWLR},
location = {Chongqing, China},
series = {ICAL 2021}
}

@article{10.1007/s10664-020-09881-0,
author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
title = {Testing machine learning based systems: a systematic mapping},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09881-0},
doi = {10.1007/s10664-020-09881-0},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5193–5254},
numpages = {62},
keywords = {Machine learning, Software testing, Systematic review, Systematic mapping}
}

@article{10.1016/j.neunet.2018.03.019,
author = {Kulkarni, Shruti R. and Rajendran, Bipin},
title = {Spiking neural networks for handwritten digit recognition—Supervised learning and network optimization},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2018.03.019},
doi = {10.1016/j.neunet.2018.03.019},
journal = {Neural Netw.},
month = jul,
pages = {118–127},
numpages = {10},
keywords = {Neuromorphic computing, Approximate computing, Pattern recognition, Supervised learning, Spiking neurons, Neural networks}
}

@article{10.1007/s10845-021-01841-9,
author = {Gao, Zhenyang and Dong, Guoying and Tang, Yunlong and Zhao, Yaoyao Fiona},
title = {Machine learning aided design of conformal cooling channels for injection molding},
year = {2021},
issue_date = {Mar 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01841-9},
doi = {10.1007/s10845-021-01841-9},
abstract = {During the injection molding process, the cooling process represents the largest portion of the cycle time. The effectiveness of the cooling system significantly affects the production efficiency and part quality, where it is limited by the conventional cooling channels manufactured by the drilling and casting process. Although the maturing advanced additive manufacturing (AM) technology allows the design and fabrication of complex conformal cooling channels, the temperature variance caused by non-uniform thickness distribution of the part remains unsolved. This issue is caused by the fact that the existing conformal cooling designs do not create the channels conformal to the part thickness distributions. In this work, a machine learning aided design method is proposed to generate cooling systems which conform not only to the part surface but also to the part thickness values. Three commonly used conformal cooling channel topologies including spiral, zig-zag, and porous are selected. A surrogate model is derived for each cooling channel topology to approximate the relationship between the design parameters of the cooling channels, part thickness, and the resulting part surface temperature. Based on the surrogate model, the design parameters of each type of cooling channels are optimized to minimize the part surface temperature variation. At the end of the paper, design cases are studied to validate the effectiveness of the proposed method. Based on the proposed method, much lower temperature variance and a smaller coolant pressure drop are achieved compared with the conventional conformal cooling design.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1183–1201},
numpages = {19},
keywords = {Additive manufacturing, Design and optimization, Artificial neural network, Machine learning, Conformal cooling, Injection mold}
}

@inproceedings{10.1145/3292500.3330744,
author = {Bernardi, Lucas and Mavridis, Themistoklis and Estevez, Pablo},
title = {150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330744},
doi = {10.1145/3292500.3330744},
abstract = {Booking.com is the world's largest online travel agent where millions of guests find their accommodation and millions of accommodation providers list their properties including hotels, apartments, bed and breakfasts, guest houses, and more. During the last years we have applied Machine Learning to improve the experience of our customers and our business. While most of the Machine Learning literature focuses on the algorithmic or mathematical aspects of the field, not much has been published about how Machine Learning can deliver meaningful impact in an industrial environment where commercial gains are paramount. We conducted an analysis on about 150 successful customer facing applications of Machine Learning, developed by dozens of teams in Booking.com, exposed to hundreds of millions of users worldwide and validated through rigorous Randomized Controlled Trials. Following the phases of a Machine Learning project we describe our approach, the many challenges we found, and the lessons we learned while scaling up such a complex technology across our organization. Our main conclusion is that an iterative, hypothesis driven process, integrated with other disciplines was fundamental to build 150 successful products enabled by Machine Learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1743–1751},
numpages = {9},
keywords = {product development, machine learning, experimentation, e-commerce, data science, business impact},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3459637.3481999,
author = {Draschner, Carsten Felix and Stadler, Claus and Bakhshandegan Moghaddam, Farshad and Lehmann, Jens and Jabeen, Hajira},
title = {DistRDF2ML - Scalable Distributed In-Memory Machine Learning Pipelines for RDF Knowledge Graphs},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481999},
doi = {10.1145/3459637.3481999},
abstract = {This paper presents DistRDF2ML, the generic, scalable, and distributed framework for creating in-memory data preprocessing pipelines for Spark-based machine learning on RDF knowledge graphs. This framework introduces software modules that transform large-scale RDF data into ML-ready fixed-length numeric feature vectors. The developed modules are optimized to the multi-modal nature of knowledge graphs. DistRDF2ML provides aligned software design and usage principles as common data science stacks that offer an easy-to-use package for creating machine learning pipelines. The modules used in the pipeline, the hyper-parameters and the results are exported as a semantic structure that can be used to enrich the original knowledge graph. The semantic representation of metadata and machine learning results offers the advantage of increasing the machine learning pipelines' reusability, explainability, and reproducibility. The entire framework of DistRDF2ML is open source, integrated into the holistic SANSA stack, documented in scala-docs, and covered by unit tests. DistRDF2ML demonstrates its scalable design across different processing power configurations and (hyper-)parameter setups within various experiments. The framework brings the three worlds of knowledge graph engineers, distributed computation developers, and data scientists closer together and offers all of them the creation of explainable ML pipelines using a few lines of code.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4465–4474},
numpages = {10},
keywords = {scalable semantic, resource description framework, processing, preprocessing pipeline, open source framework, machine learning, knowledge graphs, explainable artificial intelligence, distributed computing, data science, big data, apache spark, SANSA, RDF},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3368089.3409737,
author = {Gaaloul, Khouloud and Menghi, Claudio and Nejati, Shiva and Briand, Lionel C. and Wolfe, David},
title = {Mining assumptions for software components using machine learning},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409737},
doi = {10.1145/3368089.3409737},
abstract = {Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {159–171},
numpages = {13},
keywords = {Search-based software testing, Model checking, Machine learning, Environment assumptions, Decision trees},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@phdthesis{10.5555/AAI28259275,
author = {Zhang, Liming and Yang, Ruixin and Z\"{u}fle, Andreas and Purohit, Hemant},
advisor = {Dieter, Pfoser,},
title = {Explainable Machine Learning for Activity Modeling in GeoAi},
year = {2020},
isbn = {9798557032087},
publisher = {George Mason University},
address = {USA},
abstract = {GeoAI is a recent cutting-edge discipline that combines advancements in Big Geospatial Data Management (Geo) and Artificial Intelligence (AI). This thesis focuses on two GeoAI aspects, (1) AI for Geo, which applies advanced machine-learning-based models to emerging geospatial data; and (2) Geo for AI, which proposes novel machine learning algorithms that have better explainability aligned with geospatial knowledge.AI for Geo applies machine learning to a wealth of emerging spatiotemporal datasets generated by users, such as OpenstreetMap, Twitter, Yelp, or farecard data from transportation systems. This data captures user activities and the dynamics of a changing environment. Our approach to derive knowledge is based on so-called Activity Modeling for spatiotemporal data by proposing specific machine learning methods to tackle the following novel challenges.(i) The data quality and validity of user-generated data are still of some concern and we show the feasibility of using OpenstreetMap Edits for assessing urban change using statistical modeling. (ii) Another challenge is using explainable latent temporal patterns in machine learning models, e.g., the spatial proximity and temporal auto-correlation for user clustering and trajectory synthesis; (iii) The last challenge is how to introduce new data representation (such as continuous-time temporal graphs) and latest deep learning methods (including Factorized Variational Autoencoder and Generative Adversarial Neural Networks) to capture complex high-dimensional information beyond conventional data representation and methods.From the perspective of Geo for AI, machine learning (ML) models help solve many challenging problems such as computer vision, speech processing, and also spatiotemporal data. However, people are expecting good explainability of results for decision making (e.g. healthcare, law enforcement, and self-driving systems) or first-principle scientific domain knowledge (e.g. chemical bonds, physics movement, and biological linkage). As such, this thesis is also motivated by promoting machine learning explainability in Activity Modeling to generate and enforce better explainability for spatiotemporal data and specific applications beyond what is possible with generic models. Specifically, this work addresses(i) the predictive modeling of urban change using a novel autoregression approach based on power-law growth principles and spatiotemporal auto-correlation, (ii) explainable user clustering based on matrix factorization as part of a transfer learning framework leveraging tidal traffic and commuting behaviors, (iii) spatiotemporal trajectory generation using Variantional Autoencoders through a factorized generative model and spatiotemporal-validity constraints on driving behavior and the physical limitations of vehicular movement, and finally (iv) Generative Adversarial Network (GAN) based temporal graph generation using novel deep structures on temporal dynamics and location representation.Overall, this thesis comprises six chapters. Chapter 1 discusses GeoAI and Activity Modeling and highlights how the two perspectives of GeoAI motivate explainability in machine learning. Chapters 2 through 5 cover four specific issues related to Activity Modeling. Chapter 6 summarizes this thesis and identifies future works.},
note = {AAI28259275}
}

@article{10.3233/JIFS-189575,
author = {Xu, Xiaoying and Zeng, Zhijian and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Analysis of regional economic evaluation based on machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189575},
doi = {10.3233/JIFS-189575},
abstract = {The regional economic evaluation and analysis has guiding significance for the subsequent economic strategy formulation. Due to the influence of various factors, the volatility of some current economic evaluation models is relatively large. According to the needs of regional economic evaluation, this study uses computer technology combined with regional economic development to build an economic development evaluation model to evaluate and analyze the regional economy. Through comparative analysis, this study selects the entropy weight-TOPSIS model as the comprehensive evaluation model of regional economy, uses the entropy weight method to determine the weight of each index, and then uses the TOPSIS method to conduct comprehensive evaluation. In addition, this study designs a control experiment to analyze the performance of this study model. Moreover, this study uses the model proposed in this study to conduct regional economic evaluation in recent years, and compares it with real data, and observes the test results with statistical charts and table data. The research results show that this research model has a certain effect, which can provide analytical tools for the follow-up economic strategy research and analysis.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7543–7553},
numpages = {11},
keywords = {Machine learning, regional economy, simulation model, economic evaluation}
}

@inproceedings{10.1109/ICSE43902.2021.00138,
author = {Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan},
title = {Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00138},
doi = {10.1109/ICSE43902.2021.00138},
abstract = {Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {testing machine learning libraries, test case generation, Empirical software engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3394885.3431629,
author = {Jiang, Weiwen and Xiong, Jinjun and Shi, Yiyu},
title = {When Machine Learning Meets Quantum Computers: A Case Study},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431629},
doi = {10.1145/3394885.3431629},
abstract = {Along with the development of AI democratization, the machine learning approach, in particular neural networks, has been applied to wide-range applications. In different application scenarios, the neural network will be accelerated on the tailored computing platform. The acceleration of neural networks on classical computing platforms, such as CPU, GPU, FPGA, ASIC, has been widely studied; however, when the scale of the application consistently grows up, the memory bottleneck becomes obvious, widely known as memory-wall. In response to such a challenge, advanced quantum computing, which can represent 2N states with N quantum bits (qubits), is regarded as a promising solution. It is imminent to know how to design the quantum circuit for accelerating neural networks. Most recently, there are initial works studying how to map neural networks to actual quantum processors. To better understand the state-of-the-art design and inspire new design methodology, this paper carries out a case study to demonstrate an end-to-end implementation. On the neural network side, we employ the multilayer perceptron to complete image classification tasks using the standard and widely used MNIST dataset. On the quantum computing side, we target IBM Quantum processors, which can be programmed and simulated by using IBM Qiskit. This work targets the acceleration of the inference phase of a trained neural network on the quantum processor. Along with the case study, we will demonstrate the typical procedure for mapping neural networks to quantum circuits.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {593–598},
numpages = {6},
keywords = {quantum computing, neural networks, MNIST dataset, IBM Quantum, IBM Qiskit},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.1155/2021/2788161,
author = {Lv, Yan and Lu, Laijun and Nagaraj, Balakrishnan},
title = {Geological Mineral Energy and Classification Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/2788161},
doi = {10.1155/2021/2788161},
abstract = {In order to mine geological mineral energy and study on geological mineral energy classification, a method based on a wireless sensor was proposed. Of logistic regression, artificial neural networks, random forests, and main wireless sensor algorithms of support vector machine (SVM) with the model in the application of the energy mineral resource prediction practice effects are reviewed and discuss the practical application in the process of sample selection, the wrong points existing in the cost, the uncertainty evaluation, and performance evaluation of the model using wireless sensor algorithm, random forest of the probability distribution of mineralization in the study area is calculated, and five prospecting potential areas are delineated. The results show that the ratio of ore-bearing unit and non-ore-bearing unit is 1 : 1, and the best random forest training model is obtained. 70% of the training sample set was randomly selected as the training set, and the remaining 30% was used as the test set to construct the random forest model. The training accuracy of the model is 96.7%, and the testing accuracy is 96.5%. Both model training accuracy and model testing accuracy are very high, which proves the accuracy of RF model construction and achieves satisfactory results. In this study, a wireless sensor is successfully applied to 3D mineral energy prediction, which makes a positive exploration for mineral resource prediction and evaluation in the future. Finally, the prediction of mineral resource energy based on a wireless sensor is an important trend of future development.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3419604.3419772,
author = {Haddouchi, Maissae and Berrado, Abdelaziz},
title = {An implementation of a multivariate discretization for supervised learning using Forestdisc},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419772},
doi = {10.1145/3419604.3419772},
abstract = {Discretization is a key pre-processing step in Machine Learning that transforms continuous attributes into discrete ones, through different methods available in the literature. In this regard, this work provides the ForestDisc framework that discretizes data based on a supervised, multivariate and hybrid approach. It uses, at first, a splitting process relying on a tree learning ensemble to generate a large set of cut points. It then uses a merging process based on moment matching optimization, to transform this set into a reduced and representative one. ForestDisc is a non-parametric discretizer in the sense that it does not require the user to introduce any initial setting parameters. We implemented ForestDisc algorithm in the "ForestDisc" R package.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {8},
numpages = {6},
keywords = {Tree Ensembles, Split Points Selection, Random Forest, Multivariate Discretization, Moment Matching, Data Preprocessing},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3339311.3339337,
author = {K, Saritha and Abraham, Sajimon},
title = {Accuracy evaluation of prediction using supervised learning techniques},
year = {2019},
isbn = {9781450366526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339311.3339337},
doi = {10.1145/3339311.3339337},
abstract = {The term Big data is used to refer the huge volume of complex and growing data generated from many distinct electronic gadgets. In the case of Big Data, the commonly used programming methods are not adequate to collect, store and analyze the data within a short period. Statistics as well as machine learning techniques are used for finding patterns and information from large data through data driven decision-making. Big Data analytics gives competitive opportunities in designing business plans for Business Analytics. For analytical purpose, traditionally we use Multiple Linear Regression (MLR) model in the statistical method, a type of Supervised Machine Learning Algorithm. We implemented Cross-Validation Resampling technique with MLR model. The performance of new MLR-Leave-One-Out (MLR-LOOCV) model evaluated using partitioning the whole data set. This technique used to validate the model developed from training data with test data to control the problem like over fitting. The accuracy of such prediction model is very poor. So we propose to build a Multilayer Perceptron Neural Network (MPNN) model with gradient descent learning method to improve the efficiency of prediction model. The new proposed model, MPNN with GD shows accuracy much greater than normal MLR. The data set from UCI machine learning repository is used for simulation methods to check the performance.},
booktitle = {Proceedings of the Third International Conference on Advanced Informatics for Computing Research},
articleno = {26},
numpages = {6},
keywords = {predictive analytics, perceptron neural network, multiple linear regression, gradient descent, cross-validation, big data analytics},
location = {Shimla, India},
series = {ICAICR '19}
}

@article{10.1145/3491234,
author = {Rasoulinezhad, Seyedramin and Roorda, Esther and Wilton, Steve and Leong, Philip H. W. and Boland, David},
title = {Rethinking Embedded Blocks for Machine Learning Applications},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3491234},
doi = {10.1145/3491234},
abstract = {The underlying goal of FPGA architecture research is to devise flexible substrates that implement a wide variety of circuits efficiently. Contemporary FPGA architectures have been optimized to support networking, signal processing, and image processing applications through high-precision digital signal processing (DSP) blocks. The recent emergence of machine learning has created a new set of demands characterized by: (1) higher computational density and (2) low precision arithmetic requirements. With the goal of exploring this new design space in a methodical manner, we first propose a problem formulation involving computing nested loops over multiply-accumulate (MAC) operations, which covers many basic linear algebra primitives and standard deep neural network (DNN) kernels. A quantitative methodology for deriving efficient coarse-grained compute block architectures from benchmarks is then proposed together with a family of new embedded blocks, called MLBlocks. An MLBlock instance includes several multiply-accumulate units connected via a flexible routing, where each configuration performs a few parallel dot-products in a systolic array fashion. This architecture is parameterized with support for different data movements, reuse, and precisions, utilizing a columnar arrangement that is compatible with existing FPGA architectures. On synthetic benchmarks, we demonstrate that for 8-bit arithmetic, MLBlocks offer 6\texttimes{} improved performance over the commercial Xilinx DSP48E2 architecture with smaller area and delay; and for time-multiplexed 16-bit arithmetic, achieves 2\texttimes{} higher performance per area with the same area and frequency. All source codes and data, along with documents to reproduce all the results in this article, are available at .},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = nov,
articleno = {9},
numpages = {30},
keywords = {digital signal processing, neural networks, reconfigurable architecture, coarse-grained compute blocks, FPGA Architectures}
}

@article{10.1016/j.jco.2021.101587,
author = {Dick, Josef and Feischl, Michael},
title = {A quasi-Monte Carlo data compression algorithm for machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0885-064X},
url = {https://doi.org/10.1016/j.jco.2021.101587},
doi = {10.1016/j.jco.2021.101587},
journal = {J. Complex.},
month = dec,
numpages = {25},
keywords = {Higher-order methods, Statistical learning, Big data, Quasi-Monte Carlo, 65D32, 65D30, 65C05}
}

@inproceedings{10.1007/978-3-030-64583-0_7,
author = {Silva, Stefan and Crispim, Jos\'{e}},
title = {An Application of Machine Learning to Study Utilities Expenses in the Brazilian Navy},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_7},
doi = {10.1007/978-3-030-64583-0_7},
abstract = {The extensive Brazilian territory endows its Navy with more than 350 facilities with several distinct activities that transcend military operations. Understanding the variation of all the essential and common costs of those facilities proved to be a challenging and relevant task. This paper presents a machine learning approach to support the decision-making process based on data that represents several facilities attributes, where models were trained, and those with the best performance were further analyzed. Besides data limitations, our results show that predictions and explanations derived from the models can be applied to support decision-making within the organization and contribute with insights to improve management over its resources.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {60–71},
numpages = {12},
keywords = {Data-driven organization, Decision-making, Machine learning, Military expenditures},
location = {Siena, Italy}
}

@article{10.1016/j.procs.2021.01.171,
author = {Anastasi, Sara and Madonna, Marianna and Monica, Luigi},
title = {Implications of embedded artificial intelligence - machine learning on safety of machinery},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {180},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.01.171},
doi = {10.1016/j.procs.2021.01.171},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {338–343},
numpages = {6},
keywords = {Machinery Directive, Machine Learning, Artificial Intelligence, Safety of machinery}
}

@inproceedings{10.5555/2969033.2969226,
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
title = {Semi-supervised learning with deep generative models},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3581–3589},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.1145/3468264.3468536,
author = {Biswas, Sumon and Rajan, Hridesh},
title = {Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468536},
doi = {10.1145/3468264.3468536},
abstract = {In recent years, many incidents have been reported where machine learning models exhibited discrimination among people based on race, sex, age, etc. Research has been conducted to measure and mitigate unfairness in machine learning models. For a machine learning task, it is a common practice to build a pipeline that includes an ordered set of data preprocessing stages followed by a classifier. However, most of the research on fairness has considered a single classifier based prediction task. What are the fairness impacts of the preprocessing stages in machine learning pipeline? Furthermore, studies showed that often the root cause of unfairness is ingrained in the data itself, rather than the model. But no research has been conducted to measure the unfairness caused by a specific transformation made in the data preprocessing stage. In this paper, we introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ML pipeline. We leveraged existing metrics to define the fairness measures of the stages. Then we conducted a detailed fairness evaluation of the preprocessing stages in 37 pipelines collected from three different sources. Our results show that certain data transformers are causing the model to exhibit unfairness. We identified a number of fairness patterns in several categories of data transformers. Finally, we showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline. We used the fairness composition to choose appropriate downstream transformer that mitigates unfairness in the machine learning pipeline.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {981–993},
numpages = {13},
keywords = {preprocessing, pipeline, models, machine learning, fairness},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3341069.3341084,
author = {Chen, I-Ching and Hu, Shueh-Cheng},
title = {Realizing Specific Weather Forecast through Machine Learning Enabled Prediction Model},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341084},
doi = {10.1145/3341069.3341084},
abstract = {To general people, it is more convenient to know weather condition at a specific location and particular time. However, current weather forecasting services offered by meteorological observation organizations only provide a wide-range or coarse-grained forecast. This research work tried to utilize historical weather observation data and machine learning (ML) techniques to build models enabling specific weather forecast. Different settings of models were applied and the corresponding results were compared and analyzed in terms of training cost and prediction quality. The preliminary results indicate that the ML-enabled forecast model can serve as a supplementary source for people who need to know finer-grained whether condition. To improve the quality of the ML forecasting models, besides more fine-tuning and algorithms renovation, large volume of long-term historical weather data are critical since climate changes to a large extent, possess subtle periodical characteristics.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {71–74},
numpages = {4},
keywords = {tensorflow, neuron network configuration, machine learning model, Specific weather forecasting},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3183713.3197387,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3197387},
doi = {10.1145/3183713.3197387},
abstract = {There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1645–1650},
numpages = {6},
keywords = {machine learning, data integration, data enrichment},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3453688.3461483,
author = {Chen, Zhiyang and Ji, Weiqing and Peng, Yihao and Chen, Datao and Liu, Mingyu and Yao, Hailong},
title = {Machine Learning Based Acceleration Method for Ordered Escape Routing},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461483},
doi = {10.1145/3453688.3461483},
abstract = {Escape routing, especially ordered escape routing, is a critical design stage for both printed circuit boards (PCBs) and integrated fan-out (InFO) wafer-level chip-scale packages. Previous works formulate ordered escape routing as boolean satisfiability (SAT) or integer linear programming (ILP) problems. Although optimal routing solutions can be obtained by above-mentioned approaches, the runtime is unacceptable for large-scale designs due to the exponential time complexity of SAT and ILP solvers. In this paper, we first attempt to address ordered escape routing problems with machine learning. We propose a learning-based method to accelerate existing solvers by reducing the solution space of the original problem. The proposed method is flexible, which can be combined with different ordered escape routing algorithms. Specifically, a fully convolutional neural network is trained to predict the probability of each routing grid to be occupied by routing paths. Thus, routing grids with low-probability usage can be removed to reduce the solution space. Experimental results show that the proposed method is effective for both SAT and ILP solvers of ordered escape routing. It achieves an acceleration of 4∼ 370x on average, with a slight increase in the total wirelength. Also, our model has a strong generalization ability. Although it is trained on $10times 10$ pin array problems, it works well on larger problem sizes such as 14 x 14.},
booktitle = {Proceedings of the 2021 Great Lakes Symposium on VLSI},
pages = {365–370},
numpages = {6},
keywords = {convolutional neural networks, machine learning, ordered escape routing, printed circuit boards},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/3349341.3349460,
author = {Wang, Xiaojuan and Wang, Defu and Zhang, Yong and Jin, Lei and Song, Mei},
title = {Unsupervised Learning for Log Data Analysis Based on Behavior and Attribute Features},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349460},
doi = {10.1145/3349341.3349460},
abstract = {In some special application environments, network fault can lead to loss of important information or even mission failures, resulting in unpredictable losses. Therefore, it has certain research significance and practical value to evaluate the network status and predict the possible faults before performing the key tasks. Based on the logs collected by the router board in the real network, this paper analyses the behavior type, attribute information and the corresponding status value, and detects the hidden fault or network attack, so as to provide early warning information for operators. We propose a deep neural network model utilizing Long Short-Term Memory (LSTM) to predict the current number of level-1 logs. By comparing the predicted number of level-1 logs, it can detect abnormal behavior such as a surge in the number of logs. What's more, we perform semantic analysis on attribute information to construct attribute syntax forest, which assists maintenance staff to monitor the system through key fingerprint information in the log. In addition, we adopt attribute information and status value to train the unsupervised learning algorithm models such as Isolation Forest, OneClassSVM and LocalOutlierFactor. What's more, this paper analyses the results to find out the causes of log surge, and to assist operators in subsequent maintenance of the system.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {510–518},
numpages = {9},
keywords = {Unsupervised Machine Learning, Network Fault, Log Analysis, LSTM},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1007/978-3-030-60248-2_13,
author = {Li, Ningwei and Gao, Hang and Liu, Liang and Peng, Jianfei},
title = {Machine Learning-Based Attack Detection Method in Hadoop},
year = {2020},
isbn = {978-3-030-60247-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60248-2_13},
doi = {10.1007/978-3-030-60248-2_13},
abstract = {With the development of big data and cloud computing, big data clusters have become the main targets of attackers. However, The security of big data clusters is not guaranteed well. Based on our previous work, this paper proposes a big data cluster attack detection method based on Machine Learning. This method extracts 46 types of features related to the execution of cluster tasks, which solves the problem that the existing detection scheme has few features and detects a single attack type. Considering the impact of too many features on the detection results, this paper uses filter to extract key features, which can improve the accuracy of detection; Finally, this paper uses different machine learning algorithms to train different attack data extracted on different tasks. Experiments show that the attack detection scheme in this paper has an accuracy of over 88% for different attack.},
booktitle = {Algorithms and Architectures for Parallel Processing: 20th International Conference, ICA3PP 2020, New York City, NY, USA, October 2–4, 2020, Proceedings, Part III},
pages = {184–196},
numpages = {13},
keywords = {Attack detection, Machine learning, Security, Big data, Hadoop},
location = {New York, NY, USA}
}

@article{10.1109/TNET.2021.3112082,
author = {Zhang, Xiaoxi and Wang, Jianyu and Lee, Li-Feng and Yang, Tom and Kalra, Akansha and Joshi, Gauri and Joe-Wong, Carlee},
title = {Machine Learning on Volatile Instances: Convergence, Runtime, and Cost Tradeoffs},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3112082},
doi = {10.1109/TNET.2021.3112082},
abstract = {Due to the massive size of the neural network models and training datasets used in machine learning today, it is imperative to distribute stochastic gradient descent (SGD) by splitting up tasks such as gradient evaluation across multiple worker nodes. However, running distributed SGD can be prohibitively expensive because it may require specialized computing resources such as GPUs for extended periods of time. We propose cost-effective strategies to exploit volatile cloud instances that are cheaper than standard instances, but may be interrupted by higher priority workloads. To the best of our knowledge, this work is the first to quantify how variations in the number of active worker nodes (as a result of preemption) affect SGD convergence and the time to train the model. By understanding these trade-offs between preemption probability of the instances, accuracy, and training time, we are able to derive practical strategies for configuring distributed SGD jobs on volatile instances such as Amazon EC2 spot instances and other preemptible cloud instances. Experimental results show that our strategies achieve good training performance at substantially lower cost.},
journal = {IEEE/ACM Trans. Netw.},
month = nov,
pages = {215–228},
numpages = {14}
}

@article{10.1504/ijcvr.2021.115165,
author = {Sharma, Vipul and Mir, Roohie Naaz},
title = {Maximum entropy-based semi-supervised learning for automatic detection and recognition of objects using deep ConvNets},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {3},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2021.115165},
doi = {10.1504/ijcvr.2021.115165},
abstract = {Object detection and localisation is one of the major research areas in computer vision that is growing very rapidly. Currently, there is a plethora of pre-trained models for object detection including YOLO, mask RCNN, RCNN, fast RCNN, multi-box, etc. In this paper, we proposed a new framework for object detection called 'maximum entropy-based semi-supervised learning for automatic detection and recognition of objects'. The main objective of this paper is to recognise objects from a number of visual object classes in a realistic scene simultaneously. The major operations of our proposed approach are preprocessing, localisation, segmentation and object detection. In the preprocessing, three processes, noise reduction, intensity normalisation, and morphology are considered. Then localisation and object segmentation is performed using maximum entropy in which optimal threshold is detected and in the end, object detection is performed using deep ConvNet. The performance of the proposed framework is evaluated using MATLAB-R2018b and it is compared with some previous state of the art techniques in terms of localisation error, detection and segmentation accuracy along with computation time.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {328–356},
numpages = {28},
keywords = {segmentation and localisation, deep convolutional neural networks, weakly supervised learning, object detection, maximum entropy}
}

@inproceedings{10.1145/3472674.3473978,
author = {De Stefano, Manuel and Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea},
title = {Comparing within- and cross-project machine learning algorithms for code smell detection},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473978},
doi = {10.1145/3472674.3473978},
abstract = {Code smells represent a well-known problem in software engineering, since they are a notorious cause of loss of comprehensibility and maintainability. The most recent efforts in devising automatic machine learning-based code smell detection techniques have achieved unsatisfying results so far. This could be explained by the fact that all these approaches follow a within-project classification, i.e. training and test data are taken from the same source project, which combined with the imbalanced nature of the problem, produces datasets with a very low number of instances belonging to the minority class (i.e. smelly instances). In this paper, we propose a cross-project machine learning approach and compare its performance with a within-project alternative. The core idea is to use transfer learning to increase the overall number of smelly instances in the training datasets. Our results have shown that cross-project classification provides very similar performance with respect to within-project. Despite this finding does not yet provide a step forward in increasing the performance of ML techniques for code smell detection, it sets the basis for further investigations.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {1–6},
numpages = {6},
keywords = {Transfer Learning, Empirical Software Engineering, Code smells},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1145/3448612,
author = {Wan, Liangtian and Zhang, Mingyue and Sun, Lu and Wang, Xianpeng},
title = {Machine Learning Empowered IoT for Intelligent Vehicle Location in Smart Cities},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3448612},
doi = {10.1145/3448612},
abstract = {Intelligent Transportation System (ITS) can boost the development of smart cities, and artificial intelligence and edge computing are key technologies that support the implementation of ITS. Vehicle localization is critical for ITS since the safety driving and location-aware serves highly depend on the accurate location information. In this article, we construct a vehicle localization system architecture composed of multiple Internet of Things (IoT) with arbitrary array configuration and a large amount of vehicles in smart cities. In order to deal with the coexisting of circular and non-circular signals transmitted by vehicles, we proposed several vehicle number estimation methods for non-circular signals. Based on the machine learning technique, we extend the vehicle number estimation method into mixed signals in more complex scenario of smart cities. Then the DOA estimation method for non-circular signals based on IoT is proposed, and then the performance of this method is analyzed as well. Simulation outcomes verify the excellent performance of the proposed vehicle number estimation methods and the DOA estimation method in smart cities, and the vehicle positions can be achieved with high estimation accuracy.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {71},
numpages = {25},
keywords = {vehicle location, machine learning, IoT, Smart cities}
}

@article{10.1287/trsc.2021.1084,
author = {Tahir, Adil and Quesnel, Fr\'{e}d\'{e}ric and Desaulniers, Guy and El Hallaoui, Issmail and Yaakoubi, Yassine},
title = {An Improved Integral Column Generation Algorithm Using Machine Learning for Aircrew Pairing},
year = {2021},
issue_date = {November–December 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {6},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2021.1084},
doi = {10.1287/trsc.2021.1084},
abstract = {The crew-pairing problem (CPP) is solved in the first step of the crew-scheduling process. It consists of creating a set of pairings (sequence of flights, connections, and rests forming one or multiple days of work for an anonymous crew member) that covers a given set of flights at minimum cost. Those pairings are assigned to crew members in a subsequent crew-rostering step. In this paper, we propose a new integral column-generation algorithm for the CPP, called improved integral column generation with prediction (I2CGp), which leaps from one integer solution to another until a near-optimal solution is found. Our algorithm improves on previous integral column-generation algorithms by introducing a set of reduced subproblems. Those subproblems only contain flight connections that have a high probability of being selected in a near-optimal solution and are, therefore, solved faster. We predict flight-connection probabilities using a deep neural network trained in a supervised framework. We test I2CGp on several real-life instances and show that it outperforms a state-of-the-art integral column-generation algorithm as well as a branch-and-price heuristic commonly used in commercial airline planning software, in terms of both solution costs and computing times. We highlight the contributions of the neural network to I2CGp.},
journal = {Transportation Science},
month = nov,
pages = {1411–1429},
numpages = {19},
keywords = {deep neural network, integral column generation, machine learning, crew pairing}
}

@inproceedings{10.5555/3540261.3541531,
author = {Cousins, Cyrus},
title = {An axiomatic theory of provably-fair welfare-centric machine learning},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We address an inherent difficulty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justified alternative setting, and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimization tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We define a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justification via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss and maximizing welfare. Building upon these concepts, we define fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justified) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on firm theoretical ground, as it yields statistical — and in some cases computational — efficiency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1270},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-21290-2_39,
author = {Nalchigar, Soroosh and Yu, Eric and Obeidi, Yazan and Carbajales, Sebastian and Green, John and Chan, Allen},
title = {Solution Patterns for Machine Learning},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_39},
doi = {10.1007/978-3-030-21290-2_39},
abstract = {Despite the hype around machine learning (ML), many organizations are struggling to derive business value from ML capabilities. Design patterns have long been used in software engineering to enhance design effectiveness and to speed up the development process. The contribution of this paper is two-fold. First, it introduces solution patterns as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. Second, it reports on the feasibility, expressiveness, and usefulness of solution patterns for ML, in collaboration with an industry partner. It provides a prototype architecture for supporting the use of solution patterns in real world scenarios. It presents a proof-of-concept implementation of the architecture and illustrates its feasibility. Findings from the collaboration suggest that solution patterns can have a positive impact on ML design and development efforts.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {627–642},
numpages = {16},
keywords = {Conceptual modeling, Machine learning, Advanced analytics, Business analytics, Design patterns},
location = {Rome, Italy}
}

@inproceedings{10.1145/3214834.3214841,
author = {Keller, Alexander and K\v{r}iv\'{a}nek, Jaroslav and Nov\'{a}k, Jan and Kaplanyan, Anton and Salvi, Marco},
title = {Machine learning and rendering},
year = {2018},
isbn = {9781450358095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214834.3214841},
doi = {10.1145/3214834.3214841},
abstract = {Machine learning techniques just recently enabled dramatic improvements in both realtime and offline rendering. In this course, we introduce the basic principles of machine learning and review their relations to rendering. Besides fundamental facts like the mathematical identity of reinforcement learning and the rendering equation, we cover efficient and surprisingly elegant solutions to light transport simulation, participating media, noise removal, and anti-aliasing.},
booktitle = {ACM SIGGRAPH 2018 Courses},
articleno = {19},
numpages = {2},
keywords = {rendering, reinforcement learning, path tracing, parametric mixture models, neural networks, integral equations, anti-aliasing},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3444685.3446265,
author = {Petit, Thomas and Letessier, Pierre and Duffner, Stefan and Garcia, Christophe},
title = {Unsupervised learning of co-occurrences for face images retrieval},
year = {2021},
isbn = {9781450383080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444685.3446265},
doi = {10.1145/3444685.3446265},
abstract = {Despite a huge leap in performance of face recognition systems in recent years, some cases remain challenging for them while being trivial for humans. This is because a human brain is exploiting much more information than the face appearance to identify a person. In this work, we aim at capturing the social context of unlabeled observed faces in order to improve face retrieval. In particular, we propose a framework that substantially improves face retrieval by exploiting the faces occurring simultaneously in a query's context to infer a multi-dimensional social context descriptor. Combining this compact structural descriptor with the individual visual face features in a common feature vector considerably increases the correct face retrieval rate and allows to disambiguate a large proportion of query results of different persons that are barely distinguishable visually.To evaluate our framework, we also introduce a new large dataset of faces of French TV personalities organised in TV shows in order to capture the co-occurrence relations between people. On this dataset, our framework is able to improve the mean Average Precision over a set of internal queries from 67.93% (using only facial features extracted with a state-of-the-art pre-trained model) to 78.16% (using both facial features and faces co-occurrences), and from 67.88% to 77.36% over a set of external queries.},
booktitle = {Proceedings of the 2nd ACM International Conference on Multimedia in Asia},
articleno = {12},
numpages = {7},
keywords = {retrieval, person recognition, facial recognition, co-occurrences},
location = {Virtual Event, Singapore},
series = {MMAsia '20}
}

@article{10.1007/s10115-017-1144-z,
author = {Chen, Zhiyuan and Khoa, Le Dinh and Teoh, Ee Na and Nazir, Amril and Karuppiah, Ettikan Kandasamy and Lam, Kim Sim},
title = {Machine learning techniques for anti-money laundering (AML) solutions in suspicious transaction detection: a review},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1144-z},
doi = {10.1007/s10115-017-1144-z},
abstract = {Money laundering has been affecting the global economy for many years. Large sums of money are laundered every year, posing a threat to the global economy and its security. Money laundering encompasses illegal activities that are used to make illegally acquired funds appear legal and legitimate. This paper aims to provide a comprehensive survey of machine learning algorithms and methods applied to detect suspicious transactions. In particular, solutions of anti-money laundering typologies, link analysis, behavioural modelling, risk scoring, anomaly detection, and geographic capability have been identified and analysed. Key steps of data preparation, data transformation, and data analytics techniques have been discussed; existing machine learning algorithms and methods described in the literature have been categorised, summarised, and compared. Finally, what techniques were lacking or under-addressed in the existing research has been elaborated with the purpose of pinpointing future research directions.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {245–285},
numpages = {41},
keywords = {Unsupervised learning, Supervised learning, Risk scoring, Link analysis, Geographic capability, Data mining methods and algorithms, Behavioural modelling, Anti-money laundering typologies, Anti-money laundering, Anomaly detection}
}

@phdthesis{10.5555/AAI28225425,
author = {Liang, Ming},
advisor = {Min, Chi, and David, Lubkeman, and Mesut, Baran, and Ning, Lu,},
title = {A Machine Learning-Based Approach for Synthetic Distribution Feeder Generation},
year = {2020},
isbn = {9798684618680},
publisher = {North Carolina State University},
abstract = {Test systems are widely used by researchers and engineers to test conceptual designs, optimize parameter settings, and validate performance. However, developing high-fidelity distribution feeder models requires access to utility network models and customer data, which is a major barrier for the research community to have unrestrictive, unlimited number of customizable, realistic test systems for research and development purpose. So far, there has been very little attempt made towards the manual and static test system design principles, making creating an ensemble of test systems from actual feeder models a daunting task. Motivated by this, the dissertation aims at developing an end-to-end, machine-learning-based approach for automated, customizable test feeder generation using actual feeder models as inputs.This dissertation presents a novel, automated, generative adversarial networks (GAN) based synthetic feeder generation mechanism, abbreviated as FeederGAN. FeederGAN digests real feeder models represented by directed graphs via a deep learning framework powered by GAN and graph convolutional networks (GCN). Information of a distribution feeder circuit is extracted from its model input files so that the device connectivity is mapped onto the adjacency matrix and the device characteristics, such as circuit types (i.e., 3-phase, 2-phase, and 1-phase) and component attributes (e.g., length and current ratings), are mapped onto the attribute matrix. Then, Wasserstein distance is used to optimize the GAN and GCN is used to discriminate the generated graphs from the actual ones. A greedy method based on graph theory is developed to reconstruct the feeder using the generated adjacency and attribute matrices. After the feeder topologies and attributes are generated, we then use a statistical, rule-based method to generate the load transformers. The rules make the transformers follows line capacity constraints, line attributes constraints and topology&nbsp;constraints. Finally, we write the generated feeder file with load information into OpenDSS format and run combined case studies. The results show that the GAN generated feeders resemble the actual feeder in both topology and attributes verified by visual inspection and by empirical statistics obtained from actual distribution feeders.In the second part of the dissertation, a synthetic load generation method is developed via a novel sequential energy disaggregation (SED) algorithm. The SED algorithm is presented for extracting heating and cooling energy consumptions from residential and small commercial building loads using low-resolution (i.e. 15-minute, 30-minute, and 60-minute) smart meter data. The method is validated using data collected from 137 households in the PECAN street project. Results show that the proposed SED method is computationally efficient, simple to implement, and robust in performance. Based on the SED algorithm developed, case study is conducted on buildings with photovoltaic (PV) systems and electric vehicles (EVs). Among the cases, load database of zero-net energy (ZNE) cases, ZNE ready cases, ZNE with EV cases is built up. Therefore, those load data can be then used together with the generated synthetic feeders to test different planning and operational strategies.},
note = {AAI28225425}
}

@article{10.1007/s11042-019-08206-8,
author = {Dornaika, F. and Elorza, A. and Wang, K. and Arganda-Carreras, I.},
title = {Image-based face beauty analysis via graph-based semi-supervised learning},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {3–4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08206-8},
doi = {10.1007/s11042-019-08206-8},
abstract = {Automatic facial beauty analysis has become an emerging research topic. Despite some achieved advances, current methods and systems suffer from at least two limitations. Firstly, many developed systems rely on the use of ad-hoc hand-crafted features that were designed for generic pattern recognition problems. Secondly, while Deep Convolutional Neural Nets (DCNN) have been recently demonstrated to be a promising area of research in statistical machine learning, their use for automatic face beauty analysis may not guarantee optimal performances due to the use of a limited amount of face images with beauty scores. In this paper, we attempt to overcome these two main limitations by jointly exploiting two tricks. First, instead of using hand-crafted face features we use deep features of a pre-trained DCNN able to generate a high-level representation of a face image. Second, we exploit manifold learning theory and deploy three graph-based semi-supervised learning methods in order to enrich model learning without the need of additional labeled face images. These schemes perform graph-based score propagation. The proposed schemes were tested on three public datasets for beauty analysis: SCUT-FBP, M2B, and SCUT-FBP5500. These experiments, as well as many comparisons with supervised schemes, show that the scheme coined Kernel Flexible Manifold Embedding compares favorably with many supervised schemes. They also show that its performances in terms of error prediction and Pearson Correlation are better than those reported for the used datasets.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {3005–3030},
numpages = {26},
keywords = {Deep face features, Graph-based label propagation, Semi-supervised learning, Image-based face beauty analysis}
}

@article{10.1016/j.patcog.2018.01.036,
author = {Sanakoyeu, Artsiom and Bautista, Miguel A. and Ommer, Bjrn},
title = {Deep unsupervised learning of visual similarities},
year = {2018},
issue_date = {June 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.01.036},
doi = {10.1016/j.patcog.2018.01.036},
abstract = {Unsupervised visual similarity learning is framed as a surrogate classification task.Use weak estimates of local similarities to group samples into compact cliques.Train a ConvNet to learn visual similarities by learning to categorize cliques.Optimization problem to sample training minibatches without conflicting relations.Competitive performance on detailed posture analysis and object classification. Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to computer vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.},
journal = {Pattern Recogn.},
month = jun,
pages = {331–343},
numpages = {13},
keywords = {Visual similarity learning, Self-supervised learning, Object retrieval, Human pose analysis, Deep learning}
}

@article{10.1007/s11390-020-9668-1,
author = {Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali and Abran, Alain},
title = {Machine Learning Techniques for Software Maintainability Prediction: Accuracy Analysis},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9668-1},
doi = {10.1007/s11390-020-9668-1},
abstract = {Maintaining software once implemented on the end-user side is laborious and, over its lifetime, is most often considerably more expensive than the initial software development. The prediction of software maintainability has emerged as an important research topic to address industry expectations for reducing costs, in particular, maintenance costs. Researchers and practitioners have been working on proposing and identifying a variety of techniques ranging from statistical to machine learning (ML) for better prediction of software maintainability. This review has been carried out to analyze the empirical evidence on the accuracy of software product maintainability prediction (SPMP) using ML techniques. This paper analyzes and discusses the findings of 77 selected studies published from 2000 to 2018 according to the following criteria: maintainability prediction techniques, validation methods, accuracy criteria, overall accuracy of ML techniques, and the techniques offering the best performance. The review process followed the well-known systematic review process. The results show that ML techniques are frequently used in predicting maintainability. In particular, artificial neural network (ANN), support vector machine/regression (SVM/R), regression &amp; decision trees (DT), and fuzzy &amp; neuro fuzzy (FNF) techniques are more accurate in terms of PRED and MMRE. The N-fold and leave-one-out cross-validation methods, and the MMRE and PRED accuracy criteria are frequently used in empirical studies. In general, ML techniques outperformed non-machine learning techniques, e.g., regression analysis (RA) techniques, while FNF outperformed SVM/R, DT, and ANN in most experiments. However, while many techniques were reported superior, no specific one can be identified as the best.},
journal = {J. Comput. Sci. Technol.},
month = oct,
pages = {1147–1174},
numpages = {28},
keywords = {maintainability prediction, machine learning technique, accuracy value, accuracy criterion}
}

@inproceedings{10.1145/3416013.3426458,
author = {Ul Mustafa, Raza and Ferlin, Simone and Esteve Rothenberg, Christian and Raca, Darijo and J. Quinlan, Jason},
title = {A Supervised Machine Learning Approach for DASH Video QoE Prediction in 5G Networks},
year = {2020},
isbn = {9781450381208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416013.3426458},
doi = {10.1145/3416013.3426458},
abstract = {Future fifth generation (5G) networks are envisioned to provide improved Quality-of-Experience (QoE) for applications by means of higher data rates, low and ultra-reliable latency and very high reliability. Proving increasing beneficial for mobile devices running multimedia applications. However, there exist two main co-related challenges in multimedia delivery in 5G. Namely, balancing operator provisioning and client expectations. To this end, we investigate how to build a QoE-aware network that guarantees at run-time that the end-to-end user experience meets the end users' expectations at the same that the network's Quality of Service (QoS) varies. The contribution of this paper is twofold: first, we consider a Dynamic Adaptive Streaming over HTTP (DASH) video application in a realistic emulation environment derived from real 5G traces in static and mobility scenarios to assess the QoE performance of three state-of-art Adaptive Bitrate Streaming (ABS) algorithm categories: Hybrid - Elastic and Arbiter+; buffer-based - BBA and Logistic; and rate-based - Exponential and Conventional. Second, we propose a Machine Learning (ML) classifier to predict user satisfaction which considers network metrics, such as RTT, throughput, and number of packets. Our proposed model does not rely on knowledge about the application or specific traffic information. We show that our ML classifiers achieve a QoE prediction accuracy of 87.63 % and 79 % for static and mobility scenarios, respectively.},
booktitle = {Proceedings of the 16th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {57–64},
numpages = {8},
keywords = {video streaming, machine learning, QoS, QoE prediction, DASH, 5G},
location = {Alicante, Spain},
series = {Q2SWinet '20}
}

@article{10.1016/j.ins.2015.01.019,
author = {Peng, Hong and Wang, Jun and P\'{e}rez-Jim\'{e}nez, Mario J. and Riscos-N\'{u}\~{n}ez, Agust\'{\i}n},
title = {An unsupervised learning algorithm for membrane computing},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {304},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.01.019},
doi = {10.1016/j.ins.2015.01.019},
abstract = {This paper focuses on the unsupervised learning problem within membrane computing, and proposes an innovative solution inspired by membrane computing techniques, the fuzzy membrane clustering algorithm. An evolution-communication P system with nested membrane structure is the core component of the algorithm. The feasible cluster centers are represented by means of objects, and three types of membranes are considered: evolution, local store, and global store. Based on the designed membrane structure and the inherent communication mechanism, a modified differential evolution mechanism is developed to evolve the objects in the system. Under the control of the evolution-communication mechanism of the P system, the proposed fuzzy clustering algorithm achieves good fuzzy partitioning for a data set. The proposed fuzzy clustering algorithm is compared to three recently-developed and two classical clustering algorithms for five artificial and five real-life data sets.},
journal = {Inf. Sci.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Unsupervised learning, P system, Membrane computing, Fuzzy clustering, Evolution-communication P system, Data clustering}
}

@phdthesis{10.5555/AAI28866778,
author = {Sommer, David E. and C., Fu, Kai-Mei and V, Andreev, Anton},
advisor = {T, Dunham, Scott},
title = {Modeling Complex Multicomponent Materials with First-Principles Based Statistical Mechanics and Machine Learning},
year = {2021},
isbn = {9798780638827},
publisher = {University of Washington},
abstract = {Recent progress in the engineering of multicomponent, solid-state compounds for optoelectronic applications has entailed an ever expanding range of material chemistries and a rapid increase in material complexity. For example, within the classes of chalcogenide and halide-perovskite semiconductors, fundamental material properties can be effectively tuned by alloying various isovalent chemical species and by the controlled incorporation of dopants. In characterizing these materials at an atomistic level, one has to contend not only with the presence of a multitude of point defects, but also with the potential formation of ordering and instabilities against secondary phases. This poses a fundamental challenge to first-principles modeling that is only exacerbated by an exponentially large configuration space. This work is primarily concerned with the development and application of methods, rooted in statistical mechanics and machine learning, for modeling these complex, multicomponent semiconductors. Much of this manuscript focuses on modeling defects and configurational disorder in specific chalcogenides and halide-perovskites of particular technological relevance, employing well-established multiscale methods such as density functional theory, point defect thermodynamics, statistical learning, cluster expansions and Monte Carlo simulation. The latter portion of this work concerns more recent developments in the fields of deep learning and tensor networks, adapted and applied to material structure-property prediction.},
note = {AAI28866778}
}

@inproceedings{10.1145/3409963.3410492,
author = {Chen, Jingde and Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {Machine learning for load balancing in the Linux kernel},
year = {2020},
isbn = {9781450380690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409963.3410492},
doi = {10.1145/3409963.3410492},
abstract = {The OS load balancing algorithm governs the performance gains provided by a multiprocessor computer system. The Linux's Completely Fair Scheduler (CFS) scheduler tracks process loads by average CPU utilization to balance workload between processor cores. That approach maximizes the utilization of processing time but overlooks the contention for lower-level hardware resources. In servers running compute-intensive workloads, an imbalanced need for limited computing resources hinders execution performance. This paper solves the above problem using a machine learning (ML)-based resource-aware load balancer. We describe (1) low-overhead methods for collecting training data; (2) an ML model based on a multi-layer perceptron model that imitates the CFS load balancer based on the collected training data; and (3) an in-kernel implementation of inference on the model. Our experiments demonstrate that the proposed model has an accuracy of 99% in making migration decisions and while only increasing the latency by 1.9 μs.},
booktitle = {Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {67–74},
numpages = {8},
keywords = {operating system, neural network, machine learning, load balancing, completely fair scheduler, Linux kernel},
location = {Tsukuba, Japan},
series = {APSys '20}
}

@article{10.14778/3352063.3352115,
author = {Sabek, Ibrahim and Mokbel, Mohamed F.},
title = {Machine learning meets big spatial data},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352115},
doi = {10.14778/3352063.3352115},
abstract = {The proliferation in amounts of generated data has propelled the rise of scalable machine learning solutions to efficiently analyze and extract useful insights from such data. Meanwhile, spatial data has become ubiquitous, e.g., GPS data, with increasingly sheer sizes in recent years. The applications of big spatial data span a wide spectrum of interests including tracking infectious disease, climate change simulation, drug addiction, among others. Consequently, major research efforts are exerted to support efficient analysis and intelligence inside these applications by either providing spatial extensions to existing machine learning solutions or building new solutions from scratch. In this 90-minutes tutorial, we comprehensively review the state-of-the-art work in the intersection of machine learning and big spatial data. We cover existing research efforts and challenges in three major areas of machine learning, namely, data analysis, deep learning and statistical inference, as well as two advanced spatial machine learning tasks, namely, spatial features extraction and spatial sampling. We also highlight open problems and challenges for future research in this area.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1982–1985},
numpages = {4},
keywords = {scalability, machine learning, big spatial data}
}

@article{10.1007/s00500-018-3597-8,
author = {Candelieri, Antonio and Archetti, Francesco},
title = {Global optimization in machine learning: the design of a predictive analytics application},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {9},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3597-8},
doi = {10.1007/s00500-018-3597-8},
abstract = {Global optimization, especially Bayesian optimization, has become the tool of choice in hyperparameter tuning and algorithmic configuration to optimize the generalization capability of machine learning algorithms. The contribution of this paper was to extend this approach to a complex algorithmic pipeline for predictive analytics, based on time-series clustering and artificial neural networks. The software environment R has been used with mlrMBO, a comprehensive and flexible toolbox for sequential model-based optimization. Random forest has been adopted as surrogate model, due to the nature of decision variables (i.e., conditional and discrete hyperparameters) of the case studies considered. Two acquisition functions have been considered: Expected improvement and lower confidence bound, and results are compared. The computational results, on a benchmark and a real-world dataset, show that even in a complex search space, up to 80 dimensions related to integer, categorical, and conditional variables (i.e., hyperparameters), sequential model-based optimization is an effective solution, with lower confidence bound requiring a lower number of function evaluations than expected improvement to find the same optimal solution.},
journal = {Soft Comput.},
month = may,
pages = {2969–2977},
numpages = {9},
keywords = {Machine learning, Hyperparameters optimization, Global optimization}
}

@article{10.1145/3485133,
author = {Machado, Gabriel Resende and Silva, Eug\^{e}nio and Goldschmidt, Ronaldo Ribeiro},
title = {Adversarial Machine Learning in Image Classification: A Survey Toward the Defender’s Perspective},
year = {2021},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485133},
doi = {10.1145/3485133},
abstract = {Deep Learning algorithms have achieved state-of-the-art performance for Image Classification. For this reason, they have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been proposed recently in the literature. However, devising an efficient defense mechanism has proven to be a difficult task, since many approaches demonstrated to be ineffective against adaptive attackers. Thus, this article aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, nevertheless, with a defender’s perspective. This article introduces novel taxonomies for categorizing adversarial attacks and defenses, as well as discuss possible reasons regarding the existence of adversarial examples. In addition, relevant guidance is also provided to assist researchers when devising and evaluating defenses. Finally, based on the reviewed literature, this article suggests some promising paths for future research.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {8},
numpages = {38},
keywords = {defense methods, adversarial attacks, deep neural networks, adversarial images, image classification, Computer vision}
}

@inproceedings{10.1145/3410352.3410747,
author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
title = {Machine Learning Techniques for Automated Software Fault Detection via Dynamic Execution Data: Empirical Evaluation Study},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410747},
doi = {10.1145/3410352.3410747},
abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {15},
numpages = {12},
keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1007/978-3-030-26061-3_28,
author = {Levonevskiy, Dmitriy and Malov, Dmitrii and Vatamaniuk, Irina},
title = {Estimating Aggressiveness of Russian Texts by Means of Machine Learning},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_28},
doi = {10.1007/978-3-030-26061-3_28},
abstract = {This paper considers emotional assessment of texts in Russian using machine learning on the example of aggression detection. It summarizes the related work, methods, models and datasets, describes actual problems, proposes a text processing pipeline and a software system for training neural networks on heterogeneous datasets. The experiments show that neural networks trained on the annotated corpora both in Russian and English, allow to determine whether a text item in Russian contains an aggressive message. Authors thoroughly compare different assessment methods, particularly corpus-based approaches, machine learning solutions and hybrid variants. Results, obtained here, can be used to estimate the aggressiveness probability, for example, to rank messages for subsequent manual verification. They also enable feasibility studies on the possibilities of detecting a particular type of emotion in a text using corpora in other languages. The paper highlights further research directions, where different Python toolkits (NLTK, Keras) could be used for better model performance.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {270–279},
numpages = {10},
keywords = {Machine learning, Neural networks, Aggressive text detection, Text analysis, Natural language processing, Sentiment Analysis, Emotion detection},
location = {Istanbul, Turkey}
}

@article{10.1016/j.mejo.2021.105198,
author = {Abazyan, Suren and Melikyan, Vazgen},
title = {Enhanced pin-access prediction and design optimization with machine learning integration},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {116},
number = {C},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2021.105198},
doi = {10.1016/j.mejo.2021.105198},
journal = {Microelectron. J.},
month = oct,
numpages = {5},
keywords = {Prediction and optimization, Machine learning, Pin access}
}

@article{10.1016/j.patrec.2015.08.009,
author = {Yang, Yun and Liu, Xingchen},
title = {A robust semi-supervised learning approach via mixture of label information},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {68},
number = {P1},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2015.08.009},
doi = {10.1016/j.patrec.2015.08.009},
abstract = {Due to the fact that limited amounts of labeled data are normally available in real-world, semi-supervised learning has become a popular option, where we expect to use unlabeled data information to improve the learning performance. However, how to use such unlabeled information to make the predicted labels more reliable remains to be a key for any successful learning. In this paper, we propose a semi-supervised learning framework via combination of semi-supervised clustering and semi-supervised classification. In our approach, the predicted labels are selected by both the constrained k-means and safe semi-supervised SVM (S4VMs) to improve the reliability of the predicted labels. Extensive evaluations on collection of benchmarks and real-world action recognition datasets show that the proposed technique outperforms the others.},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {15–21},
numpages = {7},
keywords = {Semi-supervised learning, Clustering, Classification}
}

@article{10.1145/3453158,
author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior},
title = {Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453158},
doi = {10.1145/3453158},
abstract = {In recent years, machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This article comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker’ s goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. To the best of our knowledge, this work is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {108},
numpages = {36},
keywords = {poisoning attacks, evasion attacks, deep learning, cyber security, adversarial machine learning, adversarial examples, Adversarial learning}
}

@inproceedings{10.1145/3461702.3462527,
author = {Hopkins, Aspen and Booth, Serena},
title = {Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462527},
doi = {10.1145/3461702.3462527},
abstract = {Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community---for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints---tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {134–145},
numpages = {12},
keywords = {machine learning practice, contextual inquiry, big tech, ML developers},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3301275.3302324,
author = {Gil, Yolanda and Honaker, James and Gupta, Shikhar and Ma, Yibo and D'Orazio, Vito and Garijo, Daniel and Gadewar, Shruti and Yang, Qifan and Jahanshad, Neda},
title = {Towards human-guided machine learning},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302324},
doi = {10.1145/3301275.3302324},
abstract = {Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user's knowledge about the data available. We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {614–624},
numpages = {11},
keywords = {task analysis, scientific workflows, human-guided machine learning, automated machine learning (AutoML)},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.5555/3495724.3496555,
author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
title = {Unsupervised learning of visual features by contrasting cluster assignments},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of con-trastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or "views") of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a "swapped" prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {831},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3360322.3360854,
author = {Murthy, Akshay and Green, Curtis and Stoleru, Radu and Bhunia, Suman and Swanson, Charles and Chaspari, Theodora},
title = {Machine Learning-based Irrigation Control Optimization},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360854},
doi = {10.1145/3360322.3360854},
abstract = {Irrigation schedules on traditional irrigation controllers tend to disperse too much water by design and cause runoff, which results in wastage of water and pollution of water sources. Previous attempts at tackling this problem either used expensive sensors or ignored site-specific factors. In this paper, we propose Weather-aware Runoff Prevention Irrigation Control (WaRPIC), a low-cost, practical solution that optimally applies water, while preventing runoff for each sprinkler zone. WaRPIC involves homeowner-assisted data collection on the landscape. The gathered data is used to build site-specific machine learning models that can accurately predict the Maximum Allowable Runtime (MAR) for each sprinkler zone given weather data obtained from the nearest weather station. We have also developed a low-cost module that can retrofit irrigation controllers in order to modify its irrigation schedule. We built a neural network-based model that predicts the MAR for any set of antecedent conditions. The model's prediction is compared with a state-of-the-art irrigation controller and the volume of water wasted by WaRPIC is only 2.6% of that of the state-of-the-art. We have deployed our modules at residences and estimate that the average homeowner can save 38,826 gallons of water over the course of May-Oct 2019, resulting in savings of $192.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {213–222},
numpages = {10},
keywords = {turf-grass, smart irrigation, machine learning, control, Internet-of-Things},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3485279.3485302,
author = {Swieso, Sloan and Yao, Powen and Miller, Mark and Jothi, Adityan and Zhao, Andrew and Zyda, Michael},
title = {Toward Using Machine Learning-Based Motion Gesture for 3D Text Input},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485302},
doi = {10.1145/3485279.3485302},
abstract = {In this paper, we discuss our implementation of a gesture-based 3-dimensional typing system within virtual reality. Rather than the conventional point-and-click keyboard commonly found in immersive technologies, we explore using unique gestures with the controller to enter a specific key. To map these gestures and movements to their respective key, we utilize machine learning techniques to avoid naive hard-coded implementations. The result of the trained model is a text input system that adapts to the user’s gestures, rather than forcing the user to conform to the system’s definition of a gesture. Our goal is to work toward a viable alternative to standard virtual reality keyboards and typing systems.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {28},
numpages = {2},
keywords = {virtual reality, text entry, machine learning, gesture typing},
location = {Virtual Event, USA},
series = {SUI '21}
}

@article{10.1007/s00500-020-05253-4,
author = {Al-Yarimi, Fuad Ali Mohammed and Munassar, Nabil Mohammed Ali and Bamashmos, Mohammed Hasan Mohammed and Ali, Mohammed Yousef Salem},
title = {RETRACTED ARTICLE: Feature optimization by discrete weights for heart disease prediction using supervised learning},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {3},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05253-4},
doi = {10.1007/s00500-020-05253-4},
abstract = {The topic predictive analytics is the ray that lightning the way to patch the gap between accuracy in decision-making by the expertise and the inexperience. In particular, the health domain is more crucial about disease prediction accuracy. The disease diagnosis by clinical practitioner correlates to his exposer toward the clinical observations of the disease. However, the perceptions of an experienced clinical practitioner on a medical record often fail to identify the premature states of the disease, which costs patient life in the sector of critical diseases such as heart diseases. Hence, contemporary computer science engineering research has more attention to define substantial predictive analytics built by machine learning toward heart disease prediction. The critical objective to define predictive analytics with minimal false alarming is centric to potential training data corpus, and the optimal feature selection. In order to these arguments, the contribution of this manuscript aimed to portray the feature selection approach to perform supervised learning and label the given patient record is prone to heart disease or not with minimal false alarming. The contribution is a dynamic n-gram Features Optimization by Discrete Weights of the feature correlation. The experimental study signified the performance of the proposed model compared to the contemporary methods of feature selection for heart disease prediction.},
journal = {Soft Comput.},
month = feb,
pages = {1821–1831},
numpages = {11},
keywords = {Coronary Heart Disease (CHD), Decision Trees (DT), Nearest Neighbor Algorithm (K-NN), Support vector machines (SVM), Decision support system (DSS)}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Interacting products, Machine learning, Multi-objective search, Rule mining, Configuration, Product line}
}

@inproceedings{10.1007/978-3-030-52200-1_29,
author = {Brown, Christopher W. and Daves, Glenn Christopher},
title = {Applying Machine Learning to Heuristics for Real Polynomial Constraint Solving},
year = {2020},
isbn = {978-3-030-52199-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52200-1_29},
doi = {10.1007/978-3-030-52200-1_29},
abstract = {This paper considers the application of machine learning to automatically generating heuristics for real polynomial constraint solvers. We consider a specific choice-point in the algorithm for constructing an open Non-uniform Cylindrical Algebraic Decomposition (NuCAD) for a conjunction of constraints, and we learn a heuristic for making that choice. Experiments demonstrate the effectiveness of the learned heuristic. We hope that the approach we take to learning this heuristic, which is not a natural fit to machine learning, can be applied effectively to other choices in constraint solving algorithms.},
booktitle = {Mathematical Software – ICMS 2020: 7th International Conference, Braunschweig, Germany, July 13–16, 2020, Proceedings},
pages = {292–301},
numpages = {10},
keywords = {Machine learning, Non-linear polynomial constraints},
location = {Braunschweig, Germany}
}

@article{10.1016/j.knosys.2016.05.044,
author = {Lopes, Lucas A. and Machado, Vinicius P. and Rab\^{e}lo, Ricardo A.L. and Fernandes, Ricardo A.S. and Lima, Bruno V.A.},
title = {Automatic labelling of clusters of discrete and continuous data with supervised machine learning},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.044},
doi = {10.1016/j.knosys.2016.05.044},
abstract = {This study presents a definition of the labelling problem and a solution that is based on techniques for supervised learning, unsupervised learning and a discretisation model.A method with unsupervised learning is applied to the clustering problem, and a supervised learning algorithm will detect the relevant attributes to define each formed cluster.Some strategies are used to form a methodology that presents a label (based on attributes and values) for each provided cluster.Discretisation methods 226 will be used to determine the ranges of values of the attributes presented in the 227 labels.This methodology is applied to three different databases, in which acceptable results were achieved with an average that exceeds 92.89% of correctly labelled elements. The clustering problem has been considered one of the most relevant problems in the research area of unsupervised learning. However, the comprehension and definition of such clusters is not a trivial task, making necessary their identification, i.e., assign a label to each cluster. To address the problem of labelling clusters, this paper presents a methodology based on techniques for supervised learning, unsupervised learning and a discretization model. Thus, a method with unsupervised learning is applied to the clustering problem, and the supervised learning algorithm is responsible for detecting the meaningful attributes to define each formed cluster. Some strategies are used to form a methodology that presents a label (based on attributes and values) for each provided cluster. Such methodology is applied to three different databases, in which acceptable results were achieved with an average that exceeds 92.89% of correctly labelled elements.},
journal = {Know.-Based Syst.},
month = aug,
pages = {231–241},
numpages = {11},
keywords = {labelling, clustering, artificial neural networks, Machine learning}
}

@inproceedings{10.1145/3387168.3387219,
author = {Mohammad, Nurul Izzati and Ismail, Saiful Adli and Kama, Mohd Nazri and Yusop, Othman Mohd and Azmi, Azri},
title = {Customer Churn Prediction In Telecommunication Industry Using Machine Learning Classifiers},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387219},
doi = {10.1145/3387168.3387219},
abstract = {Customer churn is one of the main problems in telecommunication industry. This study aims to identify the factors that influence customer churn and develop an effective churn prediction model as well as provide best analysis of data visualization results. The dataset has been collected from Kaggle open data website. The proposed methodology for analysis of churn prediction covers several phases: data pre-processing, analysis, implementing machine learning algorithms, evaluation of the classifiers and choose the best one for prediction. Data preprocessing process involved three major action, which are data cleaning, data transformation and feature selection. Machine learning classifiers was chosen are Logistic Regression, Artificial Neural Network and Random Forest. Then, classifiers were evaluated by using performance measurement which are accuracy, precision, recall and error rate in order to find the best classifier. Based on this study, the output shows that logistic regression outperform compared to artificial neural network and random forest.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {34},
numpages = {7},
keywords = {Telecommunication Industry, Prediction, Machine Learning, Customer Churn},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.1007/978-3-030-58811-3_5,
author = {Chlioui, Imane and Abnane, Ibtissam and Idri, Ali},
title = {Comparing Statistical and Machine Learning Imputation Techniques in Breast Cancer Classification},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_5},
doi = {10.1007/978-3-030-58811-3_5},
abstract = {Missing data imputation is an important task when dealing with crucial data that cannot be discarded such as medical data. This study evaluates and compares the impacts of two statistical and two machine learning imputation techniques when classifying breast cancer patients, using several evaluation metrics. Mean, Expectation-Maximization (EM), Support Vector Regression (SVR) and K-Nearest Neighbor (KNN) were applied to impute 18% of missing data missed completely at random in the two Wisconsin datasets. Thereafter, we empirically evaluated these four imputation techniques when using five classifiers: decision tree (C4.5), Case Based Reasoning (CBR), Random Forest (RF), Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP). In total, 1380 experiments were conducted and the findings confirmed that classification using imputation based machine learning outperformed classification using statistical imputation. Moreover, our experiment showed that SVR was the best imputation method for breast cancer classification.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {61–76},
numpages = {16},
keywords = {Breast cancer, Data mining, Missing data imputation},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3359992.3366640,
author = {Sanchez, Odnan Ref and Ferlin, Simone and Pelsser, Cristel and Bush, Randy},
title = {Comparing Machine Learning Algorithms for BGP Anomaly Detection using Graph Features},
year = {2019},
isbn = {9781450369992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359992.3366640},
doi = {10.1145/3359992.3366640},
abstract = {The Border Gateway Protocol (BGP) coordinates the connectivity and reachability among Autonomous Systems, providing efficient operation of the global Internet. Historically, BGP anomalies have disrupted network connections on a global scale, i.e., detecting them is of great importance. Today, Machine Learning (ML) methods have improved BGP anomaly detection using volume and path features of BGP's update messages, which are often noisy and bursty. In this work, we identified different graph features to detect BGP anomalies, which are arguably more robust than traditional features. We evaluate such features through an extensive comparison of different ML algorithms, i.e., Naive Bayes classifier (NB), Decision Trees (DT), Random Forests (RF), Support Vector Machines (SVM), and Multi-Layer Perceptron (MLP), to specifically detect BGP path leaks. We show that SVM offers a good trade-off between precision and recall. Finally, we provide insights into the graph features' characteristics during the anomalous and non-anomalous interval and provide an interpretation of the ML classifier results.},
booktitle = {Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks},
pages = {35–41},
numpages = {7},
keywords = {machine learning, graph features, anomaly detection, BGP},
location = {Orlando, FL, USA},
series = {Big-DAMA '19}
}

@inproceedings{10.1145/3430036.3430058,
author = {Grimmeisen, Benedikt and Theissler, Andreas},
title = {The machine learning model as a guide: pointing users to interesting instances for labeling through visual cues},
year = {2020},
isbn = {9781450387507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430036.3430058},
doi = {10.1145/3430036.3430058},
abstract = {The labeling of datasets is an important task for supervised and semi-supervised machine learning that can be addressed with visual analytics. With model-based active learning and user-based interactive labeling, there are two complementary strategies for this task. We present an approach that combines the strengths of both areas and aims to guide users through model-based recommendations and highlighting in one interface when selecting and labeling instances. For this purpose, an active learning strategy is used to recommend useful instances in addition to the user-based selection of instances. We have implemented the approach and conducted a user survey to research the effects guidance by visual cues has on the users' selection strategies. The proposed approach combines both perspectives in a single interactive visualization to support the user with different degrees of guidance in the selection of instances. Our results of the user survey suggest that user guidance has a positive influence on the users' perceived confidence and difficulty in selecting instances, on their orientation, and on their perceived impression of the models' performance. A video of the approach is available: https://youtu.be/TYPWG85Akn0.},
booktitle = {Proceedings of the 13th International Symposium on Visual Information Communication and Interaction},
articleno = {6},
numpages = {8},
keywords = {visual-interactive labeling, visual analytics, machine learning, classification, active learning},
location = {Eindhoven, Netherlands},
series = {VINCI '20}
}

@inproceedings{10.1145/3417990.3420057,
author = {Moin, Armin and R\"{o}ssler, Stephan and Sayih, Marouane and G\"{u}nnemann, Stephan},
title = {From things' modeling language (ThingML) to things' machine learning (ThingML2)},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3420057},
doi = {10.1145/3417990.3420057},
abstract = {In this paper, we illustrate how to enhance an existing state-of-the-art modeling language and tool for the Internet of Things (IoT), called ThingML, to support machine learning on the modeling level. To this aim, we extend the Domain-Specific Language (DSL) of ThingML, as well as its code generation framework. Our DSL allows one to define things, which are in charge of carrying out data analytics. Further, our code generators can automatically produce the complete implementation in Java and Python. The generated Python code is responsible for data analytics and employs APIs of machine learning libraries, such as Keras, Tensorflow and Scikit Learn. Our prototype is available as open source software on Github.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {19},
numpages = {2},
keywords = {machine learning, internet of things, domain-specific modeling},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.5555/3495724.3496119,
author = {Mallis, Dimitrios and Sanchez, Enrique and Bell, Matt and Tzimiropoulos, Georgios},
title = {Unsupervised learning of object landmarks via self-training correspondence},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper addresses the problem of unsupervised discovery of object landmarks. We take a different path compared to existing works, based on 2 novel perspectives: (1) Self-training: starting from generic keypoints, we propose a self-training approach where the goal is to learn a detector that improves itself, becoming more and more tuned to object landmarks. (2) Correspondence: we identify correspondence as a key objective for unsupervised landmark discovery and propose an optimization scheme which alternates between recovering object landmark correspondence across different images via clustering and learning an object landmark descriptor without labels. Compared to previous works, our approach can learn landmarks that are more flexible in terms of capturing large changes in viewpoint. We show the favourable properties of our method on a variety of difficult datasets including LS3D, BBCPose and Human3.6M.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {395},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.3233/JIFS-189481,
author = {Wang, Linuo and Ramachandran, Varatharajan},
title = {Simulation of sports movement training based on machine learning and brain-computer interface},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189481},
doi = {10.3233/JIFS-189481},
abstract = {Injuries and hidden dangers in training have a greater impact on athletes ’careers. In particular, the brain function that controls the motor function area has a greater impact on the athlete ’s competitive ability. Based on this, it is necessary to adopt scientific methods to recognize brain functions. In this paper, we study the structure of motor brain-computer and improve it based on traditional methods. Moreover, supported by machine learning and SVM technology, this study uses a DSP filter to convert the preprocessed EEG signal X into a time series, and adjusts the distance between the time series to classify the data. In order to solve the inconsistency of DSP algorithms, a multi-layer joint learning framework based on logistic regression model is proposed, and a brain-machine interface system of sports based on machine learning and SVM is constructed. In addition, this study designed a control experiment to improve the performance of the method proposed by this study. The research results show that the method in this paper has a certain practical effect and can be applied to sports.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6409–6420},
numpages = {12},
keywords = {Machine learning, SVM, sports, brain-computer interface}
}

@article{10.1016/j.patrec.2021.10.008,
author = {Shin, Yu-Hyun and Baek, Seung Jun},
title = {Hopfield-type neural ordinary differential equation for robust machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.10.008},
doi = {10.1016/j.patrec.2021.10.008},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {180–187},
numpages = {8},
keywords = {Neural ODE, Adversarial defense, Hopfield-type network, Image classification, 41A05, 41A10, 65D05, 65D17}
}

@article{10.5555/2627435.2627447,
author = {Fox-Roberts, Patrick and Rosten, Edward},
title = {Unbiased generative semi-supervised learning},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs--an unusual property for a semi-supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {367–443},
numpages = {77},
keywords = {semi-supervised, generative model, bias, asymptotic bounds, Kullback-Leibler}
}

@inproceedings{10.1145/3458817.3476181,
author = {Dryden, Nikoli and B\"{o}hringer, Roman and Ben-Nun, Tal and Hoefler, Torsten},
title = {Clairvoyant prefetching for distributed machine learning I/O},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476181},
doi = {10.1145/3458817.3476181},
abstract = {I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments. Indeed, at large scale, I/O takes as much as 85% of training time. Addressing this I/O bottleneck necessitates careful optimization, as optimal data ingestion pipelines differ between systems, and require a delicate balance between access to local storage, external filesystems, and remote nodes. We introduce NoPFS, a machine learning I/O middleware, which provides a scalable, flexible, and easy-to-use solution to the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the random access pattern for training with SGD, it can exactly predict when and where a sample will be accessed. We combine this with an analysis of access patterns and a performance model to provide distributed caching policies that adapt to different datasets and storage hierarchies. NoPFS reduces I/O times and improves end-to-end training by up to 5.4\texttimes{} on the ImageNet-1k, ImageNet-22k, and CosmoFlow datasets.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {92},
numpages = {15},
keywords = {high-performance computing, deep learning, I/O},
location = {St. Louis, Missouri},
series = {SC '21}
}

@phdthesis{10.5555/AAI28716476,
author = {Gangopadhyay, Ahana and ShiNung, Ching, and Carlos, Ponce, and Baranidharan, Raman, and Shen, Zeng,},
advisor = {Shantanu, Chakrabartty,},
title = {A Neuromorphic Machine Learning Framework Based on the Growth Transform Dynamical System},
year = {2021},
isbn = {9798538124060},
publisher = {Washington University in St. Louis},
abstract = {As computation increasingly moves from the cloud to the source of data collection, there is a growing demand for specialized machine learning algorithms that can perform learning and inference at the edge in energy and resource-constrained environments. In this regard, we can take inspiration from small biological systems like insect brains that exhibit high energy-efficiency within a small form-factor, and show superior cognitive performance using fewer, coarser neural operations (action potentials or spikes) than the high-precision floating-point operations used in deep learning platforms. Attempts at bridging this gap using neuromorphic hardware has produced silicon brains that are orders of magnitude inefficient in energy dissipation as well as performance. This is because neuromorphic machine learning (ML) algorithms are traditionally built bottom-up, starting with neuron models that mimic the response of biological neurons and connecting them together to form a network. Neural responses and weight parameters are therefore not optimized w.r.t. any system objective, and it is not evident how individual spikes and the associated population dynamics are related to a network objective. On the other hand, conventional ML algorithms follow a top-down synthesis approach, starting from a system objective (that usually only models task efficiency), and reducing the problem to the model of a non-spiking neuron with non-local updates and little or no control over the population dynamics. I propose that a reconciliation of the two approaches may be key to designing scalable spiking neural networks that optimize for both energy and task efficiency under realistic physical constraints, while enabling spike-based encoding and learning based on local updates in an energy-based framework like traditional ML models.To this end, I first present a neuron model implementing a mapping based on polynomial growth transforms, which allows for independent control over spike forms and transient firing statistics. I show how spike responses are generated as a result of constraint violation while minimizing a physically plausible energy functional involving a continuous-valued neural variable, that represents the local power dissipation in a neuron. I then show how the framework could be extended to coupled neurons in a network by remapping synaptic interactions in a standard spiking network. I show how the network could be designed to perform a limited amount of learning in an energy-efficient manner even without synaptic adaptation by appropriate choices of network structure and parameters - through spiking SVMs that learn to allocate switching energy to neurons that are more important for classification and through spiking associative memory networks that learn to modulate their responses based on global activity. Lastly, I describe a backpropagation-less learning framework for synaptic adaptation where weight parameters are optimized w.r.t. a network-level loss function that represents spiking activity across the network, but which produces updates that are local. I show how the approach can be used for unsupervised and supervised learning such that minimizing a training error is equivalent to minimizing the network-level spiking activity. I build upon this framework to introduce end-to-end spiking neural network (SNN) architectures and demonstrate their applicability for energy and resource-efficient learning using a benchmark dataset.},
note = {AAI28716476}
}

@article{10.1016/j.procs.2021.07.023,
author = {Sarah, Neamat Al and Rifat, Fahmida Yasmin and Hossain, Md. Shohrab and Narman, Husnu S.},
title = {An Efficient Android Malware Prediction Using Ensemble machine learning algorithms},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {191},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.07.023},
doi = {10.1016/j.procs.2021.07.023},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {184–191},
numpages = {8},
keywords = {Drebin Dataset, Ensemble learning, Feature Selection, Machine Learning, Malware, Android}
}

@article{10.1007/s00165-021-00538-3,
author = {Bu, Lei and Liang, Yongjuan and Xie, Zhunyi and Qian, Hong and Hu, Yi-Qi and Yu, Yang and Chen, Xin and Li, Xuandong},
title = {Machine learning steered symbolic execution framework for complex software code},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00538-3},
doi = {10.1007/s00165-021-00538-3},
abstract = {During program traversing, symbolic execution collects path
conditions and feeds them to a constraint solver to obtain feasible
solutions. However, complex path conditions, like nonlinear
constraints, which widely appear in programs, are hard to be handled
efficiently by the existing solvers. In this paper, we adapt the
classical symbolic execution framework with a machine learning
approach for constraint satisfaction. The approach samples and
learns from different solutions to identify potentially feasible
area. This sampling-learning style solving can be applied in
different class of complex problems easily. Therefore, incorporating
this approach, our framework, MLBSE, supports the symbolic
execution of not only simple linear path conditions, but also
nonlinear arithmetic operations, and even black-box function calls
of library methods. Meanwhile, thanks to the theoretical foundation
of the machine learning based approach, when the solver fails to
solve a path condition, we can have an estimation of the confidence
in the satisfiability (ECS) of the problem to give users insights
about how the problem is analyzed and whether they could ultimately
find a solution. We implement MLBSE on the basis of Symbolic
Path Finder (SPF) into a fully automatic Java symbolic execution
engine. Users can feed their code to MLBSE directly, which is
very convenient to use. To evaluate its performance, 22 real case
programs are used as the benchmarks for MLBSE to generate test
cases, which involve a total number of 1042 methods that are full of
nonlinear operations, floating-point arithmetic as well as native
method calls. Experiment results show that the coverage achieved by
MLBSE is much higher than the state-of-the-art tools.},
journal = {Form. Asp. Comput.},
month = jun,
pages = {301–323},
numpages = {23},
keywords = {Constraint solving, Nonlinear path condition, Machine learning, Symbolic execution}
}

@inproceedings{10.1007/978-3-030-77772-2_21,
author = {Hart, Kyle M. and Goodman, Ari B. and O’Shea, Ryan P.},
title = {Automatic Generation of Machine Learning Synthetic Data Using ROS},
year = {2021},
isbn = {978-3-030-77771-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77772-2_21},
doi = {10.1007/978-3-030-77772-2_21},
abstract = {Data labeling is a time intensive process. As such, many data scientists use various tools to aid in the data generation and labeling process. While these tools help automate labeling, many still require user interaction throughout the process. Additionally, most target only a few network frameworks. Any researchers exploring multiple frameworks must find additional tools or write conversion scripts. This paper presents an automated tool for generating synthetic data in arbitrary network formats. It uses Robot Operating System (ROS) and Gazebo, which are common tools in the robotics community. Through ROS paradigms, it allows extensive user customization of the simulation environment and data generation process. Additionally, a plugin-like framework allows the development of arbitrary data format writers without the need to change the main body of code. Using this tool, the authors were able to generate an arbitrarily large image dataset for three unique training formats using approximately 15&nbsp;min of user setup time and a variable amount of hands-off run time, depending on the dataset size. The source code for this data generation tool is available at},
booktitle = {Artificial Intelligence in HCI: Second International Conference, AI-HCI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {310–325},
numpages = {16},
keywords = {Machine learning, Data generation, ROS}
}

@article{10.1007/s11192-021-03951-w,
author = {Mihaljevi\'{c}, Helena and Santamar\'{\i}a, Luc\'{\i}a},
title = {Disambiguation of author entities in ADS using supervised learning and graph theory methods},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {126},
number = {5},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-021-03951-w},
doi = {10.1007/s11192-021-03951-w},
abstract = {Disambiguation of authors in digital libraries is essential for many tasks, including efficient bibliographical searches and scientometric analyses to the level of individuals. The question of how to link documents written by the same person has been given much attention by academic publishers and information retrieval researchers alike. Usual approaches rely on publications’ metadata such as affiliations, email addresses, co-authors, or scholarly topics. Lack of homogeneity in the structure of bibliographic collections and discipline-specific dissimilarities between them make the creation of general-purpose disambiguators arduous. We present an algorithm to disambiguate authorships in the Astrophysics Data System (ADS) following an established semi-supervised approach of training a classifier on authorship pairs and clustering the resulting graphs. Due to the lack of high-signal features such as email addresses and citations, we engineer additional content- and location-based features via text embeddings and named-entity recognition. We train various nonlinear tree-based classifiers and detect communities from the resulting weighted graphs through label propagation, a fast yet efficient algorithm that requires no tuning. The resulting procedure reaches reasonable complexity and offers possibilities for interpretation. We apply our method to the creation of author entities in a recent ADS snapshot. The algorithm is evaluated on 39 manually-labeled author blocks comprising 9545 authorships from 562 author profiles. Our best approach utilizes the Random Forest classifier and yields a micro- and macro-averaged BCubed F1 score of 0.95 and 0.87, respectively. We release our code and labeled data publicly to foster the development of further disambiguation procedures for ADS.},
journal = {Scientometrics},
month = may,
pages = {3893–3917},
numpages = {25},
keywords = {Digital libraries, Information retrieval, Label Propagation, Supervised learning, Record linkage, Author name disambiguation}
}

@inproceedings{10.1145/3448016.3452759,
author = {Grafberger, Stefan and Guha, Shubha and Stoyanovich, Julia and Schelter, Sebastian},
title = {MLINSPECT: A Data Distribution Debugger for Machine Learning Pipelines},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452759},
doi = {10.1145/3448016.3452759},
abstract = {Machine Learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this wide-spread use are garnering attention from policymakers, scientists, and the media. ML applications are often very brittle with respect to their input data, which leads to concerns about their reliability, accountability, and fairness. While bias detection cannot be fully automated, computational tools can help pinpoint particular types of data issues.We recently proposed mlinspect, a library that enables lightweight lineage-based inspection of ML preprocessing pipelines. In this demonstration, we show how mlinspect can be used to detect data distribution bugs in a representative pipeline. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines, can handle both relational and matrix data, and does not require manual code instrumentation. The library is publicly available at https://github.com/stefan-grafberger/mlinspect.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2736–2739},
numpages = {4},
keywords = {technical bias, responsible data science, machine learning pipelines, data distribution debugging},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@phdthesis{10.5555/AAI28314022,
author = {Yanxon, Howard and Salamat, Ashkan and Zygelman, Bernard and Thompson, Aidan and Mo, Yifei and Neda, Monika},
advisor = {Qiang, Zhu,},
title = {Developments of Machine Learning Potentials for Atomistic Simulations},
year = {2020},
isbn = {9798738630514},
publisher = {University of Nevada, Las Vegas},
address = {USA},
abstract = {Atomistic modeling methods such as molecular dynamics play important roles in investigating time-dependent physical and chemical processes at the microscopic level. In the simulations, energy and forces, sometimes including stress tensor, need to be recalculated iteratively as the atomic configuration evolves. Consequently, atomistic simulations crucially depend on the accuracy of the underlying potential energy surface. Modern quantum mechanical modeling based on density functional theory can consistently generate an accurate description of the potential energy surface. In most cases, molecular dynamics simulations based on density functional theory suffer from highly demanding computational costs. On the other hand, atomistic simulations based on classical force fields have proven to be essential in the computational modeling community due to their unrivaled computational efficiency. However, classical force fields are only useful for inspecting the qualitative insights because they fail to provide confidence in the quantitative results for a lot of cases. In this thesis, I will show the power of machine learning potentials, resolving the predicaments described above. First, the machine learning potential methods will be applied to SiO2 for investigating the implications of different machine learning potentials. Second, the machine learning potentials will be extended to predict physical properties of crystalline silicon, Ni-Mo system, high entropy alloy of NbMoTaW, and Pt for nanoparticle systems. Finally, the diffusion barriers of Pt adsorptions on Pt(111) and Pt(100) surfaces will be examined in detail.},
note = {AAI28314022}
}

@article{10.1007/s42979-020-00233-9,
author = {Das, Dibakar and Bapat, Jyotsna and Das, Debabrata},
title = {Unsupervised Learning Based Capacity Augmentation in SDN Assisted Wireless Networks},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {4},
url = {https://doi.org/10.1007/s42979-020-00233-9},
doi = {10.1007/s42979-020-00233-9},
abstract = {Future tactile internet is likely to have a combination of underlying wired and wireless networks with heterogeneous (legacy and new) access technologies to support diverse applications, e.g., Internet of Things (IoT). In this context, Opportunistic Network (ON) can be an important paradigm in wireless networks to help augment capacity of network for varying internet traffic requirements. Software Defined Networking (SDN), with its logically centralized control plane, is expected to ease implementation of functions, such as radio resource management, across wireless networks with multiple Radio Access Technologies (multi-RAT). Hence, tactile internet is likely to work over an intelligent SDN controlled cloud-based implementation of wired and wireless technologies, and necessitating opportunistic network capacity augmentation with appropriate RAT. This paper presents a novel SDN assisted architecture for futuristic wireless networks which augments network capacity on need basis using unsupervised Machine Learning (ML) to create ON cells with appropriate RAT. Subsequently, we define utilities for the Wireless Network Infrastructure (WNI) and the User Equipment (UE) to evaluate the benefit of creation of ON cells. A game theoretic model is developed to understand the strategies of the two players, i.e., WNI and UE, while using the ON cell resources. The Nash Equilibria (NE) of the game reveal that both UE and WNI gain by co-operating with each other and lose otherwise in utilizing the augmented network capacity. Simulation results also confirm this observation.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {13},
keywords = {Wireless access networks, Multi-RAT networks, 5G, Opportunistic network, SDN}
}

@inproceedings{10.1145/3314221.3322485,
author = {Iyer, Arun and Jonnalagedda, Manohar and Parthasarathy, Suresh and Radhakrishna, Arjun and Rajamani, Sriram K.},
title = {Synthesis and machine learning for heterogeneous extraction},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3322485},
doi = {10.1145/3314221.3322485},
abstract = {We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data.  We use machine learning models ("ML models") such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called "field constraints") to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity.  More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds --- handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {301–315},
numpages = {15},
keywords = {Program synthesis, Machine Learning, Heterogeneous data, Data extraction},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{10.1145/3418526,
author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/3418526},
doi = {10.1145/3418526},
abstract = {Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of GD and SGD and show that they are truly efficient—their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical first-order stationary points.},
journal = {J. ACM},
month = feb,
articleno = {11},
numpages = {29},
keywords = {perturbations, efficiency, Saddle points, (stochastic) gradient descent}
}

@phdthesis{10.5555/AAI28717263,
author = {Ogidi-Ekoko, Onoriode Nathaniel and J., Wierer, Jonathan and Sushil, Kumar, and Michael, Stavola,},
advisor = {Nelson, Tansu,},
title = {Machine Learning–Inspired III-Nitride Photonics Design and Power Devices},
year = {2021},
isbn = {9798460408559},
publisher = {Lehigh University},
address = {USA},
abstract = {III-Nitrides are a very versatile class of semiconductors which have found great success for a number of applications including ultra-bright LEDs and laser diodes. One notable promising application still at an earlier stage of advancement is power electronics. Power electronics technology is a critical component of modern-day energy infrastructure. From electric grids to electric vehicles to consumer electronics, the need for highly efficient electrical power conversion to achieve huge energy savings on a global scale cannot be overstated. For decades, silicon, and more recently, silicon carbide (SiC), have been the semiconductors of choice for the low voltage and high voltage tiers of power electronics, respectively. However, while silicon technology is significantly more mature and cheaper than other material systems, due to its smaller bandgap, it is also fundamentally limited for high voltage, high temperature and high frequency applications. These fundamental limits are gradually being reached at current levels of technological advancement. There is therefore a need to explore other material systems in order to make the much-needed breakthroughs for the next generation of power devices.Gallium Nitride, owing to its wide bandgap property and consequently higher critical field, has a much higher theoretical limit (power figure of merit) than Si and SiC and can support significantly higher breakdown voltages than both for a given specific on resistance. This fact allows it to compete very favorably against both, if certain critical challenges can be addressed, including the higher substrate cost and some practical device limitations. For instance, for MOS-based devices, there has been a significant challenge with finding a high-quality dielectric for GaN that has excellent insulating properties and low interface state density. This work focuses on this issue, specifically investigating the electrical properties of MgO/GaN MOS capacitors. MgO has been considered to be a very good dielectric match for GaN, given its similar lattice structure, high k-value, wide bandgap and favorably high conduction band offset to GaN. In this work, the electrical properties of atomic layer deposited MgO/GaN MOS capacitors are investigated using current-voltage (I-V), capacitance-voltage (C-V), and conductance-voltage (G-V) characterization methods to determine leakage current characteristics, capacitive behavior, and interface trap density of the MOS capacitor structure, all of which can give insights into actual device performance. A study of the effect of annealing and mitigation efforts using Al2O3 in a gate stack design is also presented.The second part of this work addresses the design of GaN-based subwavelength gratings using machine learning–based approaches. Traditionally, III-Nitride photonics design has been driven by human effort and creativity which has been remarkably successful, evidenced by signature breakthroughs in LED technology and laser design. However, with advances in computing technology, the potential immense benefits of leveraging machine-driven optimization techniques for photonics design cannot be ignored. There is a need to capitalize on these gains. To this end, as an example, this work takes up the design of complex-shaped and conventional binary GaN-based subwavelength gratings using machine learning–based techniques, notably the differential evolution algorithm, to design the gratings for ultrabroadband reflectivity at two wavelength regimes of 500 nm and 1.55 μm. GaN subwavelength gratings are designed for high reflectivity as mirrors for vertical cavity surface emitting lasers (VCSELs) as a potential replacement for conventional distributed Bragg reflectors (DBR) used to achieve high reflectivity in VCSEL devices. The III-Nitride DBR challenges of low refractive index contrast, low p-type doping and lateral current control, which have to this point hampered the commercialization of III-Nitride VCSELs, may be surmountable by replacing the DBRs with monolithic subwavelength gratings, as has been demonstrated for other material systems such as InP, hence the motivation for this work. Rather than designing by a trial-and-error process, the algorithm optimizes the design to satisfy tight constraints for stopband width maximization (reflectivity greater than 99%), achieving a relative bandwidth (Δλ/λcenter) of 30% or more, well above what has hitherto been achievable using III-Nitride DBRs.},
note = {AAI28717263}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Software product lines, NLP model inference, Feature models, Domain engineering}
}

@inproceedings{10.1145/3324884.3415281,
author = {Abdelkader, Hala},
title = {Towards robust production machine learning systems: managing dataset shift},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415281},
doi = {10.1145/3324884.3415281},
abstract = {The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1164–1166},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3376920,
author = {Navarro, Osvaldo and Yudi, Jones and Hoffmann, Javier and Hernandez, Hector Gerardo Mu\~{n}oz and H\"{u}bner, Michael},
title = {A Machine Learning Methodology for Cache Memory Design Based on Dynamic Instructions},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3376920},
doi = {10.1145/3376920},
abstract = {Cache memories are an essential component of modern processors and consume a large percentage of their power consumption. Its efficacy depends heavily on the memory demands of the software. Thus, finding the optimal cache for a particular program is not a trivial task and usually involves exhaustive simulation. In this article, we propose a machine learning–based methodology that predicts the optimal cache reconfiguration for any given application, based on its dynamic instructions. Our evaluation shows that our methodology reaches 91.1% accuracy. Moreover, an additional experiment shows that only a small portion of the dynamic instructions (10%) suffices to reach 89.71% accuracy.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = mar,
articleno = {12},
numpages = {20},
keywords = {machine learning, classification, cache memory design, cache memory, Supervised learning}
}

@article{10.3233/JIFS-202099,
author = {Recal, F\"{u}sun and Demirel, Tufan},
title = {Comparison of machine learning methods in predicting binary and multi-class occupational accident severity},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-202099},
doi = {10.3233/JIFS-202099},
abstract = {Although Machine Learning (ML) is widely used to examine hidden patterns in complex databases and learn from them to predict future events in many fields, utilization of it for predicting the outcome of occupational accidents is relatively sparse. This study utilized diversified ML algorithms; Multinomial Logistic Regression (MLR), Support Vector Machines (SVM), Single C5.0 Tree (C5), Stochastic Gradient Boosting (SGB), and Neural Network (NN) in classifying the severity of occupational accidents in binary (Fatal/NonFatal) and multi-class (Fatal/Major/Minor) outcomes. Comparison of the performance of models showed Balanced Accuracy to be the best for SVM and SGB methods in 2-Class and SGB in 3-Class. Algorithms performed better at predicting fatal accidents compared to major and minor accidents. Results obtained revealed that, ML unveils factors contributing to severity to better address the corrective actions. Furthermore, taking action related to even some of the most significant factors in complex accidents database with many attributes can prevent majority of severe accidents. Interpretation of most significant factors identified for accident prediction suggest the following corrective measures: taking fall prevention actions, prioritizing workplace inspections based on the number of employees, and supplementing safety actions according to worker’s age and experience.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {10981–10998},
numpages = {18},
keywords = {feature selection; machine learning, data mining, classification, Accidents severity}
}

@article{10.1016/j.cosrev.2021.100413,
author = {Jain, Praphula Kumar and Pamula, Rajendra and Srivastava, Gautam},
title = {A systematic literature review on machine learning applications for consumer sentiment analysis using online reviews},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100413},
doi = {10.1016/j.cosrev.2021.100413},
journal = {Comput. Sci. Rev.},
month = aug,
numpages = {17},
keywords = {Hospitality and tourism, Recommendation prediction, Fake review, Sentiment analysis, Online reviews, Machine Learning}
}

@inproceedings{10.1145/3079628.3079630,
author = {Arevalillo-Herr\'{a}ez, Miguel and Ayesh, Aladdin and Santos, Olga C. and Arnau-Gonz\'{a}lez, Pablo},
title = {Combining Supervised and Unsupervised Learning to Discover Emotional Classes},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079630},
doi = {10.1145/3079628.3079630},
abstract = {Most previous work in emotion recognition has fixed the available classes in advance, and attempted to classify samples into one of these classes using a supervised learning approach. In this paper, we present preliminary work on combining supervised and unsupervised learning to discover potential latent classes which were not initially considered. To illustrate the potential of this hybrid approach, we have used a Self-Organizing Map (SOM) to organize a large number of Electroencephalogram (EEG) signals from subjects watching videos, according to their internal structure. Results suggest that a more useful labelling scheme could be produced by analysing the resulting topology in relation to user reported valence levels (i.e., pleasantness) for each signal, refining the original set of target classes.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {355–356},
numpages = {2},
keywords = {user modelling, personalization, eeg, cluster analysis, class discovery, affective computing},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@article{10.1016/j.compag.2021.106382,
author = {Maldaner, Leonardo Felipe and Molin, Jos\'{e} Paulo and Canata, Tatiana Fernanda and Martello, Maur\'{\i}cio},
title = {A system for plant detection using sensor fusion approach based on machine learning model},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {189},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106382},
doi = {10.1016/j.compag.2021.106382},
journal = {Comput. Electron. Agric.},
month = oct,
numpages = {11},
keywords = {Data fusion, Ultrasonic sensor, Optic sensor, Machine learning, Sugarcane, Precision agriculture}
}

@inproceedings{10.1109/ICSE43902.2021.00024,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Are Machine Learning Cloud APIs Used Correctly?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00024},
doi = {10.1109/ICSE43902.2021.00024},
abstract = {Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {125–137},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.3233/JIFS-200862,
author = {Elavarasan, Dhivya and Vincent, Durai Raj},
title = {Reinforced XGBoost machine learning model for sustainable intelligent agrarian applications},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200862},
doi = {10.3233/JIFS-200862},
abstract = {The development in science and technical intelligence has incited to represent an extensive amount ofdata from various fields of agriculture. Therefore an objective rises up for the examination of the available data and integrating with processes like crop enhancement, yield prediction, examination of plant infections etc. Machine learning has up surged with tremendous processing techniques to perceive new contingencies in the multi-disciplinary agrarian advancements. In this pa- per a novel hybrid regression algorithm, reinforced extreme gradient boosting is proposed which displays essentially improved execution over traditional machine learning algorithms like artificial neural networks, deep Q-Network, gradient boosting, ran- dom forest and decision tree. Extreme gradient boosting constructs new models, which are essentially, decision trees learning from the mistakes of their predecessors by optimizing the gradient descent loss function. The proposed hybrid model performs reinforcement learning at every node during the node splitting process of the decision tree construction. This leads to effective utilizationofthesamplesbyselectingtheappropriatesplitattributeforenhancedperformance. Model’sperformanceisevaluated by means of Mean Square Error, Root Mean Square Error, Mean Absolute Error, and Coefficient of Determination. To assure a fair assessment of the results, the model assessment is performed on both training and test dataset. The regression diagnostic plots from residuals and the results obtained evidently delineates the fact that proposed hybrid approach performs better with reduced error measure and improved accuracy of 94.15% over the other machine learning algorithms. Also the performance of probability density function for the proposed model delineates that, it can preserve the actual distributional characteristics of the original crop yield data more approximately when compared to the other experimented machine learning models.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7605–7620},
numpages = {16},
keywords = {intelligent agrarian application, extreme gradient boosting, reinforcement learning, Crop yield prediction}
}

@inproceedings{10.1007/978-3-030-74251-5_9,
author = {Z\"{o}ller, Marc-Andr\'{e} and Nguyen, Tien-Dung and Huber, Marco F.},
title = {Incremental Search Space Construction for Machine Learning Pipeline Synthesis},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_9},
doi = {10.1007/978-3-030-74251-5_9},
abstract = {Automated machine learning (AutoML) aims for constructing machine learning (ML) pipelines automatically. Many studies have investigated efficient methods for algorithm selection and hyperparameter optimization. However, methods for ML pipeline synthesis and optimization considering the impact of complex pipeline structures containing multiple preprocessing and classification algorithms have not been studied thoroughly. In this paper, we propose a data-centric approach based on meta-features for pipeline construction and hyperparameter optimization inspired by human behavior. By expanding the pipeline search space incrementally in combination with meta-features of intermediate data sets, we are able to prune the pipeline structure search space efficiently. Consequently, flexible and data set specific ML pipelines can be constructed. We prove the effectiveness and competitiveness of our approach on 28 data sets used in well-established AutoML benchmarks in comparison with state-of-the-art AutoML frameworks.},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {103–115},
numpages = {13},
keywords = {AutoML, Meta-learning, Pipeline structure search},
location = {Porto, Portugal}
}

@inproceedings{10.1145/3243250.3243254,
author = {Wu, YanTong and Liu, Yang and Li, XueMing},
title = {Position Estimation of Camera Based on Unsupervised Learning},
year = {2018},
isbn = {9781450364829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243250.3243254},
doi = {10.1145/3243250.3243254},
abstract = {It is an exciting task to recover the scene's 3D structure and camera pose from the video sequence. Most of the current solutions divide it into two parts, monocular depth recovery and camera pose estimation. The monocular depth recovery is often studied as an independent part, and a better depth estimation is used to solve the pose. While camera pose is still estimated by traditional SLAM (Simultaneous Localization And Mapping) methods in most cases. The application of unsupervised method for monocular depth recovery and pose estimation has benefited from the study of [1] and achieved good results. In this paper, we improve the method of [1]. Our emphasis is laid on the improvement of the idea and related theory, introducing a more reasonable inter frame constraints and finally synthesize the camera trajectory with inter frame pose estimation in the unified world coordinate system. And our results show better performance.},
booktitle = {Proceedings of the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {30–35},
numpages = {6},
keywords = {Unsupervised learning, Track stitching, Pose estimation},
location = {Union, NJ, USA},
series = {PRAI 2018}
}

@article{10.1145/3214306,
author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
title = {Machine Learning for Survival Analysis: A Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3214306},
doi = {10.1145/3214306},
abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {110},
numpages = {36},
keywords = {survival data, survival analysis, regression, hazard rate, concordance index, censoring, Machine learning, Cox model}
}

@article{10.1109/TITS.2020.3003111,
author = {Gatto, Rubens Cruz and Forster, Carlos Henrique Quartucci},
title = {Audio-Based Machine Learning Model for Traffic Congestion Detection},
year = {2021},
issue_date = {Nov. 2021},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {1524-9050},
url = {https://doi.org/10.1109/TITS.2020.3003111},
doi = {10.1109/TITS.2020.3003111},
abstract = {The present work approaches intelligent traffic evaluation and congestion detection using sound sensors and machine learning. For this, two important problems are addressed: traffic condition assessment from audio data, and analysis of audio under uncontrolled environments. By modeling the traffic parameters and the sound generation from passing vehicles and using the produced audio as a source of data for learning the traffic audio patterns, we provide a solution that copes with the time, the cost and the constraints inherent to the activity of traffic monitoring. External noise sources were introduced to produce more realistic acoustic scenes and to verify the robustness of the methods presented. Audio-based monitoring becomes a simple and low-cost option, comparing to other methods based on detector loops, or GPS, and as good as camera-based solutions, without some of the common problems of image-based monitoring, such as occlusions and light conditions. The approach is evaluated with data from audio analysis of traffic registered in locations around the city of São Jose dos Campos, Brazil, and audio files from places around the world, downloaded from YouTube. Its validation shows the feasibility of traffic automatic audio monitoring as well as using machine learning algorithms to recognize audio patterns under noisy environments.},
journal = {Trans. Intell. Transport. Sys.},
month = nov,
pages = {7200–7207},
numpages = {8}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.1016/j.comcom.2020.02.008,
author = {Gupta, Rajesh and Tanwar, Sudeep and Tyagi, Sudhanshu and Kumar, Neeraj},
title = {Machine Learning Models for Secure Data Analytics: A taxonomy and threat model},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {153},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2020.02.008},
doi = {10.1016/j.comcom.2020.02.008},
journal = {Comput. Commun.},
month = mar,
pages = {406–440},
numpages = {35},
keywords = {Data security and privacy, Threat model, Machine learning models, Data reduction, Secure Data Analytics, Big data}
}

@inproceedings{10.1145/3437963.3441736,
author = {Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
title = {Theoretical Understandings of Product Embedding for E-commerce Machine Learning},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441736},
doi = {10.1145/3437963.3441736},
abstract = {Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {256–264},
numpages = {9},
keywords = {sufficient dimension reduction, representation learning, product relation, machine learning theory, information theory},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inbook{10.5555/3454287.3455249,
author = {Kulkarni, Tejas and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
title = {Unsupervised learning of object keypoints for perception and control},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains – (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards. Code for the model is available at: https://github. com/deepmind/deepmind-research/tree/master/transporter.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {962},
numpages = {11}
}

@article{10.1007/s00521-020-04874-y,
author = {Adi, Erwin and Anwar, Adnan and Baig, Zubair and Zeadally, Sherali},
title = {Machine learning and data analytics for the IoT},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04874-y},
doi = {10.1007/s00521-020-04874-y},
abstract = {The Internet of Things (IoT) applications have grown in exorbitant numbers, generating a large amount of data required for intelligent data processing. However, the varying IoT infrastructures (i.e., cloud, edge, fog) and the limitations of the IoT application layer protocols in transmitting/receiving messages become the barriers in creating intelligent IoT applications. These barriers prevent current intelligent IoT applications to adaptively learn from other IoT applications. In this paper, we critically review how IoT-generated data are processed for machine learning analysis and highlight the current challenges in furthering intelligent solutions in the IoT environment. Furthermore, we propose a framework to enable IoT applications to adaptively learn from other IoT applications and present a case study in how the framework can be applied to the real studies in the literature. Finally, we discuss the key factors that have an impact on future intelligent applications for the IoT.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {16205–16233},
numpages = {29},
keywords = {Machine learning, Intelligent systems, Internet of Things, Cybersecurity}
}

@article{10.1109/TNSM.2020.3031333,
author = {Gij\'{o}n, Carolina and Toril, Mat\'{\i}as and Luna-Ram\'{\i}rez, Salvador and Bejarano-Luque, Juan L. and Mar\'{\i}-Altozano, Mar\'{\i}a Luisa},
title = {Estimating Pole Capacity From Radio Network Performance Statistics by Supervised Learning},
year = {2020},
issue_date = {Dec. 2020},
publisher = {IEEE Press},
volume = {17},
number = {4},
issn = {1932-4537},
url = {https://doi.org/10.1109/TNSM.2020.3031333},
doi = {10.1109/TNSM.2020.3031333},
abstract = {Network dimensioning is a critical task for cellular operators to avoid degraded user experience and unnecessary upgrades of network resources with changing mobile traffic patterns. For this purpose, smart network planning tools require accurate cell and user capacity estimates. In these tools, throughput is often used as a capacity metric due to its close relationship with user satisfaction. In this work, a comprehensive analysis is carried out to compare different well-known Supervised Learning (SL) algorithms for estimating cell and user throughput in the DownLink in busy hours from radio measurements collected on a cell basis in the Operation Support System (OSS). The considered SL approaches include random forest, shallow multi-layer perceptron, support vector regression and k-nearest neighbors. Such algorithms are compared with classical multiple linear regression and deep learning approaches considered in previous works. All these algorithms are tested in two radio access technologies: High Speed DownLink Packet Access (HSDPA) and Long Term Evolution (LTE). To this end, two datasets with the most relevant performance indicators per technology are collected from live cellular networks. Results show that non-deep SL algorithms are the most appropriate option for applications with storage constraints, such as network planning tools, since they provide a higher accuracy with reduced datasets.},
journal = {IEEE Trans. on Netw. and Serv. Manag.},
month = dec,
pages = {2090–2101},
numpages = {12}
}

@inproceedings{10.1145/3459104.3459144,
author = {Jan Hagendorfer, Elias},
title = {Knowledge Incorporation for Machine Learning in Condition Monitoring: A Survey},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459144},
doi = {10.1145/3459104.3459144},
abstract = {Model-based condition monitoring (MBCM) solves the inverse problem of inferring a systems state, including possible faults, from sensor observations. Constructing these models in a knowledge-based manner following the laws of physics is hard due to the inverse nature of the problem and unknown fault types. As a result, it has become more attractive to build a model solely from past observations via machine learning (ML). Although highly promising, shortcomings of ML in the scientific domain, including physically inconsistent results and lack of interpretability, became apparent. This led to recent efforts to enhance machine learning with scientific knowledge including a combination of knowledge-based and data-driven modelling, often referred to as hybrid models. The main contributions of this work are: (1) a link of shortcomings of machine learning in CM to a lack of knowledge; (2) a categorization of unique approaches with respect to required knowledge and mechanism of incorporation that have either been applied in condition monitoring or show potential from their application to scientific problems; (3) derivation of promising research directions uncovered as vacant spaces in the categorization.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {230–240},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3461615.3486575,
author = {Vo\ss{}, Hendric and Wersing, Heiko and Kopp, Stefan},
title = {Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning},
year = {2021},
isbn = {9781450384711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461615.3486575},
doi = {10.1145/3461615.3486575},
abstract = {Detecting mental states of human users is crucial for the development of cooperative and intelligent robots, as it enables the robot to understand the user’s intentions and desires. Despite their importance, it is difficult to obtain a large amount of high quality data for training automatic recognition algorithms as the time and effort required to collect and label such data is prohibitively high. In this paper we present a multimodal machine learning approach for detecting dis-/agreement and confusion states in a human-robot interaction environment, using just a small amount of manually annotated data. We collect a data set by conducting a human-robot interaction study and develop a novel preprocessing pipeline for our machine learning approach. By combining semi-supervised and supervised architectures, we are able to achieve an average F1-score of 81.1% for dis-/agreement detection with a small amount of labeled data and a large unlabeled data set, while simultaneously increasing the robustness of the model compared to the supervised approach.},
booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction},
pages = {317–323},
numpages = {7},
keywords = {unsupervised, supervised, semi-supervised, neural networks, deep learning, confusion detection, complex user states, agreement - disagreement detection},
location = {Montreal, QC, Canada},
series = {ICMI '21 Companion}
}

@inproceedings{10.1145/3375627.3375808,
author = {Leben, Derek},
title = {Normative Principles for Evaluating Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375808},
doi = {10.1145/3375627.3375808},
abstract = {There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {86–92},
numpages = {7},
keywords = {algorithmic decision-making, discrimination, fairness, machine learning, political philosophy},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{10.1016/j.finel.2021.103572,
author = {Mai, Hau T. and Kang, Joowon and Lee, Jaehong},
title = {A machine learning-based surrogate model for optimization of truss structures with geometrically nonlinear behavior},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {196},
number = {C},
issn = {0168-874X},
url = {https://doi.org/10.1016/j.finel.2021.103572},
doi = {10.1016/j.finel.2021.103572},
journal = {Finite Elem. Anal. Des.},
month = nov,
numpages = {14},
keywords = {Truss optimization, Geometric nonlinear, Machine learning, Neural network, Deep neural network, Surrogate model}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Transplant recipient, Survival analysis, Support vector machines, Semi-supervised learning, Machine learning, Liver transplantation, Imbalanced classification}
}

@article{10.1007/s10796-020-10056-x,
author = {Alahakoon, Damminda and Nawaratne, Rashmika and Xu, Yan and De Silva, Daswin and Sivarajah, Uthayasankar and Gupta, Bhumika},
title = {Self-Building Artificial Intelligence and Machine Learning to Empower Big Data Analytics in Smart Cities},
year = {2020},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-020-10056-x},
doi = {10.1007/s10796-020-10056-x},
abstract = {The emerging information revolution makes it necessary to manage vast amounts of unstructured data rapidly. As the world is increasingly populated by IoT devices and sensors that can sense their surroundings and communicate with each other, a digital environment has been created with vast volumes of volatile and diverse data. Traditional AI and machine learning techniques designed for deterministic situations are not suitable for such environments. With a large number of parameters required by each device in this digital environment, it is desirable that the AI is able to be adaptive and self-build (i.e. self-structure, self-configure, self-learn), rather than be structurally and parameter-wise pre-defined. This study explores the benefits of self-building AI and machine learning with unsupervised learning for empowering big data analytics for smart city environments. By using the growing self-organizing map, a new suite of self-building AI is proposed. The self-building AI overcomes the limitations of traditional AI and enables data processing in dynamic smart city environments. With cloud computing platforms, the self-building AI can integrate the data analytics applications that currently work in silos. The new paradigm of the self-building AI and its value are demonstrated using the IoT, video surveillance, and action recognition applications.},
journal = {Information Systems Frontiers},
month = aug,
pages = {221–240},
numpages = {20},
keywords = {Self-organizing maps, Smart cities, Machine learning, Self-building AI, Big data analytics}
}

@inproceedings{10.1109/WAIN52551.2021.00021,
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
title = {Practices for Engineering Trustworthy Machine Learning Applications},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00021},
doi = {10.1109/WAIN52551.2021.00021},
abstract = {Following the recent surge in adoption of machine learning (ML), the negative impact that improper use of ML can have on users and society is now also widely recognised. To address this issue, policy makers and other stakeholders, such as the European Commission or NIST, have proposed high-level guidelines aiming to promote trustworthy ML (i.e., lawful, ethical and robust). However, these guidelines do not specify actions to be taken by those involved in building ML systems. In this paper, we argue that guidelines related to the development of trustworthy ML can be translated to operational practices, and should become part of the ML development life cycle. Towards this goal, we ran a multi-vocal literature review, and mined operational practices from white and grey literature. Moreover, we launched a global survey to measure practice adoption and the effects of these practices. In total, we identified 14 new practices, and used them to complement an existing catalogue of ML engineering practices. Initial analysis of the survey results reveals that so far, practice adoption for trustworthy ML is relatively low. In particular, practices related to assuring security of ML components have very low adoption. Other practices enjoy slightly larger adoption, such as providing explanations to users. Our extended practice catalogue can be used by ML development teams to bridge the gap between high-level guidelines and actual development of trustworthy ML systems; it is open for review and contributions.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {97–100},
numpages = {4},
location = {Madrid, Spain}
}

@inproceedings{10.1007/978-3-030-60884-2_4,
author = {Jara-Maldonado, Miguel and Alarcon-Aquino, Vicente and Rosas-Romero, Roberto},
title = {A Multiresolution Machine Learning Technique to Identify Exoplanets},
year = {2020},
isbn = {978-3-030-60883-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60884-2_4},
doi = {10.1007/978-3-030-60884-2_4},
abstract = {The discovery of planets outside our Solar System, called exoplanets, allows us to study the feasibility of life outside Earth. Different techniques such as the transit method have been employed to detect and identify exoplanets. The amount of time and effort required to perform such a task, hinder the manual examination of the existing data. Several machine learning approaches have been proposed to deal with this matter, though they are not yet unerring. Therefore, new models continue to be proposed. In this work, we present experimental results using the K-Nearest Neighbors, Random Forests, Convolutional Neural Network and the Ridge classifier models to identify simulated transit signals. Furthermore, we propose a methodology based on the Empirical Mode Decomposition and Ensemble Empirical Mode Decomposition techniques for light curve preprocessing. Following this methodology we prove that multiresolution analysis can be used to improve the robustness of the presented models.},
booktitle = {Advances in Soft Computing: 19th Mexican International Conference on Artificial Intelligence, MICAI 2020, Mexico City, Mexico, October 12–17, 2020, Proceedings, Part I},
pages = {50–64},
numpages = {15},
keywords = {Synthetic transits, Multiresolution Analysis, Machine learning, Light curves, Exoplanets, Empirical Mode Decomposition},
location = {Mexico City, Mexico}
}

@inproceedings{10.1007/978-3-030-84532-2_23,
author = {Alatrany, A. and Hussain, A. and Mustafina, J. and Al-Jumeily, D.},
title = {A Novel Hybrid Machine Learning Approach Using Deep Learning for the Prediction of Alzheimer Disease Using Genome Data},
year = {2021},
isbn = {978-3-030-84531-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-84532-2_23},
doi = {10.1007/978-3-030-84532-2_23},
abstract = {Genome-wide association studies are aimed at identifying associations between commonly occurring variations in a group of individuals and a phonotype, in which the Deoxyribonucleic acid is genotyped in the form of single nucleotide polymorphisms. Despite the exsistence of various research studies for the prediction of chronic diseases using human genome data, more investigations are still required. Machine learning algorithms are widely used for prediction and genome-wide association studies. In this research, Random Forest was utilised for selecting most significant single nucleotide polymorphisms associated to Alzheimer’s Disease. Deep learning model for the prediction of the disease was then developed. Our extesnive similation results indicated that this hybrid model is promising in predicting individuals that suffer from Alzheimer’s disease, achieving area under the curve of 0.9 and 0.93 using Convolutional Neural Network and Multilayer perceptron respectively.},
booktitle = {Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Shenzhen, China, August 12–15, 2021, Proceedings, Part III},
pages = {253–266},
numpages = {14},
keywords = {ANN, CNN, Random forest, Machine learning, GWAS},
location = {Shenzhen, China}
}

@inproceedings{10.1007/978-3-030-86044-8_4,
author = {C\'{a}mara, Javier and Silva, Mariana and Garlan, David and Schmerl, Bradley},
title = {Explaining Architectural Design Tradeoff Spaces: A Machine Learning Approach},
year = {2021},
isbn = {978-3-030-86043-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86044-8_4},
doi = {10.1007/978-3-030-86044-8_4},
abstract = {In software design, guaranteeing the correctness of run-time system behavior while achieving an acceptable balance among multiple quality attributes remains a challenging problem. Moreover, providing guarantees about the satisfaction of those requirements when systems are subject to uncertain environments is even more challenging. While recent developments in architectural analysis techniques can assist architects in exploring the satisfaction of quantitative guarantees across the design space, existing approaches are still limited because they do not explicitly link design decisions to satisfaction of quality requirements. Furthermore, the amount of information they yield can be overwhelming to a human designer, making it difficult to distinguish the forest through the trees. In this paper, we present an approach to analyzing architectural design spaces that addresses these limitations and provides a basis to enable the explainability of design tradeoffs. Our approach combines dimensionality reduction techniques employed in machine learning pipelines with quantitative verification to enable architects to understand how design decisions contribute to the satisfaction of strict quantitative guarantees under uncertainty across the design space. Our results show feasibility of the approach in two case studies and evidence that dimensionality reduction is a viable approach to facilitate comprehension of tradeoffs in poorly-understood design spaces.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021, Virtual Event, Sweden, September 13-17, 2021, Proceedings},
pages = {49–65},
numpages = {17},
keywords = {Dimensionality reduction, Uncertainty, Tradeoff analysis}
}

@inproceedings{10.1145/3387906.3388618,
author = {Cruz, Daniel and Santana, Amanda and Figueiredo, Eduardo},
title = {Detecting bad smells with machine learning algorithms: an empirical study},
year = {2020},
isbn = {9781450379601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387906.3388618},
doi = {10.1145/3387906.3388618},
abstract = {Bad smells are symptoms of bad design choices implemented on the source code. They are one of the key indicators of technical debts, specifically, design debt. To manage this kind of debt, it is important to be aware of bad smells and refactor them whenever possible. Therefore, several bad smell detection tools and techniques have been proposed over the years. These tools and techniques present different strategies to perform detections. More recently, machine learning algorithms have also been proposed to support bad smell detection. However, we lack empirical evidence on the accuracy and efficiency of these machine learning based techniques. In this paper, we present an evaluation of seven different machine learning algorithms on the task of detecting four types of bad smells. We also provide an analysis of the impact of software metrics for bad smell detection using a unified approach for interpreting the models' decisions. We found that with the right optimization, machine learning algorithms can achieve good performance (F1 score) for two bad smells: God Class (0.86) and Refused Parent Bequest (0.67). We also uncovered which metrics play fundamental roles for detecting each bad smell.},
booktitle = {Proceedings of the 3rd International Conference on Technical Debt},
pages = {31–40},
numpages = {10},
keywords = {software quality, software measurement, machine learning, empirical software engineering, bad smells detection},
location = {Seoul, Republic of Korea},
series = {TechDebt '20}
}

@article{10.1155/2021/6643763,
author = {Chen, Binjie and Wei, Fushan and Gu, Chunxiang and Choo, Kim-Kwang Raymond},
title = {Bitcoin Theft Detection Based on Supervised Machine Learning Algorithms},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/6643763},
doi = {10.1155/2021/6643763},
abstract = {Since its inception, Bitcoin has been subject to numerous thefts due to its enormous economic value. Hackers steal Bitcoin wallet keys to transfer Bitcoin from compromised users, causing huge economic losses to victims. To address the security threat of Bitcoin theft, supervised learning methods were used in this study to detect and provide warnings about Bitcoin theft events. To overcome the shortcomings of the existing work, more comprehensive features of Bitcoin transaction data were extracted, the unbalanced dataset was equalized, and five supervised methods—the k-nearest neighbor (KNN), support vector machine (SVM), random forest (RF), adaptive boosting (AdaBoost), and multi-layer perceptron (MLP) techniques—as well as three unsupervised methods—the local outlier factor (LOF), one-class support vector machine (OCSVM), and Mahalanobis distance-based approach (MDB)—were used for detection. The best performer among these algorithms was the RF algorithm, which achieved recall, precision, and F1 values of 95.9%. The experimental results showed that the designed features are more effective than the currently used ones. The results of the supervised methods were significantly better than those of the unsupervised methods, and the results of the supervised methods could be further improved after equalizing the training set.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {10}
}

@inproceedings{10.1007/978-3-030-95481-9_4,
author = {d’Amato, Claudia},
title = {Mining the Semantic Web with Machine Learning: Main Issues that Need to Be Known},
year = {2021},
isbn = {978-3-030-95480-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95481-9_4},
doi = {10.1007/978-3-030-95481-9_4},
abstract = {The Semantic Web (SW) is characterized by the availability of a vast amount of semantically annotated data collections. Annotations are provided by exploiting ontologies acting as shared vocabularies. Additionally ontologies are endowed with deductive reasoning capabilities which allow to make explicit knowledge that is formalized implicitly. Along the years a large number of data collections have been developed and interconnected, as testified by the Linked Open Data Cloud. Currently, seminal examples are represented by the numerous Knowledge Graphs (KGs) that have been built, either as enterprise KGs or open KGs, that are freely available. All of them are characterized by very large data volumes, but also incompleteness and noise. These characteristics have made the exploitation of deductive reasoning services less feasible from a practical viewpoint, opening up&nbsp;to alternative solutions, grounded on Machine Learning (ML), for mining knowledge from the vast amount of information available. Actually, ML methods have been exploited in the SW for solving several problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level), and concept leaning. Whilst initially symbol-based solutions have been mostly targeted, recently numeric-based approaches are receiving major attention because of the need to scale on the very large data volumes. Nevertheless, data collections in the SW have peculiarities that can hardly be found in other fields. As such the application of ML methods for solving the targeted problems is not straightforward. This paper extends&nbsp;[20], by surveying the most representative symbol-based and numeric-based solutions and related problems, with a special focus on the main issues that need to be considered and solved when ML methods are adopted in the SW field as well as by analyzing the main peculiarities and drawbacks for each solution.},
booktitle = {Reasoning Web. Declarative Artificial Intelligence : 17th International Summer School 2021, Leuven, Belgium, September 8–15, 2021, Tutorial Lectures},
pages = {76–93},
numpages = {18},
keywords = {Numeric-based methods, Symbol-based methods, Machine learning, Semantic Web},
location = {Leuven, Belgium}
}

@inproceedings{10.1007/978-3-030-77977-1_21,
author = {Ngo, Ngoc-Tri and Pham, Anh-Duc and Truong, Ngoc-Son and Truong, Thi Thu Ha and Huynh, Nhat-To},
title = {Hybrid Machine Learning for Time-Series Energy Data for Enhancing Energy Efficiency in Buildings},
year = {2021},
isbn = {978-3-030-77976-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77977-1_21},
doi = {10.1007/978-3-030-77977-1_21},
abstract = {Buildings consume about 40% of the world's energy use. Energy efficiency in buildings is an increasing concern for the building owners. A reliable energy use prediction model is crucial for decision-makers. This study proposed a hybrid machine learning model for predicting one-day-ahead time-series electricity use data in buildings. The proposed SAMFOR model combined support vector regression (SVR) and firefly algorithm (FA) with conventional time-series seasonal autoregressive integrated moving average (SARIMA) forecasting model. Large datasets of electricity use in office buildings in Vietnam were used to develop the forecasting model. Results show that the proposed SAMFOR model was more effective than the baselines machine learning models. The proposed model has the lowest errors, which yielded 0.90 kWh in RMSE, 0.96 kWh in MAE, 9.04% in MAPE, 0.904 in R in the test phase. The prediction results provide building managers with useful information to enhance energy-saving solutions.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part V},
pages = {273–285},
numpages = {13},
keywords = {Prediction model, Data analytics, Machine learning, Energy consumption data},
location = {Krakow, Poland}
}

@article{10.1007/s11277-021-08879-1,
author = {Hosseini, Soodeh and Fard, Reyhane Hafezi},
title = {Machine Learning Algorithms for Predicting Electricity Consumption of Buildings},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {121},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08879-1},
doi = {10.1007/s11277-021-08879-1},
abstract = {Given that the population is increasing and also energy resources are decreasing, in this study we examine the amount of domestic energy consumption. The purpose of this study is to predict the factors affecting energy consumption in buildings. For this prediction, algorithms of decision tree, random forests and K-nearest neighbors have been used. These algorithms are available in Orange software. In this study, univariate regression algorithm is used to select the best factors. This algorithm identifies the most important factors affecting energy consumption and their impact. The results of this study show that the overall height, roof area, surface and relative compaction have the greatest impact on energy consumption of buildings. The percentage of forecast error for cooling load and heating load are 1.128 and 0.404, respectively. Also, among the tested algorithms, random forest gets the best result.},
journal = {Wirel. Pers. Commun.},
month = dec,
pages = {3329–3341},
numpages = {13},
keywords = {Random forest algorithm, K-nearest neighbor algorithm (KNN), Decision tree algorithm, Regression, Building energy consumption, Energy}
}

@article{10.4018/IJVAR.290044,
author = {Yokoyama, Yuto and Nagao, Katashi},
title = {VR Presentation Training System Using Machine Learning Techniques for Automatic Evaluation},
year = {2021},
issue_date = {Jan 2021},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {1},
issn = {2473-537X},
url = {https://doi.org/10.4018/IJVAR.290044},
doi = {10.4018/IJVAR.290044},
abstract = {In this paper, the authors build an immersive training space using building-scale VR, a technology that makes a virtual space based on an entire building existing in the real world. The space is used for presentations, allowing students to self-train. The results of a presentation are automatically evaluated by using machine learning or the like and fed back to the user. In this space, users can meet their past selves (more accurately, their avatars), so they can objectively observe their presentations and recognize weak points. The authors developed a mechanism for recording and reproducing activities in virtual space in detail and a mechanism for applying machine learning to activity records. With these mechanisms, a system for recording, reproducing, and automatically evaluating presentations was developed.},
journal = {Int. J. Virtual Augment. Real.},
month = jan,
pages = {20–42},
numpages = {23},
keywords = {VR Recording, VR Playback, Virtual Environment, Presentation Training, Machine Learning, Building-Scale VR, Automatic Evaluation of Presentations, 3D Avatars}
}

@inproceedings{10.1145/3313831.3376177,
author = {Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
title = {Understanding and Visualizing Data Iteration in Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376177},
doi = {10.1145/3313831.3376177},
abstract = {Successful machine learning (ML) applications require iterations on both modeling and the underlying data. While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models. We also identify common types of data iterations and associated analysis tasks and challenges. To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions. We present two case studies where developers apply system to their own evolving datasets on production ML projects. Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {data iteration, evolving datasets, interactive interfaces, machine learning iteration, visual analytics},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3357526.3357544,
author = {Sen, Satyabrata and Imam, Neena},
title = {Machine learning based design space exploration for hybrid main-memory design},
year = {2019},
isbn = {9781450372060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357526.3357544},
doi = {10.1145/3357526.3357544},
abstract = {We develop a machine learning (ML) based design space exploration (DSE) method that builds predictive models for various responses of a hybrid main-memory system. To overcome the challenges associated with latency, capacity, and power of memory systems in future extreme-scale machines, the hybrid memory architectures are being considered in which novel non-volatile memory (NVM) systems augment the traditional DRAM. However, way before their actual design and implementation, these emerging hybrid memory systems need to be simulated and analyzed to fully understand their capabilities and limitations. As the conventional architectural-level memory simulators require significant amounts of computational costs and time, we propose to utilize ML techniques for developing various memory-response models that can instantly provide a predicted response corresponding to any new memory configuration. Specifically, in this work, we apply four supervised ML techniques to build regression models for memory latency, bandwidth, power, and total read/write responses. The training and validation data for the ML methods are generated using NVMain memory simulator for DRAM, NVM, and their hybrid combinations. We demonstrate the results of the ML based memory-DSE method in terms of the learning curve characteristics for hyperparameter tuning and the statistical error analyses of the designed predictive models.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
pages = {480–489},
numpages = {10},
keywords = {predictive models, nonvolatile memory, machine learning, learning curves, hybrid main memory, design space exploration},
location = {Washington, District of Columbia, USA},
series = {MEMSYS '19}
}

@article{10.1007/s00371-019-01746-y,
author = {Liu, Li and Chen, Siqi and Chen, Xiuxiu and Wang, Tianshi and Zhang, Long},
title = {Fuzzy weighted sparse reconstruction error-steered semi-supervised learning for face recognition},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {8},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-019-01746-y},
doi = {10.1007/s00371-019-01746-y},
abstract = {Since the number of labeled data is limited in the semi-supervised learning settings, we propose a fuzzy weighted sparse reconstruction error-steered semi-supervised learning method for face recognition. The fuzzy membership functions are introduced to the reconstruction error calculation for the unlabeled data. A weight function is utilized to capture the locality property of data when learning the sparse coefficients. The fuzzy weighted sparse reconstruction error-steered semi-supervised learning not only inherits the advantages of sparse representation classification techniques and neighborhood methods, but also steers the reconstruction errors of unlabeled data. Experimental studies on well-known face image datasets demonstrate that the proposed method outperforms the comparative approaches.},
journal = {Vis. Comput.},
month = aug,
pages = {1521–1534},
numpages = {14},
keywords = {Fuzzy, Sparse representation, Membership function}
}

@article{10.1016/j.cageo.2021.104862,
author = {Zhu, Mengbo and Cheng, Jianyuan and Zhang, Zheng},
title = {Quality control of microseismic P-phase arrival picks in coal mine based on machine learning},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2021.104862},
doi = {10.1016/j.cageo.2021.104862},
journal = {Comput. Geosci.},
month = nov,
numpages = {12},
keywords = {Convolutional neural network, Machine learning, Quality control, P-phase arrival, Microseismic monitoring, Coal mine}
}

@inproceedings{10.1145/3460418.3479316,
author = {Hamanaka, Satoki and Sasaki, Wataru and Okoshi, Tadashi and Nakazawa, Jin and Yagasaki, Kaori and Komatsu, Hiroko},
title = {A Comparative Study of CIPN Symptom Estimation Methods Based on Machine Learning},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479316},
doi = {10.1145/3460418.3479316},
abstract = {Chemotherapy-induced peripheral neuropathy (CIPN) is a common side effect of anticancer medicines that causes muscular weakness and falls in cancer patients. Therefore, we compared the methods for identifying and estimating the factors that cause symptoms in CIPN based on factor analysis and four different machine learning algorithms. The results of factor analysis showed that the latent factors causing CIPN can be classified into three categories: “Morning body condition”, “Pain in the legs” and “Dizziness”. We chose models for machine learning based on the premise of multi-task learning and built numerous machine learning models to estimate various CIPN symptoms. According to the results of the comparison studies, TabNet has the best generalization performance.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {117–120},
numpages = {4},
keywords = {mobile sensing, machine learning, fall detection, chemotherapy-induced peripheral neuropathy},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@phdthesis{10.5555/AAI28652512,
author = {Chen, You-Lin and Biao, Wu, Wei and Mihai, Anitescu, and S, Tsay, Ruey},
advisor = {Mladen, Kolar,},
title = {Stochastic and Online Learning with Applications to Machine Learning and Statistics},
year = {2021},
isbn = {9798460477845},
publisher = {The University of Chicago},
abstract = {This dissertation develops rigorous analyses of stochastic and online learning with applications to modern machine learning models and statistical methodologies. There is no doubt that big data are rapidly expanding nowadays in all fields like science and engineering domains. While the potential of massive data is considerable, in many scenarios of employing machine learning and statistic algorithms to imaging recognizing, natural language processing, and genetic studies, practitioners face many challenges, including high-dimensional data (the feature size is much large than the sample size), over-fitting, discrimination, and hidden confounding. Those problems impose additional difficulties due to over-parameterization, ill-conditioning, non-convex objectives and constraints, and limited power of computation and data storage. Therefore, fully exploiting data's value requires novel learning techniques. In this thesis, we discuss the following topics. First, the theorems of accelerated first-order methods on strongly convex objective functions with the growth condition are derived. The growth condition, which is more realistic but rarely addressed in the literature, states the variance of the stochastic gradients can be dominated by a multiplicative part and an additive part. We show that there exists a trade-off between the convergence rate and robustness for multiplicative noise. Next, the no-regret analysis of online learning for non-linear models is considered. We establish an error control for biased stochastic gradient descent, which leads to a no-regret analysis for the circumstance where we only receive a non-convex approximation of a convex loss function. These results can be applied to a game-theoretical framework for building neural network classifiers with fairness constraints. Finally, canonical correlation analysis with low-rank constraints is studied. We prove algorithmic and statistical properties of two-dimensional canonical correlation analysis under mild conditions of the data generating process and further develop the error bound of using first-order stochastic methods, an effective initialization scheme, and a deflation procedure for extracting multiple canonical components. We believe that our works contribute not only to cutting-edge research in the field of optimization but also to complex and large-scale data analysis.},
note = {AAI28652512}
}

@article{10.1504/ijhpcn.2020.113779,
author = {Chouhan, Lokesh and Chauhan, Nancy and Mahapatra, Amitosh Swain and Agarwal, Vidushi},
title = {A survey on the applications of machine learning in wireless sensor networks},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {4},
issn = {1740-0562},
url = {https://doi.org/10.1504/ijhpcn.2020.113779},
doi = {10.1504/ijhpcn.2020.113779},
abstract = {With the dawn of the 21st century and the growth of fast, always available and low power networks, wireless sensor networks are being implemented in diverse use-cases. Wireless sensor networks are being deployed to observe, explore and control the physical world. Wireless sensor networks are generally deployed in dynamic environments. Sensor networks utilise machine learning techniques to avoid the unnecessary redesign of a wireless sensor network deployment for adapting to changing requirements. Machine learning is also used to maximise the security, efficiency, lifetime, and resource utilisation in such networks. In this paper, we present an extensive literature survey of various machine learning applications that are used or are in research to address the operational and non-operational challenges in wireless sensor networks.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jan,
pages = {197–220},
numpages = {23},
keywords = {wireless sensor networks, security, machine learning, data aggregation, clustering}
}

@inproceedings{10.5555/3504035.3504515,
author = {Park, Sungrae and Park, JunKeon and Shin, Su-Jin and Moon, Il-Chul},
title = {Adversarial dropout for supervised and semi-supervised learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Recently, training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has improved the generalization performance of neural networks. In contrast to the biased individual inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between 1) the training supervision and 2) the outputs from the network with the dropouts. The identified adversarial dropouts are used to automatically reconfigure the neural network in the training process, and we demonstrated that the simultaneous training on the original and the reconfigured network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST, SVHN, and CIFAR-10. We analyzed the trained model to find the performance improvement reasons. We found that adversarial dropout increases the sparsity of neural networks more than the standard dropout. Finally, we also proved that adversarial dropout is a regularization term with a rank-valued hyper parameter that is different from a continuous-valued parameter to specify the strength of the regularization.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {480},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.compbiomed.2020.104027,
author = {Petrovi\'{c}, Nata\v{s}a and Moy\`{a}-Alcover, Gabriel and Jaume-i-Cap\'{o}, Antoni and Gonz\'{a}lez-Hidalgo, Manuel},
title = {Sickle-cell disease diagnosis support selecting the most appropriate machine learning method: Towards a general and interpretable approach for cell morphology analysis from microscopy images},
year = {2020},
issue_date = {Nov 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104027},
doi = {10.1016/j.compbiomed.2020.104027},
journal = {Comput. Biol. Med.},
month = nov,
numpages = {14},
keywords = {Morphology analysis, Interpretability, Machine learning, Microscopy image, Sickle-cell disease, Red blood cell}
}

@article{10.1016/j.knosys.2021.107164,
author = {Han, Yuzhang and Modaresnezhad, Minoo and Nemati, Hamid},
title = {An Adaptive Machine Learning System for predicting recurrence of child maltreatment: A routine activity theory perspective},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107164},
doi = {10.1016/j.knosys.2021.107164},
journal = {Know.-Based Syst.},
month = sep,
numpages = {19},
keywords = {Big data, Child maltreatment, Routine activity theory, Predictive risk modeling, Adaptive machine learning system}
}

@article{10.1016/j.neucom.2018.11.053,
author = {V. Utkin, Lev},
title = {An imprecise extension of SVM-based machine learning models},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.053},
doi = {10.1016/j.neucom.2018.11.053},
journal = {Neurocomput.},
month = feb,
pages = {18–32},
numpages = {15},
keywords = {Imprecise model, Interval-valued data, Regression, Classification, Duality, Support vector machine, Machine learning}
}

@article{10.1016/j.neucom.2019.08.036,
author = {Zhao, Mingbo and Zhang, Yue and Zhang, Zhao and Liu, Jiao and Kong, Weijian},
title = {ALG: Adaptive low-rank graph regularization for scalable semi-supervised and unsupervised learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.036},
doi = {10.1016/j.neucom.2019.08.036},
journal = {Neurocomput.},
month = dec,
pages = {16–27},
numpages = {12},
keywords = {Adaptive low-rank model, Spectral clustering, Unsupervised learning, Semi-supervised learning}
}

@inproceedings{10.1145/3416014.3424615,
author = {Coutinho, Rodolfo W. L.},
title = {Machine Learning for Self-Adaptive Internet of Underwater Things},
year = {2020},
isbn = {9781450381215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416014.3424615},
doi = {10.1145/3416014.3424615},
abstract = {Internet of Underwater Things (IoUTs) has gained increased momentum thanks to the advancements in underwater nodes, sensing, and communication technologies. This novel paradigm has tremendous potential to empower smart ocean applications. However, the harsh and dynamic nature of the underwater environment and underwater communication, the stringent requirements of underwater applications, and the difficulty and cost for IoUT management and maintenance have limited the development and application of IoUTs. In this regard, machine learning has been proposed to create self-adaptive IoUTs and boost the performance of smart oceans applications. In this paper, we shed light on the design of machine learning models for the on-the-fly intelligent and autonomous management of IoUT networking parameters and configurations aimed at boosting data delivery. We discuss the recent proposals for IoUT network management and how machine learning algorithms can improve such solutions at different networking layers. Finally, we point out some future research directions in need of further attention.},
booktitle = {Proceedings of the 10th ACM Symposium on Design and Analysis of Intelligent Vehicular Networks and Applications},
pages = {65–69},
numpages = {5},
keywords = {networking protocols, network operation, machine learning, internet of underwater things},
location = {Alicante, Spain},
series = {DIVANet '20}
}

@inproceedings{10.1145/3442188.3445918,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Remi and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {datasets, machine learning, requirements engineering},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/3357160.3357670,
author = {Gon, Shuangping and Ma, Huajuan and Wan, Yihang and Xu, Anran},
title = {Machine Learning in Human-computer Nonverbal Communication},
year = {2019},
isbn = {9781450369725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357160.3357670},
doi = {10.1145/3357160.3357670},
abstract = {Nonverbal communication is an indispensable and omnipresent element of human behaviors which includes various ways such as human action, hand gesture, and facial expression, etc. As a basic means to express a person's attitudes, feelings, and emotions, it is essential in human's communication, and also play an important role in the multimodal interactions between human and computer system. Due to the richness, flexibility, and ambiguity existing in human's nonverbal expressions, as well as being affected by personalized behavior habits, the understanding and recognition of above human's expressions involves complex intelligent technologies such as visual analysis and situational awareness based on machine learning. This paper aims to provide a systematic summary and analysis of machine learning methods and technologies in human-computer nonverbal communication. It starts with the evolution of nonverbal communication research, goes on to present the definition and classification of nonverbal communication, proceeds to review and analyze the machine learning techniques which are frequently employed in human-computer nonverbal communication, and finally expounds the development of smart learning based on neural mechanism.},
booktitle = {NeuroManagement and Intelligent Computing Method on Multimodal Interaction},
articleno = {4},
numpages = {7},
keywords = {Smart learning, Nonverbal communication, Multimodal interaction, Machine learning, Human action recognition},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3447545.3451185,
author = {Hegeman, Tim and Jansen, Matthijs and Iosup, Alexandru and Trivedi, Animesh},
title = {GradeML: Towards Holistic Performance Analysis for Machine Learning Workflows},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451185},
doi = {10.1145/3447545.3451185},
abstract = {Today, machine learning (ML) workloads are nearly ubiquitous. Over the past decade, much effort has been put into making ML model-training fast and efficient, e.g., by proposing new ML frameworks (such as TensorFlow, PyTorch), leveraging hardware support (TPUs, GPUs, FPGAs), and implementing new execution models (pipelines, distributed training). Matching this trend, considerable effort has also been put into performance analysis tools focusing on ML model-training. However, as we identify in this work, ML model training rarely happens in isolation and is instead one step in a larger ML workflow. Therefore, it is surprising that there exists no performance analysis tool that covers the entire life-cycle of ML workflows. Addressing this large conceptual gap, we envision in this work a holistic performance analysis tool for ML workflows. We analyze the state-of-practice and the state-of-the-art, presenting quantitative evidence about the performance of existing performance tools. We formulate our vision for holistic performance analysis of ML workflows along four design pillars: a unified execution model, lightweight collection of performance data, efficient data aggregation and presentation, and close integration in ML systems. Finally, we propose first steps towards implementing our vision as GradeML, a holistic performance analysis tool for ML workflows. Our preliminary work and experiments are open source at https://github.com/atlarge-research/grademl.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {57–63},
numpages = {7},
keywords = {performance analysis, modeling, machine learning workflow, gradeML, data gathering, MLdevops},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3501409.3501576,
author = {Han, Ao and Zhao, Zhenyu and Feng, Chaochao and Zhang, Shuzheng},
title = {Stage-based Path Delay Prediction with Customized Machine Learning Technique},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501576},
doi = {10.1145/3501409.3501576},
abstract = {Static timing analysis is an important timing analysis technique in the physical design process of integrated circuits, facing the challenge of speed and accuracy trade-off in advanced nodes. Expensive and burdensome path-based analysis (PBA) forces designers to adopt faster graph-based analysis (GBA) in more early flows at the cost of pessimism. Existing work focuses on reducing pessimism but ignores the degree of optimism. In this paper, we propose a stage-based delay model based on machine learning technique with customized loss function to rapidly generate predicted PBA timing results from the pessimistic GBA timing report with considering the asymmetric loss. The model could also enable the designers to identify the false violation path in GBA report with less time cost to reduce the over-design and margin in post-route optimization phase. Experimental results demonstrate that the mean absolute error of predicted PBA slack divergence reduces 66.7%~79.8% compared to GBA-PBA slack divergence (from 17.79ps to 5.92ps and 3.6ps) with about 3X runtime overhead reduction on a 28nm industrial ASIC for each corner. It can also correct about 75.6% false violation paths in GBA timing report.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {926–933},
numpages = {8},
keywords = {Customized machine learning, False violation path calibration, PBA prediction, Static timing analysis},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1145/3398069,
author = {Thieme, Anja and Belgrave, Danielle and Doherty, Gavin},
title = {Machine Learning in Mental Health: A Systematic Review of the HCI Literature to Support the Development of Effective and Implementable ML Systems},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3398069},
doi = {10.1145/3398069},
abstract = {High prevalence of mental illness and the need for effective mental health care, combined with recent advances in AI, has led to an increase in explorations of how the field of machine learning (ML) can assist in the detection, diagnosis and treatment of mental health problems. ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. Despite the potential opportunities for using ML within mental health, this is an emerging research area, and the development of effective ML-enabled applications that are implementable in practice is bound up with an array of complex, interwoven challenges. Aiming to guide future research and identify new directions for advancing development in this important domain, this article presents an introduction to, and a systematic review of, current ML work regarding psycho-socially based mental health conditions from the computing and HCI literature. A quantitative synthesis and qualitative narrative review of 54 papers that were included in the analysis surfaced common trends, gaps, and challenges in this space. Discussing our findings, we (i) reflect on the current state-of-the-art of ML work for mental health, (ii) provide concrete suggestions for a stronger integration of human-centered and multi-disciplinary approaches in research and development, and (iii) invite more consideration of the potentially far-reaching personal, social, and ethical implications that ML models and interventions can have, if they are to find widespread, successful adoption in real-world mental health contexts.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {34},
numpages = {53},
keywords = {systematic review, society + AI, real-world interventions, mental illness, machine learning, interpretability, interaction design, health care, ethics, Mental health, AI applications}
}

@phdthesis{10.5555/AAI28715432,
author = {Wadekar, Digvijay and Glennys, Farrar, and Yacine, Ali-Haimoud, and Ken, Van Tilburg, and Shirley, Ho,},
advisor = {Roman, Scoccimarro,},
title = {Accelerating Cosmological Inference with Novel Analytic Methods and Machine Learning},
year = {2021},
isbn = {9798496509169},
publisher = {New York University},
address = {USA},
abstract = {Astronomical observations of millions of galaxies in the Universe help us shed light upon questions such as: What is the Universe made up of? What is its origin and what will be its ultimate fate? In order to answer these questions, we need to infer cosmological parameters from galaxy survey data. We typically use summary statistics such as the power spectrum and we need an accurate estimate of their covariance matrix. The traditional process of obtaining the covariance for spectroscopic surveys involves simulating thousands of mocks. In Chapter 1, I developed an analytic approach for the covariance matrix which is more than four orders of magnitude faster. In Chapter 2, I validated our method with an analysis of the BOSS DR12 data. Furthermore, our analytic approach is free of sampling noise which makes it useful for upcoming surveys like DESI, Euclid and many others.In order to extract the wealth of cosmological information that lies beyond the linear scales, one needs to account for baryonic effects which are captured by hydrodynamic simulations. However, such simulations have a huge computational cost (~10 million CPU hours for 0.001 Gpc3 volume) and cannot therefore be directly used in predictions for upcoming surveys which will probe ~100 Gpc3 volumes. Focusing on neutral hydrogen (HI), I trained neural networks on hydro simulations in Chapter 3 to quickly generate accurate HI maps from gravity-only dark matter simulations. I took the initiative of interpreting the neural network and we learnt useful additions in Chapter 4 to theoretical models like HOD. In particular, a basic HOD model relies on a widely used assumption that the baryonic properties of a dark matter halo depend only on its mass. However, I inferred from the neural network that the halo environment also has a crucial dependence on the HI content of the halo. I also inferred novel symbolic expressions for encoding the effect of halo environment on the clustering of HI using a newly developed tool called symbolic regression. These help in better understanding the galaxy-halo connection and marginalizing over baryonic effects to extract cosmology from non-linear scales.I also explored the direction of using observations of individual astrophysical systems like dwarf galaxies to probe alternatives to the standard cold, collisionless dark matter (CDM) paradigm. Due to a lack of baryonic feedback, dwarf galaxies are pristine objects for such analyses and therefore are a subject of interest for numerous upcoming surveys like the Rubin observatory. In Chapter 5, I used the optical and 21cm observations of a gas-rich dwarf galaxy of the Milky Way called Leo T to set strong constraints on DM-baryon interactions. Such constraints are complementary to the early-universe constraints as they are not affected by assumptions about cosmology. Furthermore, for the popular dark photon DM model, we obtained constraints stronger than all the previous literature.},
note = {AAI28715432}
}

@inproceedings{10.1145/3338503.3357719,
author = {Tofighi-Shirazi, Ramtine and Asavoae, Irina-Mariuca and Elbaz-Vincent, Philippe and Le, Thanh-Ha},
title = {Defeating Opaque Predicates Statically through Machine Learning and Binary Analysis},
year = {2019},
isbn = {9781450368353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338503.3357719},
doi = {10.1145/3338503.3357719},
abstract = {We present a new approach that bridges binary analysis techniques with machine learning classification for the purpose of providing a static and generic evaluation technique for opaque predicates, regardless of their constructions. We use this technique as a static automated deobfuscation tool to remove the opaque predicates introduced by obfuscation mechanisms. According to our experimental results, our models have up to 98% accuracy at detecting and deobfuscating state-of-the-art opaque predicates patterns. By contrast, the leading edge deobfuscation methods based on symbolic execution show less accuracy mostly due to the SMT solvers constraints and the lack of scalability of dynamic symbolic analyses. Our approach underlines the efficiency of hybrid symbolic analysis and machine learning techniques for a static and generic deobfuscation methodology.},
booktitle = {Proceedings of the 3rd ACM Workshop on Software Protection},
pages = {3–14},
numpages = {12},
keywords = {symbolic execution, software protection, opaque predicate, obfuscation, machine learning, deobfuscation},
location = {London, United Kingdom},
series = {SPRO'19}
}

@article{10.1007/s10208-020-09472-x,
author = {Unser, Michael},
title = {A Unifying Representer Theorem for Inverse Problems and Machine Learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {1615-3375},
url = {https://doi.org/10.1007/s10208-020-09472-x},
doi = {10.1007/s10208-020-09472-x},
abstract = {Regularization addresses the ill-posedness of the training problem in machine learning or the reconstruction of a signal from a limited number of measurements. The method is applicable whenever the problem is formulated as an optimization task. The standard strategy consists in augmenting the original cost functional by an energy that penalizes solutions with undesirable behavior. The effect of regularization is very well understood when the penalty involves a Hilbertian norm. Another popular configuration is the use of an ℓ1-norm (or some variant thereof) that favors sparse solutions. In this paper, we propose a higher-level formulation of regularization within the context of Banach spaces. We present a general representer theorem that characterizes the solutions of a remarkably broad class of optimization problems. We then use our theorem to retrieve a number of known results in the literature such as the celebrated representer theorem of machine leaning for RKHS, Tikhonov regularization, representer theorems for sparsity promoting functionals, the recovery of spikes, as well as a few new ones.},
journal = {Found. Comput. Math.},
month = aug,
pages = {941–960},
numpages = {20},
keywords = {68T05, 65J20, 47A52, 46N10, Banach space, Machine learning, Inverse problem, Representer theorem, Regularization, Convex optimization}
}

@article{10.1145/3205942,
author = {Fiebrink, Rebecca and Gillies, Marco},
title = {Introduction to the Special Issue on Human-Centered Machine Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3205942},
doi = {10.1145/3205942},
abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {7},
numpages = {7},
keywords = {interactive machine learning, Human-centered machine learning}
}

@inproceedings{10.1145/3357384.3357914,
author = {Kang, SeongKu and Hwang, Junyoung and Lee, Dongha and Yu, Hwanjo},
title = {Semi-Supervised Learning for Cross-Domain Recommendation to Cold-Start Users},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357914},
doi = {10.1145/3357384.3357914},
abstract = {Providing accurate recommendations to newly joined users (or potential users, so-called cold-start users) has remained a challenging yet important problem in recommender systems. To infer the preferences of such cold-start users based on their preferences observed in other domains, several cross-domain recommendation (CDR) methods have been studied. The state-of-the-art Embedding and Mapping approach for CDR (EMCDR) aims to infer the latent vectors of cold-start users by supervised mapping from the latent space of another domain. In this paper, we propose a novel CDR framework based on semi-supervised mapping, called SSCDR, which effectively learns the cross-domain relationship even in the case that only a few number of labeled data is available. To this end, it first learns the latent vectors of users and items for each domain so that their interactions are represented by the distances, then trains a cross-domain mapping function to encode such distance information by exploiting both overlapping users as labeled data and all the items as unlabeled data. In addition, SSCDR adopts an effective inference technique that predicts the latent vectors of cold-start users by aggregating their neighborhood information. Our extensive experiments on different CDR scenarios show that SSCDR outperforms the state-of-the-art methods in terms of CDR accuracy, particularly in the realistic settings that a small portion of users overlap between two domains.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1563–1572},
numpages = {10},
keywords = {semi-supervised learning, neighborhood inference, metric learning, cross-domain recommendation, collaborative filtering},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3463858.3463869,
author = {Junior Mele, Umberto and Maria Gambardella, Luca and Montemanni, Roberto},
title = {Machine Learning Approaches for the Traveling Salesman Problem: A Survey},
year = {2021},
isbn = {9781450389921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463858.3463869},
doi = {10.1145/3463858.3463869},
abstract = {Machine Learning techniques have been applied in many contexts with great success. In this survey, we focus on their applications in the Combinatorial Optimization (CO) domain, and in particular to the Traveling Salesman Problem (TSP). We propose an intuitive and simple mind map helpful to navigate through the wide existing literature, indicating the approaches we consider most promising. Different ML techniques introduced to solve the TSP are discussed and reviewed; and their differences and limitations are delved. Open problems for future research in this area are finally highlighted.},
booktitle = {Proceedings of the 2021 8th International Conference on Industrial Engineering and Applications (Europe)},
pages = {182–186},
numpages = {5},
keywords = {Traveling Salesman Problem, Machine Learning, Combinatorial Optimization, Artificial Intelligence},
location = {Barcelona, Spain},
series = {ICIEA 2021-Europe}
}

@article{10.1016/j.jcp.2019.05.039,
author = {Vesselinov, V.V. and Mudunuru, M.K. and Karra, S. and O'Malley, D. and Alexandrov, B.S.},
title = {Unsupervised machine learning based on non-negative tensor factorization for analyzing reactive-mixing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Academic Press Professional, Inc.},
address = {USA},
volume = {395},
number = {C},
issn = {0021-9991},
url = {https://doi.org/10.1016/j.jcp.2019.05.039},
doi = {10.1016/j.jcp.2019.05.039},
journal = {J. Comput. Phys.},
month = oct,
pages = {85–104},
numpages = {20},
keywords = {Non-negative solutions, Anisotropic dispersion, Reactive-mixing, Structure-preserving feature extraction, Unsupervised machine learning, Non-negative tensor factorization}
}

@inproceedings{10.1145/3394885.3431547,
author = {Dhar, Tonmoy and Poojary, Jitesh and Li, Yaguang and Kunal, Kishor and Madhusudan, Meghna and Sharma, Arvind K. and Manasi, Susmita Dey and Hu, Jiang and Harjani, Ramesh and Sapatnekar, Sachin S.},
title = {Fast and Efficient Constraint Evaluation of Analog Layout Using Machine Learning Models},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431547},
doi = {10.1145/3394885.3431547},
abstract = {Placement algorithms for analog circuits explore numerous layout configurations in their iterative search. To steer these engines towards layouts that meet the electrical constraints on the design, this work develops a fast feasibility predictor to guide the layout engine. The flow first discerns rough bounds on layout parasitics and prunes the feature space. Next, a Latin hypercube sampling technique is used to sample the reduced search space, and the labeled samples are classified by a linear support vector machine (SVM). If necessary, a denser sample set is used for the SVM, or if the constraints are found to be nonlinear, a multilayer perceptron (MLP) is employed. The resulting machine learning model demonstrated to rapidly evaluate candidate placements in a placer, and is used to build layouts for several analog blocks.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {158–163},
numpages = {6},
keywords = {performance analysis, machine learning, Analog layout},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.1007/s10844-019-00582-9,
author = {Ram\'{\i}rez, Jaime and Flores, M. Julia},
title = {Machine learning for music genre: multifaceted review and experimentation with audioset},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {55},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-019-00582-9},
doi = {10.1007/s10844-019-00582-9},
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {469–499},
numpages = {31},
keywords = {Feed-forward neural networks, Music, Classification algorithms, Music information retrieval, Datasets, Machine learning}
}

@article{10.1145/3264745,
author = {Chen, Chaochao and Chang, Kevin Chen-Chuan and Li, Qibing and Zheng, Xiaolin},
title = {Semi-supervised Learning Meets Factorization: Learning to Recommend with Chain Graph Model},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3264745},
doi = {10.1145/3264745},
abstract = {Recently, latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predict missing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which is mainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this article, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field. The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {73},
numpages = {24},
keywords = {latent factor model, data sparsity, chain graph model, Semi-supervised learning}
}

@article{10.1016/j.ijinfomgt.2021.102353,
author = {Young, Amber Grace and Majchrzak, Ann and Kane, Gerald C.},
title = {Organizing workers and machine learning tools for a less oppressive workplace},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2021.102353},
doi = {10.1016/j.ijinfomgt.2021.102353},
journal = {Int. J. Inf. Manag.},
month = aug,
numpages = {9},
keywords = {Machine learning, Normative theory, Organizing for the future of work, Oppression, Emancipatory organizing theory, Artificial intelligence}
}

@phdthesis{10.5555/AAI28552648,
author = {Li, Li and Karen, Chen, and Xiaolei, Fang, and Jing, Feng, and Shiyan, Jiang,},
advisor = {Xu, Xu,},
title = {Automated Ergonomics Assessment Through Computer Vision and Machine Learning},
year = {2021},
isbn = {9798522941499},
publisher = {North Carolina State University},
abstract = {Musculoskeletal disorders (MSDs) are major nonfatal occupational injuries in the U.S. and led to billions of cost for workers' compensation annually. The reported MSD cases are also significantly underestimated worldwide due to under-reporting. Therefore, there is a great importance to develop an assessment tool for the prevention of MSDs. The risk factors for developing work-related MSDs include physical, psychosocial, and individual risk factors. This dissertation mainly focuses on the physical risk factor assessment at the workplace. In general, the assessment methods can be categorized as 1) selfreports, 2) direct measurements (e.g., using wearable inertial measurement units (IMUs)), and 3) observational methods (e.g., using rapid upper limb assessment (RULA)). However, the traditional practice of them suffers from several limitations. For example, self-reports have relatively low reliability, direct measurements could be intrusive to the workers, and observational methods require well-trained and highly-skilled safety practitioners. The recent advancement in computer vision provides a possibility to develop a reliable, efficient, and non-intrusive vision-based ergonomics assessment tool. In this dissertation, we aim to develop computer vision-based methods to perform ergonomics assessment by combining observational methods and monocular images collected by a regular camera. Particularly, we have examined the possibility of performing RULA estimation from a 2-D pose and proposed a novel end-to-end pipeline for performing RULA estimation from a monocular 2-D image. The results showed 93% accuracy in RULA estimation and 29 frames per second in terms of efficiency. We have also created a multimodal dataset of full-body pose and motion in occupational tasks to address the lack of a consistent and biomechanically meaningful human pose dataset. This dataset includes videos and motion data of 25 tasks collected on 11 subjects, which could be used for validating and comparing different computer vision-based ergonomics assessment tools. As human-robot collaboration is becoming a flourishing work configuration in modern industries, in this dissertation, we also developed a lifting posture predictor using generative models. This lifting posture predictor can help robots predict the working posture of their human teammates, and in turn, help robots choose a collaborative work location to reduce the risk of MSDs for the human workers. The algorithm was trained with postures extracted from the motion dataset we created. The results show that the proposed algorithm is able to predict postures with satisfying accuracy and validity and to partially capture the variability of lifting postures.},
note = {AAI28552648}
}

@inproceedings{10.1145/3366424.3383418,
author = {G. Harris, Christopher},
title = {Methods to Evaluate Temporal Cognitive Biases in Machine Learning Prediction Models},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383418},
doi = {10.1145/3366424.3383418},
abstract = {When asked to rank or rate a list of items, humans are often affected by cognitive biases, which may lead to inconsistent decisions over time. These inconsistencies become part of machine learning prediction algorithms trained on human judgments, leading to misalignment and consequently affecting the metrics used to evaluate their correctness. In this paper, we propose new accuracy metrics, built upon commonly used statistics- and decision support-based metrics. Each of these metrics is designed to address the varying nature of human judgment and to evaluate the importance of decisions that change over time due to cognitive biases.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {572–575},
numpages = {4},
keywords = {Temporal Evaluation, Cognitive Bias, Fairness, Machine Learning, Decision Making, Data Science},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3368756.3369072,
author = {Daanouni, Othmane and Cherradi, Bouchaib and Tmiri, Amal},
title = {Predicting diabetes diseases using mixed data and supervised machine learning algorithms},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369072},
doi = {10.1145/3368756.3369072},
abstract = {Diabetes is considered as one of the deadliest and chronic diseases in several countries. All of them are working to prevent this disease at early stage by diagnosing and predicting the symptoms of diabetes using several methods. The motive of this study is to compare the performance of some Machine Learning algorithms, used to predict type 2 diabetes diseases. In this paper, we apply and evaluate four Machine Learning algorithms (Decision Tree, K-Nearest Neighbours, Artificial Neural Network and Deep Neural Network) to predict patients with or without type 2 diabetes mellitus. These techniques have been trained and tested on two diabetes databases: The first obtained from Frankfurt hospital (Germany), and the second is the well-known Pima Indian dataset. These datasets contain the same features composed of mixed data; risk factors and some clinical data. The performances of the experimented algorithms have been evaluated in both the cases i.e. dataset with noisy data (before pre-processing/some data with missing values) and dataset set without noisy data (after preprocessing). The results compared using different similarity metrics like Accuracy, Sensitivity, and Specificity gives best performance with respect to state of the art.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {85},
numpages = {6},
keywords = {prediction systems, machine learning, diabetes diseases, deep learning, computer aided diagnosis (CAD)},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@article{10.1007/s00146-019-00910-1,
author = {Yalur, Tolga},
title = {Interperforming in AI: question of ‘natural’ in machine learning and recurrent neural networks},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-019-00910-1},
doi = {10.1007/s00146-019-00910-1},
abstract = {This article offers a critical inquiry of contemporary neural network models as an instance of machine learning, from an interdisciplinary perspective of AI studies and performativity. It shows the limits on the architecture of these network systems due to the misemployment of ‘natural’ performance, and it offers ‘context’ as a variable from a performative approach, instead of a constant. The article begins with a brief review of machine learning-based natural language processing systems and continues with a concentration on the relevant model of recurrent neural networks, which is applied in most commercial research such as Facebook AI Research. It demonstrates that the logic of performativity is not brought into account in all recurrent nets, which is an integral part of human performance and languaging, and it argues that recurrent network models, in particular, fail to grasp human performativity. This logic works similarly to the theory of performativity articulated by Jacques Derrida in his critique of John L. Austin’s concept of the performative. Applying Jacques Derrida’s work on performativity, and linguistic traces as spatially organized entities that allow for this notion of performance, the article argues that recurrent nets fall into the trap of taking ‘context’ as a constant, of treating human performance as a ‘natural’ fix to be encoded, instead of performative. Lastly, the article applies its proposal more concretely to the case of Facebook AI Research’s Alice and Bob.},
journal = {AI Soc.},
month = sep,
pages = {737–745},
numpages = {9},
keywords = {Facebook, Derrida, Recurrent neural networks, Natural language processing, Machine learning, Performativity}
}

@article{10.1162/neco_a_01094,
author = {Abarbanel, Henry D. I. and Rozdeba, Paul J. and Shirman, Sasha},
title = {Machine learning: Deepest learning as statistical data assimilation problems},
year = {2018},
issue_date = {August 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {30},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco_a_01094},
doi = {10.1162/neco_a_01094},
abstract = {We formulate an equivalence between machine learning and the formulation of statistical data assimilation as used widely in physical and biological sciences. The correspondence is that layer number in a feedforward artificial network setting is the analog of time in the data assimilation setting. This connection has been noted in the machine learning literature. We add a perspective that expands on how methods from statistical physics and aspects of Lagrangian and Hamiltonian dynamics play a role in how networks can be trained and designed. Within the discussion of this equivalence, we show that adding more layers making the network deeper is analogous to adding temporal resolution in a data assimilation framework. Extending this equivalence to recurrent networks is also discussed.We explore how one can find a candidate for the global minimum of the cost functions in the machine learning context using a method from data assimilation. Calculations on simple models from both sides of the equivalence are reported.Also discussed is a framework in which the time or layer label is taken to be continuous, providing a differential equation, the Euler-Lagrange equation and its boundary conditions, as a necessary condition for a minimum of the cost function. This shows that the problem being solved is a two-point boundary value problem familiar in the discussion of variational methods. The use of continuous layers is denoted "deepest learning."These problems respect a symplectic symmetry in continuous layer phase space. Both Lagrangian versions and Hamiltonian versions of these problems are presented. Their well-studied implementation in a discrete time/layer, while respecting the symplectic structure, is addressed. The Hamiltonian version provides a direct rationale for backpropagation as a solution method for a certain two-point boundary value problem.},
journal = {Neural Comput.},
month = aug,
pages = {2025–2055},
numpages = {31}
}

@article{10.1007/s10916-018-0940-7,
author = {Maniruzzaman, Md. and Rahman, Md. Jahanur and Al-Mehedihasan, Md. and Suri, Harman S. and Abedin, Md. Menhazul and El-Baz, Ayman and Suri, Jasjit S.},
title = {Accurate Diabetes Risk Stratification Using Machine Learning: Role of Missing Value and Outliers},
year = {2018},
issue_date = {May 2018},
publisher = {Plenum Press},
address = {USA},
volume = {42},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-018-0940-7},
doi = {10.1007/s10916-018-0940-7},
abstract = {Diabetes mellitus is a group of metabolic diseases in which blood sugar levels are too high. About 8.8% of the world was diabetic in 2017. It is projected that this will reach nearly 10% by 2045. The major challenge is that when machine learning-based classifiers are applied to such data sets for risk stratification, leads to lower performance. Thus, our objective is to develop an optimized and robust machine learning (ML) system under the assumption that missing values or outliers if replaced by a median configuration will yield higher risk stratification accuracy. This ML-based risk stratification is designed, optimized and evaluated, where: (i) the features are extracted and optimized from the six feature selection techniques (random forest, logistic regression, mutual information, principal component analysis, analysis of variance, and Fisher discriminant ratio) and combined with ten different types of classifiers (linear discriminant analysis, quadratic discriminant analysis, na ve Bayes, Gaussian process classification, support vector machine, artificial neural network, Adaboost, logistic regression, decision tree, and random forest) under the hypothesis that both missing values and outliers when replaced by computed medians will improve the risk stratification accuracy. Pima Indian diabetic dataset (768 patients: 268 diabetic and 500 controls) was used. Our results demonstrate that on replacing the missing values and outliers by group median and median values, respectively and further using the combination of random forest feature selection and random forest classification technique yields an accuracy, sensitivity, specificity, positive predictive value, negative predictive value and area under the curve as: 92.26%, 95.96%, 79.72%, 91.14%, 91.20%, and 0.93, respectively. This is an improvement of 10% over previously developed techniques published in literature. The system was validated for its stability and reliability. RF-based model showed the best performance when outliers are replaced by median values.},
journal = {J. Med. Syst.},
month = may,
pages = {1–17},
numpages = {17},
keywords = {Risk stratification, Outliers, Missing values, Machine learning, Feature selection, Diabetes}
}

@inproceedings{10.1145/3426020.3426138,
author = {Bhandari, Khadak Singh and Seo, Changho and Cho, Gi Hwan},
title = {Towards Sensor-Cloud Based Efficient Smart Healthcare Monitoring Framework using Machine Learning},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426138},
doi = {10.1145/3426020.3426138},
abstract = {The adaptation of sensor-cloud in healthcare infrastructure has enabled the use of machine learning techniques for efficient healthcare provisioning. In the context of the smart healthcare system, biomedical wireless sensor networks (BWSNs) are one of the key infrastructure enabling the development of healthcare applications and services. With the increasing number of healthcare information collected through BWSN, different types of medical data can be exploited to design a predictive analytics system, thereby transforming the traditional healthcare system. In this paper, we propose and highlight smart healthcare monitoring framework using state-of-the-art technologies. In particular, we focus on sensor-cloud computing and machine learning as emerging technologies, which are suitable for a proactive healthcare system by the advancement in various aspects, including computational capability, data storage, and learning techniques. Besides, we describe the components of our proposed framework with data analysis techniques and sensor-cloud layered architecture.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {380–383},
numpages = {4},
keywords = {Sensor-cloud, Machine learning, Healthcare system, Body area network},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@inproceedings{10.1007/978-3-030-73280-6_15,
author = {Lorente-Leyva, Leandro L. and Alemany, M. M. E. and Peluffo-Ord\'{o}\~{n}ez, Diego H. and Araujo, Roberth A.},
title = {Demand Forecasting for Textile Products Using Statistical Analysis and Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-73279-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73280-6_15},
doi = {10.1007/978-3-030-73280-6_15},
abstract = {The generation of an accurate forecast model to estimate the future demand for textile products that favor decision-making around an organization's key processes is very important. The minimization of the model's uncertainty allows the generation of reliable results, which prevent the textile industry's economic commitment and improve the strategies adopted around production planning and decision making. That is why this work is focused on the demand forecasting for textile products through the application of artificial neural networks, from a statistical analysis of the time series and disaggregation in different time horizons through temporal hierarchies, to develop a more accurate forecast. With the results achieved, a comparison is made with statistical methods and machine learning algorithms, providing an environment where there is an adequate development of demand forecasting, improving accuracy and performance. Where all the variables that affect the productive environment of this sector under study are considered. Finally, as a result of the analysis, multilayer perceptron achieved better performance compared to conventional and machine learning algorithms. Featuring the best behavior and accuracy in demand forecasting of the analyzed textile products.},
booktitle = {Intelligent Information and Database Systems: 13th Asian Conference, ACIIDS 2021, Phuket, Thailand, April 7–10, 2021, Proceedings},
pages = {181–194},
numpages = {14},
keywords = {Temporal hierarchies, Artificial neural networks, Machine learning algorithms, Statistical analysis, Textile products, Demand forecasting},
location = {Phuket, Thailand}
}

@article{10.1016/j.image.2019.04.003,
author = {Cui, Yan and Jiang, Jielin and Hu, Zuojin and Jiang, Xiaoyan and Yan, Wuxia and Zhang, Min-ling},
title = {Neighborhood kinship preserving hashing for supervised learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.003},
doi = {10.1016/j.image.2019.04.003},
journal = {Image Commun.},
month = aug,
pages = {31–40},
numpages = {10},
keywords = {Robust distance metric, Discriminant hashing, Discriminant information, Kinship preserving hashing, Hashing learning}
}

@article{10.1007/s00158-020-02819-6,
author = {Wu, Rih-Teng and Liu, Ting-Wei and Jahanshahi, Mohammad R. and Semperlotti, Fabio},
title = {Design of one-dimensional acoustic metamaterials using machine learning and cell concatenation},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {5},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-020-02819-6},
doi = {10.1007/s00158-020-02819-6},
abstract = {Metamaterial systems have opened new, unexpected, and exciting paths for the design of acoustic devices that only few years ago were considered completely out of reach. However, the development of an efficient design methodology still remains challenging due to highly intensive search in the design space required by the conventional optimization-based approaches. To address this issue, this study develops two machine learning (ML)-based approaches for the design of one-dimensional periodic and non-periodic metamaterial systems. For periodic metamaterials, a reinforcement learning (RL)-based approach is proposed to design a metamaterial that can achieve user-defined frequency band gaps. This RL-based approach surpasses conventional optimization-based methods in the reduction of computation cost when a near-optimal solution is acceptable. Leveraging the capability of exploration in RL, the proposed approach does not require any training datasets generation and therefore can be deployed for online metamaterial design. For non-periodic metamaterials, a neural network (NN)-based approach capable of learning the behavior of individual material units is presented. By assembling the NN representation of individual material units, a surrogate model of the whole metamaterial is employed to determine the properties of the resulting assembly. Interestingly, the proposed approach is capable of modeling different metamaterial assemblies satisfying user-defined properties while requiring only a one-time network training procedure. Also, the NN-based approach does not need a pre-defined number of material unit cells, and it works when the physical model of the unit cell is not well understood, or the situation where only the sensor measurements of the unit cell are available. The robustness of the proposed two approaches is validated through numerical simulations and design examples.},
journal = {Struct. Multidiscip. Optim.},
month = may,
pages = {2399–2423},
numpages = {25},
keywords = {Neural network, Reinforcement learning, Machine learning, Phononic crystal, Acoustic metamaterial}
}

@inproceedings{10.1145/3485730.3493448,
author = {Toussaint, Wiebke and Mathur, Akhil and Ding, Aaron Yi and Kawsar, Fahim},
title = {Characterising the Role of Pre-Processing Parameters in Audio-based Embedded Machine Learning},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493448},
doi = {10.1145/3485730.3493448},
abstract = {When deploying machine learning (ML) models on embedded and IoT devices, performance encompasses more than an accuracy metric: inference latency, energy consumption, and model fairness are necessary to ensure reliable performance under heterogeneous and resource-constrained operating conditions. To this end, prior research has studied model-centric approaches, such as tuning the hyperparameters of the model during training and later applying model compression techniques to tailor the model to the resource needs of an embedded device. In this paper, we take a data-centric view of embedded ML and study the role that pre-processing parameters in the data pipeline can play in balancing the various performance metrics of an embedded ML system. Through an in-depth case study with audio-based keyword spotting (KWS) models, we show that pre-processing parameter tuning is a remarkable tool that model developers can adopt to trade-off between a model's accuracy, fairness, and system efficiency, as well as to make an embedded ML model resilient to unseen deployment conditions.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {439–445},
numpages = {7},
keywords = {pre-processing parameters, fairness, embedded machine learning, audio keyword spotting},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1016/j.adhoc.2021.102685,
author = {Sharma, Parjanay and Jain, Siddhant and Gupta, Shashank and Chamola, Vinay},
title = {Role of machine learning and deep learning in securing 5G-driven industrial IoT applications},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2021.102685},
doi = {10.1016/j.adhoc.2021.102685},
journal = {Ad Hoc Netw.},
month = dec,
numpages = {34},
keywords = {Smart city, Block chain, Artificial intelligence, Deep learning, Machine learning, Security, Industrial internet of things}
}

@article{10.1016/j.compind.2021.103510,
author = {Crawford, Bryn and Sourki, Reza and Khayyam, Hamid and S. Milani, Abbas},
title = {A machine learning framework with dataset-knowledgeability pre-assessment and a local decision-boundary crispness score: An industry 4.0-based case study on composite autoclave manufacturing},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2021.103510},
doi = {10.1016/j.compind.2021.103510},
journal = {Comput. Ind.},
month = nov,
numpages = {13},
keywords = {Machine learning, Anomaly detection, Model explainability, Knowledge engineering, Quality control, Industry 4.0, Composite}
}

@inproceedings{10.1007/978-3-031-12429-7_23,
author = {Casimiro, Maria and Garlan, David and C\'{a}mara, Javier and Rodrigues, Lu\'{\i}s and Romano, Paolo},
title = {A Probabilistic Model Checking Approach to&nbsp;Self-adapting Machine Learning Systems},
year = {2021},
isbn = {978-3-031-12428-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12429-7_23},
doi = {10.1007/978-3-031-12429-7_23},
abstract = {Machine Learning (ML) is increasingly used in domains such as cyber-physical systems and enterprise systems. These systems typically operate in non-static environments, prone to unpredictable changes that can adversely impact the accuracy of the ML models, which are usually in the critical path of the systems. Mispredictions of ML components can thus affect other components in the system, and ultimately impact overall system utility in non-trivial ways. From this perspective, self-adaptation techniques appear as a natural solution to reason about how to react to environment changes via adaptation tactics that can potentially improve the quality of ML models (e.g., model retrain), and ultimately maximize system utility. However, adapting ML components is non-trivial, since adaptation tactics have costs and it may not be clear in a given context whether the benefits of ML adaptation outweigh its costs. In this paper, we present a formal probabilistic framework, based on model checking, that incorporates the essential governing factors for reasoning at an architectural level about adapting ML classifiers in a system context. The proposed framework can be used in a self-adaptive system to create adaptation strategies that maximize rewards of a multi-dimensional utility space. Resorting to a running example from the enterprise systems domain, we show how the proposed framework can be employed to determine the gains achievable via ML adaptation and to find the boundary that renders adaptation worthwhile.},
booktitle = {Software Engineering and Formal Methods. SEFM 2021 Collocated Workshops: CIFMA, CoSim-CPS, OpenCERT, ASYDE, Virtual Event, December 6–10, 2021, Revised Selected Papers},
pages = {317–332},
numpages = {16},
keywords = {Architectural framework, Probabilistic model checking, Self-adaptation, Machine-learning based systems}
}

@inproceedings{10.1145/3449726.3459541,
author = {Faustmann, Georg and Mrkvicka, Christoph and Musliu, Nysret and Winter, Felix},
title = {Automated configuration of parallel machine dispatching rules by machine learning},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459541},
doi = {10.1145/3449726.3459541},
abstract = {Finding optimized machine schedules is a very important task that arises in many areas of industrial manufacturing. In practice dispatching rules are often used to create schedules within short run times. This paper investigates a method for automated specification of parameters for weighted dispatching rules. We define this problem as a machine learning task and propose a novel set of features to characterize problem instances. Experimental results with a practical dispatching rule show that our approach can obtain high quality solutions in short run times.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {125–126},
numpages = {2},
keywords = {parallel machine scheduling, dispatching rules, automatic configuration},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.3103/S0147688220060076,
author = {Shelmanov, A. O. and Devyatkin, D. A. and Isakov, V. A. and Smirnov, I. V.},
title = {Open Information Extraction from Texts: Part II. Extraction of Semantic Relationships Using Unsupervised Machine Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {47},
number = {6},
issn = {0147-6882},
url = {https://doi.org/10.3103/S0147688220060076},
doi = {10.3103/S0147688220060076},
journal = {Sci. Tech. Inf. Process.},
month = dec,
pages = {340–347},
numpages = {8},
keywords = {autoencoder, neural networks, unsupervised machine learning, semantic relations, open information extraction}
}

@article{10.1016/j.neunet.2019.09.007,
author = {Hao, Yunzhe and Huang, Xuhui and Dong, Meng and Xu, Bo},
title = {A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {121},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.09.007},
doi = {10.1016/j.neunet.2019.09.007},
journal = {Neural Netw.},
month = jan,
pages = {387–395},
numpages = {9},
keywords = {Biologically plausibility, Supervised learning, Pattern recognition, Dopamine-modulated spike-timing dependent plasticity, Spiking neural networks}
}

@article{10.1016/j.envsoft.2021.105170,
author = {Cui, Tao and Pagendam, Dan and Gilfedder, Mat},
title = {Gaussian process machine learning and Kriging for groundwater salinity interpolation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {144},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2021.105170},
doi = {10.1016/j.envsoft.2021.105170},
journal = {Environ. Model. Softw.},
month = oct,
numpages = {12},
keywords = {Cokriging, Musgrave Province Australia, Airborne electromagnetic (AEM), Groundwater salinity}
}

@inproceedings{10.1007/978-3-031-04083-2_4,
author = {Molnar, Christoph and K\"{o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
title = {General Pitfalls of&nbsp;Model-Agnostic Interpretation Methods for&nbsp;Machine Learning Models},
year = {2020},
isbn = {978-3-031-04082-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-04083-2_4},
doi = {10.1007/978-3-031-04083-2_4},
abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
booktitle = {XxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers},
pages = {39–68},
numpages = {30},
keywords = {Explainable AI, Interpretable machine learning},
location = {Vienna, Austria}
}

@article{10.1145/3466689,
author = {Marshall, Byron and Curry, Michael and Crossler, Robert E. and Correia, John},
title = {Machine Learning and Survey-based Predictors of InfoSec Non-Compliance},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3466689},
doi = {10.1145/3466689},
abstract = {Survey items developed in behavioral Information Security (InfoSec) research should be practically useful in identifying individuals who are likely to create risk by failing to comply with InfoSec guidance. The literature shows that attitudes, beliefs, and perceptions drive compliance behavior and has influenced the creation of a multitude of training programs focused on improving ones’ InfoSec behaviors. While automated controls and directly observable technical indicators are generally preferred by InfoSec practitioners, difficult-to-monitor user actions can still compromise the effectiveness of automatic controls. For example, despite prohibition, doubtful or skeptical employees often increase organizational risk by using the same password to authenticate corporate and external services. Analysis of network traffic or device configurations is unlikely to provide evidence of these vulnerabilities but responses to well-designed surveys might. Guided by the relatively new IPAM model, this study administered 96 survey items from the Behavioral InfoSec literature, across three separate points in time, to 217 respondents. Using systematic feature selection techniques, manageable subsets of 29, 20, and 15 items were identified and tested as predictors of non-compliance with security policy. The feature selection process validates IPAM's innovation in using nuanced self-efficacy and planning items across multiple time frames. Prediction models were trained using several ML algorithms. Practically useful levels of prediction accuracy were achieved with, for example, ensemble tree models identifying 69% of the riskiest individuals within the top 25% of the sample. The findings indicate the usefulness of psychometric items from the behavioral InfoSec in guiding training programs and other cybersecurity control activities and demonstrate that they are promising as additional inputs to AI models that monitor networks for security events.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {13},
numpages = {20},
keywords = {machine learning, compliance behavior, Information security}
}

@inproceedings{10.1145/3356401.3356409,
author = {Mai, Tieu Long and Navet, Nicolas and Migge, J\"{o}rn},
title = {On the use of supervised machine learning for assessing schedulability: application to ethernet TSN},
year = {2019},
isbn = {9781450372237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356401.3356409},
doi = {10.1145/3356401.3356409},
abstract = {In this work, we ask if Machine Learning (ML) can provide a viable alternative to conventional schedulability analysis to determine whether a real-time Ethernet network meets a set of timing constraints. Otherwise said, can an algorithm learn what makes it difficult for a system to be feasible and predict whether a configuration will be feasible without executing a schedulability analysis? To get insights into this question, we apply a standard supervised ML technique, k-nearest neighbors (k-NN), and compare its accuracy and running times against precise and approximate schedulability analyses developed in Network-Calculus. The experiments consider different TSN scheduling solutions based on priority levels combined for one of them with traffic shaping. The results obtained on an automotive network topology suggest that k-NN is efficient at predicting the feasibility of realistic TSN networks, with an accuracy ranging from 91.8% to 95.9% depending on the exact TSN scheduling mechanism and a speedup of 190 over schedulability analysis for 106 configurations. Unlike schedulability analysis, ML leads however to a certain rate "false positives" (i.e., configurations deemed feasible while they are not). Nonetheless ML-based feasibility assessment techniques offer new trade-offs between accuracy and computation time that are especially interesting in contexts such as design-space exploration where false positives can be tolerated during the exploration process.},
booktitle = {Proceedings of the 27th International Conference on Real-Time Networks and Systems},
pages = {143–153},
numpages = {11},
keywords = {timing verification, time sensitive networking (TSN), schedulability analysis, machine learning},
location = {Toulouse, France},
series = {RTNS '19}
}

@article{10.1016/j.comnet.2021.108217,
author = {Khan, Ammar Ahmed and Khan, Muhammad Mubashir and Khan, Kashif Mehboob and Arshad, Junaid and Ahmad, Farhan},
title = {A blockchain-based decentralized machine learning framework for collaborative intrusion detection within UAVs},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108217},
doi = {10.1016/j.comnet.2021.108217},
journal = {Comput. Netw.},
month = sep,
numpages = {13},
keywords = {Collaborative intrusion detection, Decentralized machine learning, Machine learning, Blockchain, UAV, Unmanned aerial vehicles}
}

@inproceedings{10.5555/3540261.3540696,
author = {Lee, Kookjin and Trask, Nathaniel and Stinis, Panos},
title = {Machine learning structure preserving brackets for forecasting irreversible processes},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either "black-box" or penalty-based approaches.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {435},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1145/3447545.3451172,
author = {Nguyen, Minh-Tri and Truong, Hong-Linh},
title = {Demonstration Paper: Monitoring Machine Learning Contracts with QoA4ML},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451172},
doi = {10.1145/3447545.3451172},
abstract = {Using machine learning (ML) services, both service customers and providers need to monitor complex contractual constraints of ML service that are strongly related to ML models and data. Therefore, establishing and monitoring comprehensive ML contracts are crucial in ML serving. This paper demonstrates a set of features and utilities of the QoA4ML framework for ML contracts.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {169–170},
numpages = {2},
keywords = {system monitoring, service contract, SLO/SLA, ML serving},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3211346.3211348,
author = {Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
title = {Relay: a new IR for machine learning frameworks},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211348},
doi = {10.1145/3211346.3211348},
abstract = {Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {58–68},
numpages = {11},
keywords = {machine learning, intermediate representation, differentiable programming, compilers},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@article{10.3233/JIFS-189556,
author = {Liu, Tao and Xin, Baogui and Wu, Fan and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Urban green economic planning based on improved genetic algorithm and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189556},
doi = {10.3233/JIFS-189556},
abstract = {There are many factors that need to be considered when planning a city’s green economy, so it is difficult to simulate the planning effect through manual models. In order to improve the effect of urban green economic planning, this paper improves the traditional algorithm and combines the principle of machine learning algorithm to build a model that can be used in urban green economic planning. Moreover, this paper considers the measurement of green economic efficiency from the perspective of input, expected output and undesired output. In addition, this paper compares and analyzes the green efficiency calculated by the SE-SBM model, including horizontal comparison analysis and vertical comparison analysis, and conducts model simulation analysis in combination with data simulation research. Finally, this paper sets the simulation area, combines the data to perform model performance analysis, summarizes the data with statistical analysis methods, and draws charts. The research results show that the model constructed in this paper has a certain effect and can be applied to the design stage of urban green planning.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7309–7322},
numpages = {14},
keywords = {Genetic algorithm, improved algorithm, machine learning, green economy}
}

@article{10.1007/s10470-021-01885-0,
author = {Lopes, Alba and Pereira, Monica},
title = {Fast DSE of reconfigurable accelerator systems via ensemble machine learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {3},
issn = {0925-1030},
url = {https://doi.org/10.1007/s10470-021-01885-0},
doi = {10.1007/s10470-021-01885-0},
abstract = {Reconfigurable hardware accelerators (RAs) attached to processors have become a frequent choice to meet the performance demand of current embedded applications. However, answering when the combination of general purpose processors (GPPs) and RAs can provide the expected performance at the additional area and energy cost demands an extensive design space exploration (DSE). Performing DSE through hardware synthesis is an extremely time-consuming and costly task. High-level simulations are a faster and simpler DSE method at the cost of accuracy loss. Even so, the use of high-level simulation does not allow simulating all design solutions and meeting time-to-market. In this scenario, machine learning (ML) has become a promising solution to provide robustness to the DSE of large hardware designs by predicting aspects such as performance and energy. A main challenge in the design of a high-accuracy predictor is to select one ML algorithm to encompass a wide range of applications. In this context, ensemble learning is a promising solution since it can use multiple models and combine their predictions. In this work we employ the use of ensemble methods to simplify and speed up the DSE of GPPs with RAs. In our investigation, we evaluate three ensemble methods, Random Forest, AdaBoosting and Gradient Boosting. We compare them to the most used regression algorithms found in literature to perform DSE of computer architectures. Results show an error prediction rate below 2% for some benchmarks when using ensemble methods and a throughput of more than 6000 predictions per second when using Gradient Boosting.},
journal = {Analog Integr. Circuits Signal Process.},
month = sep,
pages = {495–509},
numpages = {15},
keywords = {DSE, Reconfigurable accelerators, Ensemble learning}
}

@phdthesis{10.5555/AAI28028890,
author = {Banjade, Huta and Perdew, John and Ruzsinzsky, Adrienn and Carnevale, Vincenzo},
advisor = {Qimin, Yan,},
title = {Machine Learning and Computation: Exploring Structure-property Correlations in Inorganic Crystalline Materials},
year = {2020},
isbn = {9798664751116},
publisher = {Temple University},
address = {USA},
abstract = {Kohn-Sham Density Functional Theory (DFT) has been the most successful tool to probe the electronic structure, mainly the ground-state total energies and densities of many condensed matter systems has led to the development of various databases such as Materials Project (MP), Inorganic Crystal Structure Database (ICSD), and many others. These databases ignited the interest of the material science community towards Machine Learning (ML), leading to the development of a new sub-field in material science called material-informatics, which aims to uncover the interrelation between known features and material properties. ML techniques can handle and identify relationships in complex and arbitrarily high-dimensional spaces data, which are almost impossible for human reasoning. Unlike DFT, the ML approach uses data from past computations or experiments. In many cases, ML models have shown their superiority over DFT in terms of accuracy and efficiency in predicting various physical and chemical properties of materials. The incorporation of material property data obtained from atomistic simulations is crucial important to make continuous progress in data-driven methods. In this direction, we use DFT with Perdew-Burke-Ernzerhof (PBE), and Heyd–Scuseria–Ernzerhof (HSE) functionals, to introduce a family of mono-layer isostructural semiconducting tellurides M2N2Te8, with M = {Ti, Zr, Hf} and N = {Si, Ge}. These compounds have been identified to possess direct band gaps that are tunable from 1.0 eV to 1.3 eV, which are well suited for photonics and optoelectronics applications. Additionally, in-plane transport behavior is observed, and small electron and hole (0.11-015 Me) masses are identified along the dominant transport direction. High carrier mobility is found in these compounds, which shows great promise for applications in high-speed electronic devices. Detailed analysis of electronic structures reveals the presence of metal center bicapped trigonal prism as the structural building blocks in these compounds; a common feature in most of the group V chalcogenides helps to understand the atomic origins of promising properties of this unique class of 2D telluride materials.Atomistic simulations based on DFT theory played a vital role in the development of data-driven materials discovery process. However, the resource-based constraints have limited the high-throughput discovery process by using DFT. The main motivation of our work towards the application of machine learning in material science is to assist the discovery process using available material property data in various databases. Incorporation of physical principles in a network-based machine learning (ML) architecture is a fundamental step toward the continued development of artificial intelligence for materials science and condensed matter physics. In this work, as inspired by the Pauling's rule, we propose that structure motifs (polyhedral formed by cations and surrounding anions) in inorganic crystals can serve as a central input to a machine learning framework for crystalline inorganic materials. We demonstrated that an unsupervised learning algorithm Motif2Vec is able to convert the presence of structural motifs and their connections in a large set of crystalline compounds into unique vectors. The connections among complex materials can be largely determined by the presence of different structural motifs, and their clustering information is identified by our Motif2Vec algorithm. To demonstrate the novel use of structure motif information, we show that a motif-centric learning framework can be effectively created by combining motif information with the recently developed atom-based graph neural networks to form an atom-motif hybrid graph network (AMDNet). Taking advantage of node and edge information on both atomic and motif level, the AMDNet is more accurate than a single graph network in predicting electronic structure related material properties such as band gaps. The work illustrates the route toward the fundamental design of graph neural network learning architecture for complex materials properties by incorporating beyond-atom physical principles. Due to the limitations in resources, it is not feasible to synthesize hundreds of thousands of materials listed in various databases by experiment or compute their detailed properties by using various electronic structure codes and state-of-the-art computational tools. Hence, the identification of an alternative route to screen such databases is very desirable. If identified, this route would be very helpful in reducing the material search space for any application. Categorizing materials based on their structural building blocks is very important to study the underlying physics and to understand the possible mechanisms for any application. Based on structure motifs, we purpose a novel way to categorize, analyze, and visualize the material space called a material network. The connection between any two nodes in this network is determined by using the calculated similarity value (Tanimoto-coeffecient) between each motif and its surrounding information, encoded in terms of a feature vector of length 64. By mapping a known compound, the network thus constructed can be used to screen compounds for the desired application. All the connections of the mapped compound are identified and extracted as a subgraph for further analysis. In our test screening for the transparent conducting oxides (TCO), the proposed network is successful in identifying compounds that are already listed as TCO in the literature. Thus, this indicates its usefulness in reducing the search space for the new TCO materials and various applications. This motif-based material network can serve as an alternate route for functional material discovery and design.},
note = {AAI28028890}
}

@inproceedings{10.1145/3316781.3323470,
author = {Zizzo, Giulio and Hankin, Chris and Maffeis, Sergio and Jones, Kevin},
title = {Adversarial Machine Learning Beyond the Image Domain},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3323470},
doi = {10.1145/3316781.3323470},
abstract = {Machine learning systems have had enormous success in a wide range of fields from computer vision, natural language processing, and anomaly detection. However, such systems are vulnerable to attackers who can cause deliberate misclassification by introducing small perturbations. With machine learning systems being proposed for cyber attack detection such attackers are cause for serious concern. Despite this the vast majority of adversarial machine learning security research is focused on the image domain. This work gives a brief overview of adversarial machine learning and machine learning used in cyber attack detection and suggests key differences between the traditional image domain of adversarial machine learning and the cyber domain. Finally we show an adversarial machine learning attack on an industrial control system.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {176},
numpages = {4},
keywords = {neural networks, intrusion detection, adversarial machine learning},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@article{10.1016/j.cmpb.2021.106180,
author = {De Brouwer, Edward and Becker, Thijs and Moreau, Yves and Havrdova, Eva Kubala and Trojano, Maria and Eichau, Sara and Ozakbas, Serkan and Onofrj, Marco and Grammond, Pierre and Kuhle, Jens and Kappos, Ludwig and Sola, Patrizia and Cartechini, Elisabetta and Lechner-Scott, Jeannette and Alroughani, Raed and Gerlach, Oliver and Kalincik, Tomas and Granella, Franco and Grand'Maison, Francois and Bergamaschi, Roberto and Jos\'{e} S\'{a}, Maria and Van Wijmeersch, Bart and Soysal, Aysun and Sanchez-Menoyo, Jose Luis and Solaro, Claudio and Boz, Cavit and Iuliano, Gerardo and Buzzard, Katherine and Aguera-Morales, Eduardo and Terzi, Murat and Trivio, Tamara Castillo and Spitaleri, Daniele and Van Pesch, Vincent and Shaygannejad, Vahid and Moore, Fraser and Oreja-Guevara, Celia and Maimone, Davide and Gouider, Riadh and Csepany, Tunde and Ramo-Tello, Cristina and Peeters, Liesbet},
title = {Longitudinal machine learning modeling of MS patient trajectories improves predictions of disability progression},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {208},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106180},
doi = {10.1016/j.cmpb.2021.106180},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {14},
keywords = {Real-world data, Disability progression, Electronic health records, Recurrent neural networks, Longitudinal data, Machine learning, Multiple sclerosis}
}

@inproceedings{10.1145/3430984.3430999,
author = {Moghe, Ritwik Prashant and Rathee, Sunil and Nayak, Bharath and Adusumilli, Kranthi Mitra},
title = {Machine Learning based Batching Prediction System for Food&nbsp;Delivery},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3430999},
doi = {10.1145/3430984.3430999},
abstract = {Delivery time estimates are an important factor for online food delivery platforms. These platforms also depend on batching - delivering two orders together - to increase efficiency and reduce cost. In this paper we propose a novel system for enhanced delivery time estimates for batched orders. The system is based on multiple machine learning algorithms that work together to make the predictions. We observe that the system leads to an increase in the number of times the food is delivered within the estimated delivery times by about 6%.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {316–322},
numpages = {7},
keywords = {Zero Inflated Regression, Machine Learning, Food Delivery, Deep Learning, Customer Experience},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1609/aaai.v33i01.33018827,
author = {Peng, Guozhu and Wang, Shangfei},
title = {Dual semi-supervised learning for facial action unit recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33018827},
doi = {10.1609/aaai.v33i01.33018827},
abstract = {Current works on facial action unit (AU) recognition typically require fully AU-labeled training samples. To reduce the reliance on time-consuming manual AU annotations, we propose a novel semi-supervised AU recognition method leveraging two kinds of readily available auxiliary information. The method leverages the dependencies between AUs and expressions as well as the dependencies among AUs, which are caused by facial anatomy and therefore embedded in all facial images, independent on their AU annotation status. The other auxiliary information is facial image synthesis given AUs, the dual task of AU recognition from facial images, and therefore has intrinsic probabilistic connections with AU recognition, regardless of AU annotations. Specifically, we propose a dual semi-supervised generative adversarial network for AU recognition from partially AU-labeled and fully expression-labeled facial images. The proposed network consists of an AU classifier C, an image generator G, and a discriminator D. In addition to minimize the supervised losses of the AU classifier and the face generator for labeled training data, we explore the probabilistic duality between the tasks using adversary learning to force the convergence of the face-AU-expression tuples generated from the AU classifier and the face generator, and the ground-truth distribution in labeled data for all training data. This joint distribution also includes the inherent AU dependencies. Furthermore, we reconstruct the facial image using the output of the AU classifier as the input of the face generator, and create AU labels by feeding the output of the face generator to the AU classifier. We minimize reconstruction losses for all training data, thus exploiting the informative feedback provided by the dual tasks. Within-database and cross-database experiments on three benchmark databases demonstrate the superiority of our method in both AU recognition and face synthesis compared to state-of-the-art works.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1083},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s00778-021-00664-7,
author = {Guo, Yunyan and Zhang, Zhipeng and Jiang, Jiawei and Wu, Wentao and Zhang, Ce and Cui, Bin and Li, Jianzhong},
title = {Model averaging in distributed machine learning: a case study with Apache Spark},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00664-7},
doi = {10.1007/s00778-021-00664-7},
abstract = {The increasing popularity of Apache Spark has attracted many users to put their data into its ecosystem. On the other hand, it has been witnessed in the literature that Spark is slow when it comes to distributed machine learning (ML). One resort is to switch to specialized systems such as parameter servers, which are claimed to have better performance. Nonetheless, users have to undergo the painful procedure of moving data into and out of Spark. In this paper, we investigate performance bottlenecks of MLlib (an official Spark package for ML) in detail, by focusing on analyzing its implementation of stochastic gradient descent (SGD)—the workhorse under the training of many ML models. We show that the performance inferiority of Spark is caused by implementation issues rather than fundamental flaws of the bulk synchronous parallel (BSP) model that governs Spark’s execution: we can significantly improve Spark’s performance by leveraging the well-known “model averaging” (MA) technique in distributed ML. Indeed, model averaging is not limited to SGD, and we further showcase an application of MA to training latent Dirichlet allocation (LDA) models within Spark. Our implementation is not intrusive and requires light development effort. Experimental evaluation results reveal that the MA-based versions of SGD and LDA can be orders of magnitude faster compared to their counterparts without using MA.},
journal = {The VLDB Journal},
month = apr,
pages = {693–712},
numpages = {20},
keywords = {Latent Dirichlet allocation, Generalized linear models, Apache Spark MLlib, Distributed machine learning}
}

@inproceedings{10.1145/3359986.3361204,
author = {Allen, Nathan and Raje, Yash and Ro, Jin Woo and Roop, Partha},
title = {A compositional approach for real-time machine learning},
year = {2019},
isbn = {9781450369978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359986.3361204},
doi = {10.1145/3359986.3361204},
abstract = {Cyber-Physical Systems are highly safety critical, especially since they have to provide both functional and timing guarantees. Increasingly, Cyber-Physical Systems such as autonomous vehicles are relying on Artificial Neural Networks in their decision making and this has obvious safety implications. While many formal approaches have been recently developed for ensuring functional correctness of machine learning modules involving Artificial Neural Networks, the issue of timing correctness has received scant attention.This paper proposes a new compiler from the well known Keras Neural Network library to hardware to mitigate the above problem. In the developed approach, we compile networks of Artificial Neural Networks, called Meta Neural Networks, to hardware implementations using a new synchronous semantics for their execution. The developed semantics enables compilation of Meta Neural Networks to a parallel hardware implementation involving limited hardware resources. The developed compiler is semantics driven and guarantees that the generated implementation is deterministic and time predictable. The compiler also provides a better alternative for the realisation of non-linear functions in hardware. Overall, we show that the developed approach is significantly more efficient than a software approach, without the burden of complex algorithms needed for software Worst Case Execution Time analysis.},
booktitle = {Proceedings of the 17th ACM-IEEE International Conference on Formal Methods and Models for System Design},
articleno = {7},
numpages = {5},
keywords = {hardware, machine learning, neural networks, semantics},
location = {La Jolla, California},
series = {MEMOCODE '19}
}

@inproceedings{10.1145/3461702.3462554,
author = {Segal, Shahar and Adi, Yossi and Pinkas, Benny and Baum, Carsten and Ganesh, Chaya and Keshet, Joseph},
title = {Fairness in the Eyes of the Data: Certifying Machine-Learning Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462554},
doi = {10.1145/3461702.3462554},
abstract = {We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {926–935},
numpages = {10},
keywords = {privacy, machine-learning, fairness, cryptography},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1016/j.knosys.2020.105479,
author = {Chen, Yiyang and Zhou, Yingwei},
title = {Machine learning based decision making for time varying systems: Parameter estimation and performance optimization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.105479},
doi = {10.1016/j.knosys.2020.105479},
journal = {Know.-Based Syst.},
month = feb,
numpages = {9},
keywords = {Time varying system, Model predictive control, Machine learning}
}

@inproceedings{10.1007/978-3-030-58520-4_42,
author = {Devaranjan, Jeevan and Kar, Amlan and Fidler, Sanja},
title = {Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation},
year = {2020},
isbn = {978-3-030-58519-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58520-4_42},
doi = {10.1007/978-3-030-58520-4_42},
abstract = {Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII},
pages = {715–733},
numpages = {19},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3540261.3540878,
author = {Alabdulmohsin, Ibrahim and Lucic, Mario},
title = {A near-optimal algorithm for debiasing trained machine learning models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {617},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.compbiomed.2021.104794,
author = {Liu, Minliang and Liang, Liang and Ismail, Yasmeen and Dong, Hai and Lou, Xiaoying and Iannucci, Glen and Chen, Edward P. and Leshnower, Bradley G. and Elefteriades, John A. and Sun, Wei},
title = {Computation of a probabilistic and anisotropic failure metric on the aortic wall using a machine learning-based surrogate model},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104794},
doi = {10.1016/j.compbiomed.2021.104794},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {11},
keywords = {Neural network, Machine learning, Aortic aneurysm, Surrogate model, Failure metric}
}

@inproceedings{10.1007/978-3-030-94437-7_7,
author = {Ihde, Nina and Marten, Paula and Eleliemy, Ahmed and Poerwawinata, Gabrielle and Silva, Pedro and Tolovski, Ilin and Ciorba, Florina M. and Rabl, Tilmann},
title = {A Survey of&nbsp;Big Data, High Performance Computing, and&nbsp;Machine Learning Benchmarks},
year = {2021},
isbn = {978-3-030-94436-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-94437-7_7},
doi = {10.1007/978-3-030-94437-7_7},
abstract = {In recent years, there has been a convergence of Big Data (BD), High Performance Computing (HPC), and Machine Learning (ML) systems. This convergence is due to the increasing complexity of long data analysis pipelines on separated software stacks. With the increasing complexity of data analytics pipelines comes a need to evaluate their systems, in order to make informed decisions about technology selection, sizing and scoping of hardware. While there are many benchmarks for each of these domains, there is no convergence of these efforts. As a first step, it is also necessary to understand how the individual benchmark domains relate.In this work, we analyze some of the most expressive and recent benchmarks of BD, HPC, and ML systems. We propose a taxonomy of those systems based on individual dimensions such as accuracy metrics and common dimensions such as workload type. Moreover, we aim at enabling the usage of our taxonomy in identifying adapted benchmarks for their BD, HPC, and ML systems. Finally, we identify challenges and research directions related to the future of converged BD, HPC, and ML system benchmarking.},
booktitle = {Performance Evaluation and Benchmarking: 13th TPC Technology Conference, TPCTC 2021, Copenhagen, Denmark, August 20, 2021, Revised Selected Papers},
pages = {98–118},
numpages = {21},
keywords = {Machine Learning, HPC, Big Data, Benchmarking},
location = {Copenhagen, Denmark}
}

@article{10.1007/s10664-021-09955-7,
author = {Daoudi, Nadia and Allix, Kevin and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
title = {Lessons Learnt on Reproducibility in Machine Learning Based Android Malware Detection},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09955-7},
doi = {10.1007/s10664-021-09955-7},
abstract = {A well-known curse of computer security research is that it often produces systems that, while technically sound, fail operationally. To overcome this curse, the community generally seeks to assess proposed systems under a variety of settings in order to make explicit every potential bias. In this respect, recently, research achievements on machine learning based malware detection are being considered for thorough evaluation by the community. Such an effort of comprehensive evaluation supposes first and foremost the possibility to perform an independent reproduction study in order to sharpen evaluations presented by approaches’ authors. The question Can published approaches actually be reproduced? thus becomes paramount despite the little interest such mundane and practical aspects seem to attract in the malware detection field. In this paper, we attempt a complete reproduction of five Android Malware Detectors from the literature and discuss to what extent they are “reproducible”. Notably, we provide insights on the implications around the guesswork that may be required to finalise a working implementation. Finally, we discuss how barriers to reproduction could be lifted, and how the malware detection field would benefit from stronger reproducibility standards—like many various fields already have.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {53},
keywords = {Replicability, Reproducibility, Machine learning, Android malware dection}
}

@inproceedings{10.1007/978-3-030-27192-3_17,
author = {Audah, M. Z. Fatimah and Chin, Tan Saw and Zulfadzli, Y. and Lee, C. K. and Rizaluddin, K.},
title = {Towards Efficient and Scalable Machine Learning-Based QoS Traffic Classification in Software-Defined Network},
year = {2019},
isbn = {978-3-030-27191-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27192-3_17},
doi = {10.1007/978-3-030-27192-3_17},
abstract = {Internet Service Provider (ISP) has the responsibility to fulfill the Quality of Service (QoS) of various types of applications. The centralized network controller in Software Defined Networking (SDN) provides the chance to instil intelligence in managing network resources based on QoS requirements. A fined-grained QoS Traffic Engineering can be realized by identifying different traffic flow types and categorizing them according to various application/classes. Previous methods include port-based classification and Deep Packet Inspection (DPI), which have been found non-accurate and highly computational. Thus, machine learning (ML) based traffic classifier has gained much attention from the research community, which can be seen from an increase number of works being published. This paper identifies the issues in ML-based traffic classification (TC) in order to devised the best solution; i.e. the TC framework should be scalable to accommodate network expansion, can accurately identify flows according to their source applications/classes, while maintaining an efficient run-time and memory requirement. Therefore, based on these findings, this work proposed a TC engine comprises of Training and Feature Selection Module and Classifier Model, which is placed at the data plane. The training and feature selection will be done offline and regularly to keep the Classifier Model updated. In the proposed solution, the SDN switch forwards the packets the Classifier Model, which classify the packets with accurate applications and send them to the control plane. Finally, the controller will perform resource and queue management according to the labeled packets and updates the flow tables via the switch. The proposed solution will be the starting point in solving efficiency and scalability issues in SDN-ISP TC.},
booktitle = {Mobile Web and Intelligent Information Systems: 16th International Conference, MobiWIS 2019, Istanbul, Turkey, August 26–28, 2019, Proceedings},
pages = {217–229},
numpages = {13},
keywords = {Machine Learning, Software Defined Networking, Traffic Classification},
location = {Istanbul, Turkey}
}

@article{10.1111/coin.12018,
author = {Dee Miller, L. and Soh, Leen-Kiat and Scott, Stephen},
title = {Genetic Algorithm Classifier System for Semi-Supervised Learning},
year = {2015},
issue_date = {May 2015},
publisher = {Blackwell Publishers, Inc.},
address = {USA},
volume = {31},
number = {2},
issn = {0824-7935},
url = {https://doi.org/10.1111/coin.12018},
doi = {10.1111/coin.12018},
abstract = {Real-world datasets often contain large numbers of unlabeled data points, because there is additional cost for obtaining the labels. Semi-supervised learning SSL algorithms use both labeled and unlabeled data points for training that can result in higher classification accuracy on these datasets. Generally, traditional SSLs tentatively label the unlabeled data points on the basis of the smoothness assumption that neighboring points should have the same label. When this assumption is violated, unlabeled points are mislabeled injecting noise into the final classifier. An alternative SSL approach is cluster-then-label CTL, which partitions all the data points labeled and unlabeled into clusters and creates a classifier by using those clusters. CTL is based on the less restrictive cluster assumption that data points in the same cluster should have the same label. As shown, this allows CTLs to achieve higher classification accuracy on many datasets where the cluster assumption holds for the CTLs, but smoothness does not hold for the traditional SSLs. However, cluster configuration problems e.g., irrelevant features, insufficient clusters, and incorrectly shaped clusters could violate the cluster assumption. We propose a new framework for CTLs by using a genetic algorithm GA to evolve classifiers without the cluster configuration problems e.g., the GA removes irrelevant attributes, updates number of clusters, and changes the shape of the clusters. We demonstrate that a CTL based on this framework achieves comparable or higher accuracy with both traditional SSLs and CTLs on 12 University of California, Irvine machine learning datasets.},
journal = {Comput. Intell.},
month = may,
pages = {201–232},
numpages = {32},
keywords = {unsupervised clustering, semi-supervised learning, genetic algorithm, cluster-then-label}
}

@article{10.1007/s00542-020-04888-5,
author = {Kumari, Rashmi and Nigam, Akriti and Pushkar, Shashank},
title = {Machine learning technique for early detection of Alzheimer’s disease},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {12},
issn = {0946-7076},
url = {https://doi.org/10.1007/s00542-020-04888-5},
doi = {10.1007/s00542-020-04888-5},
abstract = {Alzheimer’s disease (AD) is non-repairable brain disorder which impacts a person’s thinking along with shrinking the size of the brain, ultimately resulting in the death of the patient. It is necessary for the treatment of initial stages in AD so that the further degeneration could be delayed. This diagnosis can be achieved with the application of machine learning techniques which employ various optimization and probabilistic techniques. Hence with an objective of distinguishing people with normal brain ageing from those who would develop Alzheimer’s disease, this paper presents an effective machine learning model that successfully diagnosed AD, cMCI, ncMCI and CN which are being detected during pre-stages by itself.},
journal = {Microsyst. Technol.},
month = dec,
pages = {3935–3944},
numpages = {10}
}

@article{10.1504/ijcse.2021.115101,
author = {Muraleedharan, N. and Janet, B.},
title = {Flow-based machine learning approach for slow HTTP distributed denial of service attack classification},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {2},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.115101},
doi = {10.1504/ijcse.2021.115101},
abstract = {Distributed denial of service (DDoS) attack is one of the common threats to the availability of services on the internet. The DDoS attacks are evolved from volumetric attack to slow DDoS. Unlike the volumetric DDoS attack, the slow DDoS traffic rate looks similar to the normal traffic. Hence, it is difficult to detect using traditional security mechanism. In this paper, we propose a flow-based classification model for slow HTTP DDoS traffic. The important flow level features were selected using CICIDS2017 dataset. Impacts of time, packet length and transmission rate for slow DDoS are analysed. Using the selected features, three classification models were trained and evaluated using two benchmark datasets. The results obtained reveal the proposed classifiers can achieve higher accuracy of 0.997 using RF classifiers. A comparison of the results obtained with state-of-the-art approaches shows that the proposed approach can improve the detection rate by 19.7%.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {147–161},
numpages = {14},
keywords = {slow read, slowloris, slow HTTP DDoS, network flow, machine learning, application layer DoS, slow DDoS, denial of service}
}

@article{10.1016/j.compag.2021.106476,
author = {Marin, Diego Bedin and Ferraz, Gabriel Ara\'{u}jo e Silva and Santana, Lucas Santos and Barbosa, Brenon Diennevan Souza and Barata, Rafael Alexandre Pena and Osco, Lucas Prado and Ramos, Ana Paula Marques and Guimar\~{a}es, Paulo Henrique Sales},
title = {Detecting coffee leaf rust with UAV-based vegetation indices and decision tree machine learning models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106476},
doi = {10.1016/j.compag.2021.106476},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {10},
keywords = {Logistic model tree, Plant disease, Precision agriculture, Multispectral imagery}
}

@inproceedings{10.1007/978-3-030-90525-5_36,
author = {Nong, Jinjin and Zhou, Zikang and Xian, Xiaoming and Huang, Guowei and Li, Peiwen and Xie, Longhan},
title = {Using Plantar Pressure and Machine Learning to Automatically Evaluate Strephenopodia for Rehabilitation Exoskeleton: A Pilot Study},
year = {2021},
isbn = {978-3-030-90524-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90525-5_36},
doi = {10.1007/978-3-030-90525-5_36},
abstract = {Stroke patients often suffer from strephenopodia, which seriously affects their walking ability and rehabilitation. However, lower limb rehabilitation robots lack the evaluation and automatic correction function of strephenopodia. There are practical demands for convenient, automatic, and quantitative assessments of the angle of strephenopodia to adjust the orthopedic strength in time to remind stroke patients to use their muscles to realize the movements. In this study, we proposed a novel methodology for automatically predicting the angles of strephenopodia based on a plantar pressure system using machine learning methods. Three machine learning methods were implemented to build stochastic function mapping from gait features to strephenopodia angles, showing good reliability and precision prediction of the strephenopodia angle [determination coefficient (R2)&nbsp;≥&nbsp;0.80]. Results showed that our method is convenient to implement and outperforms previous methods in accuracy. Therefore, measurements derived from the plantar pressure system are proper estimators of the strephenopodia angle and are beneficial to lower limb rehabilitation exoskeleton for stroke population training.},
booktitle = {Social Robotics: 13th International Conference, ICSR 2021, Singapore, Singapore,  November 10–13, 2021, Proceedings},
pages = {421–431},
numpages = {11},
keywords = {Machine learning, Plantar pressure, Strephenopodia, Stroke},
location = {Singapore, Singapore}
}

@article{10.1007/s11277-021-08283-9,
author = {Mishra, Kamta Nath and Pandey, Subhash Chandra},
title = {Fraud Prediction in Smart Societies Using Logistic Regression and k-fold Machine Learning Techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {119},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08283-9},
doi = {10.1007/s11277-021-08283-9},
abstract = {The credit/debit card deceit detection is an enormously difficult task. However, it is a well known problem of our cloud based mobile internet society and it must be solved by technocrats in the welfare of societal mental harassments. The main problem in executing credit/debit card fraud detection technique is the availability of limited amount of fraud related data like transaction amount, transaction date, transaction time, address, and vendor category code related to the frauds. It is the truth of mobile internet world that there are billions of potential places and e-commerce websites where a credit/debit card can be used by fraudulent people for online transactions and payments which make it exceedingly thorny to trace the pattern of frauds. Moreover, the problem of fraud detection in cloud— Internet of Things (IoT) based smart societies has numerous constraints like continuous change in the behavior of normal and fraudulent persons, the fraudulent people try to develop and use new method for executing frauds, and very little availability of frauds related bench mark data sets. In this research article, the authors have presented logistic regression based k-fold machine learning technique (MLT) for fraud detection and prevention in cloud-IoT based smart societal environment. The k-fold method creates multiple folds of bank transactions related data before implementing logistic regression and MLT. The logistic regression performs logic based regression analysis and the intelligent machine learning approach performs registration, classification, clustering, dimensionality reduction, deep learning, training, and reinforcement learning steps on the received bank transactions data. The implementation of proposed methodology and its further analysis using intelligent machine learning tools like ROC (Receiver Operating Characteristic) curve, confusion matrix, mean-recall score value, and precision recall curves for European banks day-to-day transactions related bench mark data set reveal that the proposed methodology is efficient, accurate, and reliable for detecting frauds in cloud-IoT based smart societal environment.},
journal = {Wirel. Pers. Commun.},
month = jul,
pages = {1341–1367},
numpages = {27},
keywords = {ROC curve, Mean-recall-score, Machine learning, Logistic regression, Fraudulent, Fraud detection, Confusion matrix, Cloud-IoT based distributed environment}
}

@article{10.1155/2021/6511552,
author = {Chen, Qiaoshan and Cai, Shousong and Gu, Xiaomin and Gupta, Punit},
title = {Construction of the Luxury Marketing Model Based on Machine Learning Classification Algorithm},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6511552},
doi = {10.1155/2021/6511552},
abstract = {China has become the world’s largest luxury goods consumer market due to its population base. In view of the bright prospects of the luxury consumer market, major companies have entered and want to get a share. For the luxury goods industry, traditional mass marketing methods are not able to serve corporate sales and marketing strategies more effectively, and targeted marketing is clearly much more efficient than randomized marketing. Therefore, in this paper, based on consumer buying habits and characteristics data of luxury goods, the paper uses a machine learning algorithm to build a personalized marketing strategy model. And the paper uses historical data to model and form deductions to predict the purchase demand of each consumer and evaluate the possibility of customers buying different goods, including cosmetics, jewelry, and clothing.},
journal = {Sci. Program.},
month = jan,
numpages = {11}
}

@article{10.1007/s00371-021-02287-z,
author = {Tian, Ye and Zhang, Liguo and Sun, Jianguo and Yin, Guisheng and Dong, Yuxin},
title = {Consistency regularization teacher–student semi-supervised learning method for target recognition in SAR images},
year = {2021},
issue_date = {Dec 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {12},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-021-02287-z},
doi = {10.1007/s00371-021-02287-z},
abstract = {Synthetic aperture radar automatic target recognition (SAR-ATR) is a hotspot in the field of remote sensing, which has been widely used in disaster monitoring, environmental monitoring, resource exploration, and crop yield estimates. In recent years, deep convolutional neural networks (DCNNs) have achieved promising performance among a variety of supervised classification methods under the condition of sufficiently labeled samples. However, it is expensive and time-consuming to collect large amounts of labeled samples suitable for DCNNs in SAR domains. To reduce the dependence of SAR-ATR on labeled samples, in this work, a consistency regularization teacher–student semi-supervised (CRTS) method for SAR-ATR is proposed, in which consistency regularization is applied to analyze and divide unlabeled samples and the teacher–student structure is introduced to generate pseudo-labels for the divided unlabeled samples. Firstly, by using consistent pseudo-label prediction, unlabeled samples are divided into consistent unlabeled samples and confident unlabeled samples. Then, in order to improve the quality of pseudo-labeled labels, the student model is used to generate a pseudo-label for consistent unlabeled samples, and the teacher model which is ensembled by the other two student models labels the confident unlabeled samples. Finally, these pseudo-label unlabeled samples are mixed with the labeled samples and trained together to improve recognition performance. Experiments are conducted on the MSTAR dataset, and the results demonstrate the effectiveness of the proposed method. As compared with several state-of-the-art methods, the recognition accuracy shows the superiority of the proposed method, especially when the training dataset is limited.},
journal = {Vis. Comput.},
month = aug,
pages = {4179–4192},
numpages = {14},
keywords = {Teacher–student structure, Consistency regularization, Semi-supervised learning, SAR automatic target recognition}
}

@inproceedings{10.1007/978-3-030-53552-0_15,
author = {Lucas, Flavien and Billot, Romain and Sevaux, Marc and S\"{o}rensen, Kenneth},
title = {Reducing Space Search in Combinatorial Optimization Using Machine Learning Tools},
year = {2020},
isbn = {978-3-030-53551-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-53552-0_15},
doi = {10.1007/978-3-030-53552-0_15},
abstract = {A new metaheuristic, called Feature-Guided MNS (FG-MNS) is proposed, combining well-known local search with simple machine learning techniques. In this metaheuristic, a solution is represented by features (mean depth of each route, standard deviation of the length of each route, etc.). The solver uses decision trees to define promising areas in the features space. The search is mainly focused on the promising areas, in order to minimize the exploration time, and to improve the quality of the found solutions. Additional neighborhoods, guided by the features are proposed.},
booktitle = {Learning and Intelligent Optimization: 14th International Conference, LION 14, Athens, Greece, May 24–28, 2020, Revised Selected Papers},
pages = {143–150},
numpages = {8},
keywords = {Vehicle routing, Data mining, Machine learning, Metaheuristic},
location = {Athens, Greece}
}

@article{10.1007/s11554-020-01063-x,
author = {Grellert, Mateus and da Silva Cruz, Luis A. and Zatt, Bruno and Bampi, Sergio},
title = {Coding mode decision algorithm for fast HEVC transrating using heuristics and machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {6},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-020-01063-x},
doi = {10.1007/s11554-020-01063-x},
abstract = {This article describes a framework to speed up the HEVC encoding decisions for on-demand transrating of bitstreams. The methods proposed collect information from a high-quality reference bitstream which after processing is used to limit the number of modes evaluated in subsequent re-encodings at different bitrates. In this way, the time required to process re-encode-time computing-intensive decisions, such as partitioning and motion estimation is significantly reduced. The methods proposed are a combination of heuristics with a statistical basis and fast decision techniques trained using automatic learning methodologies. Experimental results using the HEVC reference encoder show that jointly the methods proposed reduce the transcoding computational complexity by up to 78.8%, with Bjontegaard bitrate deltas penalties smaller than 1.06%. A comparison with related works showed that the proposed method is able to outperform state-of-the-art solutions in terms of combined rate-distortion–complexity performance indicators.},
journal = {J. Real-Time Image Process.},
month = dec,
pages = {1881–1896},
numpages = {16},
keywords = {Random forests, Video coding, Machine learning, HEVC, Transrating}
}

@inproceedings{10.5555/3045390.3045589,
author = {Bai, Qinxun and Rosenberg, Steven and Wu, Zheng and Sclaroff, Stan},
title = {Differential geometric regularization for supervised learning of classifiers},
year = {2016},
publisher = {JMLR.org},
abstract = {We study the problem of supervised learning for both binary and multiclass classification from a unified geometric perspective. In particular, we propose a geometric regularization technique to find the submanifold corresponding to an estimator of the class probability P(y|x). The regularization term measures the volume of this sub-manifold, based on the intuition that overfitting produces rapid local oscillations and hence large volume of the estimator. This technique can be applied to regularize any classification function that satisfies two requirements: firstly, an estimator of the class probability can be obtained; secondly, first and second derivatives of the class probability estimator can be calculated. In experiments, we apply our regularization technique to standard loss functions for classification, our RBF-based implementation compares favorably to widely used regularization methods for both binary and multiclass classification.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1879–1888},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@phdthesis{10.5555/AAI28714415,
author = {Li, Yanying and TH, Han, Tian and RL, Liu, Rong and YN, Ning, Yue},
advisor = {HW, Wang, Hui},
title = {Fair Machine Learning over Crowdsourced and Graph-Structured Data},
year = {2021},
isbn = {9798544292791},
publisher = {Stevens Institute of Technology},
address = {USA},
abstract = {Fair machine learning has gained considerable interests in recent years, and has been widely adapted in every aspect of life. A multitude of formal, mathematical definitions of  fairness in machine learning has been proposed in the last few years. Recently many fairness-enhancing machine learning methods have been designed to deal with various applications and scenarios. However, the research of fairness in learning over crowdsourced data and graph-structured data is largely limited. The goal of my thesis is to study algorithmic fairness in the contexts of crowd-powered data analytics and graph analytics. We have made the following research achievements. First, we focus on fair truth discovery for crowdsourcing systems. One important research problem for crowdsourcing systems is truth discovery, which aims to aggregate noisy answers contributed by the workers to obtain the correct answer (truth) of each task. However, since the collected answers are highly prone to the workers' bias, aggregating these biased answers without proper treatment will unavoidably lead to discriminatory truth discovery results. In this thesis, we address the fairness issue for truth discovery from biased crowdsourced answers. We design new algorithms that incorporate truth discovery with fairness. The algorithms estimate both worker bias and truth iteratively, and dynamically select bias to be removed from the answers during truth inference. We perform an extensive set of experiments on both synthetic and real-world crowdsourcing datasets. Experimental results demonstrate the effectiveness of our mitigation algorithms.Second, we focus on the fair classification problem under the circumstance of using applicants' social information in peer-to-peer lending. Peer-to-peer (P2P) lending marketplaces on the Web have been growing over the last decade. Since the applicants on P2P lending platforms may lack sufficient financial history for assessment, quite a few P2P lending service providers have been utilizing the applicants' social relationships to improve the risk prediction accuracy of loan applications. However, utilizing the information of applicants' social relationships may introduce discrimination in lending decisions. In this thesis, we analyze and evaluate the impact of users'  social relationships on the fairness of risk prediction for P2P lending. We consider two types of fairness notions in the literature, namely individual fairness and counterfactual fairness, and design two new algorithms that mitigate bias by generalizing social features for both fairness metrics. Experimental results show that our mitigation algorithms can reduce bias while utilizing social relationships effectively. Third, we focus on fair link prediction on social networks. Link prediction has been widely applied in social network analysis. Despite its importance, link prediction algorithms can be biased by disfavoring the links between individuals in particular demographic groups. We study one particular type of bias, namely, the bias in predicting inter-group links (i.e., the links across different demographic groups), on social network graphs. First, we formalize the definition of bias in link prediction by providing quantitative measurements of accuracy disparity, which measures the difference in prediction accuracy of inter-group and intra-group links. Second, by extensive empirical studies over real-world datasets, we unveil the existence of bias in  five existing state-of-the-art link prediction algorithms. Third, we identify one of the underlying causes of bias in link prediction as the imbalanced density across intra-group and inter-group edges in the training graph. Based on the identified cause, we design a pre-processing bias mitigation method named FairLP that modifies the training graph to balance the distribution of intra-group and inter-group links. FairLP is model-agnostic and thus is compatible with any existing link prediction algorithm. Our experimental results on real-world social network graph data demonstrate that FairLP achieves better trade-off between fairness and prediction accuracy compared with the existing fairness-enhancing link prediction methods.  Fourth, we focus on fair and privacy-preserving heterogeneous graph embedding. Due to the popularity of heterogeneous graphs (HGs) in real-world scenarios, HG embeddings have drawn considerable attentions in recent years. Despite the significant efforts on developing embedding models, two important issues are largely ignored. First, as machine learning algorithms may be systematically discriminated against particular group of individuals, it is important to ensure that HG embeddings are bias-free for downstream tasks. Second, as HG embeddings are typically shared among multiple parties instead of the original graphs, it is vital to ensure that these embeddings are privacy-preserving. Both fairness and privacy are equally important. In this thesis, we address the trade-off between fairness, privacy, and utility of HG embeddings. Regarding fairness, we consider compositional fairness that specifies multiple fairness constraints on different node types in HGs. Regarding privacy, we consider differential privacy, the privacy standard for data analysis. Meeting both requirements of compositional fairness and differential privacy may lead to significant utility loss of embeddings. To address this challenge, we design the Compositionally Fair, Privacy-preserving Graph Embedding (CoPE) algorithm which consists of two components: (1) the one-discriminator encoder (ODE) that realizes compositional fairness; and (2) the privacy enhancer that equips ODE with differential privacy. Our experiment results on two real-world HG datasets show that the utility of the embeddings learned by systemc are much better than the existing work, especially when strong privacy is required and the number of fairness constraints is larger than two. Besides the issue of algorithmic fairness in machine learning, we consider data acquisition on online data marketplaces. We design DANCE, a middleware that provides the desired data acquisition service. DANCE searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.},
note = {AAI28714415}
}

@phdthesis{10.5555/AAI28027596,
author = {Shah Mohammadi, Fatemeh and Ptucha, Raymond and Maywar, Drew and Markopoulos, Panos and Cahill, Nathan and Hensel, Edward},
advisor = {Andres, Kwasinski,},
title = {Machine Learning-enabled Resource Allocation for Underlay Cognitive Radio Networks},
year = {2020},
isbn = {9798664705553},
publisher = {Rochester Institute of Technology},
abstract = {Due to the rapid growth of new wireless communication services and applications, much attention has been directed to frequency spectrum resources and the way they are regulated. Considering that the radio spectrum is a natural limited resource, supporting the ever increasing demands for higher capacity and higher data rates for diverse sets of users, services and applications is a challenging task which requires innovative technologies capable of providing new ways of efficiently exploiting the available radio spectrum. Consequently, dynamic spectrum access (DSA) has been proposed as a replacement for static spectrum allocation policies. The DSA is implemented in three modes including interweave, overlay and underlay mode [1]. The key enabling technology for DSA is cognitive radio (CR), which is among the core prominent technologies for the next generation of wireless communication systems. Unlike conventional radio which is restricted to only operate in designated spectrum bands, a CR has the capability to operate in different spectrum bands owing to its ability in sensing, understanding its wireless environment, learning from past experiences and proactively changing the transmission parameters as needed. These features for CR are provided by an intelligent software package called the cognitive engine (CE). In general, the CE manages radio resources to accomplish cognitive functionalities and allocates and adapts the radio resources to optimize the performance of the network. Cognitive functionality of the CE can be achieved by leveraging machine learning techniques. Therefore, this thesis explores the application of two machine learning techniques in enabling the cognition capability of CE. The two considered machine learning techniques are neural network-based supervised learning and reinforcement learning. Specifically, this thesis develops resource allocation algorithms that leverage the use of machine learning techniques to find the solution to the resource allocation problem for heterogeneous underlay cognitive radio networks (CRNs). The proposed algorithms are evaluated under extensive simulation runs.The first resource allocation algorithm uses a neural network-based learning paradigm to present a fully autonomous and distributed underlay DSA scheme where each CR operates based on predicting its transmission effect on a primary network (PN). The scheme is based on a CE with an artificial neural network that predicts the adaptive modulation and coding configuration for the primary link nearest to a transmitting CR, without exchanging information between primary and secondary networks. By managing the effect of the secondary network (SN) on the primary network, the presented technique maintains the relative average throughput change in the primary network within a prescribed maximum value, while also finding transmit settings for the CRs that result in throughput as large as allowed by the primary network interference limit. The second resource allocation algorithm uses reinforcement learning and aims at distributively maximizing the average quality of experience (QoE) across transmission of CRs with different types of traffic while satisfying a primary network interference constraint. To best satisfy the QoE requirements of the delay-sensitive type of traffics, a cross-layer resource allocation algorithm is derived and its performance is compared against a physical-layer algorithm in terms of meeting end-to-end traffic delay constraints. Moreover, to accelerate the learning performance of the presented algorithms, the idea of transfer learning is integrated. The philosophy behind transfer learning is to allow well-established and expert cognitive agents (i.e. base stations or mobile stations in the context of wireless communications) to teach newly activated and naive agents. Exchange of learned information is used to improve the learning performance of a distributed CR network. This thesis further identifies the best practices to transfer knowledge between CRs so as to reduce the communication overhead.    The investigations in this thesis propose a novel technique which is able to accurately predict the modulation scheme and channel coding rate used in a primary link without the need to exchange information between the two networks (e.g. access to feedback channels), while succeeding in the main goal of determining the transmit power of the CRs such that the interference they create remains below the maximum threshold that the primary network can sustain with minimal effect on the average throughput. The investigations in this thesis also provide a physical-layer as well as a cross-layer machine learning-based algorithms to address the challenge of resource allocation in underlay cognitive radio networks, resulting in better learning performance and reduced communication overhead.},
note = {AAI28027596}
}

@inproceedings{10.1145/3387940.3391489,
author = {Schumacher, Max Eric Henry and Le, Kim Tuyen and Andrzejak, Artur},
title = {Improving Code Recommendations by Combining Neural and Classical Machine Learning Approaches},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391489},
doi = {10.1145/3387940.3391489},
abstract = {Code recommendation systems for software engineering are designed to accelerate the development of large software projects. A classical example is code completion or next token prediction offered by modern integrated development environments. A particular challenging case for such systems are dynamic languages like Python due to limited type information at editing time. Recently, researchers proposed machine learning approaches to address this challenge. In particular, the Probabilistic Higher Order Grammar technique (Bielik et al., ICML 2016) uses a grammar-based approach with a classical machine learning schema to exploit local context. A method by Li et al., (IJCAI 2018) uses deep learning methods, in detail a Recurrent Neural Network coupled with a Pointer Network. We compare these two approaches quantitatively on a large corpus of Python files from GitHub. We also propose a combination of both approaches, where a neural network decides which schema to use for each prediction. The proposed method achieves a slightly better accuracy than either of the systems alone. This demonstrates the potential of ensemble-like methods for code completion and recommendation tasks in dynamically typed languages.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {476–482},
numpages = {7},
keywords = {neural networks, machine learning, code recommendations},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.5555/3306127.3332143,
author = {Strobel, Martin},
title = {Aspects of Transparency in Machine Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The fact that machine learning is growing more and more entrenched in almost every aspect of society, combined with the opacity of various of its algorithms has induced the relatively young research area of transparent machine learning. The aim of this domain is to provide explanations for automated decisions to increase public trust. In my thesis, I am going to consider certain problems that arise from this research agenda. Particularly, I so far considered, the dilemma of conflicting explanations and the issue of privacy concerns arising from transparency.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2449–2451},
numpages = {3},
keywords = {values in mas (privacy, safety, security, transparency, $dots$, transparent machine learning, social choice theory, game theory for practical applications, cooperative games: theory &amp; analysis},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.3103/S0146411621060043,
author = {Gochhait, S. and Butt, Sh. Aziz and De-La-Hoz-Franco, E. and Shaheen, Q. and Luis, D. M. Jorge and Pi\~{n}eres-Espitia, G. and Mercado-Polo, D.},
title = {A Machine Learning Solution for Bed Occupancy Issue for Smart Healthcare Sector},
year = {2021},
issue_date = {Nov 2021},
publisher = {Allerton Press, Inc.},
address = {USA},
volume = {55},
number = {6},
issn = {0146-4116},
url = {https://doi.org/10.3103/S0146411621060043},
doi = {10.3103/S0146411621060043},
journal = {Autom. Control Comput. Sci.},
month = nov,
pages = {546–556},
numpages = {11},
keywords = {algorithms, bed occupancy rate, machine learning, healthcare}
}

@inproceedings{10.1145/3460268.3460272,
author = {Wu, Jiaming and Liu, Zunhao and Yang, Bowen},
title = {DQ-DPS Data Partition Strategy Based on Distributed Machine Learning},
year = {2021},
isbn = {9781450389273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460268.3460272},
doi = {10.1145/3460268.3460272},
abstract = {With the expansion of the data scale, machine learning develops from centralized to distributed. Generally, distributed machine learning uses parameter server architecture to train in synchronous mode. At this time, data samples are statically and symmetrically allocated to each computing node according to the batch size. Each worker trains synchronously and iterates until the model converges. However, due to the different number of resources at each compute node in a mixed-load scenario, the traditional data partition strategy is usually to statically configure batch size parameters or require manual setting of batch size parameters, which makes the computational efficiency of distributed machine learning model training operations inefficient, and the data adjustment for each node will have an impact on the accuracy of the model. To solve this problem, on the premise of ensuring the accuracy of the distributed machine learning model training task, this paper proposes an optimal configuration scheme for a batch size of distributed machine learning model training task data: a data partition strategy based on distributed machine learning (DQ-DPS). DQ-DPS solves the problem of low computational efficiency caused by static data partitioning, improves the computational efficiency of distributed machine learning tasks, and ensures the accuracy of distributed machine learning training model. Through a large number of experiments, we have proved the effectiveness of DQ-DPS. Compared with the traditional data partition strategy, DQ-DPS improves the computing efficiency of each training round by 38%.},
booktitle = {Proceedings of the 2021 2nd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {20–26},
numpages = {7},
keywords = {Parallel Computing, Model Training, Distributed Machine Learning, Data Partition},
location = {Phuket, Thailand},
series = {AIEE '21}
}

@inproceedings{10.1145/3313831.3376447,
author = {Yan, Jing Nathan and Gu, Ziwei and Lin, Hubert and Rzeszotarski, Jeffrey M.},
title = {Silva: Interactively Assessing Machine Learning Fairness Using Causality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376447},
doi = {10.1145/3313831.3376447},
abstract = {Machine learning models risk encoding unfairness on the part of their developers or data sources. However, assessing fairness is challenging as analysts might misidentify sources of bias, fail to notice them, or misapply metrics. In this paper we introduce Silva, a system for exploring potential sources of unfairness in datasets or machine learning models interactively. Silva directs user attention to relationships between attributes through a global causal view, provides interactive recommendations, presents intermediate results, and visualizes metrics. We describe the implementation of Silva, identify salient design and technical challenges, and provide an evaluation of the tool in comparison to an existing fairness optimization tool.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {bias, interactive system, machine learning fairness},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1007/s00521-021-05820-2,
author = {Mastoi, Qurat-ul-ain and Memon, Muhammad Suleman and Lakhan, Abdullah and Mohammed, Mazin Abed and Qabulio, Mumtaz and Al-Turjman, Fadi and Abdulkareem, Karrar Hameed},
title = {Machine learning-data mining integrated approach for premature ventricular contraction prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {18},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05820-2},
doi = {10.1007/s00521-021-05820-2},
abstract = {Cardiac arrhythmias impose a significant burden on the healthcare environment due to the increasing ratio of mortality worldwide. Arrhythmia and abnormal ECG heartbeat are the possible symptoms of severe heart diseases that can lead to death. Premature ventricular contraction (PVC) is a common form of cardiac arrhythmia which begins from the lower chamber of the heart, and frequent occurrence of PVC beat might lead to mortality. ECG signals are the noninvasive and primary tool used to identify the actual life threat related to the heart. Nowadays, in society, the computer-assisted technique reduces doctors' burden to evaluate heart disease and heart arrhythmia automatically. Regardless of well-equipped and well-developed health facilities that are available for monitoring the cardiac condition, the success stories are yet unsatisfactorily due to the complexity of the cardiac disorder. The most challenging part in ECG signal analysis is to extract the accurate features relevant to the arrhythmia for classification due to the inter-patient variation. There are many morphological changes present in the ECG signals. Hence, there is a gap in the usage of appropriate methods for the extraction of features and classification models, which reduce the biased diagnosis of PVC arrhythmia. To predict PVC arrhythmia accurately is a quite challenging task owing to (a) QRS negative (b) long compensatory pause (c) p-wave (d) biased diagnosis of PVC detection due to the small feature set. This study presents a new approach for PVC prediction using derived predictor variables from the electrocardiograph (ECG-MLII) signals: R–R wave interval, previous R–R wave interval, QRS duration, and verification of P-wave whether it is present or absent using threshold technique. We propose the machine learning-data mining MACDM integrated approach using five different models of multiple logistic regression and four classifiers, namely, Random Forest (RF), K-Nearest Neighbor (KNN), Support vector machine (SVM), and Na\"{\i}ve Bayes (NB). The experiment was conducted on the public benchmark MIT-BIH-AR to evaluate the performance of our proposed MACDM technique. The multiple logistic regression models constructed as a function of all independent variables achieved an accuracy of 99.96%, sensitivity 98.9%, specificity 99.20%, PPV 99.25%, and Youden's index parameter 98.24%. Thus, it is proved that this computer-aided method helps our medical practitioners improve the efficiency of their services.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {11703–11719},
numpages = {17},
keywords = {Data mining, Machine learning, Multiple logistic regression, Heart, ECG, PVC prediction, Premature ventricular contraction}
}

@article{10.1145/3466826.3466835,
author = {Eduardo A. Sousa, Jose and Oliveira, Vinicius C. and Almeida Valadares, Julia and Borges Vieira, Alex and Bernardino, Heder S. and Moraes Villela, Saulo and Dias Goncalves, Glauber},
title = {Fighting Under-price DoS Attack in Ethereum with Machine Learning Techniques},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3466826.3466835},
doi = {10.1145/3466826.3466835},
abstract = {Ethereum is one of the most popular cryptocurrency currently and it has been facing security threats and attacks. As a consequence, Ethereum users may experience long periods to validate transactions. Despite the maintenance on the Ethereum mechanisms, there are still indications that it remains susceptible to a sort of attacks. In this work, we analyze the Ethereum network behavior during an under-priced DoS attack, where malicious users try to perform denial-of-service attacks that exploit flaws in the fee mechanism of this cryptocurrency. We propose the application of machine learning techniques and ensemble methods to detect this attack, using the available transaction attributes. The proposals present notable performance as the Decision Tree models, with AUC-ROC, F-score and recall larger than 0.94, 0.82, and 0.98, respectively.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = may,
pages = {24–27},
numpages = {4},
keywords = {machine learning, ethereum, dos, blockchain, attacks}
}

@inproceedings{10.1145/3442705.3442711,
author = {Viktor, Malichenko and Honggui, Han},
title = {Traffic Sign and Vehicle Classification based on Machine Learning},
year = {2021},
isbn = {9781450388931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442705.3442711},
doi = {10.1145/3442705.3442711},
abstract = {In this paper, we offer a machine learning classifier model, later considered as MLCM, for classifying objects such as road signs and vehicles. Showing the influence of vocabulary size on accuracy of SVM using SURF. Based on SURF method used bag-of-words model as feature extractor. Due to its simplifying representation, it accelerates the first stage of our MLCM. We tested and analyzed accuracy of Support Vector Machines, including Linear, Quadratic and Medium Gaussian SVM as flowed step model and automatically use best result for further estimation. Furthermore, we provide a brief introduction of applied methods and experimental results analysis. MLCM introduces combination of SURF method and several SVMs as well as optimized SVM. This technique shows good performance with minimum failures. Thereafter, it will be implemented for real-time video sequences. The achieved goal can be implemented in the use of self-driving of industrial machines with a safe speed.},
booktitle = {Proceedings of the 2020 2nd International Conference on Video, Signal and Image Processing},
pages = {35–41},
numpages = {7},
keywords = {self-driving vehicle, Support Vector Machines, SURF method, Machine learning},
location = {Jakarta, Indonesia},
series = {VSIP '20}
}

@inproceedings{10.1145/3299869.3314050,
author = {Agrawal, Pulkit and Arya, Rajat and Bindal, Aanchal and Bhatia, Sandeep and Gagneja, Anupriya and Godlewski, Joseph and Low, Yucheng and Muss, Timothy and Paliwal, Mudit Manu and Raman, Sethu and Shah, Vishrut and Shen, Bochao and Sugden, Laura and Zhao, Kaiyu and Wu, Ming-Chuan},
title = {Data Platform for Machine Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314050},
doi = {10.1145/3299869.3314050},
abstract = {In this paper, we present a purpose-built data management system, MLdp, for all machine learning (ML) datasets. ML applications pose some unique requirements different from common conventional data processing applications, including but not limited to: data lineage and provenance tracking, rich data semantics and formats, integration with diverse ML frameworks and access patterns, trial-and-error driven data exploration and evolution, rapid experimentation, reproducibility of the model training, strict compliance and privacy regulations, etc. Current ML systems/services, often named MLaaS, to-date focus on the ML algorithms, and offer no integrated data management system. Instead, they require users to bring their own data and to manage their own data on either blob storage or on file systems. The burdens of data management tasks, such as versioning and access control, fall onto the users, and not all compliance features, such as terms of use, privacy measures, and auditing, are available. MLdp offers a minimalist and flexible data model for all varieties of data, strong version management to guarantee re-producibility of ML experiments, and integration with major ML frameworks. MLdp also maintains the data provenance to help users track lineage and dependencies among data versions and models in their ML pipelines. In addition to table-stake features, such as security, availability and scalability, MLdp's internal design choices are strongly influenced by the goal to support rapid ML experiment iterations, which cycle through data discovery, data exploration, feature engineering, model training, model evaluation, and back to data discovery. The contributions of this paper are: 1) to recognize the needs and to call out the requirements of an ML data platform, 2) to share our experiences in building MLdp by adopting existing database technologies to the new problem as well as by devising new solutions, and 3) to call for actions from our communities on future challenges.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1803–1816},
numpages = {14},
keywords = {physical data layout, dataset management for machine learning, data version control, data streaming access, data platform},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.1007/s10614-020-10013-5,
author = {Zhang, Jun and Li, Lan and Chen, Wei},
title = {Predicting Stock Price Using Two-Stage Machine Learning Techniques},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {4},
issn = {0927-7099},
url = {https://doi.org/10.1007/s10614-020-10013-5},
doi = {10.1007/s10614-020-10013-5},
abstract = {Stock market forecasting is considered to be a challenging topic among time series forecasting. This study proposes a novel two-stage ensemble machine learning model named SVR-ENANFIS for stock price prediction by combining features of support vector regression (SVR) and ensemble adaptive neuro fuzzy inference system (ENANFIS). In the first stage, the future values of technical indicators are forecasted by SVR. In the second stage, ENANFIS is utilized to forecast the closing price based on prediction results of first stage. Finally, the proposed model SVR-ENANFIS is tested on 4 securities randomly selected from the Shanghai and Shenzhen Stock Exchanges with data collected from 2012 to 2017, and the predictions are completed 1–10, 15 and 30 days in advance. The experimental results show that the proposed model SVR-ENANFIS has superior prediction performance than single-stage model ENANFIS and several two-stage models such as SVR-Linear, SVR-SVR, and SVR-ANN.},
journal = {Comput. Econ.},
month = apr,
pages = {1237–1261},
numpages = {25},
keywords = {Support vector regression (SVR), Stock market, Adaptive neuro fuzzy inference system (ANFIS), Fusion models}
}

@inproceedings{10.1145/3450508.3464564,
author = {Marshall, Carl S.},
title = {Practical machine learning for rendering: from research to deployment},
year = {2021},
isbn = {9781450383615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450508.3464564},
doi = {10.1145/3450508.3464564},
abstract = {• Give insights into closing the gap between taking a research neural model to deployment• Understand the challenges in development, training, deployment, and iteration of neural networks for rendering• Show practical use cases, tools, and networks to start your path toward neural rendering in production software},
booktitle = {ACM SIGGRAPH 2021 Courses},
articleno = {10},
numpages = {239},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@article{10.1007/s42979-021-00872-6,
author = {Sakhrawi, Zaineb and Sellami, Asma and Bouassida, Nadia},
title = {Software Enhancement Effort Prediction Using Machine-Learning Techniques: A Systematic Mapping Study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00872-6},
doi = {10.1007/s42979-021-00872-6},
abstract = {Accurate prediction of software enhancement effort is a key success in software project management. To increase the accuracy of estimates, several proposals used machine-learning (ML) techniques for predicting the software project effort. However, there is no clear evidence for determining which techniques to select for predicting more accurate effort within the context of enhancement projects. This paper aims to present a systematic mapping study (SMS) related to the use of ML techniques for predicting software enhancement effort (SEME). A SMS was performed by reviewing relevant papers from 1995 through 2020. We followed well-known guidelines. We selected 30 relevant studies; 19 from journals and 11 conferences proceedings through 4 search engines. Some of the key findings indicate that (1) there is relatively little activity in the area of SEME, (2) most of the successful studies cited focused on regression problems for enhancement maintenance effort prediction, (3) SEME is the dependent variable the most commonly used in software enhancement project planning, and the enhancement size (or the functional change size) is the most used independent variables, (4) several private datasets were used in the selected studies, and there is a growing demand for the use of commonly published datasets, and (5) only single models were employed for SEME prediction. Results indicate that much more work is needed to develop repositories in all prediction models. Based on the findings obtained in this SMS, estimators should be aware that SEME using ML techniques as part of non-algorithmic models demonstrated increased accuracy prediction over the algorithmic models. The use of ML techniques generally provides a reasonable accuracy when using the enhancement functional size as independent variables.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {15},
keywords = {Machine learning (ML), Software enhancement effort (SEME) prediction, Functional change (FC), Systematic mapping study (SMS)}
}

@phdthesis{10.5555/AAI28860524,
author = {Lin, Fanghua and Gonzalo, Arce, and David, Edwards, and David, Stockman, and Xiao, Fang, and Mei, Wang,},
advisor = {Paul, Laux,},
title = {Corporate Leverage in the US: Economic Explanation and Machine Learning Exploration},
year = {2021},
isbn = {9798780624196},
publisher = {University of Delaware},
address = {USA},
abstract = {This dissertation studies corporate leverage in the United States from the dual perspectives of economic explanation and machine learning prediction. Corporate leverage in our study is defined as the ratio of debt to assets. Our economic study proposes and investigates an explanation for the coexistence of two leverage patterns across US firms: low (or even zero) leverage for some firms and also some firms with very high leverage, even as the mean leverage ratio remains fairly stable over time. These patterns of the cross-sectional distribution of leverage have become even more pronounced in recent years. The explanation we propose centers on production flexibility, defined as the ability of firms' to quickly adjust their output in response to demand conditions. Production flexibility is found to have increased in recent years, and is a companion pattern to the low-leverage pattern and the increasing high-leverage pattern. We posit that the combined influence of production flexibility along with the state of a firm's financial constraints and its lenders' concerns about being taken advantage of by managers or shareholders leads to a plausible explanation for the  patterns in leverage. The core idea is that when financial constraints do not exist, high production flexibility indicates a low operation risk, which enables firms to take a high financial risk from debt as a substitute. Furthermore, firms where agency problem concerns are not so great can easily benefit from production flexibility to achieve growth, and the growth leads to lower leverage. However, for firms where agency problems are of greater concern to lenders, the production flexibility might be used to the detriment of lenders, leading to a positive relationship between leverage and production flexibility in the cross-section. Our model predicts that, as production flexibility has increased over the decades, we should observe a stronger low-leverage pattern for firms with less serious agency problems, and we should observe growing leverage for firms with more serious agency problems in extreme cases. We conduct an empirical investigation of these issues, and find results consistent with this reasoning, especially for firms listed in Nasdaq. We do not propose that growing production flexibility is the only reason for the leverage patterns we investigate, but we believe the reasoning and evidence suggests it is part of the explanation. Secondly, we conduct a study from a machine learning perspective. Our empirical finance study is conducted using standard panel-data regressions methods. Such methods typically involve controlling for a large set of unexplained ``fixed effects" in the cross-section, leaving only part of the variation to be explained by economic factors. The fixed effects are, mechanically, dummy variables whose coefficients need to be estimated. In the context of relatively short time series relative to the breadth of the cross-section, panel-data regressions are not usually considered to be useful for prediction. For prediction purposes, we reason that it may be useful to avoid using the information in the data to estimate pre-specified fixed effects, but instead to introduce clustering dummies only as needed in the model training process sequentially. The training algorithm we suggest is a variation on the traditional machine learning method "Gradient Boosting Machine" (GBM for short) and is called "PanelGBM" in our study. This approach relaxes assumptions of linearity which are inherent in a standard panel-data regression approach as well as other assumptions regarding clusters and data in other machine learning models that are designed for panel data analysis. Via a simulation study, we establish that our algorithm provides good accuracy and stability relative to other prediction methods for a variety of underlying true models and these clustering dummy variables that are learned in the model training process help improving model performance significantly. The algorithm could have applications in a variety of settings; in the context of this dissertation, we apply the algorithm to predict leverage for US firms and find that it performs much better than other models. Even a linear model with the clustering dummies learned from the PanelGBM performs much better than traditional panel regression model.},
note = {AAI28860524}
}

@inproceedings{10.5555/3157382.3157469,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of opportunity in supervised learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1145/3459990.3465193,
author = {Aki Tamashiro, Mariana and Van Mechelen, Maarten and Schaper, Marie-Monique and Sejer Iversen, Ole},
title = {Introducing Teenagers to Machine Learning through Design Fiction: An Exploratory Case Study},
year = {2021},
isbn = {9781450384520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459990.3465193},
doi = {10.1145/3459990.3465193},
abstract = {In this paper, we describe an exploratory study of how design fiction can be used to introduce machine learning (ML) to 14-15 year old students in an engaging way. The study describes three sessions conducted online that combined hands-on and design fiction activities related to supervised ML, focusing on facial analysis. Based on semi-structured interviews and field notes, we discuss how the design fiction approach seemed to be engaging and supportive for students' reflection on their relationship with technology. In future work, we will expand the study to include a larger number of students, assess their learning, and further explore connections between ML and its societal implications.},
booktitle = {Proceedings of the 20th Annual ACM Interaction Design and Children Conference},
pages = {471–475},
numpages = {5},
keywords = {machine learning, emerging technologies, design fiction},
location = {Athens, Greece},
series = {IDC '21}
}

@article{10.1007/s10489-020-02160-x,
author = {G. Mart\'{\i}n, Alejandro and Fern\'{a}ndez-Isabel, Alberto and Mart\'{\i}n de Diego, Isaac and Beltr\'{a}n, Marta},
title = {A survey for user behavior analysis based on machine learning techniques: current models and applications},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02160-x},
doi = {10.1007/s10489-020-02160-x},
abstract = {Significant research has been carried out in the field of User Behavior Analysis, focused on understanding, modeling and predicting past, present and future behaviors of users. However, the heterogeneity of the approaches makes their comprehension very complicated. Thus, domain and Machine Learning experts have to work together to achieve their objectives. The main motivation for this work is to obtain an understanding of this field by providing a categorization of state-of-the-art works grouping them based on specific features. This paper presents a comprehensive survey of the existing literature in the areas of Cybersecurity, Networks, Safety and Health, and Service Delivery Improvement. The survey is organized based on four different topic-based features which categorize existing works: keywords, application domain, Machine Learning algorithm, and data type. This paper aims to thoroughly analyze the existing references, to promote the dissemination of state-of-the-art approaches discussing their strong and weak points, and to identify open challenges and prospective future research directions. In addition, 127 discussed papers have been scored and ranked according to relevance-based features: paper reputation, maximum author reputation, novelty, innovation and data quality. Both types of features, topic-based and relevance-based have been combined to build a similarity metric enabling a rich visualization of all considered publications. The obtained graphic representation provides a guide of recent advancements in User Behavior Analysis by topic, highlighting the most relevant ones.},
journal = {Applied Intelligence},
month = aug,
pages = {6029–6055},
numpages = {27},
keywords = {Relevance-based features, Topic-based features, Machine learning, Survey, Behavioral analytics, User behavior analysis}
}

@phdthesis{10.5555/AAI29074139,
author = {Li, Sibo and M., Megaridis, Constantine and David, He, and Sushant, Anand, and Michael, D’Mello, and Garcia, Martinez, Marta},
advisor = {Farzad, Mashayek, and Roberto, Paoli,},
title = {Investigation of Aircraft Icing Based on Numerical Modeling and Machine Learning Methods},
year = {2021},
isbn = {9798790639074},
publisher = {University of Illinois at Chicago},
address = {USA},
abstract = {The goal of this work is to investigate the aircraft icing severity by using numerical modeling and machine learning methods. Since the ice buildup on the wing's leading edge may alter the original aerodynamic conﬁguration and degrade the aerodynamic performances, the ice shape is predicted by the numerical simulation approach. Numerical simulations generally require computationally expensive and/or cumbersome treatments to calculate the ice displacement and accretion along the wing, such as remeshing or other sophisticated techniques. Based on that fact, a prediction model based on machine learning methods is built to extract general yet critical ice features such as the ice global coverage or maximum ice thickness.The ﬁrst stage of the work is comprised of two chapters, where the focus is modeling of ice accretion over aircraft wings. An approach based on the Eulerian two-phase ﬂow theory to numerically simulate ice accretions on an aircraft wing is developed and implemented in OpenFOAM to contribute the open-source community. The airﬂow ﬁeld is obtained by solving the Navier-Stokes equations. Turbulence can be modeled or resolved to improve the prediction accuracy, especially in evaluating the eﬀect of icing on the aerodynamics. A permeable wall condition is proposed to simulate the droplets impingement properly. The droplets collection eﬃciency is calculated according to the droplets velocity and volume fraction. The ice accretion thermodynamic model is built based on the improved Messinger model and the wall function is modiﬁed to account for roughness eﬀect in calculating the convective heat transfer coeﬃcient. The ice shape is reconstructed based on the assumption that ice grows in the direction normal to the aircraft surface. The mesh morphing model adjusts the wing's shape every time step based on the amount of ice accreted. Therefore, the air ﬂow ﬁeld is updating during the simulation. Ice accretion on a NACA0012 airfoil and an ONERA M6 wing model under diﬀerent conditions have been simulated to validate the solver's performance and investigate the eﬀect of the accreted ice on the aerodynamic performance. Then, a density based compressible solver is proposed to improve the scaling performance for time accurate simulations. The aim of developing this solver is to twofold: (i) to improve the scaling performance of rhoCentralFoam, especially in large-scale simulations; and (ii) to improve time-accuracy and overall time-to-solution using high-order Runge-Kutta scheme. The parallel scalability of the solver is demonstrated through numerical experiments conducted on a Cray XC40 parallel system Theta, at Argonne National Laboratory. The proposed solver could be easily incorporated into the icing solver when conducting large eddy simulations.The second stage of the thesis introduces a data-driven statistical model for aircraft icing severity evaluation. The complex process of ice accretion makes machine learning-based methods an attractive alternative to experiments and traditional numerical simulation-based approaches. We adapt the multiple conventional and ensemble machine learning models to six atmospheric and ﬂight conditions - liquid water content, median volumetric diameter, exposure time, static temperature, angle of attack and ﬂight speed. The prediction models are demonstrated in three cases: maximum ice thickness prediction, icing area prediction and icing severity level evaluation. Multiple performance measures are employed and the results show that proposed data-driven model has a satisfactory capability to evaluate aircraft icing severity. The eﬀects of two selected ﬂight conditions on aircraft icing are further studied.},
note = {AAI29074139}
}

@article{10.1007/s10489-018-1361-5,
author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Cri\c{s}an, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1361-5},
doi = {10.1007/s10489-018-1361-5},
abstract = {Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX---MIN Ant System (MMAS) in the background. We extended the MMAS---Algorithm in a way, that the human can directly interact and influence the ants. This is done by "traveling" with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant's behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.},
journal = {Applied Intelligence},
month = jul,
pages = {2401–2414},
numpages = {14},
keywords = {Interactive machine learning, Human-in-the-loop, Combinatorial optimization, Ant Colony Optimization}
}

@article{10.1007/s00521-020-05051-x,
author = {Yin, Xiangbao},
title = {Driven by machine learning to intelligent damage recognition of terminal optical components},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05051-x},
doi = {10.1007/s00521-020-05051-x},
abstract = {In order to realize the terminal optical element online detection system in the Shenguang III system, each optical element in each terminal optical component in the target room is detected. The research on the optical damage of terminal optical components focuses on the search for damage points, the extraction of damage information, and the classification of damage types. In addition, damage classification and identification of terminal optical components are performed through machine learning, and infrared nondestructive testing is used as technical support to improve the identification model and reduce the complexity of the spectral model. After studying the preprocessing and dimensionality reduction methods of near-infrared spectroscopy, this paper compares the effects of different preprocessing methods and screening feature methods and combines different modeling methods to conduct experiments. The research results show that the method proposed in this paper has certain effects.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {789–804},
numpages = {16},
keywords = {Infrared nondestructive testing, Damage identification, Terminal optics, Machine learning}
}

@inbook{10.5555/3454287.3454547,
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
title = {Safe exploration for interactive machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {260},
numpages = {11}
}

@article{10.1007/s11277-020-07657-9,
author = {Nawrocki, Piotr and Sniezynski, Bartlomiej},
title = {Adaptive Context-Aware Energy Optimization for Services on Mobile Devices with Use of Machine Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {115},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07657-9},
doi = {10.1007/s11277-020-07657-9},
abstract = {In this paper we present an original adaptive task scheduling system, which optimizes the energy consumption of mobile devices using machine learning mechanisms and context information. The system learns how to allocate resources appropriately: how to schedule services/tasks optimally between the device and the cloud, which is especially important in mobile systems. Decisions are made taking the context into account (e.g. network connection type, location, potential time and cost of executing the application or service). In this study, a supervised learning agent architecture and service selection algorithm are proposed to solve this problem. Adaptation is performed online, on a mobile device. Information about the context, task description, the decision made and its results such as power consumption are stored and constitute training data for a supervised learning algorithm, which updates the knowledge used to determine the optimal location for the execution of a given type of task. To verify the solution proposed, appropriate software has been developed and a series of experiments have been conducted. Results show that as a result of the experience gathered and the learning process performed, the decision module has become more efficient in assigning the task to either the mobile device or cloud resources.},
journal = {Wirel. Pers. Commun.},
month = dec,
pages = {1839–1867},
numpages = {29},
keywords = {Mobile cloud computing, Machine learning, Energy optimization, Context-aware system, Adaptation}
}

@inproceedings{10.1145/3384419.3430566,
author = {Papst, Franz},
title = {Privacy-preserving machine learning for time series data: PhD forum abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430566},
doi = {10.1145/3384419.3430566},
abstract = {Machine learning has a lot of potential when applied to time series sensor data, yet a lot of this potential is currently not utilized, due to privacy concerns of parties in charge of this data. In this work I want to apply privacy-preserving techniques to machine learning for time series data, in order to unleash the dormant potential of this type of data.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {813–814},
numpages = {2},
keywords = {time series data, sensor data, privacy preserving machine learning},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inbook{10.5555/3454287.3455619,
author = {Mercado, Pedro and Tudisco, Francesco and Hein, Matthias},
title = {Generalized matrix means for semi-supervised learning with multilayer graphs},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the task of semi-supervised learning on multilayer graphs by taking into account both labeled and unlabeled observations together with the information encoded by each individual graph layer. We propose a regularizer based on the generalized matrix mean, which is a one-parameter family of matrix means that includes the arithmetic, geometric and harmonic means as particular cases. We analyze it in expectation under a Multilayer Stochastic Block Model and verify numerically that it outperforms state of the art methods. Moreover, we introduce a matrix-free numerical scheme based on contour integral quadratures and Krylov subspace solvers that scales to large sparse multilayer graphs.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1332},
numpages = {10}
}

@inproceedings{10.1145/3397537.3397552,
author = {Reimann, Lars and Kniesel-W\"{u}nsche, G\"{u}nter},
title = {Achieving guidance in applied machine learning through software engineering techniques},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397552},
doi = {10.1145/3397537.3397552},
abstract = {Development of machine learning (ML) applications is hard. Producing successful applications requires, among others, being deeply familiar with a variety of complex and quickly evolving application programming interfaces (APIs). It is therefore critical to understand what prevents developers from learning these APIs, using them properly at development time, and understanding what went wrong when it comes to debugging. We look at the (lack of) guidance that currently used development environments and ML APIs provide to developers of ML applications, contrast these with software engineering best practices, and identify gaps in the current state of the art. We show that current ML tools fall short of fulfilling some basic software engineering gold standards and point out ways in which software engineering concepts, tools and techniques need to be extended and adapted to match the special needs of ML application development. Our findings point out ample opportunities for research on ML-specific software engineering.},
booktitle = {Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {7–12},
numpages = {6},
keywords = {usability, software engineering, machine learning, learnability, guidance},
location = {Porto, Portugal},
series = {Programming '20}
}

@inproceedings{10.5555/3540261.3541931,
author = {Jung, Yonghan and Tian, Jin and Bareinboim, Elias},
title = {Double machine learning density estimation for local treatment effects with instruments},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Local treatment effects are a common quantity found throughout the empirical sciences that measure the treatment effect among those who comply with what they are assigned. Most of the literature is focused on estimating the average of such quantity, which is called the "local average treatment effect (LATE)" [31]). In this work, we study how to estimate the density of the local treatment effect, which is naturally more informative than its average. Specifically, we develop two families of methods for this task, namely, kernel-smoothing and model-based approaches. The kernel-smoothing-based approach estimates the density through some smooth kernel functions. The model-based approach estimates the density by projecting it onto a finite-dimensional density class. For both approaches, we derive the corresponding double/debiased machine learning-based estimators [13]. We further study the asymptotic convergence rates of the estimators and show that they are robust to the biases in nuisance function estimation. The use of the proposed methods is illustrated through both synthetic and a real dataset called 401(k).},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1670},
numpages = {13},
series = {NIPS '21}
}

@article{10.1155/2021/1896953,
author = {Ali Mohamed, Mohamed and El-henawy, Ibrahim Mahmoud and Salah, Ahmad and Khalil, Ahmed Mostafa},
title = {Usages of Spark Framework with Different Machine Learning Algorithms},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/1896953},
doi = {10.1155/2021/1896953},
abstract = {Sensors, satellites, mobile devices, social media, e-commerce, and the Internet, among others, saturate us with data. The Internet of Things, in particular, enables massive amounts of data to be generated more quickly. The Internet of Things is a term that describes the process of connecting computers, smart devices, and other data-generating equipment to a network and transmitting data. As a result, data is produced and updated on a regular basis to reflect changes in all areas and activities. As a consequence of this exponential growth of data, a new term and idea known as big data have been coined. Big data is required to illuminate the relationships between things, forecast future trends, and provide more information to decision-makers. The major problem at present, however, is how to effectively collect and evaluate massive amounts of diverse and complicated data. In some sectors or applications, machine learning models are the most frequently utilized methods for interpreting and analyzing data and obtaining important information. On their own, traditional machine learning methods are unable to successfully handle large data problems. This article gives an introduction to Spark architecture as a platform that machine learning methods may utilize to address issues regarding the design and execution of large data systems. This article focuses on three machine learning types, including regression, classification, and clustering, and how they can be applied on top of the Spark platform.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {7}
}

@inproceedings{10.1007/978-3-030-61078-4_2,
author = {Huo, Dongdong and Liu, Chao and Wang, Xiao and Li, Mingxuan and Wang, Yu and Wang, Yazhe and Liu, Peng and Xu, Zhen},
title = {A Machine Learning-Assisted Compartmentalization Scheme for&nbsp;Bare-Metal Systems},
year = {2020},
isbn = {978-3-030-61077-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61078-4_2},
doi = {10.1007/978-3-030-61078-4_2},
abstract = {A primary concern in creating compartments (i.e., protection domains) for bare-metal systems is to adopt the applicable compartmentalization policy. Existing studies have proposed several typical policies in literature. However, neither of the policies consider the influence of unsafe functions on the compartment security that a vulnerable function would expose unpredictable attack surfaces, which could be exploited to manipulate any contents that are stored in the same compartment. In this paper, we design a machine learning-assisted compartmentalization scheme, which adopts a new policy that takes every function’s security into full account, to create compartments for bare-metal systems. First, the scheme takes advantage of the machine learning method to predict how likely a function holds an exploitable security bug. Second, the prediction results are used to create a new instrumented firmware that isolates vulnerable and normal functions into different compartments. Further, the scheme provides some optional optimization plans to the developer to improve the performance. The PoC of the scheme is incorporated into an LLVM-based compiler and evaluated on a Cortex-M based IoT device. Compared with the firmware adopting other typical policies, the firmware with the new policy not only shows better security but also assures the overhead basically unchanged.},
booktitle = {Information and Communications Security: 22nd International Conference, ICICS 2020, Copenhagen, Denmark, August 24–26, 2020, Proceedings},
pages = {20–35},
numpages = {16},
keywords = {Machine learning, Compartmentalization policy, Bare-metal systems},
location = {Copenhagen, Denmark}
}

@article{10.1016/j.asoc.2019.105596,
author = {Zhu, Linqi and Zhang, Chong and Zhang, Chaomo and Zhang, Zhansong and Nie, Xin and Zhou, Xueqing and Liu, Weinan and Wang, Xiu},
title = {Forming a new small sample deep learning model to predict total organic carbon content by combining unsupervised learning with semisupervised learning},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {83},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105596},
doi = {10.1016/j.asoc.2019.105596},
journal = {Appl. Soft Comput.},
month = oct,
numpages = {23},
keywords = {Total organic carbon content, Coarse-detailed feature extraction, Integrated deep learning model, Deep learning, Small sample}
}

@inproceedings{10.1145/3468264.3468615,
author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
title = {FLEX: fixing flaky tests in machine learning projects by updating assertion bounds},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468615},
doi = {10.1145/3468264.3468615},
abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions.  We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level.  We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {603–614},
numpages = {12},
keywords = {Machine Learning, Flaky tests, Extreme Value Theory},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jss.2019.110486,
author = {Barbez, Antoine and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {A machine-learning based ensemble method for anti-patterns detection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110486},
doi = {10.1016/j.jss.2019.110486},
journal = {J. Syst. Softw.},
month = mar,
numpages = {11},
keywords = {Ensemble methods, Machine learning, Anti-patterns, Software quality}
}

@inproceedings{10.1145/3301275.3302280,
author = {Arendt, Dustin and Saldanha, Emily and Wesslen, Ryan and Volkova, Svitlana and Dou, Wenwen},
title = {Towards rapid interactive machine learning: evaluating tradeoffs of classification without representation},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302280},
doi = {10.1145/3301275.3302280},
abstract = {Our contribution is the design and evaluation of an interactive machine learning interface that rapidly provides the user with model feedback after every interaction. To address visual scalability, this interface communicates with the user via a "tip of the iceberg" approach, where the user interacts with a small set of recommended instances for each class. To address computational scalability, we developed an O(n) classification algorithm that incorporates user feedback incrementally, and without consulting the data's underlying representation matrix. Our computational evaluation showed that this algorithm has similar accuracy to several off-the-shelf classification algorithms with small amounts of labeled data. Empirical evaluation revealed that users performed better using our design compared to an equivalent active learning setup.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {591–602},
numpages = {12},
keywords = {visual interactive labeling, transduction learning, representation-free classifier, interactive machine learning, hierarchical clustering, active learning},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3465480.3466928,
author = {Ren, Haoyu and Anicic, Darko and Runkler, Thomas A.},
title = {The synergy of complex event processing and tiny machine learning in industrial IoT},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466928},
doi = {10.1145/3465480.3466928},
abstract = {Focusing on comprehensive networking, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various intelligent sensors play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. Complex event processing (CEP) and machine learning (ML) have been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive, concerns are raised since transmitting such an amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. Decentralized on-device ML and CEP provide a solution where data is processed primarily on edge devices. Thus communications can be minimized. However, this is no mean feat because most IIoT edge devices are resource-constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and μCEP, we now shift the computation from the cloud to the resource-constrained IIoT devices and allow users to adapt on-device ML models and CEP reasoning rules flexibly on the fly. Lastly, we demonstrate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {126–135},
numpages = {10},
keywords = {tiny machine learning, industrial IoT, complex event processing},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@article{10.1007/s11277-018-6114-6,
author = {Jeridi, Mohamed Hechmi and Khlaifi, Hacen and Bouatay, Amine and Ezzedine, Tahar},
title = {Targets Classification Based on Multi-sensor Data Fusion and Supervised Learning for Surveillance Application},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {105},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-6114-6},
doi = {10.1007/s11277-018-6114-6},
abstract = {In surveillance application scenarios, like border security and area monitoring, potential targets to be detected may be either an unarmed person, a soldier carrying ferrous weapon or a vehicle. Detection is the first phase of a monitoring process, followed by the target classification phase and finally their tracking if required. This work focuses on classification step, where we introduce our classification approach not too resource-intensive, easy to implement and suitable for large scale environment. For that, we used probabilistic reasoning techniques to address multi sensing data correlation and take advantage of multi-sensor data fusion, then, based on adopted fusion architecture, we implemented our trained classification model in a fusion node, to make the classification more accurate.},
journal = {Wirel. Pers. Commun.},
month = mar,
pages = {313–333},
numpages = {21},
keywords = {Surveillance, Target classification, Probabilistic approach, Data fusion, Machine learning, Wireless sensor network}
}

@inproceedings{10.1145/3396743.3396778,
author = {Natsupakpong, Suriya and Nithisopa, Nahpat},
title = {Lens Quality Inspection using Image Processing and Machine Learning},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396778},
doi = {10.1145/3396743.3396778},
abstract = {This research proposes a system to inspect defective lenses with a polarization technique by using image processing and machine learning. Currently, a skilled operator checks the lens quality with the polarization method by eye and decides whether or not a lens is good (OK) or not good (NG). A 'not good' lens has a circle or a line appearing in the stress pattern of the lens. This research designs and develops a lens quality checking system with machine learning by simulating and prototyping a machine to experiment and collect persistent data, using the camera to capture and analyze images with image processing and machine learning techniques to decide on the lens quality in the computer. The experimental results show that the proposed system with a trained model with data augmentation and image preprocessing can achieve performance testing with 97.75% accuracy.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {184–188},
numpages = {5},
keywords = {polarization, machine learning, lens quality inspection, image processing},
location = {Osaka, Japan},
series = {MSIE '20}
}

@article{10.1177/1094342019852127,
author = {Dongarra, Jack and Tourancheau, Bernard and Deelman, Ewa and Mandal, Anirban and Jiang, Ming and Sakellariou, Rizos},
title = {The role of machine learning in scientific workflows},
year = {2019},
issue_date = {Nov 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {33},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342019852127},
doi = {10.1177/1094342019852127},
abstract = {Machine learning (ML) is being applied in a number of everyday contexts from image recognition, to natural language processing, to autonomous vehicles, to product recommendation. In the science realm, ML is being used for medical diagnosis, new materials development, smart agriculture, DNA classification, and many others. In this article, we describe the opportunities of using ML in the area of scientific workflow management. Scientific workflows are key to today’s computational science, enabling the definition and execution of complex applications in heterogeneous and often distributed environments. We describe the challenges of composing and executing scientific workflows and identify opportunities for applying ML techniques to meet these challenges by enhancing the current workflow management system capabilities. We foresee that as the ML field progresses, the automation provided by workflow management systems will greatly increase and result in significant improvements in scientific productivity.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {1128–1139},
numpages = {12},
keywords = {workflow composition, anomaly detection, workflow systems, machine learning, Scientific workflows}
}

@article{10.1016/j.eswa.2019.112869,
author = {Romeo, Luca and Loncarski, Jelena and Paolanti, Marina and Bocchini, Gianluca and Mancini, Adriano and Frontoni, Emanuele},
title = {Machine learning-based design support system for the prediction of heterogeneous machine parameters in industry 4.0},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {140},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112869},
doi = {10.1016/j.eswa.2019.112869},
journal = {Expert Syst. Appl.},
month = feb,
numpages = {15},
keywords = {Neighborhood component features selection, Nearest-Neighbor, Decision tree, Machine learning, Design support system}
}

@article{10.1016/j.neunet.2019.03.002,
author = {Dornaika, F. and El Traboulsi, Y.},
title = {Joint sparse graph and flexible embedding for graph-based semi-supervised learning},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {114},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.03.002},
doi = {10.1016/j.neunet.2019.03.002},
journal = {Neural Netw.},
month = jun,
pages = {91–95},
numpages = {5},
keywords = {Discriminant embedding, Inductive model, Graph construction, Non-linear projection, Semi-supervised learning, Graph-based embedding}
}

@article{10.1145/3197978,
author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
title = {A Survey on Compiler Autotuning using Machine Learning},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3197978},
doi = {10.1145/3197978},
abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {96},
numpages = {42},
keywords = {Autotuning, compilers, machine learning, optimizations, phase ordering}
}

@article{10.1007/s00291-020-00615-8,
author = {Furian, Nikolaus and O’Sullivan, Michael and Walker, Cameron and \c{C}ela, Eranda},
title = {A machine learning-based branch and price algorithm for a sampled vehicle routing problem},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {43},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/s00291-020-00615-8},
doi = {10.1007/s00291-020-00615-8},
abstract = {Planning of operations, such as routing of vehicles, is often performed repetitively in rea-world settings, either by humans or algorithms solving mathematical problems. While humans build experience over multiple executions of such planning tasks and are able to recognize common patterns in different problem instances, classical optimization algorithms solve every instance independently. Machine learning (ML) can be seen as a computational counterpart to the human ability to recognize patterns based on experience. We consider variants of the classical Vehicle Routing Problem with Time Windows and Capacitated Vehicle Routing Problem, which are based on the assumption that problem instances follow specific common patterns. For this problem, we propose a ML-based branch and price framework which explicitly utilizes those patterns. In this context, the ML models are used in two ways: (a) to predict the value of binary decision variables in the optimal solution and (b) to predict branching scores for fractional variables based on full strong branching. The prediction of decision variables is then integrated in a node selection policy, while a predicted branching score is used within a variable selection policy. These ML-based approaches for node and variable selection are integrated in a reliability-based branching algorithm that assesses their quality and allows for replacing ML approaches by other (classical) better performing approaches at the level of specific variables in each specific instance. Computational results show that our algorithms outperform benchmark branching strategies. Further, we demonstrate that our approach is robust with respect to small changes in instance sizes.},
journal = {OR Spectr.},
month = sep,
pages = {693–732},
numpages = {40},
keywords = {Branching strategies, Branch and price, Machine learning, Vehicle routing}
}

@inproceedings{10.1109/CDC45484.2021.9683128,
author = {Manchester, Ian R. and Revay, Max and Wang, Ruigang},
title = {Contraction-Based Methods for Stable Identification and Robust Machine Learning: a Tutorial},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC45484.2021.9683128},
doi = {10.1109/CDC45484.2021.9683128},
abstract = {This tutorial paper provides an introduction to recently developed tools for machine learning, especially learning dynamical systems (system identification), with stability and robustness constraints. The main ideas are drawn from contraction analysis and robust control, but adapted to problems in which large-scale models can be learnt with behavioural guarantees. We illustrate the methods with applications in robust image recognition and system identification.},
booktitle = {2021 60th IEEE Conference on Decision and Control (CDC)},
pages = {2955–2962},
numpages = {8},
location = {Austin, TX, USA}
}

@phdthesis{10.5555/AAI28320203,
author = {Wang, Wentao and Wu, Qiuwen and Ge, Yaorong and Palta, Manisha and Sheng, Yang},
advisor = {Jackie, Wu, Q.},
title = {Automated Generation of Radiotherapy Treatment Plans Using Machine Learning Methods},
year = {2021},
isbn = {9798738631467},
publisher = {Duke University},
address = {USA},
abstract = {With the development of medical linear accelerator technologies, the precision and complexity of external beam radiation therapy have increased tremendously over the years. The goal of radiation therapy has always been to push the limit to irradiate the target volume while preserving normal tissues. To achieve this goal, treatment planning for radiation therapy has become a labor-intensive and time-consuming task, which requires a high level of experience and knowledge from the planner. Therefore, automated treatment planning, or auto-planning, is of particular interest in radiation therapy research. The advantages of auto-planning are reduced planning time and increased plan quality consistency.Since the treatment planning workflow has multiple steps, auto-planning includes the automation of different planning procedures, such as contouring, beam placement, and inverse optimization, which can be achieved in different approaches. The main approaches are knowledge-based planning, automated rule implementation and reasoning, and multicriteria optimization. We can generally consider such novel auto-planning applications as artificial intelligence (AI). This study primarily focuses on treatment plan generation using knowledge-based planning and machine learning techniques. The study includes two main projects: automated beam setting for whole breast radiation therapy (WBRT) and fluence map prediction for intensity modulated radiation therapy (IMRT).In WBRT planning, tangential beams are used to irradiate the entire breast volume and avoid the organs-at-risk (OARs) (i.e., the lungs and the heart) as much as possible. The placement of the beams is vital in determining the planning target volume (PTV) coverage and normal tissue sparing. Furthermore, planners need to take multiple clinical considerations into account, e.g., avoiding the contralateral breast and the heart, and use a variety of techniques to meet the demands. Therefore, we developed an automated beam setting program which takes simple user settings and optimizes target coverage and OAR sparing. The program can be launched from the Eclipse Treatment Planning System (TPS) as a binary plug-in script, which generates a graphical user interface to accept user inputs. Several beam geometries are supported: tangential beams only (supine), tangential plus supraclavicular (SCV) beams (supine), and prone beams. For all geometries, the program calculates the optimal gantry angles, collimator angles, isocenter location, jaw sizes, and MLC shapes. The borders of the SCV beams are also matched to the tangential beams by using couch kicks on the main tangential fields. For the supine geometries, a coefficient was learned from existing clinical plans to balance between the PTV and lung coverages. The program searches from an initial setting based on breast wires and finds the optimal setting. For the prone geometry, the planner can set a margin to customize the coverage near the PTV-lung interface. The program has been implemented together with a WBRT fluence prediction program, which creates electronic compensation (ECOMP) plans from the given beam settings. This automated workflow can significantly reduce the workload of the forward planned ECOMP plans. The results showed that the AI plans achieved similar or better plan quality compared to the manual plans.In IMRT planning, inverse optimization is the standard practice to create treatment plans. Dose-volume histogram (DVH) constraints and priorities are set by the planner to start the optimization and often continuously tuned throughout the planning process until the optimal dose distribution is achieved. The actual parameters to be optimized are fluence map intensities of the IMRT beams. Numerous efforts have been devoted in KBP to predict either the DVH or the dose of the optimal plan. The rationale is that, given the patient anatomy and the physician's prescription, the DVH or dose in the final plan can be predicted based on similar previous plans. The predicted DVH or dose can then be used as a reference to either evaluate the plan quality or generate new plans by converting them into inverse optimization objectives, which is a process also known as dose mimicking. However, most dose mimicking techniques are still in the development stage and not yet commercially available. We explored the feasibility to directly predict optimal fluence maps and generate IMRT plans without inverse optimization.In order to achieve fluence map prediction, we first investigated the correlation between patient anatomy and fluence maps. A database of patient anatomy and fluence maps was built with pancreas SBRT cases. Treatment planning was done on 2D axial slices with in-house dose calculation and fluence optimization algorithms. For a new slice, an atlas matching method was developed to search for the most anatomically similar slice in the database and initialize the optimization with the existing fluence. The atlas-guided fluence optimization reduced the optimization cost and offered a small dosimetric improvement compared to uniform initialization.With more training data, deep learning methods were experimented to predict fluence maps from patient anatomy. A deep learning framework consisting of two convolutional neural networks (CNN) was developed. As each plan has several beams, all beam doses must add up to the optimal plan's total dose, while each beam dose is deposited only by said beam's fluence map. Therefore, the BD-CNN predicts the individual beam doses (BD) for an IMRT plan, which tries to minimize the prediction error for both the beam doses and the total dose. Once the beam doses are available, each fluence map (FM) is generated separately by the FM-CNN. As the fluence maps exist in the beam's eye view (BEV), a projection of the 3D beam dose onto the 2D BEV is necessary. The resulting dose map is used as the input to the FM-CNN, which predicts the fluence map as the output. The predicted fluence maps are imported into the TPS for leaf sequencing and dose calculation, generating a deliverable plan.These projects are retrospective studies using anonymized patient data for training and testing. The development of the deep learning framework was split into several stages: the initial test of the feasibility was conducted for pancreas stereotactic body radiation therapy (SBRT) with a single PTV, unified dose constraints, and a fixed 9-beam geometry; the networks were then modified to allow variable dose inputs and multiple PTVs for pancreas SBRT with simultaneous integrated boost (SIB); a transfer learning technique was applied to the training of the framework for adrenal SBRT plans with different beam settings and dose constraints, using the pancreas model as the base model. The framework has evolved to be more robust and support different sites and planning styles over time.The AI plans with predicted fluence maps achieved similar plan quality as manual plans for most cases. For some cases with particularly challenging patient anatomies, the AI plans can struggle to reach the high standard of the expert plans. Fluence map prediction is a viable way to directly generate IMRT plans without inverse optimization. This application may be especially useful for adaptive treatment planning.},
note = {AAI28320203}
}

@inbook{10.5555/3454287.3454316,
author = {Li, Xueting and Liu, Sifei and Mello, Shalini De and Wang, Xiaolong and Kautz, Jan and Yang, Ming-Hsuan},
title = {Joint-task self-supervised learning for temporal correspondence},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions and establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet. The project website can be found at https://sites.google.com/view/uvc2019/.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {29},
numpages = {11}
}

@article{10.1007/s00158-021-02896-1,
author = {Li, Zhixiang and Ma, Wen and Yao, Shuguang and Xu, Ping and Hou, Lin and Deng, Gongxun},
title = {A machine learning based optimization method towards removing undesired deformation of energy-absorbing structures},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {64},
number = {2},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-021-02896-1},
doi = {10.1007/s00158-021-02896-1},
abstract = {Optimization for the energy-absorbing structures can achieve their better crashworthiness and lightweight performance. However, traditional optimization methods cannot handle categorical responses such as deformation modes. This results some undesirable deformations which often appear in the optimization solution, making it difficult to guarantee the accuracy of optimization. To this end, a machine learning based optimization method for energy-absorbing structures was proposed in this study to remove the undesired deformations. In this method, a DOE method was used to get representative sample points in the design space; the machine learning techniques were adopted to build the prediction models for deformation modes and numerical responses; the Nondominated Sorting Genetic algorithm II (NSGA-II) was utilized for the multi-objective optimization. A case study on optimization of a shrink tube used in train energy absorption was used to verify the effectiveness of the optimization method. The optimization result for the shrink tube illustrated that the machine learning based optimization method can effectively remove the undesirable deformations for energy-absorbing structures. This study may pave a new way to improve the accuracy of energy-absorbing structure optimization.},
journal = {Struct. Multidiscip. Optim.},
month = aug,
pages = {919–934},
numpages = {16},
keywords = {Deformation mode, Energy-absorbing structure, Machine learning, Optimization}
}

@article{10.1109/TCSVT.2019.2898122,
author = {Saldanha, M\'{a}rio and Sanchez, Gustavo and Marcon, C\'{e}sar and Agostini, Luciano},
title = {Fast 3D-HEVC Depth Map Encoding Using Machine Learning},
year = {2020},
issue_date = {March 2020},
publisher = {IEEE Press},
volume = {30},
number = {3},
issn = {1051-8215},
url = {https://doi.org/10.1109/TCSVT.2019.2898122},
doi = {10.1109/TCSVT.2019.2898122},
abstract = {This paper presents a fast depth map encoding for 3D-High Efficiency Video Coding (3D-HEVC) based on static decision trees. We used data mining and machine learning to correlate the encoder context attributes, building the static decision trees. Each decision tree defines that a depth map Coding Unit (CU) must be or not be split into smaller blocks, considering the encoding context through the evaluation of the encoder attributes. Specialized decision trees for I-frames, P-frames and B-frames define the partitioning of &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$64times 64$ &lt;/tex-math&gt;&lt;/inline-formula&gt;, &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$32times 32$ &lt;/tex-math&gt;&lt;/inline-formula&gt;, and &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$16times 16$ &lt;/tex-math&gt;&lt;/inline-formula&gt; CUs. We trained the decision trees using data extracted from the 3D-HEVC Test Model considering all-intra and random-access configurations, and we evaluated the proposed approach considering the common test conditions. The experimental results demonstrated that this approach can halve the 3D-HEVC encoder computational effort with less than 0.24% of BD-rate increase on the average for all-intra configuration. When running on random-access configuration, our solution is able to reduce up to 58% the complete 3D-HEVC encoder computational effort with a BD-rate drop of only 0.13%. These results surpass all related works regarding computational effort reduction and BD-rate.},
journal = {IEEE Trans. Cir. and Sys. for Video Technol.},
month = mar,
pages = {850–861},
numpages = {12}
}

@article{10.1016/j.jbi.2021.103842,
author = {Morid, Mohammad Amin and Lau, Michael and Del Fiol, Guilherme},
title = {Predictive analytics for step-up therapy: Supervised or semi-supervised learning?},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {119},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103842},
doi = {10.1016/j.jbi.2021.103842},
journal = {J. of Biomedical Informatics},
month = jul,
numpages = {9},
keywords = {Chronic care management, Resource planning, Rheumatoid arthritis, Semi-supervised learning, Step-up therapy}
}

@article{10.1007/s10707-019-00365-y,
author = {Chatzikokolakis, Konstantinos and Zissis, Dimitrios and Spiliopoulos, Giannis and Tserpes, Konstantinos},
title = {A comparison of supervised learning schemes for the detection of search and rescue (SAR) vessel patterns},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {1384-6175},
url = {https://doi.org/10.1007/s10707-019-00365-y},
doi = {10.1007/s10707-019-00365-y},
abstract = {The overall aim of this work is to perform a systematic analysis of several off-the-shelf machine learning classification algorithms and to assess their ability to classify Search And Rescue (SAR) patterns from noisy Automatic Identification System (AIS) data. Specifically, we evaluate Decision Trees, Random Forests and Gradient Boosted Trees on a large volume of historical AIS data so as to detect SAR activity from vessel trajectories, in a scalable, data-driven supervised way, with no reliance on external sources of information (e.g. coast guard reports). Our analysis verifies that it is possible to identify SAR patterns, while the results show that although all algorithms are capable of achieving high accuracy, Random Forests marginally outperform the others in terms of performance and speed of execution.},
journal = {Geoinformatica},
month = oct,
pages = {601–622},
numpages = {22},
keywords = {Big spatiotemporal data, Trajectory mining, Classification algorithms evaluation, AIS, Search and rescue patterns}
}

@article{10.3233/SW-180308,
author = {Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia and Westphal, Patrick and B\"{u}hmann, Lorenz and Bin, Simon and Jabeen, Hajira and Lehmann, Jens and Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia},
title = {SML-Bench&nbsp;– A benchmarking framework for structured machine learning},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {10},
number = {2},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-180308},
doi = {10.3233/SW-180308},
abstract = {The availability of structured data has increased significantly over the past decade and several approaches to learn from structured data have been proposed. These logic-based, inductive learning methods are often conceptually similar, which would allow a comparison among them even if they stem from different research communities. However, so far no efforts were made to define an environment for running learning tasks on a variety of tools, covering multiple knowledge representation languages. With SML-Bench, we propose a benchmarking framework to run inductive learning tools from the ILP and semantic web communities on a selection of learning problems. In this paper, we present the foundations of SML-Bench, discuss the systematic selection of benchmarking datasets and learning problems, and showcase an actual benchmark run on the currently supported tools.},
journal = {Semant. Web},
month = jan,
pages = {231–245},
numpages = {15},
keywords = {structured machine learning, Benchmark}
}

@phdthesis{10.5555/AAI28869486,
author = {Kaptanoglu, Alan and Uri, Shumlak, and Kai-Mei, Fu, and Gerald, Seidler,},
advisor = {Steven, Brunton,},
title = {An Exploration of Data-Driven System Identification and Machine Learning for Plasma Physics},
year = {2021},
isbn = {9798780639763},
publisher = {University of Washington},
abstract = {Plasma is the most common state of visible matter in the universe and provides a myriad of scientific and engineering applications. However, the complexity of these systems poses a significant challenge for understanding and controlling plasmas. Fortunately, machine learning is increasingly used to handle complex, nonlinear systems, and the field of machine learning is advancing at an unprecedented pace, propelled forward by advances in sensing technology and computing power. This thesis summarizes work towards applying modern machine learning algorithms for fluid and plasma physics applications, with a focus on the understanding and control of magnetohydrodynamic (MHD) phenomena and fusion-relevant plasmas. Although this work is primarily focused on machine learning, first conventional numerical techniques are used to implement a two-temperature Hall-MHD model into the 3D PSI-Tet code, followed by an investigation of the plasma dynamics in the HIT-SI experiment. These simulations agree well with experimental measurements, and indicate that low-densities are required for significant closed flux surfaces—a recommendation that is now helping to guide the next generation of experimental design. Next, plasma modeling with machine learning is discussed in the context of the hierarchy of plasma models and it is illustrated that there is "plenty of room at the bottom" for physics-constrained reduced order models that approximate more complex MHD or kinetic plasma models. Variants of the dynamic mode decomposition are explored on experimental data and simulations of the HIT-SI plasma device and indicate promise for magnetic mode spectroscopy and forecasting diagnostic measurements. Continuing, analytic reduced-order modeling methods are extended using techniques in system identification for extracting reduced-order models directly from data. In the process, new methods are invented to enforce physical constraints and stability in data-driven fluid and plasma models. For instance, the ability to build data-driven models that obey global conservation of energy or global conservation of cross-helicity is demonstrated, with promise for efficient simulations of ideal and resistive MHD turbulence. With the new functionality implemented into the open-source PySINDy code as part of this work, advanced system identification methods that can robustly extract dynamical equations from data are available to the larger scientific community. In total, this work illustrates that new machine learning methods can be directly tied with known physical laws in plasma physics, have promise to significantly impact much of the plasma physics and nonlinear systems fields, and can provide complementary, interpretable methods to the relatively black-box deep learning techniques that are frequently used in the plasma physics field for extracting diagnostic information, building reduced-order models, and performing real-time control.},
note = {AAI28869486}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Structural complexity, Software product line, Software prediction model, Quality attributes, Maintainability, Feature model, Controlled experimentation}
}

@inproceedings{10.5555/3465085.3465095,
author = {Najari, Arman and Pajarito, Diego and Markopoulou, Areti},
title = {Data modeling of cities, a machine learning application},
year = {2020},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Today cities are the main humankind settlement relying on intricate systems. The technological growth in the context of urban produces a vast amount of data. Analyzing and visualizing this data have provided insights into this complex environment. However, these mere approaches are inert and, due to technical constraints, hard to be integrated into urban planning.Consequently, we adopt a research hypothesis in which the adaptability and complexity of urban systems can be replicated, or partially replicated, by the machine-learning algorithms.The current study aims to define a process for evaluating the capabilities for analyzing and predicting urban data with machine-learning (ML) algorithms. This process starts with constructing a data structure for inputting into the ML algorithm. Following this, different tests are applied to identify valid combinations of data and models that allow understanding of urban patterns. The bicycle-sharing system is used as a case study. The process ends discussing the options to replicate the experiment in different urban areas as well as to adapt it to different problems.Different datasets from different cities have been explored and considered for this experiment. Across many cities' open dataset platforms, the NYD platform offered the most reliable data. From the sub-systems of the city, the mobility network was selected as a case study for exploration. More specifically, data on shared-bicycle mobility and use were selected as a result of its exciting raise as travel choices reported by New York City's department of transportation. The urban data analysis and prediction process of this research paper identifies the neighborhood as the unit of the model. Additionally, to illustrate and analyze the relationship between the selected mobility sub-system and other urban systems, contextual indicators such as land use indexes were added in the modeling process.Despite the prediction modeling machine for the bike-sharing system coming out of this study, the main achievement is the introduction of a collection of analysis and prediction processes for urban data beyond mobility. For further advancement, implementing this approach in a different urban system and context is crucial. By this means, the replicability of the process could be evaluated and tested.},
booktitle = {Proceedings of the 11th Annual Symposium on Simulation for Architecture and Urban Design},
articleno = {10},
numpages = {8},
keywords = {urban analytics, predictive urban model, machine learning, big-data, bicycle mobility data},
location = {Virtual Event, Austria},
series = {SimAUD '20}
}

@article{10.1007/s10489-020-02118-z,
author = {Yang, Chunsheng and Gu, Wen and Ito, Takayuki and Yang, Xiaohua},
title = {Machine learning-based consensus decision-making support for crowd-scale deliberation},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02118-z},
doi = {10.1007/s10489-020-02118-z},
abstract = {With the rapid development of Internet, the online discussion system or social democratic system has become an important and effective vehicle for group decision-making support since it can continue collecting the opinions from the public at anytime. To reach a consensus in crowd-scale deliberation, the existing online discussion systems require an experienced human facilitator to navigate and guild the discussion. When human facilitator performs the required facilitation there are several issues such as heavy burden on decision-making, the 24/7 online facilitation, bias on the social issues, etc. To address these issues it is necessary and inevitable to explore intelligent facilitation. For this purpose, we propose a novel machine learning-based method for smart facilitation, in particular the intelligent consensus decision-making support (CDMS) for crowd-scale deliberation. After presenting an overview of the crowd-scale deliberation and the COLLAGREE, the paper details the proposed approach, a machine learning-based framework for CDMS in crowd-scale deliberation. To validate the developed methods the offline evaluation experiments were conducted with the online discussion platform, COLLAGREE. The preliminary experimental results obtained from offline validation demonstrated the feasibility and usefulness of the developed machine learning-based methods for CDMS.},
journal = {Applied Intelligence},
month = jul,
pages = {4762–4773},
numpages = {12},
keywords = {Online discussion systems, Facilitation, Consensus decision-making support (CDMS), Case-based reasoning, Machine learning algorithms, Crowd deliberation}
}

@article{10.1007/s00521-020-05023-1,
author = {Han, Chaoliang and Zhang, Qi},
title = {Optimization of supply chain efficiency management based on machine learning and neural network},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {5},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05023-1},
doi = {10.1007/s00521-020-05023-1},
abstract = {Supply chain management is of great significance to business operations and socioeconomic development. However, the current supply chain efficiency management cannot effectively control the risk caused by the inefficient supply chain management. In order to study the improvement in supply chain efficiency management, supported by machine learning and neural network technology, this study builds a supply chain risk management model based on learning and neural network. Moreover, this study evaluates the risk indicator system based on the current status of supply chain management. In addition, the model simulation research is carried out in the MATLAB platform, and the validity analysis of the model is performed with examples. Finally, after training the data through the training model, the risk assessment value is output, and strategies for coping with the risk are given. The research shows that the model proposed in this paper has a certain practical effect and can be considered for application.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1419–1433},
numpages = {15},
keywords = {Performance evaluation, Efficiency, Supply chain, Neural network, Machine learning}
}

@inproceedings{10.1007/978-3-030-68763-2_46,
author = {Versaci, Francesco},
title = {WaveTF: A Fast 2D Wavelet Transform for Machine Learning in Keras},
year = {2021},
isbn = {978-3-030-68762-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68763-2_46},
doi = {10.1007/978-3-030-68763-2_46},
abstract = {The wavelet transform is a powerful tool for performing multiscale analysis and it is a key subroutine in countless applications, from image processing to astronomy. Recently, it has extended its range of users to include the ever growing machine learning community. For a wavelet library to be efficiently adopted in this context, it needs to provide transformations which can be integrated seamlessly in already existing machine learning workflows and neural networks, being able to leverage the same libraries and run on the same hardware (e.g., CPU vs GPU) as the rest of the machine learning pipeline, without impacting training and evaluation performance. In this paper we present WaveTF, a wavelet library available as a Keras layer, which leverages TensorFlow to exploit GPU parallelism and can be used to enrich already existing machine learning workflows. To demonstrate its efficiency we compare its raw performance against other alternative libraries and finally measure the overhead it causes to the learning process when it is integrated in an already existing Convolutional Neural Network.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part I},
pages = {605–618},
numpages = {14},
keywords = {Neural networks, Machine learning, Discrete wavelet transforms}
}

@inproceedings{10.1007/978-3-030-61166-8_21,
author = {Ot\'{a}lora, Sebastian and Marini, Niccol\`{o} and M\"{u}ller, Henning and Atzori, Manfredo},
title = {Semi-weakly Supervised Learning for Prostate Cancer Image Classification with Teacher-Student Deep Convolutional Networks},
year = {2020},
isbn = {978-3-030-61165-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61166-8_21},
doi = {10.1007/978-3-030-61166-8_21},
abstract = {Deep Convolutional Neural Networks (CNN) are at the backbone of the state–of–the art methods to automatically analyze Whole Slide Images (WSIs) of digital tissue slides. One challenge to train fully-supervised CNN models with WSIs is providing the required amount of costly, manually annotated data. This paper presents a semi-weakly supervised model for classifying prostate cancer tissue. The approach follows a teacher-student learning paradigm that allows combining a small amount of annotated data (tissue microarrays with regions of interest traced by pathologists) with a large amount of weakly-annotated data (whole slide images with labels extracted from the diagnostic reports). The task of the teacher model is to annotate the weakly-annotated images. The student is trained with the pseudo-labeled images annotated by the teacher and fine-tuned with the small amount of strongly annotated data. The evaluation of the methods is in the task of classification of four Gleason patterns and the Gleason score in prostate cancer images. Results show that the teacher-student approach improves significatively the performance of the fully-supervised CNN, both at the Gleason pattern level in tissue microarrays (respectively κ=0.594±0.022 and κ=0.559±0.034) and at the Gleason score level in WSIs (respectively κ=0.403±0.046 and κ=0.273±0.12). Our approach opens the possibility of transforming large weakly–annotated (and unlabeled) datasets into valuable sources of supervision for training robust CNN models in computational pathology.},
booktitle = {Interpretable and Annotation-Efficient Learning for Medical Image Computing: Third International Workshop, IMIMIC 2020, Second International Workshop, MIL3ID 2020, and 5th International Workshop, LABELS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {193–203},
numpages = {11},
keywords = {Knowledge distillation, Prostate cancer, Semi-weakly supervision, Deep learning, Computational pathology},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-73280-6_34,
author = {Vecliuc, Dumitru-Daniel and Artene, Codruundefined-Georgian and Tibeic\u{a}, Marius-Nicolae and Leon, Florin},
title = {An Experimental Study of Machine Learning for Phishing Detection},
year = {2021},
isbn = {978-3-030-73279-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73280-6_34},
doi = {10.1007/978-3-030-73280-6_34},
abstract = {We approach the phishing detection problem as a data science problem with three different sub-models, each designed to handle a specific sub-task: URL classification, webpage classification based on HTML content and logo detection and recognition from the screenshot of a given webpage. The combined results from the sub-models are used as input for an ensemble model designed to do the classification. Based on the analysis performed on the results, one may conclude that ML techniques are suitable to be part of a system designed for automatic phishing detection.},
booktitle = {Intelligent Information and Database Systems: 13th Asian Conference, ACIIDS 2021, Phuket, Thailand, April 7–10, 2021, Proceedings},
pages = {427–439},
numpages = {13},
keywords = {Logo detection, Webpage classification, URL classification, Deep learning, Machine learning, Phishing detection},
location = {Phuket, Thailand}
}

@inproceedings{10.1145/3454127.3458773,
author = {Haiba, Somaya and MAZRI, TOMADER},
title = {Build a malware detection software for IOT network Using Machine learning},
year = {2021},
isbn = {9781450388719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3454127.3458773},
doi = {10.1145/3454127.3458773},
abstract = {Any system based on IOT devices must provide a reliable and secure network to transmit and manipulate the data from the smallest technologies to the final server. They are the devices of nowadays, and our future for sure, it will convert all domains of our live starting from the smart home, industry, to e-healthcare systems. To achieve the quality of performances we should have an assurance of security and trustworthiness beginning by the small device used to capture information to the final terminal of the network system, using all the exist possibilities that we can implement to ensure a secure employment. Until now, machine learning become the most powerful application, which provides for any systems the capacity of auto-learning, and improving itself taking a help from the old experiences and without any human intervention or being explicit programmed. Furthermore, the usage of IOT networks, as we know is growing and evaluate enormously so the menaces keep up to date exploiting the weakness of these tools. For that, we propose to make a look about, What machine learning is coming with, to perform more security and analyses the threats, and how can it be used to detect any endanger before it can gain control. Knowing that the reproducible use of resource-constrained IOT devices, the number of IOT Malwares has exploded variously with many ways of breakthrough and then the requirement, to have an efficient malware detection adequate with this grown-up is on immense increasing importance. In this paper, we discuss the usability of machine learning applications in malwares detection software for IOT networks to elicitation the standards, which can us, use to build a powerful model to improve the network security of this small technologies answering to pervious questions.},
booktitle = {Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security},
articleno = {64},
numpages = {8},
keywords = {Security, Malware Detection, Machine-learning, IOT networks, IOT Malwares},
location = {KENITRA, AA, Morocco},
series = {NISS '21}
}

@phdthesis{10.5555/AAI28024829,
author = {Wang, Xueshe and Virgin, Lawrence and Stanton, Samuel and Gavin, Henri},
advisor = {P, Mann, Brian},
title = {Nonlinear Energy Harvesting with Tools from Machine Learning},
year = {2020},
isbn = {9798664793543},
publisher = {Duke University},
address = {USA},
abstract = {Energy harvesting is a process where self-powered electronic devices scavenge ambient energy and convert it to electrical power. Traditional linear energy harvesters which operate based on linear resonance work well only when excitation frequency is close to its natural frequency. While various control methods applied to an energy harvester realize resonant frequency tuning, they are either energy-consuming or exhibit low efficiency when operating under multi-frequency excitations. In order to overcome these limitations in a linear energy harvester, researchers recently suggested using "nonlinearity" for broad-band frequency response.Based on existing investigations of nonlinear energy harvesting, this dissertation introduced a novel type of energy harvester designs for space efficiency and intentional nonlinearity: translational-to-rotational conversion. Two dynamical systems were presented: 1) vertically forced rocking elliptical disks, and 2) non-contact magnetic transmission. Both systems realize the translational-to-rotational conversion and exhibit nonlinear behaviors which are beneficial to broad-band energy harvesting. This dissertation also explores novel methods to overcome the limitation of nonlinear energy harvesting—the presence of coexisting attractors. A control method was proposed to render a nonlinear harvesting system operating on the desired attractor. This method is based on reinforcement learning and proved to work with various control constraints and optimized energy consumption.Apart from investigations of energy harvesting, several techniques were presented to improve the efficiency for analyzing generic linear/nonlinear dynamical systems: 1) an analytical method for stroboscopically sampling general periodic functions with arbitrary frequency sweep rates, and 2) a model-free sampling method for estimating basins of attraction using hybrid active learning.},
note = {AAI28024829}
}

@phdthesis{10.5555/AAI28744994,
author = {Xie, Hailun},
advisor = {Li, Zhang, and Shen, Wei, and Bobo, Ng,},
title = {Evolving Machine Learning and Deep Learning Models using Evolutionary Algorithms},
year = {2020},
isbn = {9798535578712},
publisher = {University of Northumbria at Newcastle (United Kingdom)},
abstract = {Despite the great success in data mining, machine learning and deep learning models are yet subject to material obstacles when tackling real-life challenges, such as feature selection, initialization sensitivity, as well as hyperparameter optimization. The prevalence of these obstacles has severely constrained conventional machine learning and deep learning methods from fulfilling their potentials. In this research, three evolving machine learning and one evolving deep learning models are proposed to eliminate above bottlenecks, i.e. improving model initialization, enhancing feature representation, as well as optimizing model configuration, respectively, through hybridization between the advanced evolutionary algorithms and the conventional ML and DL methods.  Specifically, two Firefly Algorithm based evolutionary clustering models are proposed to optimize cluster centroids in K-means and overcome initialization sensitivity as well as local stagnation. Secondly, a Particle Swarm Optimization based evolving feature selection model is developed for automatic identification of the most effective feature subset and reduction of feature dimensionality for tackling classification problems. Lastly, a Grey Wolf Optimizer based evolving Convolutional Neural Network-Long Short-Term Memory method is devised for automatic generation of the optimal topological and learning configurations for Convolutional Neural Network-Long Short-Term Memory networks to undertake multivariate time series prediction problems.  Moreover, a variety of tailored search strategies are proposed to eliminate the intrinsic limitations embedded in the search mechanisms of the three employed evolutionary algorithms, i.e. the dictation of the global best signal in Particle Swarm Optimization, the constraint of the diagonal movement in Firefly Algorithm, as well as the acute contraction of search territory in Grey Wolf Optimizer, respectively. The remedy strategies include the diversification of guiding signals, the adaptive nonlinear search parameters, the hybrid position updating mechanisms, as well as the enhancement of population leaders. As such, the enhanced Particle Swarm Optimization, Firefly Algorithm, and Grey Wolf Optimizer variants are more likely to attain global optimality on complex search landscapes embedded in data mining problems, owing to the elevated search diversity as well as the achievement of advanced trade-offs between exploration and exploitation.},
note = {AAI28744994}
}

@phdthesis{10.5555/AAI28496475,
author = {Ginley, Brandon and John, Tomaszewski, and Wen, Dong, and Scott, Doyle,},
advisor = {Pinaki, Sarder,},
title = {Investigation of Machine Learning Algorithms for Pathologic Assessment of Digitized Kidney Biopsies},
year = {2021},
isbn = {9798516939983},
publisher = {State University of New York at Buffalo},
address = {USA},
abstract = {The intent of this thesis is to study the reliability of machine learning and image analysis techniques to replicate diagnostic tests performed on kidney biopsies as part of conventional pathology analysis. The kidney biopsy is a fundamental component to the assessment of kidney disease, as the microscopic evaluation of kidney tissue can not only identify the cause of renal disease but also the severity. Conventional analysis of biopsied tissues, however, is limited for many reasons. The primary limitation is that each biopsied tissue contains overwhelming amounts of microscopic data that are not fully utilized in traditional pathology reporting due to the constraints of time in busy clinical practice. This leads to several secondary issues, the first being that diagnostic schema utilize structural evaluation of only select portions of the biopsy whose identification has high agreement among practicing pathologists and/or high prognostic value. This, combined with human nature, leads to a third limitation; i.e. regardless of the classification system used, there is always some level of inter- and intra-rater variability among pathologists. One practical example of these limitations is in the assessment of interstitial fibrosis and tubular atrophy (IFTA). IFTA is a complex morphologic manifestation of chronic kidney damage, where the tubules reduce both in physical size and function while scar tissue accumulates in the surrounding interstitium. This can involve anywhere from a small portion of a few tubules to all of the thousands of tubules captured in the biopsy. Because there is not enough time for a renal pathologist to measure the size of every tubule individually, instead, the pathologist visually estimates the total percent of the biopsy involved by IFTA in cerebro. This method of IFTA assessment has been shown to have high prognostic power, but has been consistently criticized as subjective and an oversimplification of the true underlying pathology of IFTA, with many calls to improve its pathologic definition. These conventional limitations have led to a significant amount of interest in the applications of computers to analyze biopsied data. Computers can analyze large data at a much faster rate of speed than humans, and moreover, they are indefatigable. This property opens up significant opportunity to assess the thousands of structures contained in a renal biopsy at a precise and detailed level. Despite the exceptional promise, application of computational algorithms to kidney pathology has traditionally been very limited due to the vast structural heterogeneity and complexity found in human kidney tissue. However, recent rapid development of machine learning algorithms, such as convolutional neural networks (CNN's), has created extremely powerful software for image object detection. Despite rapid adoption and significant success of these tools in oncology, validation of these tools for kidney pathology has again been very limited. Given the success in other areas, we hypothesized that optimally designed image analysis pipelines can be used to replicate pathologist assessment of kidney tissues to an equal degree of reliability as would be expected from human renal pathologists. We supported this hypothesis with two major works showing proof of concept: the first on a wholly digital pipeline to classify kidney biopsies according to the severity of diabetic nephropathy (DN), and the second to estimate prognostic markers of chronic kidney damage from whole kidney biopsies. In the first work, we developed an image analysis pipeline that was capable of classifying the stages of diabetic nephropathy (DN) with equal reliability as was observed among renal pathologists. In this study, CNN's were trained to automatically extract glomeruli from whole biopsy of 48 patients with DN and six control tissues, achieving 0.93 sensitivity and 0.99 specificity on test images. We then studied the performance of CNN's to detect glomerular nuclei in 616 cropped glomerulus images, which was found to be 0.8 sensitivity and 0.99 specificity in the test glomeruli. An additional probabilistic weighting on the output of the CNN allowed biasing towards the nucleus class, yielding an optimized performance of 0.94 sensitivity and 0.93 specificity. Leveraging the CNN nuclear detection and simple color transformation and thresholding, a three-component system modeled the complete glomerular structure (nuclei component, periodic acid-Schiff positive component, and luminal component). From this three-component model of the glomerulus, 232 digital glomerulus-specific features were hand-designed using traditional image analysis routines, such as morphological image processing. These features were measured on all glomeruli of the 54 patients, and fed as input to a recurrent neural network (RNN), whose target was prediction of the DN class. A Cohen's kappa statistic was used to measure the reliability of the RNN classifications against the renal pathologist it was trained on, achieving κ=0.55. Two separate renal pathologists respectively achieved reliabilities against the first pathologist of κ=0.48 and κ=0.68. Therefore, we concluded that the proposed digital pipeline is capable of classifying DN biopsies with similar reliability as renal pathologists.To expand the overall impact of our computational tools, we next investigated use of a machine learning algorithm to detect IFTA and glomerulosclerosis from kidney biopsies. As aforementioned, IFTA is prognostic for CKD progression and outcome. Glomerulosclerosis, the scarring of glomerular capillaries, is also a strong morphologic prognostic marker of CKD progression and outcome, and significantly associated with excess urinary protein excretion. IFTA is conventionally measured by a pathologist visually scanning the biopsy to identify regions of IFTA, and mentally estimating what percent area of the total biopsy is involved. The subjectivity of this process leads to significant reliability concerns among pathologists. Glomerulosclerosis is measured by dividing the number of glomeruli with complete (global) glomerulosclerosis by the total number of observed glomeruli, and typically shows higher reliability. We sought to investigate if machine learning methods can provide a reliable computational method to estimate these markers in kidney biopsy. A total of 205 patients pooled from 6 institutions across 3 continents were studied (refer to Section 4.4.1 for IRB approval information). One hundred sixteen biopsies were used to train and evaluate the performance of CNN's to detect IFTA and glomerulosclerosis in renal biopsy. Compared to the renal pathologist who provided the annotations to train the machine learner, the best of six configurations of CNN's achieved a Matthew's Correlation Coefficient (MCC) of 0.74 for IFTA, 0.94 for glomeruli, and 0.8 for glomerulosclerosis. The reliability of this best model was then evaluated against IFTA and glomerulosclerosis assessment made by four renal pathologists, using the intra-class correlation coefficient (ICC). The ICC among the four renal pathologists and the computer for measurement of IFTA percent was 0.94, as compared to 0.95 measured among the renal pathologists alone. For measurement of glomerulosclerosis ratio, the four pathologists alone achieved an ICC of 0.90, and including the computer raised this value to 0.91. Further, we compared the pixels labeled as IFTA by the pathologists and the computer using Cohen's kappa on the entire biopsy image. The lowest kappa scored by the computer was 0.48, compared to the lowest kappa scored by a human at 0.41. These data suggest the computer is similarly reliable to the assessment of multiple renal pathologists. Finally, for 30 transplant and 57 diabetic patients, we used a series of regression analyses and statistical testing to determine if the human and computer assessment methods show any difference in ability to predict patient outcome. It was determined that a statistically significant difference between the human and computer assessment methods could not be found. Therefore, from these experiments, we conclude that computational measurement of IFTA and glomerulosclerosis can demonstrate performance that is at least as reliable as renal pathologist assessment. These methods may be useful as a stand-in for renal pathologist expertise in scenarios where it is unavailable or infeasible. Through these works, we have demonstrated the feasibility of image analysis algorithms to replicate two common diagnostic tasks, classification of DN and estimation of chronic kidney damage. The first study demonstrated that optimal combinations of traditional image analysis and modern machine learning could create digital pipelines with great utility for pathologic analysis of kidney tissues. The second study demonstrated that these digital pipelines could even achieve similar prognostic power as measurements made by renal pathologist. These successes exposed the renal community to the power of computational processing and the ample opportunities to increase precision in kidney medicine. The tools developed in this thesis, released openly to the research community, will act as a stepping-stone for future investigators to study more interesting, complex, and hypothesis-driven questions, helping usher in the next era of precision kidney medicine.},
note = {AAI28496475}
}

@article{10.1145/3403584,
author = {Hu, Yong and Mettler, Marcel and Mueller-Gritschneder, Daniel and Wild, Thomas and Herkersdorf, Andreas and Schlichtmann, Ulf},
title = {Machine Learning Approaches for Efficient Design Space Exploration of Application-Specific NoCs},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3403584},
doi = {10.1145/3403584},
abstract = {In many Multi-Processor Systems-on-Chip (MPSoCs), traffic between cores is unbalanced. This motivates the use of an application-specific Network-on-Chip (NoC) that is customized and can provide a high performance at low cost in terms of power and area. However, finding an optimized application-specific NoC architecture is a challenging task due to the huge design space.This article proposes to apply machine learning approaches for this task. Using graph rewriting, the NoC Design Space Exploration (DSE) is modelled as a Markov Decision Process (MDP). Monte Carlo Tree Search (MCTS), a technique from reinforcement learning, is used as search heuristic. Our experimental results show that—with the same cost function and exploration budget—MCTS finds superior NoC architectures compared to Simulated Annealing (SA) and a Genetic Algorithm&nbsp;(GA). However, the NoC DSE process suffers from the high computation time due to expensive cycle-accurate SystemC simulations for latency estimation. This article therefore additionally proposes to replace latency simulation by fast latency estimation using a Recurrent Neural Network (RNN). The designed RNN is sufficiently general for latency estimation on arbitrary NoC architectures. Our experiments show that compared to SystemC simulation, the RNN-based latency estimation offers a similar speed-up as the widely used Queuing Theory (QT). Yet, in terms of estimation accuracy and fidelity, the RNN is superior to QT, especially for high-traffic scenarios. When replacing SystemC simulations with the RNN estimation, the obtained solution quality decreases only slightly, whereas it suffers significantly when QT is used.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {44},
numpages = {27},
keywords = {recurrent neural network, design space exploration, Monte-Carlo-tree search, Application-specific networks-on-chip}
}

@inproceedings{10.1007/978-3-030-01270-0_42,
author = {Janai, Joel and G\"{u}ney, Fatma and Ranjan, Anurag and Black, Michael and Geiger, Andreas},
title = {Unsupervised Learning of Multi-Frame Optical Flow with Occlusions},
year = {2018},
isbn = {978-3-030-01269-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01270-0_42},
doi = {10.1007/978-3-030-01270-0_42},
abstract = {Learning optical flow with neural networks is hampered by the need for obtaining training data with associated ground truth. Unsupervised learning is a promising direction, yet the performance of current unsupervised methods is still limited. In particular, the lack of proper occlusion handling in commonly used data terms constitutes a major source of error. While most optical flow methods process pairs of consecutive frames, more advanced occlusion reasoning can be realized when considering multiple frames. In this paper, we propose a framework for unsupervised learning of optical flow and occlusions over multiple frames. More specifically, we exploit the minimal configuration of three frames to strengthen the photometric loss and explicitly reason about occlusions. We demonstrate that our multi-frame, occlusion-sensitive formulation outperforms existing unsupervised two-frame methods and even produces results on par with some fully supervised methods.},
booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI},
pages = {713–731},
numpages = {19},
location = {Munich, Germany}
}

@article{10.1016/j.sysarc.2021.102298,
author = {Fern\'{a}ndez, Javier and Perez, Jon and Agirre, Irune and Allende, Imanol and Abella, Jaume and Cazorla, Francisco J.},
title = {Towards functional safety compliance of matrix–matrix multiplication for machine learning-based autonomous systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2021.102298},
doi = {10.1016/j.sysarc.2021.102298},
journal = {J. Syst. Archit.},
month = dec,
numpages = {14},
keywords = {Error detection, Functional safety, Machine learning}
}

@article{10.1145/3450626.3459873,
author = {Yang, Kaizhi and Chen, Xuejin},
title = {Unsupervised learning for cuboid shape abstraction via joint segmentation from point clouds},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459873},
doi = {10.1145/3450626.3459873},
abstract = {Representing complex 3D objects as simple geometric primitives, known as shape abstraction, is important for geometric modeling, structural analysis, and shape synthesis. In this paper, we propose an unsupervised shape abstraction method to map a point cloud into a compact cuboid representation. We jointly predict cuboid allocation as part segmentation and cuboid shapes and enforce the consistency between the segmentation and shape abstraction for self-learning. For the cuboid abstraction task, we transform the input point cloud into a set of parametric cuboids using a variational auto-encoder network. The segmentation network allocates each point into a cuboid considering the point-cuboid affinity. Without manual annotations of parts in point clouds, we design four novel losses to jointly supervise the two branches in terms of geometric similarity and cuboid compactness. We evaluate our method on multiple shape collections and demonstrate its superiority over existing shape abstraction methods. Moreover, based on our network architecture and learned representations, our approach supports various applications including structured shape generation, shape interpolation, and structural shape clustering.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {152},
numpages = {11},
keywords = {point clouds, joint segmentation, 3D structural representation, 3D shape abstraction}
}

@inproceedings{10.5555/2969239.2969348,
author = {Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {Unsupervised learning by program synthesis},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our techniques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures, and a technique for applying program synthesis tools to noisy data.},
booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 1},
pages = {973–981},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{10.1007/978-3-030-89432-0_5,
author = {Mogg, Raymond and Enoch, Simon Yusuf and Kim, Dong Seong},
title = {A Framework for Generating Evasion Attacks for Machine Learning Based Network Intrusion Detection Systems},
year = {2021},
isbn = {978-3-030-89431-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89432-0_5},
doi = {10.1007/978-3-030-89432-0_5},
abstract = {Intrusion Detection System (IDS) plays a vital role in detecting anomalies and cyber-attacks in networked systems. However, sophisticated attackers can manipulate the IDS’ attacks samples to evade possible detection. In this paper, we present a network-based IDS and investigate the viability of generating interpretable evasion attacks against the IDS through the application of a machine learning technique and an evolutionary algorithm. We employ a genetic algorithm to generate optimal attack features for certain attack categories, which are evaluated against a decision tree-based IDS in terms of their fitness measurements. To demonstrate the feasibility of our approach, we perform experiments based on the NSL-KDD dataset and analyze the algorithm performance.},
booktitle = {Information Security Applications: 22nd International Conference, WISA 2021, Jeju Island, South Korea, August 11–13, 2021, Revised Selected Papers},
pages = {51–63},
numpages = {13},
keywords = {Intrusion detection, Genetic algorithms, Evasion attacks, Adversarial machine learning},
location = {Jeju, Korea (Republic of)}
}

@article{10.1016/j.comcom.2021.07.009,
author = {Miglani, Arzoo and Kumar, Neeraj},
title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {178},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2021.07.009},
doi = {10.1016/j.comcom.2021.07.009},
journal = {Comput. Commun.},
month = oct,
pages = {37–63},
numpages = {27},
keywords = {6G, 5G, Deep learning, Internet of Things, Federated learning, Machine learning, Blockchain}
}

@inproceedings{10.1145/3366424.3383562,
author = {G. Harris, Christopher},
title = {Mitigating Cognitive Biases in Machine Learning Algorithms for Decision Making},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383562},
doi = {10.1145/3366424.3383562},
abstract = {Cognitive biases are an ingrained part of the human decision-making process. Nearly all machine learning algorithms that mimic human decision-making use human judgments as training data, which propagates these biases. In this paper, we conduct an empirical study in which 150 applicants are rated for suitability for three separate job openings. We develop an algorithm that learns from human judgments and consequently develops biases based on these human-generated inputs. Next, we explore and apply techniques to mitigate these algorithmic biases, using a combination of pre-processing, in-processing, and post-processing algorithms. The results from our study show that biases can be mitigated using these approaches but involve a tradeoff between complexity and effectiveness.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {775–781},
numpages = {7},
keywords = {Machine Learning, Human Resources Tasks, Fairness, Decision Making, Data Science, Cognitive Bias, Artificial Intelligence, Algorithmic Bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3351095.3372834,
author = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and van Moorsel, Aad},
title = {The relationship between trust in AI and trustworthy machine learning technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372834},
doi = {10.1145/3351095.3372834},
abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {272–283},
numpages = {12},
keywords = {artificial intelligence, machine learning, trust, trustworthiness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{10.5555/3455716.3455964,
author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
title = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {248},
numpages = {43},
keywords = {climate change, deep learning, reinforcement learning, green computing, energy efficiency}
}

@article{10.1007/s10462-020-09876-9,
author = {Goh, G. D. and Sing, S. L. and Yeong, W. Y.},
title = {A review on machine learning in 3D printing: applications, potential, and challenges},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09876-9},
doi = {10.1007/s10462-020-09876-9},
abstract = {Additive manufacturing (AM) or 3D printing is growing rapidly in the manufacturing industry and has gained a lot of attention from various fields owing to its ability to fabricate parts with complex features. The reliability of the 3D printed parts has been the focus of the researchers to realize AM as an end-part production tool. Machine learning (ML) has been applied in various aspects of AM to improve the whole design and manufacturing workflow especially in the era of industry 4.0. In this review article, various types of ML techniques are first introduced. It is then followed by the discussion on their use in various aspects of AM such as design for 3D printing, material tuning, process optimization, in situ monitoring, cloud service, and cybersecurity. Potential applications in the biomedical, tissue engineering and building and construction will be highlighted. The challenges faced by ML in AM such as computational cost, standards for qualification and data acquisition techniques will also be discussed. In the authors’ perspective, in situ monitoring of AM processes will significantly benefit from the object detection ability of ML. As a large data set is crucial for ML, data sharing of AM would enable faster adoption of ML in AM. Standards for the shared data are needed to facilitate easy sharing of data. The use of ML in AM will become more mature and widely adopted as better data acquisition techniques and more powerful computer chips for ML are developed.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {63–94},
numpages = {32},
keywords = {Process optimization, Additive manufacturing, In-situ monitoring, 3D printing, Artificial intelligence, Machine learning}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {orchestration, machine learning, deep learning, IoT}
}

@inproceedings{10.1007/978-3-030-64580-9_46,
author = {Mandarano, Nicholas and Regis, Rommel G. and Bloom, Elizabeth},
title = {Machine Learning and Statistical Models for the Prevalence of Multiple Sclerosis},
year = {2020},
isbn = {978-3-030-64579-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64580-9_46},
doi = {10.1007/978-3-030-64580-9_46},
abstract = {Multiple sclerosis is an immune-mediated disease affecting approximately 2.5 million people worldwide. Its cause is unknown and there is currently no cure. MS tends to be more prevalent in countries that are farther from the equator. Moreover, smoking and obesity are believed to increase the risk of developing the disease. This article builds machine learning and statistical models for the MS prevalence in a country in terms of its distance from the equator and the smoking and adult obesity prevalence in that country. To build the models, the center of population of a country is approximated by finding a point on the surface of the Earth that minimizes a weighted sum of squared distances from the major cities of the country. This study compares the predictive performance of several machine learning models, including first and second order multiple regression, random forest, neural network and support vector regression.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II},
pages = {559–571},
numpages = {13},
keywords = {Constrained optimization, Geographic population center, Support vector regression, Neural network, Random forest, Multiple regression, Multiple sclerosis},
location = {Siena, Italy}
}

@inproceedings{10.1007/978-3-662-60292-8_1,
author = {Baltag, Alexandru and Li, Dazhu and Pedersen, Mina Young},
title = {On the Right Path: A Modal Logic for Supervised Learning},
year = {2019},
isbn = {978-3-662-60291-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-60292-8_1},
doi = {10.1007/978-3-662-60292-8_1},
abstract = {Formal learning theory formalizes the process of inferring a general result from examples, as in the case of inferring grammars from sentences when learning a language. Although empirical evidence suggests that children can learn a language without responding to the correction of linguistic mistakes, the importance of Teacher in many other paradigms is significant. Instead of focusing only on learner(s), this work develops a general framework—the supervised learning game (SLG)—to investigate the interaction between Teacher and Learner. In particular, our proposal highlights several interesting features of the agents: on the one hand, Learner may make mistakes in the learning process, and she may also ignore the potential relation between different hypotheses; on the other hand, Teacher is able to correct Learner’s mistakes, eliminate potential mistakes and point out the facts ignored by Learner. To reason about strategies in this game, we develop a modal logic of supervised learning (SLL). Broadly, this work takes a small step towards studying the interaction between graph games, logics and formal learning theory.},
booktitle = {Logic, Rationality, and Interaction: 7th International Workshop, LORI 2019, Chongqing, China, October 18–21, 2019, Proceedings},
pages = {1–14},
numpages = {14},
keywords = {Graph games, Undecidability, Dynamic logic, Modal logic, Formal learning theory},
location = {Chongqing, China}
}

@inproceedings{10.1145/3447548.3467201,
author = {Lee, Changhun and Kim, Soohyeok and Lim, Chiehyeon and Kim, Jayun and Kim, Yeji and Jung, Minyoung},
title = {Diet Planning with Machine Learning: Teacher-forced REINFORCE for Composition Compliance with Nutrition Enhancement},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467201},
doi = {10.1145/3447548.3467201},
abstract = {Diet planning is a basic and regular human activity. Previous studies have considered diet planning a combinatorial optimization problem to generate solutions that satisfy a diet's nutritional requirements. However, this approach does not consider the composition of diets, which is critical for diet recipients' to accept and enjoy menus with high nutritional quality. Without this consideration, feasible solutions for diet planning could not be provided in practice. This suggests the necessity of diet planning with machine learning, which extracts implicit composition patterns from real diet data and applies these patterns when generating diets. This work is original research that defines diet planning as a machine learning problem; we describe diets as sequence data and solve a controllable sequence generation problem. Specifically, we develop the Teacher-forced REINFORCE algorithm to connect neural machine translation and reinforcement learning for composition compliance with nutrition enhancement in diet generation. Through a real-world application to diet planning for children, we validated the superiority of our work over the traditional combinatorial optimization and modern machine learning approaches, as well as human (i.e., professional dietitians) performance. In addition, we construct and open the databases of menus and diets to motivate and promote further research and development of diet planning with machine learning. We believe this work with data science will contribute to solving economic and social problems associated with diet planning.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3150–3160},
numpages = {11},
keywords = {teacher-forcing, reinforce, machine learning, diet planning, controllable sequence generation},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1016/j.jksuci.2018.04.002,
author = {Raj S., Sridhar and M., Nandhini},
title = {Ensemble human movement sequence prediction model with Apriori based Probability Tree Classifier (APTC) and Bagged J48 on Machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.04.002},
doi = {10.1016/j.jksuci.2018.04.002},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = may,
pages = {408–416},
numpages = {9},
keywords = {Data mining, Machine learning, Spatial-temporal-social data, Trajectory analysis, Human movement sequence prediction}
}

@inproceedings{10.1145/3463274.3463809,
author = {T. Nguyen, Phuong and Di Ruscio, Davide and Di Rocco, Juri and Di Sipio, Claudio and Di Penta, Massimiliano},
title = {Adversarial Machine Learning: On the Resilience of Third-party Library Recommender Systems},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463809},
doi = {10.1145/3463274.3463809},
abstract = {In recent years, we have witnessed a dramatic increase in the application of Machine Learning algorithms in several domains, including the development of recommender systems for software engineering (RSSE). While researchers focused on the underpinning ML techniques to improve recommendation accuracy, little attention has been paid to make such systems robust and resilient to malicious data. By manipulating the algorithms’ training set, i.e., large open-source software (OSS) repositories, it would be possible to make recommender systems vulnerable to adversarial attacks. This paper presents an initial investigation of adversarial machine learning and its possible implications on RSSE. As a proof-of-concept, we show the extent to which the presence of manipulated data can have a negative impact on the outcomes of two state-of-the-art recommender systems which suggest third-party libraries to developers. Our work aims at raising awareness of adversarial techniques and their effects on the Software Engineering community. We also propose equipping recommender systems with the capability to learn to dodge adversarial activities.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {247–253},
numpages = {7},
keywords = {Recommender systems, Adversarial Machine Learning},
location = {Trondheim, Norway},
series = {EASE '21}
}

@inproceedings{10.1007/978-3-030-60290-1_45,
author = {Yan, Yu and Wang, Hongzhi and Zou, Jian and Wang, Yixuan},
title = {Automatic Document Data Storage System Based on Machine Learning},
year = {2020},
isbn = {978-3-030-60289-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60290-1_45},
doi = {10.1007/978-3-030-60290-1_45},
abstract = {Document storage management plays a significant role in the field of database. With the advent of big data, making storage management manually becomes more and more difficult and inefficient. There are many researchers to develop algorithms for automatic storage management(ASM). However, at present, no automatic systems or algorithms related to document data has been developed. In order to realize the ASM of document data, we firstly propose an automatic document data storage system (ADSML) based on machine learning, a user-friendly management system with high efficiency for achieving storage selection and index recommendation automatically. In this paper, we present the architecture and key techniques of ADSML, and describe three demo scenarios of our system.},
booktitle = {Web and Big Data: 4th International Joint Conference, APWeb-WAIM 2020, Tianjin, China,  September 18-20, 2020, Proceedings, Part II},
pages = {551–555},
numpages = {5},
keywords = {Machine learning, Storage selection, Index recommendation, Automatic management},
location = {Tianjin, China}
}

@inproceedings{10.1145/3396851.3402122,
author = {Kolluri, Ramachandra Rao and de Hoog, Julian},
title = {Adaptive Control Using Machine Learning for Distributed Storage in Microgrids},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402122},
doi = {10.1145/3396851.3402122},
abstract = {The falling costs of solar photovoltaic systems and energy storage mean that these are being increasingly deployed in microgrids across the globe. Distributed storage can provide benefits for its owner, but can also play a key role in improving microgrid stability and resilience. However, most approaches to date assume that a central authority can control multiple nodes or households in the network. This introduces significant communication and control requirements, and may introduce points of failure. In this work we provide an initial exploration of how a machine learning model, trained on optimal control solutions, can be used locally at each node in the network to emulate a similar behaviour. The aim is for the trained model to provide benefits both for the individual energy storage owners, while also enabling community-level cooperative behaviour - all in a low communication-overhead, privacy-preserving manner. It is experimentally shown that a neural network trained on limited data from optimal schedules can learn node interactions and network characteristics, and can achieve partial voltage regulation for the entire microgrid. This can be done while still achieving a small (3%) network-wide cost savings compared to a scenario in which no distributed storage is present, can be implemented only locally, and does not introduce any significant requirements for central control and communication.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {509–515},
numpages = {7},
keywords = {voltage regulation, solar photovoltaics, optimization, neural networks, microgrids, machine learning, Energy storage},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1007/978-3-030-96282-1_2,
author = {Ahn, Robert and Supakkul, Sam and Zhao, Liping and Kolluri, Kirthy and Hill, Tom and Chung, Lawrence},
title = {Validating Business Problem Hypotheses: A Goal-Oriented and Machine Learning-Based Approach},
year = {2021},
isbn = {978-3-030-96281-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-96282-1_2},
doi = {10.1007/978-3-030-96282-1_2},
abstract = {Validating an elicited business problem hindering a business goal is often more important than finding solutions. For example, validating the impact of a client’s account balance toward an unpaid loan would be critical as a bank can take some actions to mitigate the problem. However, business organizations face difficulties confirming whether some business events are against a business goal. Some challenges to validate a problem are discovering testable factors, preparing relevant data to validate, and analyzing relationships between the business events. This paper proposes a goal-oriented and Machine Learning(ML)-based framework, Gomphy, using a problem hypothesis for validating business problems. We present an ontology and a process, an entity modeling method for a problem hypothesis to find testable factors, a data preparation method to build an ML dataset, and an evaluation method to detect relationships among the business events and goals. To see the strength and weaknesses of our framework, we have validated banking events behind an unpaid loan in one bank as an empirical study. We feel that at least the proposed approach helps validate business events against a goal, providing some insights about the validated problem.},
booktitle = {Big Data – BigData 2021: 10th International Conference, Held as Part of the Services Conference Federation, SCF 2021, Virtual Event, December 10–14, 2021, Proceedings},
pages = {17–33},
numpages = {17}
}

@article{10.1007/s10845-016-1254-6,
author = {Malaca, Pedro and Rocha, Luis F. and Gomes, D. and Silva, Jo\~{a}o and Veiga, Germano},
title = {Online inspection system based on machine learning techniques: real case study of fabric textures classification for the automotive industry},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1254-6},
doi = {10.1007/s10845-016-1254-6},
abstract = {This paper focus on the classification, in real-time and under uncontrolled lighting, of fabric textures for the automotive industry. Many industrial processes have spatial constraints that limit the effective control of illumination of their vision based systems, hindering their effectiveness. The ability to overcome these problems using robust classification methods with suitable pre-processing techniques and choice of characteristics will increase the efficiency of this type of solutions with obvious production gains and thus economical. For this purpose, this paper studied and analyzed various pre-processing techniques, and selected the most appropriate fabric characteristics for the considered industrial case scenario. The methodology followed was based on the comparison of two different machine learning classifiers, ANN and SVM, using a large set of samples with a large variability of lightning conditions to faithfully simulate the industrial environment. The obtained solution shows the sensibility of ANN over SVM considering the number of features and the size of the training set, showing the better effectiveness and robustness of the last. The characteristics vector uses histogram equalization, Laws filter and Sobel filter, and multi-scale analysis. By using a correlation based method was possible to reduce the number of features used, achieving a better balanced between processing time and classification ratio.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {351–361},
numpages = {11},
keywords = {Uncontrolled illumination, Perception and recognition, Machine learning, Fabric analyses, Computer vision, Automotive industry}
}

@inproceedings{10.1145/3368826.3377912,
author = {Cowan, Meghan and Moreau, Thierry and Chen, Tianqi and Bornholt, James and Ceze, Luis},
title = {Automatic generation of high-performance quantized machine learning kernels},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377912},
doi = {10.1145/3368826.3377912},
abstract = {Quantization optimizes machine learning inference for resource constrained environments by reducing the precision of its computation. In the extreme, even single-bit computations can produce acceptable results at dramatically lower cost. But this ultra-low-precision quantization is difficult to exploit because extracting optimal performance requires hand-tuning both high-level scheduling decisions and low-level implementations. As a result, practitioners settle for a few predefined quantized kernels, sacrificing optimality and restricting their ability to adapt to new hardware. This paper presents a new automated approach to implementing quantized inference for machine learning models. We integrate the choice of how to lay out quantized values into the scheduling phase of a machine learning compiler, allowing it to be optimized in concert with tiling and parallelization decisions. After scheduling, we use program synthesis to automatically generate efficient low-level operator implementations for the desired precision and data layout. We scale up synthesis using a novel reduction sketch that exploits the structure of matrix multiplication. On a ResNet18 model, our generated code outperforms an optimized floating-point baseline by up to 3.9\texttimes{}, and a state-of-the-art quantized implementation by up to 16.6\texttimes{}.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {305–316},
numpages = {12},
keywords = {machine learning, quantization, synthesis},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@article{10.1016/j.asoc.2021.107622,
author = {Fu, Chao and Xu, Che and Xue, Min and Liu, Weiyong and Yang, Shanlin},
title = {Data-driven decision making based on evidential reasoning approach and machine learning algorithms},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {110},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107622},
doi = {10.1016/j.asoc.2021.107622},
journal = {Appl. Soft Comput.},
month = oct,
numpages = {10},
keywords = {Diagnosis of thyroid nodule, Learning of criterion weights, Machine learning algorithms, Evidential reasoning approach, Data-driven decision making}
}

@inproceedings{10.1145/3295500.3356164,
author = {Qin, Heyang and Zawad, Syed and Zhou, Yanqi and Yang, Lei and Zhao, Dongfang and Yan, Feng},
title = {Swift machine learning model serving scheduling: a region based reinforcement learning approach},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356164},
doi = {10.1145/3295500.3356164},
abstract = {The success of machine learning has prospered Machine-Learning-as-a-Service (MLaaS) - deploying trained machine learning (ML) models in cloud to provide low latency inference services at scale. To meet latency Service-Level-Objective (SLO), judicious parallelization at both request and operation levels is utterly important. However, existing ML systems (e.g., Tensorflow) and cloud ML serving platforms (e.g., SageMaker) are SLO-agnostic and rely on users to manually configure the parallelism. To provide low latency ML serving, this paper proposes a swift machine learning serving scheduling framework with a novel Region-based Reinforcement Learning (RRL) approach. RRL can efficiently identify the optimal parallelism configuration under different workloads by estimating performance of similar configurations with that of the known ones. We both theoretically and experimentally show that the RRL approach can outperform state-of-the-art approaches by finding near optimal solutions over 8 times faster while reducing inference latency up to 79.0% and reducing SLO violation up to 49.9%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {13},
numpages = {23},
keywords = {workload scheduling, service-level-objective (SLO), reinforcement learning, parallelism parameter tuning, model inference, machine-learning-as-a-service (MLaaS)},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3397166.3413470,
author = {Sequeira, Luis and Mahmoodi, Toktam},
title = {The role of machine learning for trajectory prediction in cooperative driving},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3413470},
doi = {10.1145/3397166.3413470},
abstract = {In this paper, we study the role that machine learning can play in cooperative driving. Given the increasing rate of connectivity in modern vehicles, and road infrastructure, cooperative driving is a promising first step in automated driving. The example scenario we explored in this paper, is coordinated lane merge, with data collection, test and evaluation all conducted in an automotive test track. The assumption is that vehicles are a mix of those equipped with communication units on board, i.e. connected vehicles, and those that are not connected. However, roadside cameras are connected and can capture all vehicles including those without connectivity. We develop a Traffic Orchestrator that suggests trajectories based on these two sources of information, i.e. connected vehicles, and connected roadside cameras. Recommended trajectories are built, which are then communicated back to the connected vehicles. We explore the use of different machine learning techniques in accurately and timely prediction of trajectories.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {368–373},
numpages = {6},
keywords = {reinforcement learning, mobile edge, machine learning, lane merge, intelligent transport system, cooperative driving, V2X communications, MEC, 5G},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/2808492.2808507,
author = {Wei-Ya, Ren and Pan, Liu and Guo-Hui, Li},
title = {Semi-supervised learning via nonnegative least squares regression},
year = {2015},
isbn = {9781450335287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808492.2808507},
doi = {10.1145/2808492.2808507},
abstract = {Graph construction is the key step in graph based semi-supervised learning methods. In order to improve the quality of the graph, we consider the nonnegative constraint and noise estimation based on the least squares regression (LSR). A novel graph construction method named nonnegative least squares regression (NLSR) is proposed in this paper. The nonnegative constraint is considered to eliminate subtractive combinations of coefficients and to improve the sparsity of the graph. We consider both small Guassian noise and sparse corrupted noise to improve the robustness of the proposed method. Experiments show that the nonnegative constraint plays the dominate role. Local and global consistency (LGC) is adopted as the semi-supervised learning method. The label propagation error rate is regarded as the evaluation criterion. Extensive experiments show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms in semi-supervised learning, especially in improving LSR method significantly.},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service},
articleno = {15},
numpages = {6},
keywords = {semi-supervised learning, non-negative constraint, least squares regression, graph construction},
location = {Zhangjiajie, Hunan, China},
series = {ICIMCS '15}
}

@article{10.1016/j.compbiomed.2020.104127,
author = {Yang, Zhijian and Olszewski, Daniel and He, Chujun and Pintea, Giulia and Lian, Jun and Chou, Tom and Chen, Ronald C. and Shtylla, Blerta},
title = {Machine learning and statistical prediction of patient quality-of-life after prostate radiation therapy},
year = {2021},
issue_date = {Feb 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104127},
doi = {10.1016/j.compbiomed.2020.104127},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {12},
keywords = {Prostate cancer, Organ sensitivity, Radiation therapy, Convolutional neural network, Machine learning}
}

@inproceedings{10.1007/978-3-030-61255-9_15,
author = {Cummings, Paul and Crooks, Andrew},
title = {Development of a Hybrid Machine Learning Agent Based Model for Optimization and Interpretability},
year = {2020},
isbn = {978-3-030-61254-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61255-9_15},
doi = {10.1007/978-3-030-61255-9_15},
abstract = {The use of agent-based models (ABMs) has become more widespread over the last two decades allowing resear chers to explore complex systems composed of heterogeneous and locally interacting entities. However, there are several challenges that the agent-based modeling community face. These relate to developing accurate measurements, minimizing a large complex parameter space and developing parsimonious yet accurate models. Machine Learning (ML), specifically deep reinforcement learning has the potential to generate new ways to explore complex models, which can enhance traditional computational paradigms such as agent-based modeling. Recently, ML algorithms have proved an important contribution to the determination of semi-optimal agent behavior strategies in complex environments. What is less clear is how these advances can be used to enhance existing ABMs. This paper presents Learning-based Actor-Interpreter State Representation (LAISR), a research effort that is designed to bridge ML agents with more traditional ABMs in order to generate semi-optimal multi-agent learning strategies. The resultant model, explored within a tactical game scenario, lies at the intersection of human and automated model design. The model can be decomposed into a format that automates aspects of the agent creation process, producing a resultant agent that creates its own optimal strategy and is interpretable to the designer. Our paper, therefore, acts as a bridge between traditional agent-based modeling and machine learning practices, designed purposefully to enhance the inclusion of ML-based agents in the agent-based modeling community.},
booktitle = {Social, Cultural, and Behavioral Modeling: 13th International Conference, SBP-BRiMS 2020, Washington, DC, USA, October 18–21, 2020, Proceedings},
pages = {151–160},
numpages = {10},
keywords = {Explainable artificial intelligence, Machine Learning, Agent-based modeling},
location = {Washington, DC, USA}
}

@inproceedings{10.1007/978-3-030-64793-3_3,
author = {Granados, Alonso and Miah, Mohammad Sujan and Ortiz, Anthony and Kiekintveld, Christopher},
title = {A Realistic Approach for Network Traffic Obfuscation Using Adversarial Machine Learning},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_3},
doi = {10.1007/978-3-030-64793-3_3},
abstract = {Adversaries are becoming more sophisticated and standard countermeasures such as encryption are no longer enough to prevent traffic analysis from revealing important information about a network. Advanced encryption techniques are intended to mitigate network information exposure, but they remain vulnerable to statistical analysis of traffic features. An adversary can classify different applications and protocols from the observable statistical properties, especially from the meta-data (e.g. packet size, timing, flow directions, etc.). Several approaches are already being developed to protect computer network infrastructure from attacks using traffic analysis, but none of them are fully effective. We investigate solutions based on obfuscating the patterns in network traffic to make it more difficult to accurately use classification to extract information such as protocols or applications in use. A key problem of using obfuscation methods is to determine an appropriate algorithm that introduces minimal changes but preserves the functionality of the protocol. We apply Adversarial Machine Learning techniques to find realistic small perturbations that can improve the security and privacy of a network against traffic analysis. We introduce a novel approach for generating adversarial examples that obtains state-of-the-art performance compared to previous approaches, while considering more realistic constraints on perturbations.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {45–57},
numpages = {13},
keywords = {Adversarial machine learning, Data obfuscation, Network data analysis},
location = {College Park, MD, USA}
}

@inproceedings{10.1145/3443467.3443855,
author = {Zhang, Xiaoyan and Wang, Xiaodong},
title = {An Effective Bridge Cracks Classification Method Based on Machine Learning},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443855},
doi = {10.1145/3443467.3443855},
abstract = {Crack is the most common threat to the safety of bridges. Historical data show that the safety accidents caused by cracks account for more than 90% of the total bridge disasters. After a long period of engineering practice and rigorous theoretical analysis, it was found that 0.3 mm is the maximum allowable for bridge cracks. If the width exceeds the limit, the integrity of the bridge will be destroyed, and even a collapse accident will occur. Therefore, it is important to identify cracks in bridge structure effectively and provide information for structural disaster reduction projects in time. With the development of machine learning, bridge crack detection and classification based on deep learning has been paid more attention. This paper designs a bridge crack classification algorithm based on convolution neural network and support vector machine. Firstly, the captured image data are divided into training set and test set. Secondly, they are preprocessing and extracted features by convolution neural network. Lastly, they are classified by SVM. The proposed algorithm can solve the problems of insufficient samples and low classification accuracy, and realizes the effective and accurate classification.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {790–794},
numpages = {5},
keywords = {Bridge crack, Convolution neural network, Machine learning, Support vector machine},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3445970.3451155,
author = {Kundu, Partha Pratim and Anatharaman, Lux and Truong-Huu, Tram},
title = {An Empirical Evaluation of Automated Machine Learning Techniques for Malware Detection},
year = {2021},
isbn = {9781450383202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445970.3451155},
doi = {10.1145/3445970.3451155},
abstract = {Nowadays, it is increasingly difficult even for a machine learning expert to incorporate all of the recent best practices into their modeling due to the fast development of state-of-the-art machine learning techniques. For the applications that handle big data sets, the complexity of the problem of choosing the best performing model with the best hyper-parameter setting becomes harder. In this work, we present an empirical evaluation of automated machine learning (AutoML) frameworks or techniques that aim to optimize hyper-parameters for machine learning models to achieve the best achievable performance. We apply AutoML techniques to the malware detection problem, which requires achieving the true positive rate as high as possible while reducing the false positive rate as low as possible. We adopt two AutoML frameworks, namely AutoGluon-Tabular and Microsoft Neural Network Intelligence (NNI) to optimize hyper-parameters of a Light Gradient Boosted Machine (LightGBM) model for classifying malware samples. We carry out extensive experiments on two data sets. The first data set is a publicly available data set (EMBER data set), that has been used as a benchmarking data set for many malware detection works. The second data set is a private data set we have acquired from a security company that provides recently-collected malware samples. We provide empirical analysis and performance comparison of the two AutoML frameworks. The experimental results show that AutoML frameworks could identify the set of hyper-parameters that significantly outperform the performance of the model with the known best performing hyper-parameter setting and improve the performance of a LightGBM classifier with respect to the true positive rate from $86.8%$ to $90%$ at $0.1%$ of false positive rate on EMBER data set and from $80.8%$ to $87.4%$ on the private data set.},
booktitle = {Proceedings of the 2021 ACM Workshop on Security and Privacy Analytics},
pages = {75–81},
numpages = {7},
keywords = {malware detection, hyper-parameter optimization, automated machine learning},
location = {Virtual Event, USA},
series = {IWSPA '21}
}

@inproceedings{10.1109/ICSE43902.2021.00033,
author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
title = {An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00033},
doi = {10.1109/ICSE43902.2021.00033},
abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {238–250},
numpages = {13},
keywords = {technical debt, software repository mining, refactoring, machine learning systems, empirical studies},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3383972.3384031,
author = {Zeb, Babar and Khan, Aimal and Khan, Younas and Masood, Muhammad Faisal and Tahir, Iqra and Asad, Muhammad},
title = {Towards the Selection of the Best Machine Learning Techniques and Methods for Urinalysis},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384031},
doi = {10.1145/3383972.3384031},
abstract = {Urinalysis is a significant technique used for determining and inspecting the urinary system. Urine has numerous chemical materials secreted; these materials can be used to diagnose diseases and conditions such as urinary tract infection, diabetes, kidney diseases, and pregnancy, at the earliest. Accessing medical care and health screenings are quite essential, but it has become extremely difficult since screening techniques are either very expensive or convoluted for people of low-income communities. Despite the fact, that numerous urinalysis methods and techniques have been brought forth by researchers over the years, no research has been conducted to scrutinize and review state-of-the-art developments in the stated area. Hence, this research aims to conduct a Systematic Literature Review of urinalysis methods proposed and assessed from 2007 to 2019 and recognizes 33 studies. This leads to the identification of 10 methods, 8 technologies, 12 challenges, and 10 diseases. An insight analysis of the identified methods and models reveals that Genetic Based Fuzzy Classifying Method and Automatic Urinary Particle Recognition Method are the most optimum options due to the fact that their computational time is minimum as well as they do not require any supportive hardware. Moreover, the analysis of technologies reveals that Mobile App (Augmented Reality) is the best option among the identified technologies. It has also been discovered that machine/deep learning classification techniques have been used to classify the sample and provide reliable and accurate results within time. Nevertheless, a complete analysis of methods and technologies has been presented. The findings of this article are extremely useful for practitioners as well as academics of the area.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {127–133},
numpages = {7},
keywords = {Urinalysis Technologies, Urinalysis Strips, Urinalysis Methods, Urinalysis, Machine learning Application in Urinalysis, Deep Learning},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1145/3419111.3421296,
author = {Viswanathan, Raajay and Balasubramanian, Arjun and Akella, Aditya},
title = {Network-accelerated distributed machine learning for multi-tenant settings},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421296},
doi = {10.1145/3419111.3421296},
abstract = {Many distributed machine learning (DML) workloads are increasingly being run in shared clusters. Training in such clusters can be impeded by unexpected compute and network contention, resulting in stragglers. We present MLfabric, a contention-aware DML system that manages the performance of a DML job running in a shared cluster. The DML application hands all network communication (gradient and model transfers) to the MLfabric communication library. MLfabric then carefully orders transfers to improve convergence, opportunistically aggregates them at idle DML workers to improve resource efficiency, and replicates them to support new notions of fault tolerance, while systematically accounting for compute stragglers and network contention. We find that MLfabric achieves up to 3x speed-up in training large deep learning models in realistic dynamic cluster settings.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {447–461},
numpages = {15},
keywords = {multi-tenancy, in-network computation, distributed machine learning},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings{10.1145/3443279.3443309,
author = {To, Huy Quoc and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy and Nguyen, Anh Gia-Tuan},
title = {Gender Prediction Based on Vietnamese Names with Machine Learning Techniques},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443309},
doi = {10.1145/3443279.3443309},
abstract = {As biological gender is one of the aspects of presenting individual human, much work has been done on gender classification based on people names. The proposals for English and Chinese languages are tremendous; still, there have been few works done for Vietnamese so far. We propose a new dataset for gender prediction based on Vietnamese names. This dataset comprises over 26,000 full names annotated with genders. This dataset is available on our website for research purposes. In addition, this paper describes six machine learning algorithms (Support Vector Machine, Multinomial Naive Bayes, Bernoulli Naive Bayes, Decision Tree, Random Forrest and Logistic Regression) and a deep learning model (LSTM) with fastText word embedding for gender prediction on Vietnamese names. We create a dataset and investigate the impact of each name component on detecting gender. As a result, the best F1-score that we have achieved is up to 96% on LSTM model and we generate a web API based on our trained model.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {55–60},
numpages = {6},
keywords = {Text Classification, Machine Learning, Gender Prediction, Deep Learning},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@inproceedings{10.1145/3414274.3414275,
author = {Wang, Hongming},
title = {Stock Price Prediction Based on Machine Learning Approaches},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414275},
doi = {10.1145/3414274.3414275},
abstract = {Research of quantitate investment on stock price prediction is effective to help investors increase profits. Recently, technologies of machine learning have been well applied to explore the issue of stock trading. In this paper, Logistic Regression and Support Vector Machines (SVM) were adopted to solve the problem of predicting the trend of stock movements. The experiment showed that these two models could be effectively used in the stock market of China. Returns based on strategies we constructed were significantly better than the HS300 index. In different models, we analyzed the relationship between stock returns and different models. It found that the SVM model results are optimal. The annual return of the strategy based on SVM reached 17.13% and the maximum Drawdown was 0.32. In the future, we will not only focus on the stock market, but also plan to apply this strategy to other investment fields, such as trading of digital currency. We will also use other algorithms for research and comparison, such as andom forests, XGBoost.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {1–5},
numpages = {5},
keywords = {Support Vector Machine, Stock price prediction, Maximum Drawdown, Logistic Regression, Annual return},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1007/978-3-030-78710-3_13,
author = {Von Zuben, Andre and Heckman, Kylie and Viana, Felipe A. C. and Perotti, Luigi E.},
title = {A Multi-step Machine Learning Approach for Short Axis MR Images Segmentation},
year = {2021},
isbn = {978-3-030-78709-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78710-3_13},
doi = {10.1007/978-3-030-78710-3_13},
abstract = {Segmentation of cardiac magnetic resonance (cMR) images is often the first step necessary to compute common diagnostic biomarkers, such as myocardial mass and left ventricle (LV) ejection fraction. Often image segmentation and analysis require significant, time-consuming user input. Machine learning has been increasingly adopted to automatically segment medical images to lessen the burden on image segmentation and image analysis for model construction and validation. In this work we present a multi-step machine learning approach to segment short axis cMR images based on a heart locator and the weighted average of 2D and 2D++ UNets. The presence of a heart locator led to more accurate results and allowed to increase the neural network training batch size. Finally, the obtained segmentations are post-processed using spline interpolation and the Loop scheme to generate left ventricular endocardial and epicardial surfaces at the end of diastole and end of systole.},
booktitle = {Functional Imaging and Modeling of the Heart: 11th International Conference, FIMH 2021, Stanford, CA, USA, June 21-25, 2021, Proceedings},
pages = {122–133},
numpages = {12},
keywords = {Image segmentation, Machine learning, Heart locator, Weighted average, Left ventricular surfaces},
location = {Stanford, CA, USA}
}

@article{10.1016/j.engappai.2017.04.003,
author = {Altnel, Berna and Can Ganiz, Murat and Diri, Banu},
title = {Instance labeling in semi-supervised learning with meaning values of words},
year = {2017},
issue_date = {June 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {62},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2017.04.003},
doi = {10.1016/j.engappai.2017.04.003},
abstract = {In supervised learning systems; only labeled samples are used for building a classifier that is then used to predict the class labels of the unlabeled samples. However, obtaining labeled data is very expensive, time consuming and difficult in real-life practical situations as labeling a data set requires the effort of a human expert. On the other side, unlabeled data are often plentiful which makes it relatively inexpensive and easier to obtain. Semi-Supervised Learning methods strive to utilize this plentiful source of unlabeled examples to increase the learning capacity of the classifier particularly when amount of labeled examples are restricted. Since SSL techniques usually reach higher accuracy and require less human effort, they attract a substantial amount of attention both in practical applications and theoretical research. A novel semi-supervised methodology is offered in this study. This algorithm utilizes a new method to predict the class labels of unlabeled examples in a corpus and incorporate them into the training set to build a better classifier. The approach presented here depends on a meaning calculation, which computes the words meaning scores in the scope of classes. Meaning computation is constructed on the Helmholtz principle and utilized to various applications in the field of text mining like feature extraction, information retrieval and document summarization. Nevertheless, according to the literature, ILBOM is the first work which uses meaning calculation in a semi-supervised way to construct a semantic smoothing kernel for Support Vector Machines (SVM). Evaluation of the proposed methodology is done by performing various experiments on standard textual datasets. ILBOM's experimental results are compared with three baseline algorithms including SVM using linear kernel which is one of the most frequently used algorithms in text classification field. Experimental results show that labeling unlabeled instances based on meaning scores of words to augment the training set is valuable, and increases the classification accuracy on previously unseen test instances significantly.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {152–163},
numpages = {12},
keywords = {Text classification, Semi-supervised learning, Semantic kernel, Instance labeling, Helmholtz principle}
}

@article{10.1145/3447814,
author = {Jia, Xiaowei and Willard, Jared and Karpatne, Anuj and Read, Jordan S. and Zwart, Jacob A. and Steinbach, Michael and Kumar, Vipin},
title = {Physics-Guided Machine Learning for Scientific Discovery: An Application in Simulating Lake Temperature Profiles},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447814},
doi = {10.1145/3447814},
abstract = {Physics-based models are often used to study engineering and environmental systems. The ability to model these systems is the key to achieving our future environmental sustainability and improving the quality of human life. This article focuses on simulating lake water temperature, which is critical for understanding the impact of changing climate on aquatic ecosystems and assisting in aquatic resource management decisions. General Lake Model (GLM) is a state-of-the-art physics-based model used for addressing such problems. However, like other physics-based models used for studying scientific and engineering systems, it has several well-known limitations due to simplified representations of the physical processes being modeled or challenges in selecting appropriate parameters. While state-of-the-art machine learning models can sometimes outperform physics-based models given ample amount of training data, they can produce results that are physically inconsistent. This article proposes a physics-guided recurrent neural network model (PGRNN) that combines RNNs and physics-based models to leverage their complementary strengths and improves the modeling of physical processes. Specifically, we show that a PGRNN can improve prediction accuracy over that of physics-based models (by over 20% even with very little training data), while generating outputs consistent with physical laws. An important aspect of our PGRNN approach lies in its ability to incorporate the knowledge encoded in physics-based models. This allows training the PGRNN model using very few true observed data while also ensuring high prediction accuracy. Although we present and evaluate this methodology in the context of modeling the dynamics of temperature in lakes, it is applicable more widely to a range of scientific and engineering disciplines where physics-based (also known as mechanistic) models are used.},
journal = {ACM/IMS Trans. Data Sci.},
month = may,
articleno = {20},
numpages = {26},
keywords = {scientific discovery, deep learning, Physics-guided machine learning}
}

@phdthesis{10.5555/AAI30378048,
author = {Chen, Wei and Seppe, Kuehn, and Lance, Cooper, and Diwakar, Shukla,},
advisor = {L, Ferguson, Andrew},
title = {Machine Learning of Molecular Conformations, Kinetics and Beyond},
year = {2020},
isbn = {9798374403336},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {Machine learning has been playing an increasingly important role in many ﬁelds of computational physics, including molecular simulation. In this thesis, I report my work on machine learning method developments for molecular simulation, and their applications on learning conformations and kinetics for molecular systems. First, I present a deep-learning based accelerated sampling framework termed "Molecular Enhanced Sampling with Autoencoders" (MESA) that utilizes high-variance collective variables (CVs) to guide sampling. By applying the framework on some molecular systems, I show its efficiency for exploring conﬁguration space, and discuss several aspects for improvements. Second, I build a deep-learning based model termed "State-free Reversible VAMPnets" (SRVs) to learn slow CVs that govern the dominant kinetics of the system. By comparing SRVs with the existing kernel based method, I show that SRVs are more accurate, less sensitive to feature selection and feature scaling, and more computationally efficient. Also, extensive mathematical analysis provides theoretical guarantees for the correctness of the SRV model. Combined with Markov state models (MSMs), I show that CVs discovered by SRVs serve as excellent basis for constructing MSMs that enables high-resolution kinetics analysis, which opens the door to applications for many important physical processes. In sum, this thesis establishes new machine learning methods for learning molecular conformations, kinetics and other physical properties, builds connections between theoretical developments and computational applications, and provides new insights for both machine learning and computational physics communities.},
note = {AAI30378048}
}

@inproceedings{10.1007/978-3-031-23793-5_28,
author = {Godard, Pierre and L\"{o}ser, Kevin and Allauzen, Alexandre and Besacier, Laurent and Yvon, Fran\c{c}ois},
title = {Unsupervised Learning of&nbsp;Word Segmentation: Does Tone Matter?},
year = {2018},
isbn = {978-3-031-23792-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-23793-5_28},
doi = {10.1007/978-3-031-23793-5_28},
abstract = {In this paper, we investigate the usefulness of tonal features for unsupervised word discovery, taking Mboshi, a low-resource tonal language from the Bantu family, as our main target language. In a preliminary step, we show that tone annotation improves the performance of supervised learning when using a simplified representation of the data. To leverage this information in an unsupervised setting, we then present a probabilistic model based on a hierarchical Pitman-Yor process that incorporates tonal representations in its backoff structure. We compare our model with a tone-agnostic baseline and analyze if and how tone helps unsupervised segmentation on our small dataset.},
booktitle = {Computational Linguistics and Intelligent Text Processing: 19th International Conference, CICLing 2018, Hanoi, Vietnam, March 18–24, 2018, Revised Selected Papers, Part I},
pages = {349–359},
numpages = {11},
keywords = {Hierarchical Pitman-Yor process, Mboshi, Tonal features},
location = {Hanoi, Vietnam}
}

@article{10.1007/s11042-021-10906-z,
author = {Shahid, Eman and Arain, Qasim Ali},
title = {Indoor positioning: “an image-based crowdsource machine learning approach”},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {17},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10906-z},
doi = {10.1007/s11042-021-10906-z},
abstract = {Various technologies have been utilized today for recognizing client or user in the indoor areas. These technologies incorporate RSSI, Bluetooth Low Energy Beacons, Ultrasound waves, Vision-based advances, for example, fixed camera recordings QR codes, remote gadgets, etc. RSSI fingerprinting technique requires more effort and it is also expensive to be used for indoor localization frameworks working in real-time. In this research, indoor localization based on images is investigated as an option in contrast to other indoor positioning techniques using these days. Image-based indoor positioning is more affordable than RSSI based technologies being utilized. A mobile phone camera is utilized to take the pictures of area inside the building to find the user inside the building. Sensor data from various sensors isn’t required or no extra framework is required to find the client in the building utilizing indoor positioning based on an image. Microsoft Azure Custom Vision Services are utilized to locate the client; MS Azure classifies the pictures in one of the labels made. Strategy’s attainability is demonstrated by various investigations and accomplished accuracy and review is recorded above 90%. The average precision of the trained model is recorded above 95%.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {26213–26235},
numpages = {23},
keywords = {Non-line-of-sight, Locality sensitive hashing learning vector quantization, Location-based services, Wireless sensor network, Wireless local area networking, Received signal strength indication, Radio frequency identification, Robust crowdsourcing-based indoor localization system radio frequency, Principal component analysis scale-invariant feature transform, Indoor positioning system, Global positioning system}
}

@inproceedings{10.1145/3219819.3219838,
author = {Goh, Garrett B. and Siegel, Charles and Vishnu, Abhinav and Hodas, Nathan},
title = {Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219838},
doi = {10.1145/3219819.3219838},
abstract = {With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge and enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {302–310},
numpages = {9},
keywords = {weak supervised learning, transfer learning, natural language processing, computer vision, cheminformatics, bioinformatics},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3459637.3482397,
author = {Wang, Yongjie and Ding, Qinxu and Wang, Ke and Liu, Yue and Wu, Xingyu and Wang, Jinglong and Liu, Yong and Miao, Chunyan},
title = {The Skyline of Counterfactual Explanations for Machine Learning Decision Models},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482397},
doi = {10.1145/3459637.3482397},
abstract = {Counterfactual explanations are minimum changes of a given input to alter the original prediction by a machine learning model, usually from an undesirable prediction to a desirable one. Previous works frame this problem as a constrained cost minimization, where the cost is defined as L1/L2 distance (or variants) over multiple features to measure the change. In real-life applications, features of different types are hardly comparable and it is difficult to measure the changes of heterogeneous features by a single cost function. Moreover, existing approaches do not support interactive exploration of counterfactual explanations. To address above issues, we propose the skyline counterfactual explanations that define the skyline of counterfactual explanations as all non-dominated changes. We solve this problem as multi-objective optimization over actionable features. This approach does not require any cost function over heterogeneous features. With the skyline, the user can interactively and incrementally refine their goals on the features and magnitudes to be changed, especially when lacking prior knowledge to express their needs precisely. Intensive experiment results on three real-life datasets demonstrate that the skyline method provides a friendly way for finding interesting counterfactual explanations, and achieves superior results compared to the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2030–2039},
numpages = {10},
keywords = {skyline, multi-objective optimization, interactive query, counterfactual explanations},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1007/s00500-021-05926-8,
author = {AlZubi, Ahmad Ali and Al-Maitah, Mohammed and Alarifi, Abdulaziz},
title = {Cyber-attack detection in healthcare using cyber-physical system and machine learning techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05926-8},
doi = {10.1007/s00500-021-05926-8},
abstract = {Cyber-physical systems have been extensively utilized in healthcare domains to deliver high-quality patient treatment in multifaceted clinical scenarios. The medical device’ heterogeneity involved in these systems (mobile devices and body sensor nodes) introduces enormous attack surfaces and therefore necessitates effective security solutions for these complex environments. Hence, in this study, the cognitive machine learning assisted Attack Detection Framework has been proposed to share healthcare data securely. The Healthcare Cyber-Physical Systems will be proficient in spreading the collected data to cloud storage. Machine learning models predict cyber-attack behavior, and processing this data can offer healthcare specialists decision support. This proposed approach is based on a patient-centric design that safeguards the information on a trusted device like the end-users mobile phones and end-user control data sharing access. Experimental results demonstrate that our suggested model achieves an attack prediction ratio of 96.5%, an accuracy ratio of 98.2%, an efficiency ratio of 97.8%, less delay of 21.3%, and a communication cost of 18.9% to other existing models.},
journal = {Soft Comput.},
month = sep,
pages = {12319–12332},
numpages = {14},
keywords = {Healthcare system, Attack or malicious detection, Machine learning, Cyber-physical system}
}

@inproceedings{10.1145/3036290.3036297,
author = {Joshi, Jetendra and Polepally, Siddhanth and Kumar, Pranith and Samineni, Rohith and Rahul, S. R. and Sumedh, Kaushal and Tej, Dandu Geet Kamal and Rajapriya, Vishal},
title = {Machine Learning Based Cloud Integrated Farming},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036297},
doi = {10.1145/3036290.3036297},
abstract = {With the increasing population, the demand for more agricultural production is only surged. This increasing demand can only be attained with progressing technology. Internet of Things (IoT) is dramatically advancing the way we live our life by full-scale control over data with minimal human involvement. Using IoT to meet this high demand for production is achieved. In this paper we propose a bot that can be used for small scale farming in areas like gardens or backyard. A simple web application is provided for the user to decide the plants to be farmed and the bot does rest of the work. This bot can plant the seeds, water each plant at required intervals and even plot the weed to bury it. A database is provided with the information about several parameters to be taken care of for each kind of plant. Different sensors are used to sense the properties of soil and environment which can be used to anticipate the near changes and take necessary steps. Image processing is being used for detection and prevention of weed growth. We adopted Bayesian methods of machine learning to efficiently estimate the performance parameters by probability distribution.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {1–6},
numpages = {6},
keywords = {weed removal, smart agriculture, shadow removal, Machine Learning, Internet of Things (IoT), Bayesian Statistics},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@article{10.3233/IP-200256,
author = {Ingrams, Alex and Giest, Sarah and Grimmelikhuijsen, Stephan},
title = {A machine learning approach to open public comments for policymaking},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {4},
issn = {1570-1255},
url = {https://doi.org/10.3233/IP-200256},
doi = {10.3233/IP-200256},
abstract = {In this paper, the author argues that the conflict between the copious amount of digital data processed by public organisations and the need for policy-relevant insights to aid public participation constitutes a ‘public information paradox’. Machine learning (ML) approaches may offer one solution to this paradox through algorithms that transparently collect and use statistical modelling to provide insights for policymakers. Such an approach is tested in this paper. The test involves applying an unsupervised machine learning approach with latent Dirichlet allocation (LDA) analysis of thousands of public comments submitted to the United States Transport Security Administration (TSA) on a 2013 proposed regulation for the use of new full body imaging scanners in airport security terminals. The analysis results in salient topic clusters that could be used by policymakers to understand large amounts of text such as in an open public comments process. The results are compared with the actual final proposed TSA rule, and the author reflects on new questions raised for transparency by the implementation of ML in open rule-making processes.},
journal = {Info. Pol.},
month = jan,
pages = {433–448},
numpages = {16},
keywords = {algorithms, machine learning, open regulation, transparency, Public participation}
}

@article{10.1007/s42979-021-00751-0,
author = {Nath, Rajdeep Kumar and Thapliyal, Himanshu and Humble, Travis S.},
title = {A Review of Machine Learning Classification Using Quantum Annealing for Real-World Applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {5},
url = {https://doi.org/10.1007/s42979-021-00751-0},
doi = {10.1007/s42979-021-00751-0},
abstract = {Optimizing the training of a machine learning pipeline helps in reducing training costs and improving model performance. One such optimizing strategy is quantum annealing, which is an emerging computing paradigm that has shown potential in optimizing the training of a machine learning model. The implementation of a physical quantum annealer has been realized by D-wave systems and is available to the research community for experiments. Recent experimental results on a variety of machine learning applications using quantum annealing have shown interesting results where the performance of classical machine learning techniques is limited by limited training data and high dimensional features. This article explores the application of D-wave’s quantum annealer for optimizing machine learning pipelines for real-world classification problems. We review the application domains on which a physical quantum annealer has been used to train machine learning classifiers. We discuss and analyze the experiments performed on the D-Wave quantum annealer for applications such as image recognition, remote sensing imagery, computational biology, and particle physics. We discuss the possible advantages and the problems for which quantum annealing is likely to be advantageous over classical computation.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {11},
keywords = {Quantum computing, Quantum annealing, Optimization, Machine learning, Classification}
}

@inproceedings{10.1145/3105831.3105845,
author = {El-Kilany, Ayman and El Tazi, Neamat and Ezzat, Ehab},
title = {Building Relation Extraction Templates via Unsupervised Learning},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105845},
doi = {10.1145/3105831.3105845},
abstract = {The vast amount of text published daily over the internet pose an opportunity to build unsupervised text mining models with a better or a comparable performance than existing models. In this paper, we investigate the problem of relation extraction and generation from text using an unsupervised model learned from news published online. We propose a clustering-based method to build a dataset of relations examples. News articles are clustered and once a cluster of sentences for each event in each piece of news is formed, relations between important entities in each event cluster are extracted and considered as examples of relations. Relations examples are used to build extraction templates in order to extract and generate readable relations summaries from new instances of news. The proposed unsupervised relation extraction and generation method is evaluated against multiple methods for relation extraction over different datasets where the proposed method has shown a comparable performance.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {228–234},
numpages = {7},
keywords = {Unsupervised Learning, Relation Extraction, Ranking, Clustering},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@phdthesis{10.5555/AAI28157412,
author = {Som, Anirudh and Krishnamurthi, Narayanan and Spanias, Andreas and Li, Baoxin},
advisor = {Pavan, Turaga,},
title = {Building Invariant, Robust and Stable Machine Learning Systems Using Geometry and Topology},
year = {2020},
isbn = {9798557029346},
publisher = {Arizona State University},
address = {USA},
abstract = {Over the past decade, machine learning research has made great strides and significant impact in several fields. Its success is greatly attributed to the development of effective machine learning algorithms like deep neural networks (a.k.a. deep learning), availability of large-scale databases and access to specialized hardware like Graphic Processing Units. When designing and training machine learning systems, researchers often assume access to large quantities of data that capture different possible variations. Variations in the data is needed to incorporate desired invariance and robustness properties in the machine learning system, especially in the case of deep learning algorithms. However, it is very difficult to gather such data in a real-world setting. For example, in certain medical/healthcare applications, it is very challenging to have access to data from all possible scenarios or with the necessary amount of variations as required to train the system. Additionally, the over-parameterized and unconstrained nature of deep neural networks can cause them to be poorly trained and in many cases over-confident which, in turn, can hamper their reliability and generalizability. This dissertation is a compendium of my research efforts to address the above challenges. I propose building invariant feature representations by wedding concepts from topological data analysis and Riemannian geometry, that automatically incorporate the desired invariance properties for different computer vision applications. I discuss how deep learning can be used to address some of the common challenges faced when working with topological data analysis methods. I describe alternative learning strategies based on unsupervised learning and transfer learning to address issues like dataset shifts and limited training data. Finally, I discuss my preliminary work on applying simple orthogonal constraints on deep learning feature representations to help develop more reliable and better calibrated models.},
note = {AAI28157412}
}

@article{10.1145/3450494,
author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi (Jason) and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
title = {A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3450494},
doi = {10.1145/3450494},
abstract = {The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing numbers of smart devices and improved hardware, there is interest in performing model training on the device. Given this surge in interest, a comprehensive survey of the field from a device-agnostic perspective sets the stage for both understanding the state of the art and for identifying open challenges and future avenues of research. However, on-device learning is an expansive field with connections to a large number of related topics in AI and machine learning (including online learning, model adaptation, one/few-shot learning, etc.). Hence, covering such a large number of topics in a single survey is impractical. This survey finds a middle ground by reformulating the problem of on-device learning as resource constrained learning where the resources are compute and memory. This reformulation allows tools, techniques, and algorithms from a wide variety of research areas to be compared equitably. In addition to summarizing the state of the art, the survey also identifies a number of challenges and next steps for both the algorithmic and theoretical aspects of on-device learning.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {15},
numpages = {49}
}

@inproceedings{10.1145/3425174.3425226,
author = {Santos, Sebasti\~{a}o H. N. and da Silveira, Beatriz Nogueira Carvalho and Andrade, Stev\~{a}o A. and Delamaro, M\'{a}rcio and Souza, Simone R. S.},
title = {An Experimental Study on Applying Metamorphic Testing in Machine Learning Applications},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425226},
doi = {10.1145/3425174.3425226},
abstract = {Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {98–106},
numpages = {9},
keywords = {Metamorphic Test, Machine Learning, Experimental Study},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1007/s00146-020-01045-4,
author = {Hagendorff, Thilo},
title = {Forbidden knowledge in machine learning reflections on the limits of research and publication},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {3},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-020-01045-4},
doi = {10.1007/s00146-020-01045-4},
abstract = {Certain research strands can yield “forbidden knowledge”. This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance, with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up till now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline deliberations on how to deal with questions concerning the dissemination of such information. It proposes a tentative ethical framework for the machine learning community on how to deal with forbidden knowledge and dual-use applications.},
journal = {AI Soc.},
month = sep,
pages = {767–781},
numpages = {15},
keywords = {Publication norms, Dual-use, Governance, Artificial intelligence, Machine learning, Forbidden knowledge}
}

@phdthesis{10.5555/AAI28264908,
author = {Ferleger, Benjamin I. and Ko, Andrew},
advisor = {J., Chizeck, Howard and A, Herron, Jeffrey},
title = {Machine Learning to Optimize Embedded Adaptive Deep Brain Stimulation},
year = {2020},
isbn = {9798569995226},
publisher = {University of Washington},
abstract = {This thesis focuses on the development and application of novel machine learning approaches to the problem of optimization in adaptive deep brain stimulation. As brain-computer and brain-machine interfacing has rapidly developed in the past few years, attention in the relevant research has shifted from proof-of-concept to proof-of-feasibility. One of the first and, therefore, best-developed neurotechnologies is deep brain stimulation (DBS). DBS is a surgical intervention prescribed for several treatment-refractory neurological conditions. First, a stimulating electrode is chronically implanted into a condition-specific deep brain structure. The parameters of the stimulation provided by this electrode are then set by a clinician. Stimulation remains at these parameters continuously unless a patient actively chooses to disable their treatment. As has been repeatedly demonstrated, DBS is a safe and effective treatment for a number of movement disorders and is under active investigation for potential use in several psychiatric conditions. DBS, however, is not a panacea. Battery replacement requires a revision surgery, and even rechargeable systems' batteries must generally be replaced at least once within a device's lifetime. Additionally, DBS therapy is associated with a number of unpleasant and sometimes dangerous side effects. These side effects can range from transient paresthesias to episodes of depression and mania, and are broadly correlated with high levels of stimulation over long periods of time. In addition to concerns over battery life and side effects, the programming procedure for DBS is based primarily on a back-and-forth between clinicians and patients in a clinical setting. If we define "optimal" treatment as the most complete suppression of symptoms with the least manifestation of side effects, then achieving the corresponding settings is the goal of this procedure. The time consuming nature of this procedure, when considered in the context of clinical time constraints and patient fatigue, means that the parameters selected are far more likely to be the first passable setting than the truly optimal one. Patients are generally given the ability to disable their stimulation or select from a small range of amplitudes, but cannot actively reprogram their devices outside of a clinical setting. One technique with the potential to alleviate concerns about side effects and battery life is adaptive deep brain stimulation (aDBS). aDBS refers to any method that uses feedback on a patient's state to modulate stimulation parameters in real time. This feedback could come in the form of gyroscope and accelerometer data in the case of movement disorders, or could be derived from neural signals that are correlated with the onset of symptoms. These signals are then processed and meaningful features extracted from them, which may in turn be used to determine the appropriate stimulation parameters. This ensures that stimulation is only applied as needed. It is important to note that aDBS systems also intrinsically expand the state space forDBS programming, potentially adding yet more complexity to an already laborious procedure. As directional leads become more common in DBS devices, this expanded programming state space further reduces the likelihood of optimal settings being reached. A twin requirement to developing effective aDBS systems is thus the design of a streamlined procedure for parameter optimization in DBS programming. This may be accomplished through the introduction of an automated programming pipeline. Through the collection of quantified data on symptom severity through the use of gyroscope or accelerometer data and the digitization of patient feedback on side effects, this pipeline could considerably speed testing. In addition, the digitization of the data required to analyze aDBS parameter performance could be integrated with modern optimization techniques, such that a personalized optimal treatment may be determined to within an increased degree of certainty. This work details approaches for resolving these deeply interwoven problems in aDBS treatment through insights from the fields of machine learning and optimization. We begin by considering the current state of the art in adaptive deep brain stimulation, its accomplishments, but especially its limitations. The principal limitations are: a general reliance on distributed systems that hinder free movement in patients; a focus solely on computationally inexpensive, but potentially suboptimal, binary aDBS control strategies; and the lack of an effective pipeline to deploy optimization methods during or after programming. Furthermore, recognition that these limitations are fundamentally interrelated implies that an integrated approach is required. The three key developments detailed in this work are thus themselves closely interrelated. Despite minor reductions in power savings, fully embedded binary aDBS is specifically de-signed to maximize therapeutic efficacy and ease of programming. Our results demonstrate that such a system is prepared for widespread studies in movement disorders. Our graded aDBS system yielded inconclusive results with regards to power savings and therapeutic efficacy. However, basing our approach to feature selection for symptom estimation from neural data on a model-free foundation has yielded promising evidence for relying on data-driven feature extraction. This approach to feature extraction intrinsically requires less direct programming, and instead maximizes the insights that may be gained from the data itself. Our development of a pipeline for automated programming of DBS parameters based on inertial measurements and patient feedback on side effects was designed to generalize easily into future integration with aDBS programming procedures. Finally, our computational approach to extracting information from a tablet- and mobile-based application demonstrates that semi- or fully-automated remote symptom assessment has the potential to significantly improve the future delivery of optimized, individualized treatment. Throughout this work, integration is a key component of discussion and consideration. Each result extracted from the developments discussed herein represents a small step away from the current standard of care. Considered individually, these steps would be taken incompletely different directions. It is the hope of the author that this work instead constitutes a realignment of these disparate goals and an explicit recognition of their inter-relatedness. Only by treating these problems as different faces of the same die can we arrive at truly optimized personalized treatment in aDBS.},
note = {AAI28264908}
}

@inproceedings{10.1007/978-3-030-68007-7_4,
author = {Krause, Thomas and Andrade, Bruno G. N. and Afli, Haithem and Wang, Haiying and Zheng, Huiru and Hemmje, Matthias L.},
title = {Understanding the Role of (Advanced) Machine Learning in Metagenomic Workflows},
year = {2020},
isbn = {978-3-030-68006-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68007-7_4},
doi = {10.1007/978-3-030-68007-7_4},
abstract = {With the rapid decrease in sequencing costs there is an increased research interest in metagenomics, the study of the genomic content of microbial communities. Machine learning has also seen a revolution with regards to versatility and performance in the last decade using techniques like “Deep Learning”. Classical as well as modern machine learning (ML) techniques are already used in key areas within metagenomics. There are however several challenges that may impede broader use of ML and especially deep learning.This paper provides an overview of machine learning in metagenomics, its challenges and its relationship to biomedical pipelines. Special focus is put on modern techniques such as deep learning. The results are then discussed again in the context of the AI2VIS4BigData reference model to validate its relevancy in this research area.},
booktitle = {Advanced Visual Interfaces. Supporting Artificial Intelligence and Big Data Applications: AVI 2020 Workshops, AVI-BDA and ITAVIS, Ischia, Italy, June 9, 2020 and September 29, 2020, Revised Selected Papers},
pages = {56–82},
numpages = {27},
keywords = {AI2VIS4BigData, Big data, Metagenomics, Deep learning, Machine learning}
}

@article{10.1504/ijict.2021.117534,
author = {Xiao, Pingping and Li, Xiaoguang and Zhu, Juan},
title = {Research on adaptive prediction of multimedia data transmission path based on machine learning},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {19},
number = {3},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2021.117534},
doi = {10.1504/ijict.2021.117534},
abstract = {Aiming at the problems of high probability of data packet loss and long data receiving delay in current methods, an adaptive prediction method of multimedia data transmission path based on machine learning is proposed. This paper analyses the path length of multimedia data transmission, data integrity, energy consumption in the process of data transmission and reception delay, and ranks the advantages and disadvantages of data transmission paths, and finally realises the adaptive prediction of multimedia data transmission paths. The experimental results show that the proposed method is less likely to lose data packets and reduce the data receiving delay when predicting the path of multimedia data transmission. The experimental results verify the effectiveness of the proposed method. At the same time, the accuracy of adaptive prediction of data transmission path is discussed, and the discussion results of high accuracy of path prediction are obtained.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {310–325},
numpages = {15},
keywords = {adaptive prediction, path, data transmission, multimedia, machine learning}
}

@inproceedings{10.1145/3449726.3459459,
author = {Fellers, Justin and Quevedo, Jose and Abdelatti, Marwan and Steinhaus, Meghan and Sodhi, Manbir},
title = {Selecting between evolutionary and classical algorithms for the CVRP using machine learning: optimization of vehicle routing problems},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459459},
doi = {10.1145/3449726.3459459},
abstract = {Solutions for NP-hard problems are often obtained using heuristics that yield results relatively quickly, at some cost to the objective. Many different heuristics are usually available for the same problem type, and the solution quality of a heuristic may depend on characteristics of the instance being solved. This paper explores the use of machine learning to predict the best heuristic for solving Capacitated Vehicle Routing Problems (CVRPs). A set of 23 features related to the CVRP were identified from the literature. A large set of CVRP instances were generated across the feature space, and solved using four heuristics including a genetic algorithm and a novel self-organizing map. A neural network was trained to predict the best performing heuristic for a given problem instance. The model correctly selected the best heuristic for 79% of the CVRP test instances, while the single best heuristic was dominant for only 50% of the test instances.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {127–128},
numpages = {2},
keywords = {vehicle routing, self-organizing map, neural network, machine learning, genetic algorithm, evolutionary algorithm, algorithm selection},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1145/3401842,
author = {Carta, Silvio},
title = {Machine learning and computational design},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2020},
number = {May},
url = {https://doi.org/10.1145/3401842},
doi = {10.1145/3401842},
abstract = {The use of computers in design is substantially different today from what it was only 30 years ago, and light-years ahead of how things were designed before computers entered the scene 60 years ago. This article discusses the use of computers, more specifically computational design, as a useful tool for designers. Herein, computational design refers to the application of computational tools to design practice.},
journal = {Ubiquity},
month = may,
articleno = {2},
numpages = {10}
}

@inproceedings{10.1109/IDAACS53288.2021.9660910,
author = {Kondratenko, Yuriy and Sichevskyi, Stanislav and Kondratenko, Galyna and Sidenko, Ievgen},
title = {Manipulator's Control System with Application of the Machine Learning},
year = {2021},
isbn = {978-1-6654-4209-1},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IDAACS53288.2021.9660910},
doi = {10.1109/IDAACS53288.2021.9660910},
abstract = {This paper provides an analytical review of the subject area, approaches and methods for real-time control systems with application of the machine learning. The review helps to choose approaches to control robotic devices, methods of artificial intelligence to perform pattern recognition and classification, technologies to design a control system and simulate a robotic device. The design and software implementation stage involved the creation of a structural diagram and description of the selected technologies, training a neural network for recognition and classification of geometric objects, software implementation of control system components. The control system was implemented using the Swift programming language and other technologies, and the configuration and creation of a neural network using the CreateML framework.},
booktitle = {2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)},
pages = {363–368},
numpages = {6},
location = {Cracow, Poland}
}

@article{10.1145/3482854,
author = {Tarafdar, Naif and Di Guglielmo, Giuseppe and Harris, Philip C. and Krupa, Jeffrey D. and Loncar, Vladimir and Rankin, Dylan S. and Tran, Nhan and Wu, Zhenbin and Shen, Qianfeng and Chow, Paul},
title = {AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3482854},
doi = {10.1145/3482854},
abstract = {&nbsp;AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment &amp; Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {23},
numpages = {32},
keywords = {hardware/software co-design, data center, FPGAs}
}

@article{10.1155/2021/9923326,
author = {Gong, Fanghai and Wu, Wenqing},
title = {Workflow Scheduling Based on Mobile Cloud Computing Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9923326},
doi = {10.1155/2021/9923326},
abstract = {In recent years, cloud workflow task scheduling has always been an important research topic in the business world. Cloud workflow task scheduling means that the workflow tasks submitted by users are allocated to appropriate computing resources for execution, and the corresponding fees are paid in real time according to the usage of resources. For most ordinary users, they are mainly concerned with the two service quality indicators of workflow task completion time and execution cost. Therefore, how cloud service providers design a scheduling algorithm to optimize task completion time and cost is a very important issue. This paper proposes research on workflow scheduling based on mobile cloud computing machine learning, and this paper conducts research by using literature research methods, experimental analysis methods, and other methods. This article has deeply studied mobile cloud computing, machine learning, task scheduling, and other related theories, and a workflow task scheduling system model was established based on mobile cloud computing machine learning from different algorithms used in processing task completion time, task service costs, task scheduling, and resource usage The situation and the influence of different tasks on the experimental results are analyzed in many aspects. The algorithm in this paper speeds up the scheduling time by about 7% under a different number of tasks and reduces the scheduling cost by about 2% compared with other algorithms. The algorithm in this paper has been obviously optimized in time scheduling and task scheduling.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3340482.3342746,
author = {Viuginov, Nickolay and Filchenkov, Andrey},
title = {A machine learning based automatic folding of dynamically typed languages},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342746},
doi = {10.1145/3340482.3342746},
abstract = {The popularity of dynamically typed languages has been growing strongly lately. Elegant syntax of such languages like javascript, python, PHP and ruby pays back when it comes to finding bugs in large codebases. The analysis is hindered by specific capabilities of dynamically typed languages, such as defining methods dynamically and evaluating string expressions. For finding bugs or investigating unfamiliar classes and libraries in modern IDEs and text editors features for folding unimportant code blocks are implemented. In this work, data on user foldings from real projects were collected and two classifiers were trained on their basis. The input to the classifier is a set of parameters describing the structure and syntax of the code block. These classifiers were subsequently used to identify unimportant code fragments. The implemented approach was tested on JavaScript and Python programs and compared with the best existing algorithm for automatic code folding.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Source code analysis, Python, JavaScript, Dynamically typed languages, Automatic Folding, Abstract Syntax tree},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1007/978-3-030-30241-2_35,
author = {Castanheira, Jos\'{e} and Curado, Francisco and Tom\'{e}, Ana and Gon\c{c}alves, Edgar},
title = {Machine Learning Methods for Radar-Based People Detection and Tracking},
year = {2019},
isbn = {978-3-030-30240-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30241-2_35},
doi = {10.1007/978-3-030-30241-2_35},
abstract = {This paper describes the work developed towards the implementation of a radar-based system for people detection and tracking in indoor environments using machine learning techniques. For such, a series of experiments were carried out in an indoor scenario involving walking people and dummies representative of other moving objects. The applied machine learning methods included a neural network and a random forest classifier. The success rates (accuracies) obtained with both methods using the experimental data sets evidence the high potential of the proposed approach.},
booktitle = {Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 3–6, 2019, Proceedings, Part I},
pages = {412–423},
numpages = {12},
keywords = {Machine learning, RCS, Locomotion pattern, RADAR},
location = {Vila Real, Portugal}
}

@article{10.1007/s00521-021-06288-w,
author = {Chatterjee, Tanmoy and Essien, Aniekan and Ganguli, Ranjan and Friswell, Michael I.},
title = {The stochastic aeroelastic response analysis of helicopter rotors using deep and shallow machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {23},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06288-w},
doi = {10.1007/s00521-021-06288-w},
abstract = {This paper addresses the influence of manufacturing variability of a helicopter rotor blade on its aeroelastic responses. An aeroelastic analysis using finite elements in spatial and temporal domains is used to compute the helicopter rotor frequencies, vibratory hub loads, power required and stability in forward flight. The novelty of the work lies in the application of advanced data-driven machine learning (ML) techniques, such as convolution neural networks (CNN), multi-layer perceptron (MLP), random forests, support vector machines and adaptive Gaussian process (GP) for capturing the nonlinear responses of these complex spatio-temporal models to develop an efficient physics-informed ML framework for stochastic rotor analysis. Thus, the work is of practical significance as (i) it accounts for manufacturing uncertainties, (ii) accurately quantifies their effects on nonlinear response of rotor blade and (iii) makes the computationally expensive simulations viable by the use of ML. A rigorous performance assessment of the aforementioned approaches is presented by demonstrating validation on the training dataset and prediction on the test dataset. The contribution of the study lies in the following findings: (i) The uncertainty in composite material and geometric properties can lead to significant variations in the rotor aeroelastic responses and thereby highlighting that the consideration of manufacturing variability in analyzing helicopter rotors is crucial for assessing their behaviour in real-life scenarios. (ii) Precisely, the substantial effect of uncertainty has been observed on the six vibratory hub loads and the damping with the highest impact on the yawing hub moment. Therefore, sufficient factor of safety should be considered in the design to alleviate the effects of perturbation in the simulation results. (iii) Although advanced ML techniques are harder to train, the optimal model configuration is capable of approximating the nonlinear response trends accurately. GP and CNN followed by MLP achieved satisfactory performance. Excellent accuracy achieved by the above ML techniques demonstrates their potential for application in the optimization of rotors under uncertainty.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {16809–16828},
numpages = {20},
keywords = {Machine learning, Stochastic, Aeroelastic, Helicopter rotor}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Software product lines, Reuse, Prediction, Post-release defects, Failure-prone files, Change metrics}
}

@inproceedings{10.1007/978-3-030-58449-8_2,
author = {Bertossi, Leopoldo},
title = {Score-Based Explanations in Data Management and Machine Learning},
year = {2020},
isbn = {978-3-030-58448-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58449-8_2},
doi = {10.1007/978-3-030-58449-8_2},
abstract = {We describe some approaches to explanations for observed outcomes in data management and machine learning. They are based on the assignment of numerical scores to predefined and potentially relevant inputs. More specifically, we consider explanations for query answers in databases, and for results from classification models. The described approaches are mostly of a causal and counterfactual nature. We argue for the need to bring domain and semantic knowledge into score computations; and suggest some ways to do this.},
booktitle = {Scalable Uncertainty Management: 14th International Conference, SUM 2020, Bozen-Bolzano, Italy, September 23–25, 2020, Proceedings},
pages = {17–31},
numpages = {15},
location = {Bozen-Bolzano, Italy}
}

@inproceedings{10.1007/978-3-030-58604-1_37,
author = {Baldassarre, Federico and Smith, Kevin and Sullivan, Josephine and Azizpour, Hossein},
title = {Explanation-Based Weakly-Supervised Learning of Visual Relations with Graph Networks},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_37},
doi = {10.1007/978-3-030-58604-1_37},
abstract = {Visual relationship detection is fundamental for holistic image understanding. However, the localization and classification of (subject, predicate, object) triplets remain challenging tasks, due to the combinatorial explosion of possible relationships, their long-tailed distribution in natural images, and an expensive annotation process.This paper introduces a novel weakly-supervised method for visual relationship detection that relies on minimal image-level predicate labels. A&nbsp;graph neural network is trained to classify predicates in images from a graph representation of detected objects, implicitly encoding an inductive bias for pairwise relations. We then frame relationship detection as the explanation of such a predicate classifier, i.e. we obtain a complete relation by recovering the subject and object of a predicted predicate.We present results comparable to recent fully- and weakly-supervised methods on three diverse and challenging datasets: HICO-DET for human-object interaction, Visual Relationship Detection for generic object-to-object relations, and UnRel for unusual triplets; demonstrating robustness to non-comprehensive annotations and good few-shot generalization.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {612–630},
numpages = {19},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s11761-019-00270-0,
author = {Subramanian, E. K. and Tamilselvan, Latha},
title = {A focus on future cloud: machine learning-based cloud security},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-019-00270-0},
doi = {10.1007/s11761-019-00270-0},
abstract = {Recent days have seen an apparent shift in most of the organizations moving towards using cloud environment and various cloud-based services. In order to protect and safeguard the transactions made by organizations over cloud environment, it is highly essential to provide a secure and robust environmental solution across cloud space. Existing approaches such as linear regression and support vector machine have been tried to promote cyber-security in the market by performing static verification of cloud user behaviour in order to identify pre-defined threats. Due to their static nature, these security solutions are restricted in their functionality. When it comes to access control, the decision making involves performing a permit or block operation. Also, the earlier methods face difficulties in terms of data protection over the endpoints which are not managed by the cloud. In order to solve the above-said problems, this paper is focused on designing a novel security solution for cloud applications using machine learning (ML) approaches. The main objective of this paper is to shape the future generation of cloud security using one of the ML algorithms such as convolution neural network because CNN can provide automatic and responsive approaches to enhance security in cloud environment. Instead of focusing only on detecting and identifying sensitive data patterns, ML can provide solutions which incorporate holistic algorithms for secure enterprise data throughout all the cloud applications. The proposed ML algorithm is experimented, results are verified and performance is evaluated by comparing with the existing approaches.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {237–249},
numpages = {13},
keywords = {Malicious threats, Cloud security, Machine learning}
}

@inproceedings{10.1007/978-3-319-24888-2_32,
author = {Peikari, Mohammad and Zubovits, Judit and Clarke, Gina and Martel, Anne L.},
title = {Clustering Analysis for Semi-supervised Learning Improves Classification Performance of Digital Pathology},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_32},
doi = {10.1007/978-3-319-24888-2_32},
abstract = {Purpose: Completely labeled datasets of pathology slides are often difficult and time consuming to obtain. Semi-supervised learning methods are able to learn reliable models from small number of labeled instances and large quantities of unlabeled data. In this paper, we explored the potential of clustering analysis for semi-supervised support vector machine (SVM) classifier. Method: A clustering analysis method was proposed to find regions of high density prior to finding the decision boundary using a supervised SVM and was compared with another state-of-the-art semi-supervised technique. Different percentages of labeled instances were used to train supervised and semi-supervised SVM learners from an image dataset generated from 50 whole-mount images (8 patients) of breast specimen. Their cross-validated classification performances were compared with each other using the area under the ROC curve measure. Result: Our proposed clustering analysis for semi-supervised learning was able to produce a reliable classification model from small amounts of labeled data. Comparing the proposed method in this study with a well-known implementation of semi-supervised SVM, our method performed much faster and produced better results.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {263–270},
numpages = {8},
location = {Munich, Germany}
}

@inproceedings{10.1145/3440943.3444743,
author = {Kim, Janghoon and Choi, Hyunpyo and Shin, Jiho and Seo, Jung Taek},
title = {Study on Anomaly Detection Technique in an Industrial Control System Based on Machine Learning},
year = {2021},
isbn = {9781450383042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440943.3444743},
doi = {10.1145/3440943.3444743},
abstract = {This study proposed an anomaly detection technique in an industrial control system using supervised and unsupervised machine learning algorithms. For the dataset for learning, the HIL-based Augmented ICS (HAI) dataset provided for the study on security in industrial control systems was used. For the learning model, Light Gradient Boosted Machine -- a supervised learning algorithm and One-Class Support Vector Machine and Isolation Forest as unsupervised learning algorithms were employed. The proposed technique is presented in this paper, which is organized as follows: Feature selection, Data preprocessing, Hyperparameter optimization and verification, and Experiment and analysis of results. The performance difference according to the algorithm and model configuration was exhibited through the experimental results. In addition, issues to be considered in model configuration and future study directions for anomaly detection techniques in industrial control systems were presented based on the experimental results.},
booktitle = {Proceedings of the 2020 ACM International Conference on Intelligent Computing and Its Emerging Applications},
articleno = {47},
numpages = {5},
keywords = {OCSVM, LightGBM, Isolation Forest, ICS, HAI Dataset, Anomaly Detection},
location = {GangWon, Republic of Korea},
series = {ACM ICEA '20}
}

@inproceedings{10.1109/ICASSP.2018.8461684,
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
title = {Unsupervised Learning of Semantic Audio Representations},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2018.8461684},
doi = {10.1109/ICASSP.2018.8461684},
abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {126–130},
numpages = {5},
location = {Calgary, AB, Canada}
}

@article{10.3233/JIFS-189548,
author = {Xu, Min and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {PLC course performance evaluation based on machine learning and image feature retrieval},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189548},
doi = {10.3233/JIFS-189548},
abstract = {PLC is an indispensable technology for modern automation. The future social development will require a large number of PLC technical talents, so higher requirements are put forward for the teaching of PLC courses in colleges and universities. The intelligence and practical effect of the PLC course evaluation system are particularly important. Based on this, this article combines machine learning and image feature retrieval to construct a PLC course performance evaluation system. Moreover, this paper introduces the smoothness of the multispectral image, the smoothness of the blur function and the smoothness between the blur functions of adjacent spectral images as constraints and uses the gradient of the image blur kernel to express the smoothness of the image blur kernel itself. In addition, this article constructs the model system architecture according to the teaching requirements of the course and analyzes its realization process. Finally, in order to verify the performance of the model, this paper conducts system performance verification experiments through practical teaching methods and analyzes the results with statistical methods. The research results show that the PLC performance evaluation system constructed in this paper has a certain effect.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7209–7219},
numpages = {11},
keywords = {Machine learning, image features, feature retrieval, PLC courses, performance evaluation}
}

@article{10.3233/JIFS-189080,
author = {Yontar, Meltem and Namli, \"{O}zge H\"{u}sniye and Yanik, Seda and Kahraman, Cengiz},
title = {Using machine learning techniques to develop prediction models for detecting unpaid credit card customers},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189080},
doi = {10.3233/JIFS-189080},
abstract = {Customer behavior prediction is gaining more importance in the banking sector like in any other sector recently. This study aims to propose a model to predict whether credit card users will pay their debts or not. Using the proposed model, potential unpaid risks can be predicted and necessary actions can be taken in time. For the prediction of customers’ payment status of next months, we use Artificial Neural Network (ANN), Support Vector Machine (SVM), Classification and Regression Tree (CART) and C4.5, which are widely used artificial intelligence and decision tree algorithms. Our dataset includes 10713 customer’s records obtained from a well-known bank in Taiwan. These records consist of customer information such as the amount of credit, gender, education level, marital status, age, past payment records, invoice amount and amount of credit card payments. We apply cross validation and hold-out methods to divide our dataset into two parts as training and test sets. Then we evaluate the algorithms with the proposed performance metrics. We also optimize the parameters of the algorithms to improve the performance of prediction. The results show that the model built with the CART algorithm, one of the decision tree algorithm, provides high accuracy (about 86%) to predict the customers’ payment status for next month. When the algorithm parameters are optimized, classification accuracy and performance are increased.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6073–6087},
numpages = {15},
keywords = {CART, SVM, ANN, parameter optimization, classification, machine learning, Credit card}
}

@inproceedings{10.1007/978-3-030-59710-8_63,
author = {Yang, Hongxu and Shan, Caifeng and Kolen, Alexander F. and de With, Peter H. N.},
title = {Deep Q-Network-Driven Catheter Segmentation in 3D US by Hybrid Constrained Semi-supervised Learning and Dual-UNet},
year = {2020},
isbn = {978-3-030-59709-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59710-8_63},
doi = {10.1007/978-3-030-59710-8_63},
abstract = {Catheter segmentation in 3D ultrasound is important for computer-assisted cardiac intervention. However, a large amount of labeled images are required to train a successful deep convolutional neural network (CNN) to segment the catheter, which is expensive and time-consuming. In this paper, we propose a novel catheter segmentation approach, which requests fewer annotations than the supervised learning method, but nevertheless achieves better performance. Our scheme considers a deep Q learning as the pre-localization step, which avoids voxel-level annotation and which can efficiently localize the target catheter. With the detected catheter, patch-based Dual-UNet is applied to segment the catheter in 3D volumetric data. To train the Dual-UNet with limited labeled images and leverage information of unlabeled images, we propose a novel semi-supervised scheme, which exploits unlabeled images based on hybrid constraints from predictions. Experiments show the proposed scheme achieves a higher performance than state-of-the-art semi-supervised methods, while it demonstrates that our method is able to learn from large-scale unlabeled images.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I},
pages = {646–655},
numpages = {10},
keywords = {Hybrid constraint, Semi-supervised learning, Dual-UNet, Deep reinforcement learning, Catheter segmentation},
location = {Lima, Peru}
}

@article{10.1155/2021/9943067,
author = {Dai, Xianyan and Li, Shangbin and Wu, Wenqing},
title = {Volleyball Data Analysis System and Method Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9943067},
doi = {10.1155/2021/9943067},
abstract = {After the reform and the opening up, the economy of my country has grown rapidly and people’s lives have become better and better. As a result, there is a lot of time to pay attention to their health, which has promoted the rapid development of my country’s sports industry. Since the 2008 Beijing Olympics, the successful hosting of the Beijing Olympics has been further strengthened. With the rise of the development of sports in our country, the use of machine learning in a large amount of information can process this data and analyze it well. Based on this, this article is aimed at making volleyball players and coaches better understand the technical structure of hiking and the technique of hiking. The paper understands the characteristics of muscle activity over time and uses machine learning methods to analyze a large number of volleyball sports data. In this experiment, 12 volleyball players from a college of physical education were selected. According to the actual situation of the students’ physical fitness and skills, it is more reasonable to divide them into two arms with preswing technology (A type) group and two-arms without preswing technology (B type) group. Mainly study the volleyball spiking action, select the representative front-row 4th position strong attack and the back-row 6th position for comparison and analysis, and analyze the process from the take-off stage to the aerial shot stage in the four stages of the smash through the kinematics, dynamics, and surface electromyography parameters. Experiments have shown that for type A, the left gluteus maximus integral EMG sum value is significantly different between the front and rear rows (P&lt;0.05). The discharge volume of the left gluteus maximus during the front-row spiking process is greater than that of the back-row spiking. This difference is mainly reflected in the kicking stage and the air attack stage. It shows that volleyball data analysis has a very broad prospect of exploration and application, which can create huge social and economic benefits. How to analyze kinematics is also a very demanding research project and is also part of the future analysis of sports data. Academic value and broad practical significance are important.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {11}
}

@article{10.1016/j.future.2020.09.004,
author = {Mart\'{\i}nez Garre, Jos\'{e} Tom\'{a}s and Gil P\'{e}rez, Manuel and Ruiz-Mart\'{\i}nez, Antonio},
title = {A novel Machine Learning-based approach for the detection of SSH botnet infection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2020.09.004},
doi = {10.1016/j.future.2020.09.004},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {387–396},
numpages = {10},
keywords = {Botnet, Machine learning, Zero-day malware, Honeypot, High interaction}
}

@article{10.1007/s11042-019-08533-w,
author = {Sidhu, Ravneet Kaur and Kumar, Ravinder and Rana, Prashant Singh},
title = {Machine learning based crop water demand forecasting using minimum climatological data},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {19–20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08533-w},
doi = {10.1007/s11042-019-08533-w},
abstract = {Rice is one of the world’s most popular food crops. Since its production is dependent on intensive water use, water management is critical to ensure sustainability of water resource. However, very limited data is available on water use in rice irrigation. In the present study, traditional machine learning methods have been used to predict the irrigation schedule of rice daily. The data of year 2013-2015 is used to train the models and to further optimise it. The data of 2016-2017 is used for testing the models. Correlation thresholds are used for feature selection which helps in reducing the number of input parameters from the initial 26 to final 11. The models estimated the crop water demand as a function of weather parameters. Results show that Adaboost performed consistently well with an average accuracy of 71% as compared to other models for predicting the irrigation schedule.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13109–13124},
numpages = {16},
keywords = {Weather, Irrigation scheduling, Crop water demand, Drip irrigation, Random forest, Decision tree, Support vector regressor}
}

@article{10.1007/s00500-021-05684-7,
author = {Candelieri, Antonio and Perego, Riccardo and Archetti, Francesco},
title = {Green machine learning via augmented Gaussian processes and multi-information source optimization},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {19},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05684-7},
doi = {10.1007/s00500-021-05684-7},
abstract = {Searching for accurate machine and deep learning models is a computationally expensive and awfully energivorous process. A strategy which has been recently gaining importance to drastically reduce computational time and energy consumed is to exploit the availability of different information sources, with different computational costs and different “fidelity,” typically smaller portions of a large dataset. The multi-source optimization strategy fits into the scheme of Gaussian Process-based Bayesian Optimization. An Augmented Gaussian Process method exploiting multiple information sources (namely, AGP-MISO) is proposed. The Augmented Gaussian Process is trained using only “reliable” information among available sources. A novel acquisition function is defined according to the Augmented Gaussian Process. Computational results are reported related to the optimization of the hyperparameters of a Support Vector Machine (SVM) classifier using two sources: a large dataset—the most expensive one—and a smaller portion of it. A comparison with a traditional Bayesian Optimization approach to optimize the hyperparameters of the SVM classifier on the large dataset only is reported.},
journal = {Soft Comput.},
month = oct,
pages = {12591–12603},
numpages = {13},
keywords = {Gaussian processes, Bayesian optimization, Multi information source optimization, Green machine learning, Green AI}
}

@article{10.1007/s13748-020-00214-2,
author = {Obukhov, Artem and Krasnyanskiy, Mikhail and Nikolyukin, Maxim},
title = {Algorithm of adaptation of electronic document management system based on machine learning technology},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {4},
url = {https://doi.org/10.1007/s13748-020-00214-2},
doi = {10.1007/s13748-020-00214-2},
abstract = {The topical problem in the development of electronic document management systems (EDMS) is their adaptation and personalization to the individual characteristics of the user. This article discusses the issue of development of an adaptation algorithm using machine learning methods for solving the problem of structural-parametric synthesis of EDMS. In the framework of the presented algorithm, the approaches to the formalization of workflow processes, ways to adapt the interface to the user parameters using artificial neural networks and a comprehensive assessment of the system’s adaptability are considered. The scientific novelty of the approach consists in the algorithmic and software development for automation of the data collection, analysis and interface adaptation through the use and integration of neural networks in the information system. The application of machine learning methods for the formation and adaptation of EDMS interface allows you to automate the process of personalizing it to the user’s individual characteristics, increase the system’s flexibility and provide the best user experience at the first interaction with EDMS based on the intelligent analysis of data about other users. The main scientific results obtained in the article include: formalized criteria for adapting EDMS; algorithm for designing and adapting EDMS; and development of software for adapting EDMS, including a trained neural network and API.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {287–303},
numpages = {17},
keywords = {Artificial neural networks, Machine learning, Criteria of adaptation, Electronic document management system}
}

@article{10.1007/s10845-018-1404-0,
author = {Denkena, Berend and Bergmann, Benjamin and Witt, Matthias},
title = {Material identification based on machine-learning algorithms for hybrid workpieces during cylindrical operations},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {6},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-018-1404-0},
doi = {10.1007/s10845-018-1404-0},
abstract = {New design concepts for high-performance components are part of the current research. Because of various material properties and chemical composition, the cutting characteristics and chip formation mechanisms change during the machining process. Thus, it can be mandatory to identify the material and adapt the process parameters during machining. As a result, the workpiece quality is optimized while increasing the tool life. Therefore, this paper investigates a new approach to determine the machined material in-process by machine-learning. A cylindrical turning process is performed for friction welded EN-AW6082/20MnCr5 and C22/41Cr4 shafts. Acceleration and process force signals as well as control signals are measured and monitoring features are generated. These features are ranked and selected based on the information value by the joint mutual information method. Afterwards, four machine-learning models are trained to identify the machined material based on the signal features. The monitoring quality is evaluated during various cylindrical turning processes and the most appropriate machine-learning algorithm is determined. Thus, a new methodology for in-process material identification in CNC turning machines based on signal analysis and machine-learning algorithm is proposed.},
journal = {J. Intell. Manuf.},
month = aug,
pages = {2449–2456},
numpages = {8},
keywords = {Turning, Monitoring, Machine-learning, Hybrid parts}
}

@inproceedings{10.1145/3400302.3415763,
author = {Xie, Zhiyao and Li, Hai and Xu, Xiaoqing and Hu, Jiang and Chen, Yiran},
title = {Fast IR drop estimation with machine learning},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415763},
doi = {10.1145/3400302.3415763},
abstract = {IR drop constraint is a fundamental requirement enforced in almost all chip designs. However, its evaluation takes a long time, and mitigation techniques for fixing violations may require numerous iterations. As such, fast and accurate IR drop prediction becomes critical for reducing design turnaround time. Recently, machine learning (ML) techniques have been actively studied for fast IR drop estimation due to their promise and success in many fields. These studies target at various design stages with different emphasis, and accordingly, different ML algorithms are adopted and customized. This paper provides a review to the latest progress in ML-based IR drop estimation techniques. It also serves as a vehicle for discussing some general challenges faced by ML applications in electronics design automation (EDA), and demonstrating how to integrate ML models with conventional techniques for the better efficiency of EDA tools.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {13},
numpages = {8},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@inproceedings{10.1145/3409073.3409085,
author = {Rasheed, Zeeshan and Xiong, Wei and Cong, Gaoxiang and Niu, Hongxun and Wan, Jianxiong and Wang, Yongsheng and Li, Lixiao},
title = {Performance Evaluation and Machine Learning based Thermal Modeling of Tilted Active Tiles in Data Centers},
year = {2020},
isbn = {9781450377645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409073.3409085},
doi = {10.1145/3409073.3409085},
abstract = {Thermal management system of data center continuously face a lot of challenges, because data center industry has seen a boom growth in power density. In this paper we proposed the Tilted Active Tiles (TATs) to improve the local cold air supply and prevent the air flow blow over the rack. In traditional active tiles, fans are placed horizontally which cause the airflow blows over the rack, rather than into, the racks. To solve this issue, we adjusted the angle of the active tile to direct the airflow into the rack. We further introduced ANN based thermal models to predict the thermal performance of TATs. To train the ANN models, we adopted the data set obtained from a data center of Inner Mongolia Meteorology Information Center. The prediction accuracy of the model was extensively compared and analyzed, and the prediction accuracy and overhead of different neural network structures, i.e., BP and LSTM, were evaluated. Experimental results show that the rack with blanking panels has better thermal performance, and the temperature distribution at bottom, middle and top of the rack were same under smaller PWM. Thermal efficiency model was established by BP and LSTM, in this experiment single output model and multi output model were analyzed. The single output model can predict the temperature at different heights on the rack. In single output model the predicted effect of BP model is better than LSTM. The average prediction error is 0.57. The multi-output model can only predict the temperature at a fixed height of the rack. In multi output model LSTM model is better than BP. LSTM prediction error is less than BP. The average prediction error is 0.07.},
booktitle = {Proceedings of the 2020 5th International Conference on Machine Learning Technologies},
pages = {32–38},
numpages = {7},
keywords = {Tilted Active Tile, Machine learning based thermal modeling, Data center thermal evaluation},
location = {Beijing, China},
series = {ICMLT '20}
}

@article{10.1504/ijwbc.2019.101813,
author = {Pandey, Pratibha and Singh, Abhishek},
title = {Energy efficient resource management techniques in cloud environment for web-based community by machine learning: a survey},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1477-8394},
url = {https://doi.org/10.1504/ijwbc.2019.101813},
doi = {10.1504/ijwbc.2019.101813},
abstract = {Cloud computing is a platform where the services of information technology are delivered by retrieval of resource from the internet through web applications and web tools instead of using direct server. In order to offer better quality of service to the larger number of web-based community users, many companies are adopting cloud computing. A dynamic number of web-based community user are interacting with each other via cloud computing. The number of online users is not just limited to manual counting figure so it is extremely important to manage each and every resource efficiently by minimising the energy consumption. To increase the performance of the system, the concepts of machine learning can be used to solve the challenges occur in cloud computing while managing resources for online communities. In this paper, we discuss the application of machine learning in cloud environment and its effect on web-based community.},
journal = {Int. J. Web Based Communities},
month = jan,
pages = {238–247},
numpages = {9},
keywords = {data centre, unsupervised learning, supervised learning, resource allocation, energy efficient, cloud computing, web-based community, machine learning}
}

@article{10.1007/s12650-018-0531-1,
author = {Jiang, Liu and Liu, Shixia and Chen, Changjian},
title = {Recent research advances on interactive machine learning},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {1343-8875},
url = {https://doi.org/10.1007/s12650-018-0531-1},
doi = {10.1007/s12650-018-0531-1},
abstract = {Interactive machine learning (IML) is an iterative learning process that tightly couples a human with a machine learner, which is widely used by researchers and practitioners to effectively solve a wide variety of real-world application problems. Although recent years have witnessed the proliferation of IML in the field of visual analytics, most recent surveys either focus on a specific area of IML or aim to summarize a visualization field that is too generic for IML. In this paper, we systematically review the recent literature on IML and classify them into a task-oriented taxonomy built by us. We conclude the survey with a discussion of open challenges and research opportunities that we believe are inspiring for future work in IML.},
journal = {J. Vis.},
month = apr,
pages = {401–417},
numpages = {17},
keywords = {Machine learning, Interactive visualization, Interactive machine learning}
}

@inproceedings{10.1007/978-3-030-25965-5_27,
author = {Erazo, Yaritza P. and Chasi, Christian P. and Latta, Mar\'{\i}a A. and Andaluz, V\'{\i}ctor H.},
title = {Machine Learning for Acquired Brain Damage Treatment},
year = {2019},
isbn = {978-3-030-25964-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-25965-5_27},
doi = {10.1007/978-3-030-25965-5_27},
abstract = {This article presents an alternative rehabilitation system based on a visual feedback system for people suffering from cerebral palsy disorder. The proposed feedback system handles textures and movements in a 3D graphic environment, specially designed to develop skills that improve patient performance. The interface is developed in the Unity3D software, identifying patterns of the body is done through motion Kinect and validation of the correct execution of the exercises sensor is carried out, using the technique of machine learning for training rehabilitation system. The experimental results show the efficiency of the system that generates an improvement in the motor abilities of the upper and lower extremities of the patient.},
booktitle = {Augmented Reality, Virtual Reality, and Computer Graphics: 6th International Conference, AVR 2019, Santa Maria al Bagno, Italy, June 24–27, 2019, Proceedings, Part I},
pages = {362–375},
numpages = {14},
keywords = {Machine learning, Cerebral palsy, Virtual reality},
location = {Santa Maria al Bagno, Italy}
}

@article{10.1016/j.neunet.2019.12.006,
author = {Ieracitano, Cosimo and Mammone, Nadia and Hussain, Amir and Morabito, Francesco C.},
title = {A novel multi-modal machine learning based approach for automatic classification of EEG recordings in dementia},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {123},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.12.006},
doi = {10.1016/j.neunet.2019.12.006},
journal = {Neural Netw.},
month = mar,
pages = {176–190},
numpages = {15},
keywords = {Data fusion, Mild cognitive impairment, Alzheimer’s disease, Bispectrum, Continuous wavelet transform, Machine learning}
}

@inproceedings{10.1145/3465481.3470066,
author = {Cavalcanti, Marcos and Inacio, Pedro and Freire, Mario},
title = {Performance Evaluation of Container-Level Anomaly-Based Intrusion Detection Systems for Multi-Tenant Applications Using Machine Learning Algorithms},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470066},
doi = {10.1145/3465481.3470066},
abstract = {The virtualization of computing resources provided by containers has gained increasing attention and has been widely used in cloud computing. This new demand for container technology has been growing and the use of Docker and Kubernetes is considerable. According to recent technology surveys, containers are now mainstream. However, currently, one of the major challenges rises from the fact that multiple containers, with different owners, may cohabit on the same host. In container-based multi-tenant environments, security issues are of major concern. In this paper we investigate the performance of container-level anomaly-based intrusion detection systems for multi-tenant applications. We investigate the use of Bag of System Calls (BoSC) technique and the sliding window with the classifier and we consider eight machine learning algorithms for classification purposes. We show that among the eight machine learning algorithms, the best classification results are obtained with Decision Tree and Random Forest which lead to an F-Measure of 99.8%, using a sliding window with a size of 30 and the BoSC algorithm in both cases. We also show that, although both Decision Tree and Random Forest algorithms leads to the best classification results, the Decision Tree algorithm has a shorter execution time and consumes less CPU and memory than the Random Forest.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {109},
numpages = {9},
keywords = {Containers, Intrusion Detection Systems, Machine Learning Algorithms, System Calls},
location = {Vienna, Austria},
series = {ARES '21}
}

@article{10.1155/2021/6805151,
author = {Alsamhi, Saeed H. and Almalki, Faris A. and Al-Dois, Hatem and Ben Othman, Soufiene and Hassan, Jahan and Hawbani, Ammar and Sahal, Radyah and Lee, Brian and Saleh, Hager and Khalil, Ahmed Mostafa},
title = {Machine Learning for Smart Environments in B5G Networks: Connectivity and QoS},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/6805151},
doi = {10.1155/2021/6805151},
abstract = {The number of Internet of Things (IoT) devices to be connected via the Internet is overgrowing. The heterogeneity and complexity of the IoT in terms of dynamism and uncertainty complicate this landscape dramatically and introduce vulnerabilities. Intelligent management of IoT is required to maintain connectivity, improve Quality of Service (QoS), and reduce energy consumption in real time within dynamic environments. Machine Learning (ML) plays a pivotal role in QoS enhancement, connectivity, and provisioning of smart applications. Therefore, this survey focuses on the use of ML for enhancing IoT applications. We also provide an in-depth overview of the variety of IoT applications that can be enhanced using ML, such as smart cities, smart homes, and smart healthcare. For each application, we introduce the advantages of using ML. Finally, we shed light on ML challenges for future IoT research, and we review the current literature based on existing works.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {23}
}

@inproceedings{10.1145/3297067.3297078,
author = {Su, Shu and Yan, Hui and Ding, Ning},
title = {Machine Learning-Based Charging Network Operation Service Platform Reservation Charging Service System},
year = {2018},
isbn = {9781450366052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297067.3297078},
doi = {10.1145/3297067.3297078},
abstract = {This paper proposes a machine learning-based electric vehicle (EV) reserved charging service system, which takes into consideration the impacts from both the power system and transportation system. The proposed framework of charging network operation service platform links the power system with transportation system through the charging navigation of massive EVs. The "reserved charging + consumption" integrated service model would be great significant for dealing with large-scale integration of electric vehicles. It applies the concept of charging time window to optimization of EV charging prediction for the reserved charging service system, and designs a dynamic dispatching model based on sliding time axis to make charging process of users get rid of constraints of queuing time and charging service fee period.},
booktitle = {Proceedings of the 2018 International Conference on Signal Processing and Machine Learning},
pages = {1–5},
numpages = {5},
keywords = {reservation charging, machine learning, electric vehicle (EV), charging service fee},
location = {Shanghai, China},
series = {SPML '18}
}

@inproceedings{10.5555/3504035.3504528,
author = {Shahrampour, Shahin and Beirami, Ahmad and Tarokh, Vahid},
title = {On data-dependent random features for improved generalization in supervised learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The randomized-feature approach has been successfully employed in large-scale kernel approximation and supervised learning. The distribution from which the random features are drawn impacts the number of features required to efficiently perform a learning task. Recently, it has been shown that employing data-dependent randomization improves the performance in terms of the required number of random features. In this paper, we are concerned with the randomized-feature approach in supervised learning for good generaliz-ability. We propose the Energy-based Exploration of Random Features (EERF) algorithm based on a data-dependent score function that explores the set of possible features and exploits the promising regions. We prove that the proposed score function with high probability recovers the spectrum of the best fit within the model class. Our empirical results on several benchmark datasets further verify that our method requires smaller number of random features to achieve a certain generalization error compared to the state-of-the-art while introducing negligible pre-processing overhead. EERF can be implemented in a few lines of code and requires no additional tuning parameters.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {493},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1007/978-3-030-59413-8_3,
author = {Wang, Hao and Dartigues-Pallez, Christel and Riveill, Michel},
title = {Supervised Learning for Human Action Recognition from Multiple Kinects},
year = {2020},
isbn = {978-3-030-59412-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59413-8_3},
doi = {10.1007/978-3-030-59413-8_3},
abstract = {The research of Human Action Recognition (HAR) has made a lot of progress in recent years, and the research based on RGB images is the most extensive. However, there are two main shortcomings: the recognition accuracy is insufficient, and the time consumption of the algorithm is too large. In order to improve these issues our project attempts to optimize the algorithm based on the random forest algorithm by extracting the features of the human body 3D, trying to obtain more accurate human behavior recognition results, and can calculate the prediction results at a lower time cost. In this study, we used the 3D spatial coordinate data of multiple Kinect sensors to overcome these problems and make full use of each data feature. Then, we use the data obtained from multiple Kinects to get more accurate recognition results through post processing.},
booktitle = {Database Systems for Advanced Applications. DASFAA 2020 International Workshops: BDMS, SeCoP, BDQM, GDMA, and AIDE, Jeju, South Korea, September 24–27, 2020, Proceedings},
pages = {32–46},
numpages = {15},
location = {Jeju, Korea (Republic of)}
}

@inproceedings{10.1145/3289402.3289525,
author = {Aziz, Khadija and Zaidouni, Dounia and Bellafkih, Mostafa},
title = {Big Data Processing using Machine Learning algorithms: MLlib and Mahout Use Case},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289525},
doi = {10.1145/3289402.3289525},
abstract = {Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {25},
numpages = {6},
keywords = {Spark, Mahout, Machine Learning, MLlib, Hadoop, Collaborative Filtering, Clustering, Classification, Big Data},
location = {Rabat, Morocco},
series = {SITA'18}
}

@article{10.1016/j.ins.2019.02.030,
author = {Gonz\'{a}lez-Carrasco, Israel and Jim\'{e}nez-M\'{a}rquez, Jose Luis and L\'{o}pez-Cuadrado, Jose Luis and Ruiz-Mezcua, Bel\'{e}n},
title = {Automatic detection of relationships between banking operations using machine learning},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {485},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.030},
doi = {10.1016/j.ins.2019.02.030},
journal = {Inf. Sci.},
month = jun,
pages = {319–346},
numpages = {28},
keywords = {Finance, Business analytics, Pattern detection, Big data, Machine learning}
}

@inproceedings{10.1007/978-3-030-68780-9_10,
author = {Mattei, F. and Scardi, M.},
title = {A Machine Learning Approach to Chlorophyll a Time Series Analysis in the Mediterranean Sea},
year = {2021},
isbn = {978-3-030-68779-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68780-9_10},
doi = {10.1007/978-3-030-68780-9_10},
abstract = {Understanding the dynamics of natural system is a crucial task in ecology especially when climate change is taken into account. In this context, assessing the evolution of marine ecosystems is pivotal since they cover a large portion of the biosphere.For these reasons, we decided to develop an approach aimed at evaluating temporal and spatial dynamics of remotely-sensed chlorophyll a concentration. The concentrations of this pigment are linked with phytoplankton biomass and production, which in turn play a central role in marine environment.Machine learning techniques proved to be valuable tools in dealing with satellite data since they need neither assumptions on data distribution nor explicit mathematical formulations. Accordingly, we exploited the Self Organizing Map (SOM) algorithm firstly to reconstruct missing data from satellite time series of chlorophyll a and secondly to classify them. The missing data reconstruction task was performed using a large SOM and allowed to enhance the available information filling the gaps caused by cloud coverage. The second part of the procedure involved a much smaller SOM used as a classification tool. This dimensionality reduction enabled the analysis and visualization of over 37 000 chlorophyll a time series. The proposed approach provided insights into both temporal and spatial chlorophyll a dynamics in the Mediterranean Basin.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part VI},
pages = {98–109},
numpages = {12},
keywords = {Self organizing maps, Time series analysis, Chlorophyll a}
}

@inproceedings{10.1145/3437880.3460409,
author = {Kapusta, Katarzyna and Thouvenot, Vincent and Bettan, Olivier and Beguinet, Hugo and Senet, Hugo},
title = {A Protocol for Secure Verification of Watermarks Embedded into Machine Learning Models},
year = {2021},
isbn = {9781450382953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437880.3460409},
doi = {10.1145/3437880.3460409},
abstract = {Machine Learning is a well established tool used in a variety of applications. As training advanced models requires considerable amounts of meaningful data in addition to specific knowledge, a new business model separate models creators from model users. Pre-trained models are sold or made available as a service. This raises several security challenges, among others the one of intellectual property protection. Therefore, a new research track actively seeks to provide techniques for model watermarking that would enable model identification in case of suspicion of model theft or misuse. In this paper, we focus on the problem of secure watermarks verification, which affects all of the proposed techniques and until now was barely tackled. First, we revisit the existing threat model. In particular, we explain the possible threats related to a semi-honest or dishonest verification authority. Secondly, we show how to reduce trust requirements between participants by performing the watermarks verification on encrypted data. Finally, we describe a novel secure verification protocol as well as detail its possible implementation using Multi-Party Computation. The proposed solution does not only preserve the confidentiality of the watermarks but also helps detecting evasion attacks. It could be adopted to work with other authentication schemes based on watermarking, especially with image watermarking schemes.},
booktitle = {Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security},
pages = {171–176},
numpages = {6},
keywords = {secure verification, private intersection set, multi-party computation, machine learning watermarking, intellectual property protection},
location = {Virtual Event, Belgium},
series = {IH&amp;MMSec '21}
}

@phdthesis{10.5555/AAI28962853,
author = {Amoako, Richard and Albert, Romkes, and Purushotham, Tukkaraja, and Kelli, McCormick, and Kurt, Katzenstein,},
advisor = {Andrea, Brickey,},
title = {Applied Machine Learning in Mine Safety and Short-Term Underground Mine Production Scheduling},
year = {2021},
isbn = {9798780619109},
publisher = {South Dakota School of Mines and Technology},
abstract = {Mining is a highly mechanized industry with inherent health and safety hazards. As operations have modernized, a safety-focused culture has been adopted and research-driven innovations have been implemented to reduce injuries and fatalities. A goal of the mining industry is to achieve zero injuries on all mine sites. To this end, researchers are developing ways of incorporating minimization of occupational hazards in the mine planning phase that maximizes net present value while managing safety and health risks.This research explores the potential of machine learning in contributing to robust mine safety analysis of an accident and injury data set over a ten-year period from the Mine Safety and Health Administration through the use of multiclass logistic regression. The analysis provides a means to determine a miner's susceptibility to the following injury classes: non-fatal with no days lost or restricted activity, non-fatal with days lost and/or days of restricted work activity, and fatal and total permanent or partial permanent disability. The results reveal that a miner's experience on their current job is a significant factor in injury occurrence, even for those with decades of total mining experience. From this analysis, machine learning proves to be a tool that can be used beyond basic statistics in providing robust mine accident and injury analysis.Furthermore, the research applies an artificial neural network (ANN), a machine learning tool, to other areas of mining, namely, mine ventilation, respirable dust, and underground mine production scheduling. The purpose of the neural network is to estimate, i.e., predict, the respirable dust emissions for a given mining activity. The ability to accurately estimate respirable dust concentrations can provide valuable information to the planning and ventilation engineers. Having information indicating the potential for higher concentrations of dust allows for appropriate actions to be taken prior to the initiation of the activity, thereby managing the situation proactively, instead of reactively.A means of proactive management would be to incorporate the predicted concentrations and requisite ventilation into short-term production schedules. To this end, the research further explores short-term scheduling formulations in underground mining by applying the principles of rescheduling and deviation minimization. The author improves upon an existing formulation by incorporating more realistic penalty functions for activity and production goal deviations. Activity earliness and tardiness are penalized with exponentially increasing values as deviations increase, while goal penalties are only imposed if certain predetermined production target levels are not met. This culminates in a tool that is able to determine alternative schedules in response to unforeseen operational disruptions. The new penalty systems are evaluated using two different scenarios and the resulting schedules are compared. The comparative analysis shows how operational disruptions impact the schedule and how the new formulation makes up for the ensuing deviations. Future work will focus on developing mathematical constraints for incorporating the ANN predictions and ventilation requirements into the short-term formulation to provide an enhanced proactive approach towards the management of respirable dust exposure.},
note = {AAI28962853}
}

@inproceedings{10.1145/3464298.3494884,
author = {S\'{a}nchez-Artigas, Marc and Sarroca, Pablo Gimeno},
title = {Experience Paper: Towards enhancing cost efficiency in serverless machine learning training},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3494884},
doi = {10.1145/3464298.3494884},
abstract = {Function-as-a-Service (FaaS) has raised a growing interest in how to "tame" serverless to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional "serverful" computing. To help in this task, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two key optimizations: a significance filter and a scale-in auto-tuner, and leverages them to specialize model training to the FaaS model. Our results certify that MLLess can be 15X faster than serverful ML systems [24] at a lower cost for ML models (such as sparse logistic regression and matrix factorization) that exhibit fast convergence.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {210–222},
numpages = {13},
keywords = {serverless computing, machine learnig},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@article{10.1016/j.jpdc.2019.03.003,
author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
title = {The security of machine learning in an adversarial setting: A survey},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.03.003},
doi = {10.1016/j.jpdc.2019.03.003},
journal = {J. Parallel Distrib. Comput.},
month = aug,
pages = {12–23},
numpages = {12},
keywords = {Security model, Adversarial example, Adversarial attack, Adversarial setting, Machine learning}
}

@inproceedings{10.1145/3395363.3397366,
author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
title = {Detecting flaky tests in probabilistic and machine learning applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397366},
doi = {10.1145/3395363.3397366},
abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests – tests which fail non-deterministically when run on the same version of code.  In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain.  Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {211–224},
numpages = {14},
keywords = {Randomness, Probabilistic Programming, Non-Determinism, Machine Learning, Flaky tests},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1007/978-3-030-60887-3_38,
author = {Reyes-Campos, Josimar and Alor-Hern\'{a}ndez, Giner and Machorro-Cano, Isaac and S\'{a}nchez-Cervantes, Jos\'{e} Luis and Mu\~{n}oz-Contreras, Hilari\'{o}n and Olmedo-Aguirre, Jos\'{e} Oscar},
title = {Energy Saving by Using Internet of Things Paradigm and Machine Learning},
year = {2020},
isbn = {978-3-030-60886-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60887-3_38},
doi = {10.1007/978-3-030-60887-3_38},
abstract = {Nowadays, energy consumption is acquiring growing attention for the economic and environmental implications in our society due to the growing number of electronic home devices. From this perspective, the Internet of Things (IoT) and Machine Learning have emerged as technologies that allow monitoring and controlling devices installed in houses to detect behavioral patterns that identify feasible scenarios of energy saving. For this reason, intelligent configuration approaches for home automation are of utmost importance. This paper proposes a mobile application (called IntelihOgarT) that optimizes energy consumption through Machine Learning and IoT, while improving, at the same time, comfort at home. The proposed application makes use of Machine Learning algorithm C4.5, which automatically takes decisions based on attributes of a training data set. Furthermore, the case study presented validates the effectiveness of the mobile application, where efficient use of energy at home is a primary concern.},
booktitle = {Advances in Computational Intelligence: 19th Mexican International Conference on Artificial Intelligence, MICAI 2020, Mexico City, Mexico, October 12–17, 2020, Proceedings, Part II},
pages = {447–458},
numpages = {12},
keywords = {Machine Learning, Internet of Things, Home automation, Energy saving},
location = {Mexico City, Mexico}
}

@inproceedings{10.1145/3465416.3483302,
author = {Boykin, C. Malik and Dasch, Sophia T. and Rice Jr., Vincent and Lakshminarayanan, Venkat R. and Togun, Taiwo A. and Brown, Sarah M.},
title = {Opportunities for a More Interdisciplinary Approach to Measuring Perceptions of Fairness in Machine Learning},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483302},
doi = {10.1145/3465416.3483302},
abstract = {As machine learning (ML) is deployed in high-stakes domains, such as disease diagnosis or prison sentencing, questions of fairness have become an area of concern in its development. This interest has produced a variety of statistical fairness definitions derived from classical performance metrics which further expand the decisions that ML practitioners must make in building a system. The need to choose between these definitions raises questions about what conditions influence people to perceive an algorithm as fair or not. Recent results highlight the heavily contextual nature of fairness perceptions, and the specific conditions under which psychological principles such as framing can reliably sway these perceptions. Additional interdisciplinary insights include lessons from the replication crisis within psychology, from which we can glean best-practices for reproducible empirical research. We survey key research at the intersection of ML and psychology, focusing on psychological mechanisms underlying fairness preferences. We conclude by stating the continued need for interdisciplinary research, and underscore best-practices that can inform the state-of-the-art practice. We consider this research to be of a descriptive nature, enabling a deeper understanding and a substantiated discussion.},
booktitle = {Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {1},
numpages = {9},
keywords = {machine learning, fairness, experiment design},
location = {--, NY, USA},
series = {EAAMO '21}
}

@inproceedings{10.1145/3363347.3363364,
author = {Sajan, Kurian Karyakulam and Ramachandran, Gowri Sankar and Krishnamachari, Bhaskar},
title = {Enhancing Support for Machine Learning and Edge Computing on an IoT Data Marketplace},
year = {2019},
isbn = {9781450370134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3363347.3363364},
doi = {10.1145/3363347.3363364},
abstract = {IoT applications are increasingly employing machine learning (ML) algorithms to manage and control the operational environment autonomously while predicting future actions. To leverage these emerging technologies, the application developers require an enormous amount of data to build models. Data marketplaces enable the IoT application developers to buy data from IoT device owners to train machine learning models. Contemporary data marketplaces only focus on connecting the IoT infrastructure owner (seller) with application developers (buyer) while lacking integrated support for data analytics. Application developers are required to manually create and manage machine learning pipelines by combining edge computing resources with data sources. In this paper, we present an architectural framework to build machine learning pipelines for data marketplaces automatically. Our framework enables application developers (buyers) to leverage the edge computing resources provided by the sellers and compose low-latency IoT applications that incorporate ML-based processing. We present a proof-of-concept implementation on the I3 data marketplace and outline open challenges in combining machine-learning, AI, and edge computing technologies with data marketplaces.},
booktitle = {Proceedings of the First International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things},
pages = {19–24},
numpages = {6},
keywords = {Machine learning, IoT, Internet of Things, Edge Computing, Data marketplace, Artificial intelligence},
location = {New York, NY, USA},
series = {AIChallengeIoT'19}
}

@inproceedings{10.5555/3291291.3291297,
author = {Nascimento, Nathalia and Alencar, Paulo and Lucena, Carlos and Cowan, Donald},
title = {A context-aware machine learning-based approach},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {It is known that training a general and versatile Machine Learning (ML)-based model is more cost-effective than training several specialized ML-models for different operating contexts. However, as the volume of training information grows, the higher the probability of producing biased results. Learning bias is a critical problem for many applications, such as those related to healthcare scenarios, environmental monitoring and air traffic control. In this paper, we compare the use of a general model that was trained using all contexts against a system that is composed of a set of specialized models that was trained for each particular operating context. For this purpose, we propose a local learning approach based on context-awareness, which involves: (i) anticipating, analyzing and representing context changes; (ii) training and finding machine learning models to maximize a given scoring function for each operating context; (iii) storing trained ML-based models and associating them with corresponding operating contexts; and (iv) deploying a system that is able to select the best-fit ML-based model at runtime based on the context. To illustrate our proposed approach, we reproduce two experiments: one that uses a neural network regression-based model to perform predictions and another one that uses an evolutionary neural network-based approach to make decisions. For each application, we compare the results of the general model, which was trained based on all contexts, against the results of our proposed approach. We show that our context-aware approach can improve results by alleviating bias with different ML tasks.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {40–47},
numpages = {8},
keywords = {neural network, machine learning, learning bias, contextual modeling, context-awareness},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3229607.3229612,
author = {Mulinka, Pavol and Casas, Pedro},
title = {Stream-based Machine Learning for Network Security and Anomaly Detection},
year = {2018},
isbn = {9781450359047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229607.3229612},
doi = {10.1145/3229607.3229612},
abstract = {Data Stream Machine Learning is rapidly gaining popularity within the network monitoring community as the big data produced by network devices and end-user terminals goes beyond the memory constraints of standard monitoring equipment. Critical network monitoring applications such as the detection of anomalies, network attacks and intrusions, require fast and continuous mechanisms for on-line analysis of data streams. In this paper we consider a stream-based machine learning approach for network security and anomaly detection, applying and evaluating multiple machine learning algorithms in the analysis of continuously evolving network data streams. The continuous evolution of the data stream analysis algorithms coming from the data stream mining domain, as well as the multiple evaluation approaches conceived for benchmarking such kind of algorithms makes it difficult to choose the appropriate machine learning model. Results of the different approaches may significantly differ and it is crucial to determine which approach reflects the algorithm performance the best. We therefore compare and analyze the results from the most recent evaluation approaches for sequential data on commonly used batch-based machine learning algorithms and their corresponding stream-based extensions, for the specific problem of on-line network security and anomaly detection. Similar to our previous findings when dealing with off-line machine learning approaches for network security and anomaly detection, our results suggest that adaptive random forests and stochastic gradient descent models are able to keep up with important concept drifts in the underlying network data streams, by keeping high accuracy with continuous re-training at concept drift detection times.},
booktitle = {Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {1–7},
numpages = {7},
keywords = {Network Attacks, Machine Learning, High-Dimensional Data, Data Stream mining},
location = {Budapest, Hungary},
series = {Big-DAMA '18}
}

@inproceedings{10.1145/3176258.3176321,
author = {Liao, Cong and Zhong, Haoti and Zhu, Sencun and Squicciarini, Anna},
title = {Server-Based Manipulation Attacks Against Machine Learning Models},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176321},
doi = {10.1145/3176258.3176321},
abstract = {Machine learning approaches have been increasingly applied to various applications for data analytics (e.g. spam filtering, image classification). Further, with the growing adoption of cloud computing, various cloud services have provided an efficient way for users to train, store or deploy machine learning algorithms in an easy-to-use manner. However, the models deployed in the cloud may be exposed to potential malicious attacks launched at the server side. Attackers with access to the server can stealthily manipulate a machine learning model so as to enable misclassification or introduce bias. In this work, we study the problem of manipulation attacks as they occur at the server side. We consider not only traditional supervised learning models but also state-of-the-art deep learning models. In particular, a simple but effective gradient descent based approach is presented to exploit Logistic Regression (LR) and Convolutional Neural Networks (CNN) [16] models. We evaluate manipulation attacks against machine learning or deep learning systems using both Enron email text and MINIST image dataset [17]. Experimental results have demonstrated such attacks can manipulate the model that allows malicious samples to evade detection easily without compromising the overall performance of the systems.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {24–34},
numpages = {11},
keywords = {model manipulation, convolutional neural networks, adversarial machine learning},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1007/978-3-030-82824-0_10,
author = {Balta, Dian and Sellami, Mahdi and Kuhn, Peter and Sch\"{o}pp, Ulrich and Buchinger, Matthias and Baracaldo, Nathalie and Anwar, Ali and Ludwig, Heiko and Sinn, Mathieu and Purcell, Mark and Altakrouri, Bashar},
title = {Accountable Federated Machine Learning in Government: Engineering and Management Insights},
year = {2021},
isbn = {978-3-030-82823-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82824-0_10},
doi = {10.1007/978-3-030-82824-0_10},
abstract = {Machine learning offers promising capabilities to improve administrative procedures. At the same time, adequate training of models using traditional learning techniques requires the collection and storage of enough training data in a central place. Unfortunately, due to legislative and jurisdictional constraints, data in a central place is scarce and training a model becomes unfeasible. Against this backdrop, federated machine learning, a technique to collaboratively train models without transferring data to a centralized location, has been recently proposed. With each government entity keeping their data private, new applications that previously were impossible now can be a reality. In this paper, we demonstrate that accountability for the federated machine learning process becomes paramount to fully overcoming legislative and jurisdictional constraints. In particular, it ensures that all government entities' data are adequately included in the model and that evidence on fairness and reproducibility is curated towards trustworthiness. We also present an analysis framework suitable for governmental scenarios and illustrate its exemplary application for online citizen participation scenarios. We discuss our findings in terms of engineering and management implications: feasibility evaluation, general architecture, involved actors as well as verifiable claims for trustworthy machine learning.},
booktitle = {Electronic Participation: 13th IFIP WG 8.5 International Conference, EPart 2021, Granada, Spain, September 7–9, 2021, Proceedings},
pages = {125–138},
numpages = {14},
keywords = {Accountability, Federated learning, Framework, Verifiable claims},
location = {Granada, Spain}
}

@inproceedings{10.1007/978-3-030-45371-8_6,
author = {Briguglio, William and Saad, Sherif},
title = {Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis},
year = {2019},
isbn = {978-3-030-45370-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45371-8_6},
doi = {10.1007/978-3-030-45371-8_6},
abstract = {In cyberattack detection and prevention systems, cybersecurity analysts always prefer solutions that are as interpretable and understandable as rule-based or signature-based detection. This is because of the need to tune and optimize these solutions to mitigate and control the effect of false positives and false negatives. Interpreting machine learning models is a new and open challenge. However, it is expected that an interpretable machine learning solution will be domain specific. For instance, interpretable solutions for machine learning models in healthcare are different than solutions in malware detection. This is because the models are complex, and most of them work as a black-box. Recently, the increased ability for malware authors to bypass antimalware systems has forced security specialists to look to machine learning for creating robust detection systems. If these systems are to be relied on in the industry, then, among other challenges, they must also explain their predictions. The objective of this paper is to evaluate the current state-of-the-art ML models interpretability techniques when applied to ML-based malware detectors. We demonstrate interpretability techniques in practice and evaluate the effectiveness of existing interpretability techniques in the malware analysis domain.},
booktitle = {Foundations and Practice of Security: 12th International Symposium, FPS 2019, Toulouse, France, November 5–7, 2019, Revised Selected Papers},
pages = {82–97},
numpages = {16},
keywords = {Model reliability, Model robustness, Malware detector interpretability, N-gram, Malware detection, Machine learning, Cybersecurity},
location = {Toulouse, France}
}

@inproceedings{10.1145/3414080.3414081,
author = {Komendantskaya, Ekaterina and Kokke, Wen and Kienitz, Daniel},
title = {Continuous Verification of Machine Learning: a Declarative Programming Approach},
year = {2020},
isbn = {9781450388214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414080.3414081},
doi = {10.1145/3414080.3414081},
abstract = {In this invited talk, we discuss state of the art in neural network verification. We propose the term continuous verification to characterise the family of methods that explore continuous nature of machine learning algorithms. We argue that methods of continuous verification must rely on robust programming language infrastructure (refinement types, automated proving, type-driven program synthesis), which provides a major opportunity for the declarative programming language community. Keywords: Neural Networks, Verification, AI.},
booktitle = {Proceedings of the 22nd International Symposium on Principles and Practice of Declarative Programming},
articleno = {1},
numpages = {3},
location = {Bologna, Italy},
series = {PPDP '20}
}

@article{10.1016/j.mejo.2020.104710,
author = {Sinha, Soumendu and Bhardwaj, Rishabh and Sahu, Nishad and Ahuja, Hitesh and Sharma, Rishi and Mukhiya, Ravindra},
title = {Temperature and temporal drift compensation for Al2O3-gate ISFET-based pH sensor using machine learning techniques},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {97},
number = {C},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2020.104710},
doi = {10.1016/j.mejo.2020.104710},
journal = {Microelectron. J.},
month = mar,
numpages = {16},
keywords = {pH sensor, SPICE, Macromodel, ISFET, Bayesian inference, Machine learning}
}

@inproceedings{10.1145/3387939.3388613,
author = {Scheerer, Max and Klamroth, Jonas and Reussner, Ralf and Beckert, Bernhard},
title = {Towards classes of architectural dependability assurance for machine-learning-based systems},
year = {2020},
isbn = {9781450379625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387939.3388613},
doi = {10.1145/3387939.3388613},
abstract = {Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.},
booktitle = {Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {31–37},
numpages = {7},
keywords = {software quality, machine learning, dependability, artificial intelligence, architectural-driven self-adaptation},
location = {Seoul, Republic of Korea},
series = {SEAMS '20}
}

@article{10.1007/s11277-019-06715-1,
author = {Dash, Sanjit Kumar and Dash, Siddhant and Mishra, Jibitesh and Mishra, Sasmita},
title = {Opportunistic Mobile Data Offloading Using Machine Learning Approach},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06715-1},
doi = {10.1007/s11277-019-06715-1},
abstract = {Currently, cellular networks both 3G and 4G are heavily overloaded due to increasing usage of mobile applications. Offloading mobile data traffic through opportunistic communications is one of promising solution to solve this problem. This has huge advantage over other kinds of offloading techniques like no extra cost, significant reduction of mobile traffic, high efficiency. As a case of study, we are focusing our investigation in opportunistic communication for optimizing target set selection problem. A new algorithm is proposed for generating target set which uses machine learning paradigm. Since, machine learning is an emerging sector in computer science with lots of potential; we integrated it with offloading procedure to achieve better performance in real world scenario. The efficiency of proposed method is measured by comparing it with other methods like Greedy, Heuristic and Random. A case study is conducted incorporating this approach and performance evaluation is done. It can be ensured from this comparison that the proposed algorithm outperforms its counterparts.},
journal = {Wirel. Pers. Commun.},
month = jan,
pages = {125–139},
numpages = {15},
keywords = {Information Dissemination, Machine Learning, Opportunistic Communication, Target Set Selection, Mobile Data Offloading}
}

@article{10.1016/j.future.2018.06.028,
author = {Shaqoor Nengroo, Ab and Kuppusamy, K.S.},
title = {Machine learning based heterogeneous web advertisements detection using a diverse feature set},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.06.028},
doi = {10.1016/j.future.2018.06.028},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {68–77},
numpages = {10},
keywords = {Machine learning, Content extraction random forest, Web accessibility, Advertisements}
}

@inproceedings{10.1145/3463274.3464455,
author = {Sala, Irene and Tommasel, Antonela and Arcelli Fontana, Francesca},
title = {DebtHunter: A Machine Learning-based Approach for Detecting Self-Admitted Technical Debt},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3464455},
doi = {10.1145/3463274.3464455},
abstract = {Due to limited time, budget or resources, a team is prone to introduce code that does not follow the best software development practices. This code that introduces instability in the software projects is known as Technical Debt (TD). Often, TD intentionally manifests in source code, which is known as Self-Admitted Technical Debt (SATD). This paper presents DebtHunter, a natural language processing (NLP)- and machine learning (ML)- based approach for identifying and classifying SATD in source code comments. The proposed classification approach combines two classification phases for differentiating between the multiple debt types. Evaluations over 10 open source systems, containing more than 259k comments, showed that the approach was able to improve the performance of others in the literature. The presented approach is supported by a tool that can help developers to effectively manage SATD. The tool complements the analysis over Java source code by allowing developers to also examine the associated issue tracker. DebtHunter can be used in a continuous evolution environment to monitor the development process and make developers aware of how and where SATD is introduced, thus helping them to manage and resolve it.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {278–283},
numpages = {6},
keywords = {technical debt, self-admitted technical debt, natural language processing, machine learning},
location = {Trondheim, Norway},
series = {EASE '21}
}

@phdthesis{10.5555/AAI28496031,
author = {Bandyopadhyay, Arka Prava and Wu, Liuren and Stroebel, Johannes},
advisor = {Yildiray, Yildirim, and Linda, Allen,},
title = {Strategic Default and Moral Hazard in Real Estate: Insights from Machine Learning Applications},
year = {2021},
isbn = {9798515255688},
publisher = {City University of New York},
address = {USA},
abstract = {Strategic default has been the achilles heel in academic finance for decades. By definition, whether a default has occurred due to strategic motive is unobservable. Moreover, a household has only so many avenues of conducting a strategic default. I use the context of commercial mortgages as property value as well property cashflow co-determine the default decision of these borrowers. I tease out the different strategic aspects of default from the ones emanating from liquidity constraints. The recent advances in Deep Neural Network (DNN), the advent of big data and the computational power associated with it has enabled me to disentangle the motive of default.Also, agency conflicts of brokers during origination of a mortgage loan and the moral hazards thereof has been documented based on the soft information about the borrowers. However, there have been few, if any paper, which retains the soft information about the borrowers, post origination, during the life of the loans. There has been a plethora of research about the biases generated towards foreclosures and other adverse outcomes post securitization for the last decade. But the soft information about the borrowers obtained by the brokers have been lost during the pooling process in securitization and there have been famous papers on the loss of information during the securitization process which happens at arms' length from the original lender. I bridge this gap by using novel data on proprietary call transcripts (textual data) between borrowers and servicers. I am also in the process of procuring audio files which can capture mood, content, tone of these communications.My dissertation documents the use of machine learning (ML) techniques in commercial and residential real estate to answer long-standing questions, which could not previously be answered due to paucity of data and computational resources. In the first chapter, Irun a horserace of Deep Neural Network with other ML models and parametric models to provide a new identification strategy to disentangle liquidity-constrained default and incentives for strategic default. The second chapter attempts to answer the most pressing current socio-economic issue in the United States. Specifically, I compute the social, racial and dollar cost of the CARES Act and find these adhoc policies are as expensive as direct payment of $2,000 to households, if not worse. Finally, in the third chapter I create a novel framework to ingest quantified time-varying soft information from call transcript text data about borrowers in ML models on hard information. I alleviate the information asymmetry between the borrowers and issuers, increase mortgage market efficiency and mitigate the conflict of interest between master servicers and special servicers.There has been recent literature on the applications of supervised, unsupervised and reinforcement learning in mainstream academic finance. But, very little work is done in the highly illiquid opaque real estate literature using the cutting edge methods in Machine Learning. I take a fresh look at some of the long-debated questions in the literature using some of the machine learning techniques. I am also able to able to use the current COVID-19 pandemic as an exogenous shock for robustness check in most of my current research.},
note = {AAI28496031}
}

@inproceedings{10.1145/3397166.3409128,
author = {Zhang, Qin and Zhou, Ruiting and Wu, Chuan and Jiao, Lei and Li, Zongpeng},
title = {Online scheduling of heterogeneous distributed machine learning jobs},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3409128},
doi = {10.1145/3397166.3409128},
abstract = {Distributed machine learning (ML) has played a key role in today's proliferation of AI services. A typical model of distributed ML is to partition training datasets over multiple worker nodes to update model parameters in parallel, adopting a parameter server architecture. ML training jobs are typically resource elastic, completed using various time lengths with different resource configurations. A fundamental problem in a distributed ML cluster is how to explore the demand elasticity of ML jobs and schedule them with different resource configurations, such that the utilization of resources is maximized and average job completion time is minimized. To address it, we propose an online scheduling algorithm to decide the execution time window, the number and the type of concurrent workers and parameter servers for each job upon its arrival, with a goal of minimizing the weighted average completion time. Our online algorithm consists of (i) an online scheduling framework that groups unprocessed ML training jobs into a batch iteratively, and (ii) a batch scheduling algorithm that configures each ML job to maximize the total weight of scheduled jobs in the current iteration. Our online algorithm guarantees a good parameterized competitive ratio with polynomial time complexity. Extensive evaluations using real-world data demonstrate that it outperforms state-of-the-art schedulers in today's AI cloud systems.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {111–120},
numpages = {10},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1007/978-3-030-58802-1_16,
author = {Gallicchio, Claudio and Micheli, Alessio and Petri, Massimiliano and Pratelli, Antonio},
title = {A Preliminary Investigation of Machine Learning Approaches for Mobility Monitoring from Smartphone Data},
year = {2020},
isbn = {978-3-030-58801-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58802-1_16},
doi = {10.1007/978-3-030-58802-1_16},
abstract = {In this work we investigate the use of machine learning models for the management and monitoring of sustainable mobility, with particular reference to the transport mode recognition. The specific aim is to automatize the detection of the user’s means of transport among those considered in the data collected with an App installed on the users smartphones, i.e. bicycle, bus, train, car, motorbike, pedestrian locomotion. Preliminary results show the potentiality of the analysis for the introduction of reliable advanced, machine learning based, monitoring systems for sustainable mobility.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part II},
pages = {218–227},
numpages = {10},
keywords = {Transport mode recognition, Machine Learning, Sustainable mobility},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3234200.3234246,
author = {Mulinka, Pavol and Casas, Pedro},
title = {Adaptive Network Security through Stream Machine Learning},
year = {2018},
isbn = {9781450359153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234200.3234246},
doi = {10.1145/3234200.3234246},
abstract = {Stream Machine Learning is rapidly gaining popularity within the network monitoring community as the big data produced by network devices and end-user terminals goes beyond the memory constraints of standard monitoring equipment. We consider a stream-based machine learning approach to network security, conceiving adaptive machine learning algorithms for the analysis of continuously evolving network data streams. Using a sliding-window adaptive-size approach, we show that adaptive random forests models are able to keep up with important concept drifts in the underlying network data streams, by keeping high accuracy with continuous re-training at concept drift detection times.},
booktitle = {Proceedings of the ACM SIGCOMM 2018 Conference on Posters and Demos},
pages = {4–5},
numpages = {2},
keywords = {Data Stream mining, Machine Learning, Network Attacks},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@article{10.1016/j.compeleceng.2019.06.012,
author = {Song, Jinho and Cho, ChaeHo and Won, Yoojae},
title = {Analysis of operating system identification via fingerprinting and machine learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2019.06.012},
doi = {10.1016/j.compeleceng.2019.06.012},
journal = {Comput. Electr. Eng.},
month = sep,
pages = {1–10},
numpages = {10},
keywords = {Decision Tree, K-nearest Neighbors, NetworkMiner, Artificial Neural Network, Machine learning, Operating system fingerprinting}
}

@article{10.1007/s11277-021-08284-8,
author = {Hongsong, Chen and Yongpeng, Zhang and Yongrui, Cao and Bhargava, Bharat},
title = {Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {117},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08284-8},
doi = {10.1007/s11277-021-08284-8},
abstract = {Under big data environment, machine learning has been rapidly developed and widely used. It has been successfully applied in computer vision, natural language processing, computer security and other application fields. However, there are many security problems in machine learning under big data environment. For example, attackers can add “poisoned” sample to the data source, and big data process system will process these “poisoned” sample and use machine learning methods to train model, which will directly lead to wrong prediction results. In this paper, machine learning system and machine learning pipeline are proposed. The security problems that maybe occur in each stage of machine learning system under big data processing pipeline are analyzed comprehensively. We use four different attack methods to compare the attack experimental results.The security problems are classified comprehensively, and the defense approaches to each security problem are analyzed. Drone-deploy MapEngine is selected as a case study, we analyze the security threats and defense approaches in the Drone-Cloud machine learning application envirolment. At last,the future development drections of security issues and challenages in the machine learning system are proposed.},
journal = {Wirel. Pers. Commun.},
month = apr,
pages = {3505–3525},
numpages = {21},
keywords = {Case study, Defensive approaches, Security threats, Big data pipeline, Machine learning system}
}

@article{10.1007/s11042-020-09178-w,
author = {Carfora, Valentina and Di Massimo, Francesca and Rastelli, Rebecca and Catellani, Patrizia and Piastra, Marco},
title = {Dialogue management in conversational agents through psychology of persuasion and machine learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {47–48},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09178-w},
doi = {10.1007/s11042-020-09178-w},
abstract = {To be really effective, conversational agents must integrate well with the characteristics of the humans with whom they interact. This exploratory study focuses on a method for integrating well-assessed methods from the field of social psychology in the design of task-oriented conversational agents in which the dialogue management module is developed through machine learning. In particular, the aim is to achieve agents whose policies could take into account the psychological features of the human interactants to deliver personalized and more effective messages. The paper presents the psychological study performed and outlines the overall theoretical architecture of the software framework proposed. On the psychosocial side, we first assessed the effectiveness of differently framed messages aimed to reducing red meat consumption taking the Theory of Planned Behavior (TPB) as the psychosocial model of reference. Turning to the machine learning field, the resulting Structural Equation Model (SEM) was first translated into a probabilistic predictor using Dynamic Bayesian Network (DBN). In turn, such DBN became the fundamental element of a Partially Observable Markov Decision Processes (POMDP) in a reinforcement learning setting. The possibility to elicit complete interaction policies was then studied by applying Neural Monte Carlo Tree Search (Neural MCTS) methods. The results thus obtained introduce the possibility to develop new multidisciplinary and integrated techniques for the development of automated dialogue managing systems.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {35949–35971},
numpages = {23},
keywords = {Monte carlo tree search, Reinforcement learning, Machine learning, Psychology of persuasion, Theory of planned behavior, Conversational agent}
}

@inproceedings{10.1145/3316781.3323472,
author = {Zhang, Jeff Jun and Liu, Kang and Khalid, Faiq and Hanif, Muhammad Abdullah and Rehman, Semeen and Theocharides, Theocharis and Artussi, Alessandro and Shafique, Muhammad and Garg, Siddharth},
title = {Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3323472},
doi = {10.1145/3316781.3323472},
abstract = {Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {175},
numpages = {4},
keywords = {Timing Errors, Security, Robustness, Reliability, Permanent Faults, Machine Learning, Deep Learning, Adversarial Attacks},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.1007/978-3-030-95467-3_42,
author = {Rong, Kyle and Khant, Aditya and Flores, David and Monta\~{n}ez, George D.},
title = {The Label Recorder Method: Testing the Memorization Capacity of Machine Learning Models},
year = {2021},
isbn = {978-3-030-95466-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95467-3_42},
doi = {10.1007/978-3-030-95467-3_42},
abstract = {Highly-parameterized deep neural networks are known to have strong data-memorization capability, but does this ability to memorize random data also extend to simple standard learning methods with few parameters? Following recent work exploring memorization in deep learning, we investigate memorization in standard non-neural learning models through the label recorder method, which uses a model’s training accuracy on randomized data to estimate its memorization ability, giving a distribution- and regularization-dependent label recording score. Label recording scores can be used to measure how capacity changes in response to regularization and other hyperparameter choices. This method is fully empirical, easy to implement, and works for all black-box classification methods. The label recording score supplements existing theoretical measures of model capacity such as Rademacher complexity and Vapnik-Chervonenkis (VC) dimension, while agreeing with conventional intuitions regarding statistical learning processes. We find that memorization ability is not limited to only over-parameterized models, but instead exists as a continuum, being present (to some degree) even in simple learning models with few parameters.},
booktitle = {Machine Learning, Optimization, and Data Science: 7th International Conference, LOD 2021, Grasmere, UK, October 4–8, 2021, Revised Selected Papers, Part I},
pages = {581–595},
numpages = {15},
keywords = {Label recording score, Label recorder, Representational capacity},
location = {Grasmere, United Kingdom}
}

@article{10.1016/j.knosys.2017.04.014,
author = {Arcelli Fontana, Francesca and Zanoni, Marco},
title = {Code smell severity classification using machine learning techniques},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {128},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.04.014},
doi = {10.1016/j.knosys.2017.04.014},
abstract = {Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.880.96, measured through Spearmans .},
journal = {Know.-Based Syst.},
month = jul,
pages = {43–58},
numpages = {16},
keywords = {Refactoring prioritization, Ordinal classification, Machine learning, Code smells detection, Code smell severity}
}

@inproceedings{10.1007/978-3-030-78710-3_21,
author = {Loecher, Michael and Hannum, Ariel J. and Perotti, Luigi E. and Ennis, Daniel B.},
title = {Arbitrary Point Tracking with Machine Learning to Measure Cardiac Strains in Tagged MRI},
year = {2021},
isbn = {978-3-030-78709-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78710-3_21},
doi = {10.1007/978-3-030-78710-3_21},
abstract = {Cardiac tagged MR images allow for deformation fields to be measured in the heart by tracking the motion of tag lines throughout the cardiac cycle. Machine learning (ML) algorithms enable accurate and robust tracking of tag lines. Herein, the use of a massive synthetic physics-driven training dataset with known ground truth was used to train an ML network to enable tracking any number of points at arbitrary positions rather than anchored to the tag lines themselves. The tag tracking and strain calculation methods were investigated in a computational deforming cardiac phantom with known (ground truth) strain values. This enabled both tag tracking and strain accuracy to be characterized for a range of image acquisition and tag tracking parameters. The methods were also tested on in vivo volunteer data. Median tracking error was&lt;&nbsp;0.26&nbsp;mm in the computational phantom, and strain measurements were improved in vivo when using the arbitrary point tracking for a standard clinical protocol.},
booktitle = {Functional Imaging and Modeling of the Heart: 11th International Conference, FIMH 2021, Stanford, CA, USA, June 21-25, 2021, Proceedings},
pages = {213–222},
numpages = {10},
keywords = {Cardiac strains, Machine learning, MRI Tagging},
location = {Stanford, CA, USA}
}

@inbook{10.5555/3454287.3454483,
author = {Laue, S\"{o}ren and Mitterreiter, Matthias and Giesen, Joachim},
title = {GENO – GENeric optimization for classical machine learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although optimization is the longstanding algorithmic backbone of machine learning, new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or if there is one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure and thus cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO generates a solver from the declarative specification of an optimization problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well-engineered specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {196},
numpages = {12}
}

@inproceedings{10.1145/3167020.3167061,
author = {Pacheco, Fannia and Exposito, Ernesto and Gineste, Mathieu and Budoin, Cedric},
title = {An autonomic traffic analysis proposal using Machine Learning techniques},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167061},
doi = {10.1145/3167020.3167061},
abstract = {Network analysis has recently become in one of the most challenging tasks to handle due to the rapid growth of communication technologies. For network management, accurate identification and classification of network traffic is a key task. For example, identifying traffic from different applications is critical to manage bandwidth resources and to ensure Quality of Service objectives. Machine learning emerges as a suitable tool for traffic classification; however, it requires several steps that must be followed adequately in order to achieve the goals. In this paper, we proposed an architecture to perform traffic analysis based on Machine Learning techniques and autonomic computing. We analyze the procedures to perform Machine Learning over traffic network classification, and at the same time we give guidelines to introduce all these procedures into the architecture proposed. The main contribution of our proposal is the reconfiguration of the traffic classifier that will change according to the knowledge acquired from the traffic analysis process.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {273–280},
numpages = {8},
keywords = {traffic analysis, quality of service, autonomic computing, Machine Learning},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@article{10.1504/ijict.2020.109896,
author = {Zhu, Guiming},
title = {Research on detection method of abnormal capital transfer in electronic commerce based on machine learning},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {17},
number = {3},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2020.109896},
doi = {10.1504/ijict.2020.109896},
abstract = {In order to overcome the problems of long detection time, low detection efficiency and high false alarm rate, a new method based on machine learning is proposed. Data mining in e-commerce platform. The improved k-means algorithm was used to cluster the data, and the five steps of preparation, detection, location acquisition, modification and verification were used to clean up the clustering results and remove redundant data. The machine learning method is used to determine whether there are suspicious transaction fragments in the database through four steps: data pre-processing, generating reference sequence and query sequence, calculating similarity and sequence classification, and to complete abnormal fund transfer detection in e-commerce. Experimental results show that the detection time of this method is kept below 3 s, the highest false detection rate is only 11%, and the detection rate is always higher than 90%, with high detection efficiency, low false alarm rate, high detection rate.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {288–305},
numpages = {17},
keywords = {anomaly detection, capital transfer, e-commerce, machine learning}
}

@inproceedings{10.1007/978-3-030-65847-2_3,
author = {Lukyanenko, Roman and Castellanos, Arturo and Storey, Veda C. and Castillo, Alfred and Tremblay, Monica Chiarini and Parsons, Jeffrey},
title = {Superimposition: Augmenting Machine Learning Outputs with Conceptual Models for Explainable AI},
year = {2020},
isbn = {978-3-030-65846-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65847-2_3},
doi = {10.1007/978-3-030-65847-2_3},
abstract = {Machine learning has become almost synonymous with Artificial Intelligence (AI). However, it has many challenges with one of the most important being explainable AI; that is, providing human-understandable accounts of why a machine learning model produces specific outputs. To address this challenge, we propose superimposition as a concept which uses conceptual models to improve explainability by mapping the features that are important to a machine learning model’s decision outcomes to a conceptual model of an application domain. Superimposition is a design method for supplementing machine learning models with structural elements that are used by humans to reason about reality and generate explanations. To illustrate the potential of superimposition, we present the method and apply it to a churn prediction problem.},
booktitle = {Advances in Conceptual Modeling: ER 2020 Workshops CMAI, CMLS, CMOMM4FAIR, CoMoNoS, EmpER, Vienna, Austria, November 3–6, 2020, Proceedings},
pages = {26–34},
numpages = {9},
keywords = {Human categorization, Explainable AI, Conceptual modeling, Superimposition, Machine learning, Artificial intelligence},
location = {Vienna, Austria}
}

@book{10.5555/3235555,
author = {Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
title = {Scaling up Machine Learning: Parallel and Distributed Approaches},
year = {2018},
isbn = {1108461743},
publisher = {Cambridge University Press},
address = {USA},
abstract = {This book presents an integrated collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes, in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits, trade-offs, and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters, concurrent programming frameworks including CUDA, MPI, MapReduce, and DryadLINQ, and learning settings (supervised, unsupervised, semi-supervised, and online learning). Extensive coverage of parallelization of boosted trees, SVMs, spectral clustering, belief propagation and other popular learning algorithms and deep dives into several applications make the book equally useful for researchers, students, and practitioners.}
}

@article{10.1145/3373269,
author = {Szentimrey, Hannah and Al-Hyari, Abeer and Foxcroft, Jeremy and Martin, Timothy and Noel, David and Grewal, Gary and Areibi, Shawki},
title = {Machine Learning for Congestion Management and Routability Prediction within FPGA Placement},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3373269},
doi = {10.1145/3373269},
abstract = {Placement for Field Programmable Gate Arrays (FPGAs) is one of the most important but time-consuming steps for achieving design closure. This article proposes the integration of three unique machine learning models into the state-of-the-art analytic placement tool GPlace3.0 with the aim of significantly reducing placement runtimes. The first model, MLCong, is based on linear regression and replaces the computationally expensive global router currently used in GPlace3.0 to estimate switch-level congestion. The second model, DLManage, is a convolutional encoder-decoder that uses heat maps based on the switch-level congestion estimates produced by MLCong to dynamically determine the amount of inflation to apply to each switch to resolve congestion. The third model, DLRoute, is a convolutional neural network that uses the previous heat maps to predict whether or not a placement solution is routable. Once a placement solution is determined to be routable, further optimization may be avoided, leading to improved runtimes. Experimental results obtained using 372 benchmarks provided by Xilinx Inc. show that when all three models are integrated into GPlace3.0, placement runtimes decrease by an average of 48%.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {37},
numpages = {25},
keywords = {routing-aware, heterogeneous, field programmable gate array, congestion, UltraScale architecture, Placement}
}

@article{10.1007/s00521-019-04626-7,
author = {Ray, Upasana and Chouhan, Usha and Verma, Neha},
title = {Comparative study of machine learning approaches for classification and prediction of selective caspase-3 antagonist for Zika virus drugs},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {15},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04626-7},
doi = {10.1007/s00521-019-04626-7},
abstract = {Zika virus (ZIKV) infection is an enervating and fast-growing disease. The increasing incidences of birth defects (microcephaly) in newborns due to ZIKV represent a public health problem. The viral infection is characterized by an increase in cell death of human neural progenitors and astrocytes, which can be inhibited by suppressing infection-induced caspase-3 activity. The aim of the present work is to develop classification models for the prediction of highly active and low active caspase-3 antagonists and to seek the important structural features related to the high anti-ZIKV property. Here, machine learning (ML) is applied in quantitative structure–activity relationship (QSAR) study. QSAR study is performed on the dataset by means of ML approaches, i.e., multiple linear regression (MLR), linear discriminant analysis (LDA), least square support vector machine (LS-SVM), deep neural net (DNN), k-nearest neighbor (KNN), na\"{\i}ve Bayes (NB) and random forest (RF). MLR, LDA are used for feature selection process and DNN, LS-SVM, KNN, NB, RF classifier for classification. The obtained results confirmed the discriminative capacity of the calculated descriptors. A good correlation is found by regression analysis between the observed and predicted activities as evident by their R2 (0.895) and Rpred2 (0.716) for the molecular descriptor dataset, R2 (0.892) and Rpred2 (0.736) for fingerprint dataset. The classification model obtained using RF (85.71%, 97.57%) and DNN (85.71%, 91.07%) classifier gave better accuracy than other approaches in fingerprint dataset and molecular descriptor dataset, respectively. This work provides an effective method to screen caspase-3 antagonists that will help out further in drug design for Zika virus.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {11311–11328},
numpages = {18},
keywords = {Antagonist, Machine learning, QSAR, Caspase-3}
}

@article{10.1016/j.asoc.2021.107745,
author = {Moldovan, Dorin and Slowik, Adam},
title = {Energy consumption prediction of appliances using machine learning and multi-objective binary grey wolf optimization for feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107745},
doi = {10.1016/j.asoc.2021.107745},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {23},
keywords = {MOORA, Energy consumption prediction of appliances, Feature selection, Multi-objective, Grey wolf optimizer}
}

@article{10.1016/j.jss.2019.02.031,
author = {Abdullah, Muhammad and Iqbal, Waheed and Erradi, Abdelkarim},
title = {Unsupervised learning approach for web application auto-decomposition into microservices},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.031},
doi = {10.1016/j.jss.2019.02.031},
journal = {J. Syst. Softw.},
month = may,
pages = {243–257},
numpages = {15},
keywords = {Cloud computing, Web applications, Microservices, Scalability, Application decomposition}
}

@article{10.1155/2021/9919992,
author = {Jiang, Fengqing and Chen, Xiao and Li, Xingwang},
title = {An Action Recognition Algorithm for Sprinters Using Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/9919992},
doi = {10.1155/2021/9919992},
abstract = {The advancements in modern science and technology have greatly promoted the progress of sports science. Advanced technological methods have been widely used in sports training, which have not only improved the scientific level of training but also promoted the continuous growth of sports technology and competition results. With the development of sports science and the gradual deepening of sport practices, the use of scientific training methods and monitoring approaches has improved the effect of sports training and athletes’ performance. This paper takes sprint as the research problem and constructs the image of sprinter’s action recognition based on machine learning. In view of the shortcomings of traditional dual-stream convolutional neural network for processing long-term video information, the time-segmented dual-stream network, based on sparse sampling, is used to better express the characteristics of long-term motion. First, the continuous video frame data is divided into multiple segments, and a short sequence of data containing user actions is formed by randomly sampling each segment of the video frame sequence. Next, it is applied to the dual-stream network for feature extraction. The optical flow image extraction involved in the dual-stream network is implemented by the system using the Lucas–Kanade algorithm. The system in this paper has been tested in actual scenarios, and the results show that the system design meets the expected requirements of the sprinters.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3240765.3270589,
author = {Cammarota, Rosario and Banerjee, Indranil and Rosenberg, Ofer},
title = {Machine Learning IP Protection},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1145/3240765.3270589},
doi = {10.1145/3240765.3270589},
abstract = {Machine learning, specifically deep learning is becoming a key technology component in application domains such as identity management, finance, automotive, and healthcare, to name a few. Proprietary machine learning models - Machine Learning IP - are developed and deployed at the network edge, end devices and in the cloud, to maximize user experience. With the proliferation of applications embedding Machine Learning IPs, machine learning models and hyper-parameters become attractive to attackers, and require protection. Major players in the semiconductor industry provide mechanisms on device to protect the IP at rest and during execution from being copied, altered, reverse engineered, and abused by attackers. In this work we explore system security architecture mechanisms and their applications to Machine Learning IP protection.},
booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
pages = {1–3},
numpages = {3},
location = {San Diego, CA, USA}
}

@inproceedings{10.5555/3327757.3327765,
author = {Wynen, Daan and Schmid, Cordelia and Mairal, Julien},
title = {Unsupervised learning of artistic styles with archetypal style analysis},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to deep image representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6584–6593},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1007/978-3-319-96136-1_23,
author = {Hines, Christine and Youssef, Abdou},
title = {Machine Learning Applied to Point-of-Sale Fraud Detection},
year = {2018},
isbn = {978-3-319-96135-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96136-1_23},
doi = {10.1007/978-3-319-96136-1_23},
abstract = {This paper applies machine learning (ML) techniques including neural networks, support vector machines Random Forest, and Adaboost to detecting insider fraud in restaurant point-of-sales data. With considerable engineering of the features, and by applying under-sampling techniques we show that ML techniques deliver very high fraud-detection performance. In particular, RandomForest can achieve 91% or better across all metrics when using a model trained on one restaurant to detect fraud in a separate restaurant. However, there must be sufficient fraud samples in the model for this to occur. Knowledge and techniques from this research could be used to develop a low-cost product to automate fraud detection for restaurant owners.},
booktitle = {Machine Learning and Data Mining in Pattern Recognition: 14th International Conference, MLDM 2018, New York, NY, USA, July 15-19, 2018, Proceedings, Part I},
pages = {283–295},
numpages = {13},
keywords = {Machine learning, Classification, Outlier detection, Fraud detection, Point-of-sale data},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3318216.3363338,
author = {Roy, Abhishek and Chhabra, Anshuman and Kamhoua, Charles A. and Mohapatra, Prasant},
title = {A moving target defense against adversarial machine learning},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363338},
doi = {10.1145/3318216.3363338},
abstract = {Adversarial Machine Learning has become the latest threat with the ubiquitous presence of machine learning. In this paper we propose a Moving Target Defense approach to defend against adversarial machine learning, i.e., instead of manipulating the machine learning algorithms, we suggest a switching scheme among machine learning algorithms to defend against adversarial attack. We model the problem as a Stackelberg game between the attacker and the defender. We propose a switching strategy which is the Stackelberg equilibrium of the game. We test our method against rational, and boundedly rational attackers. We show that designing a method against a rational attacker is enough in most scenarios. We show that even under very harsh constraints, e.g., no attack-cost, and availability of attacks which can bring down the accuracy to 0, it is possible to achieve reasonable accuracy in the context of classification. This work shows, that in addition to switching among algorithms, one can think of introducing randomness in tuning parameters, and model choices to achieve better defense against adversarial machine learning.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {383–388},
numpages = {6},
keywords = {moving target defense, cybersecurity, bounded rationality, adversarial machine learning},
location = {Arlington, Virginia},
series = {SEC '19}
}

@article{10.1007/s10845-021-01817-9,
author = {Kang, SungKu and Jin, Ran and Deng, Xinwei and Kenett, Ron S.},
title = {Challenges of modeling and analysis in cybermanufacturing: a review from a machine learning and computation perspective},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01817-9},
doi = {10.1007/s10845-021-01817-9},
abstract = {In Industry 4.0, smart manufacturing is facing its next stage, cybermanufacturing, founded upon advanced communication, computation, and control infrastructure. Cybermanufacturing will unleash the potential of multi-modal manufacturing data, and provide a new perspective called computation service, as a part of service-oriented architecture (SOA), where on-demand computation requests throughout manufacturing operations are seamlessly satisfied by data analytics and machine learning. However, the complexity of information technology infrastructure leads to fundamental challenges in modeling and analysis under cybermanufacturing, ranging from information-poor datasets to a lack of reproducibility of analytical studies. Nevertheless, existing reviews have focused on the overall architecture of cybermanufacturing/SOA or its technical components (e.g., communication protocol), rather than the potential bottleneck of computation service with respect to modeling and analysis. In this paper, we review the fundamental challenges with respect to modeling and analysis in cybermanufacturing. Then, we introduce the existing efforts in computation pipeline recommendation, which aims at identifying an optimal sequence of method options for data analytics/machine learning without time-consuming trial-and-error. We envision computation pipeline recommendation as a promising research field to address the fundamental challenges in cybermanufacturing. We also expect that computation pipeline recommendation can be a driving force to flexible and resilient manufacturing operations in the post-COVID-19 industry.},
journal = {J. Intell. Manuf.},
month = aug,
pages = {415–428},
numpages = {14},
keywords = {Computation pipelines, Cybermanufacturing, Industry 4.0, Machine learning, Manufacturing modeling and analysis}
}

@inproceedings{10.1145/3485279.3485306,
author = {Miller, Mark and Yao, Powen and Jothi, Adityan and Zhao, Andrew and Swieso, Sloan and Zyda, Michael},
title = {Virtual Equipment System: Toward Peripersonal Equipment Slots with Machine Learning},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485306},
doi = {10.1145/3485279.3485306},
abstract = {Peripersonal Equipment Slots are locations to place equipment that are egocentric to the user but do not reside within the user’s personal space. Instead, these slots reside within the user’s arm reach, known as peripersonal space. In this paper, we present our initial approach and results from our attempt to realize peripersonal equipment slots through the use of machine learning.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {30},
numpages = {2},
keywords = {peripersonal space, peripersonal equipment},
location = {Virtual Event, USA},
series = {SUI '21}
}

