@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {software product lines, performance prediction, machine learning, configurable systems},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/3155562.3155625,
author = {Jamshidi, Pooyan and Siegmund, Norbert and Velez, Miguel and K\"{a}stner, Christian and Patel, Akshay and Agarwal, Yuvraj},
title = {Transfer learning for performance modeling of configurable systems: an exploratory analysis},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {497–508},
numpages = {12},
keywords = {transfer learning, Performance analysis},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1007/s10515-020-00273-8,
author = {Velez, Miguel and Jamshidi, Pooyan and Sattler, Florian and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {ConfigCrusher: towards white-box performance analysis for configurable systems},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00273-8},
doi = {10.1007/s10515-020-00273-8},
abstract = {Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems.},
journal = {Automated Software Engg.},
month = dec,
pages = {265–300},
numpages = {36},
keywords = {Dynamic analysis, Static analysis, Performance analysis, Configurable systems}
}

@article{10.1007/s00500-019-04659-z,
author = {Li, Ning and Huang, Jincai and Feng, Yanghe},
title = {Human performance modeling and its uncertainty factors affecting decision making: a survery},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04659-z},
doi = {10.1007/s00500-019-04659-z},
abstract = {This paper introduces the background and connotation of human performance modeling (HPM), HPM models, and the application of artificial intelligence algorithms in HPM. It deeply analyzes the connotation and uncertainty of each model and finally puts forward its military application. The aim is to provide relevant researchers in the field with an in-depth understanding of domain knowledge and related uncertainties and to indicate future research directions. The first part is a general overview of human factors engineering, where the definition, origin, research field, importance, and general problems of HPM are elaborated. The composition of the man–machine system and its corresponding relationship with the observe–orient–decide–act loop are described. The second part reviews the models of perception, cognition, understanding, and decision making. Among them, models of cognition consist of visual search, visual sampling, mental workload, and goals, operators, methods, and selection rules; models of action consist of Hick–Hyman law, Fitts’s law, and manual control theory. The third part is a review of the source and importance of the integrated models and focuses on the principles, composition, and successful application cases of the three models, namely SAINT, IMPRINT, and ACT-R. The fourth part is a review of the application of the algorithms and models in the fields of artificial intelligence, deep learning, and data mining in analyzing multivariate datasets in HPM. In addition, future HPM military applications are presented.},
journal = {Soft Comput.},
month = feb,
pages = {2851–2871},
numpages = {21},
keywords = {Military applications, Integrated models of HPM, Cognitive model, Uncertainty decision making, Human performance modeling, Human factors}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-41418-4_17,
author = {Chen, Yuntianyi and Gu, Yongfeng and He, Lulu and Xuan, Jifeng},
title = {Regression Models for Performance Ranking of Configurable Systems: A Comparative Study},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_17},
doi = {10.1007/978-3-030-41418-4_17},
abstract = {Finding the best configurations for a highly configurable system is challenging. Existing studies learned regression models to predict the performance of potential configurations. Such learning suffers from the low accuracy and the high effort of examining the actual performance for data labeling. A recent approach uses an iterative strategy to sample a small number of configurations from the training pool to reduce the number of sampled ones. In this paper, we conducted a comparative study on the rank-based approach of configurable systems with four regression methods. These methods are compared on 21 evaluation scenarios of 16 real-world configurable systems. We designed three research questions to check the impacts of different methods on the rank-based approach. We find out that the decision tree method of Classification And Regression Tree (CART) and the ensemble learning method of Gradient Boosted Regression Trees (GBRT) can achieve better ranks among four regression methods under evaluation; the sampling strategy in the rank-based approach is useful to save the cost of sampling configurations; the measurement, i.e., rank difference correlates with the relative error in several evaluation scenarios.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {243–258},
numpages = {16},
keywords = {Software configurations, Sampling, Performance prediction, Regression methods},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {software product line engineering, security, safety, mapping study, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2988287.2989159,
author = {Abedi, Ali and Brecht, Tim},
title = {Examining Relationships Between 802.11n Physical Layer Transmission Feature Combinations},
year = {2016},
isbn = {9781450345026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988287.2989159},
doi = {10.1145/2988287.2989159},
abstract = {To increase throughput the 802.11n standard introduced several physical layer transmission features including a short guard interval wider channels, and MIMO. Since obtaining peak throughput depends on choosing the combination of physical layer features (configuration) best suited for the channel conditions, the large number of configurations greatly complicates the decision. A deeper understanding of relationships between configurations under a variety of channel conditions should simplify the choices and improve the performance of algorithms selecting configurations. Examples of such algorithms include: rate and channel width adaptation, frame aggregation, and MIMO setting optimization.We propose a methodology for assessing the possibility of accurate estimation of the frame error rate (FER) of one configuration from the FER of another. Using devices that support up to 3 spatial streams (96 configurations), we conduct experiments under a variety of channel conditions to quantify relationships between configurations. We find that interesting relationships exist between many different configurations. Our results show that in 6 of the 7 scenarios studied at most five configurations are required to accurately estimate the error rate of all remaining 91 configurations and in the other scenario at most 15 configurations are required. Although we show that these relationships may change over time, perhaps most surprising is that relationships have been found over periods of up to one hour. These findings suggest optimization algorithms should not need to measure the FER of many configurations, but instead can sample a small subset of configurations to accurately estimate the FER of other configurtions. To demonstrate this possibility, we make simple modifications to the Minstrel HT rate adaptation algorithm to exploit relationships and observe improvements in throughput of up to 28%.},
booktitle = {Proceedings of the 19th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {229–238},
numpages = {10},
keywords = {wifi, relationship, rate adaptation, physical layer features, mobility, link adaptation, frame error rate, correlation, characterization, 802.11},
location = {Malta, Malta},
series = {MSWiM '16}
}

@inproceedings{10.1145/3477244.3477985,
author = {van der Sanden, Bram and Li, Yonghui and van den Aker, Joris and Akesson, Benny and Bijlsma, Tjerk and Hendriks, Martijn and Triantafyllidis, Kostas and Verriet, Jacques and Voeten, Jeroen and Basten, Twan},
title = {Model-driven system-performance engineering for cyber-physical systems},
year = {2021},
isbn = {9781450387125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477244.3477985},
doi = {10.1145/3477244.3477985},
abstract = {System-Performance Engineering (SysPE) encompasses modeling formalisms, methods, techniques, and industrial practices to design systems for performance, where performance is taken integrally into account during the whole system life cycle. Industrial SysPE state of practice is generally model-based. Due to the rapidly increasing complexity of systems, there is a need to develop and establish model-driven methods and techniques. To structure the field of SysPE, we identify (1) industrial challenges motivating the importance of SysPE, (2) scientific challenges that need to be addressed to establish model-driven SysPE, (3) important focus areas for SysPE and (4) best practices. We conducted a survey to collect feedback on our views. The responses were used to update and validate the identified challenges, focus areas, and best practices. The final result is presented in this paper. Interesting observations are that industry sees a need for better design-space exploration support, more than for additional performance modeling and analysis techniques. Also tools and integral methods for SysPE need attention. From the identified focus areas, scheduling and supervisory control is seen as lacking established best practices.},
booktitle = {Proceedings of the 2021 International Conference on Embedded Software},
pages = {11–22},
numpages = {12},
keywords = {system-performance engineering, model-driven design, CPS},
location = {Virtual Event},
series = {EMSOFT '21}
}

@inproceedings{10.1145/3236024.3236074,
author = {Jamshidi, Pooyan and Velez, Miguel and K\"{a}stner, Christian and Siegmund, Norbert},
title = {Learning to sample: exploiting similarities across environments to learn performance models for configurable systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236074},
doi = {10.1145/3236024.3236074},
abstract = {Most software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transfer-learning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {71–82},
numpages = {12},
keywords = {transfer learning, configurable systems, Software performance},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {regularization, configurable systems, adversarial learning, Software performance prediction},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3030207.3030216,
author = {Valov, Pavel and Petkovich, Jean-Christophe and Guo, Jianmei and Fischmeister, Sebastian and Czarnecki, Krzysztof},
title = {Transferring Performance Prediction Models Across Different Hardware Platforms},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030216},
doi = {10.1145/3030207.3030216},
abstract = {Many software systems provide configuration options relevant to users, which are often called features. Features influence functional properties of software systems as well as non-functional ones, such as performance and memory consumption. Researchers have successfully demonstrated the correlation between feature selection and performance. However, the generality of these performance models across different hardware platforms has not yet been evaluated.We propose a technique for enhancing generality of performance models across different hardware environments using linear transformation. Empirical studies on three real-world software systems show that our approach is computationally efficient and can achieve high accuracy (less than 10% mean relative error) when predicting system performance across 23 different hardware platforms. Moreover, we investigate why the approach works by comparing performance distributions of systems and structure of performance models across different platforms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {39–50},
numpages = {12},
keywords = {regression trees, performance modelling, model transfer, linear transformation},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2830772.2830780,
author = {Ardalani, Newsha and Lestourgeon, Clint and Sankaralingam, Karthikeyan and Zhu, Xiaojin},
title = {Cross-architecture performance prediction (XAPP) using CPU code to predict GPU performance},
year = {2015},
isbn = {9781450340342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830772.2830780},
doi = {10.1145/2830772.2830780},
abstract = {GPUs have become prevalent and more general purpose, but GPU programming remains challenging and time consuming for the majority of programmers. In addition, it is not always clear which codes will benefit from getting ported to GPU. Therefore, having a tool to estimate GPU performance for a piece of code before writing a GPU implementation is highly desirable. To this end, we propose Cross-Architecture Performance Prediction (XAPP), a machine-learning based technique that uses only single-threaded CPU implementation to predict GPU performance.Our paper is built on the two following insights: i) Execution time on GPU is a function of program properties and hardware characteristics. ii) By examining a vast array of previously implemented GPU codes along with their CPU counterparts, we can use established machine learning techniques to learn this correlation between program properties, hardware characteristics and GPU execution time. We use an adaptive two-level machine learning solution. Our results show that our tool is robust and accurate: we achieve 26.9% average error on a set of 24 real-world kernels. We also discuss practical usage scenarios for XAPP.},
booktitle = {Proceedings of the 48th International Symposium on Microarchitecture},
pages = {725–737},
numpages = {13},
keywords = {performance modeling, machine learning, cross-platform prediction, GPU},
location = {Waikiki, Hawaii},
series = {MICRO-48}
}

@inproceedings{10.1145/3205651.3208267,
author = {Xuan, Jifeng and Gu, Yongfeng and Ren, Zhilei and Jia, Xiangyang and Fan, Qingna},
title = {Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208267},
doi = {10.1145/3205651.3208267},
abstract = {A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.In this paper, we propose genetic configuration sampling, a new method of learning a sampling strategy for configuration-related faults. Genetic configuration sampling encodes a sequence of sampling actions as a chromosome in the genetic algorithm. Given a set of known configuration-related faults, genetic configuration sampling evolves the sequence of sampling actions and applies the learnt sequence to new configuration data. A pilot study on three highly-configurable systems shows that genetic configuration sampling performs well among nine sampling strategies in comparison.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1624–1631},
numpages = {8},
keywords = {software configurations, highly-configurable systems, genetic improvement, fault detection, configuration sampling},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.patcog.2013.06.026,
author = {Parodi, Marianela and G\'{o}mez, Juan C.},
title = {Legendre polynomials based feature extraction for online signature verification. Consistency analysis of feature combinations},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2013.06.026},
doi = {10.1016/j.patcog.2013.06.026},
abstract = {In this paper, feature combinations associated with the most commonly used time functions related to the signing process are analyzed, in order to provide some insight on their actual discriminative power for online signature verification. A consistency factor is defined to quantify the discriminative power of these different feature combinations. A fixed-length representation of the time functions associated with the signatures, based on Legendre polynomials series expansions, is proposed. The expansion coefficients in these series are used as features to model the signatures. Two different signature styles, namely, Western and Chinese, from a publicly available Signature Database are considered to evaluate the performance of the verification system. Two state-of-the-art classifiers, namely, Support Vector Machines and Random Forests are used in the verification experiments. Error rates comparable to the ones reported over the same signature datasets in a recent Signature Verification Competition, show the potential of the proposed approach. The experimental results, also show that there is a good correlation between the consistency factor and the verification errors, suggesting that consistency values could be used to select the optimal feature combination. HighlightsA feature extraction approach based on Legendre series representation of the time functions associated with the signatures is proposed.A consistency factor is proposed to quantify the discriminative power of different combinations of the time functions.The correlation between the proposed consistency factor and the verification performance of a feature combination is analyzed.A recent signature database, containing Western and Chinese signatures is used. The verification performance is quantified based on log-likelihood ratios.},
journal = {Pattern Recogn.},
month = jan,
pages = {128–140},
numpages = {13},
keywords = {Online signature verification, Legendre polynomials, Consistency factor}
}

@inproceedings{10.1109/ASE.2013.6693089,
author = {Guo, Jianmei and Czarnecki, Krzysztof and Apely, Sven and Siegmundy, Norbert and Wasowski, Andrzej},
title = {Variability-aware performance prediction: a statistical learning approach},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693089},
doi = {10.1109/ASE.2013.6693089},
abstract = {Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {301–311},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1109/ASE.2015.15,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof},
title = {Performance prediction of configurable software systems by fourier learning},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.15},
doi = {10.1109/ASE.2015.15},
abstract = {Understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant. Given a software system with n optional features, measuring all its 2n possible configurations to determine their performances is usually infeasible. Thus, various techniques have been proposed to predict software performances based on a small sample of measured configurations. We propose a novel algorithm based on Fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user, while using minimum number of samples up to a constant factor. Empirical results on the case studies constructed from real-world configurable systems demonstrate the effectiveness of our algorithm.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {365–373},
numpages = {9},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s11554-014-0483-1,
author = {Concha, David and Cabido, Ra\'{u}l and Pantrigo, Juan Jos\'{e} and Montemayor, Antonio S.},
title = {Performance evaluation of a 3D multi-view-based particle filter for visual object tracking using GPUs and multicore CPUs},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-014-0483-1},
doi = {10.1007/s11554-014-0483-1},
abstract = {This paper presents a deep and extensive performance analysis of the particle filter (PF) algorithm for a very compute intensive 3D multi-view visual tracking problem. We compare different implementations and parameter settings of the PF algorithm in a CPU platform taking advantage of the multithreading capabilities of the modern processors and a graphics processing unit (GPU) platform using NVIDIA CUDA computing environment as developing framework. We extend our experimental study to each individual stage of the PF algorithm, and evaluate the quality versus performance trade-off among different ways to design these stages. We have observed that the GPU platform performs better than the multithreaded CPU platform when handling a large number of particles, but we also demonstrate that hybrid CPU/GPU implementations can run almost as fast as only GPU solutions.},
journal = {J. Real-Time Image Process.},
month = aug,
pages = {309–327},
numpages = {19},
keywords = {Performance evaluation, Particle filtering, Multi-view, GPU computing, 3D visual tracking}
}

@inproceedings{10.1145/1370788.1370805,
author = {Azzeh, Mohammad and Neagu, Daniel and Cowling, Peter},
title = {Improving analogy software effort estimation using fuzzy feature subset selection algorithm},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370805},
doi = {10.1145/1370788.1370805},
abstract = {One of the major problems with software project management is the difficulty to predict accurately the required effort for developing software applications. Analogy Software effort estimation appears well suited to model problems of this nature. The analogy approach may be viewed as a systematic development of the expert opinion through experience learning and exposure to analogue case studies. The accuracy of such model depends on characteristics of datasets. This paper examines the impact of feature subset selection algorithms on improving the accuracy of analogy software effort estimation model. We proposed a feature subset selection algorithm based on fuzzy logic for analogy software effort estimation models. Validation using two established datasets (ISBSG, Desharnais) shows that using fuzzy features subset selection algorithm in analogy software effort estimation contribute to significant results as other algorithms: Hill climbing, Forward subset selection, and backward subset selection do.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {71–78},
numpages = {8},
keywords = {analogy software effort estimation, feature selection, fuzzy logic},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@article{10.1016/j.artmed.2009.07.010,
author = {Kim, Gilhan and Kim, Yeonjoo and Lim, Heuiseok and Kim, Hyeoncheol},
title = {An MLP-based feature subset selection for HIV-1 protease cleavage site analysis},
year = {2010},
issue_date = {February, 2010},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {48},
number = {2–3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2009.07.010},
doi = {10.1016/j.artmed.2009.07.010},
abstract = {Objective: In recent years, several machine learning approaches have been applied to modeling the specificity of the human immunodeficiency virus type 1 (HIV-1) protease cleavage domain. However, the high dimensional domain dataset contains a small number of samples, which could misguide classification modeling and its interpretation. Appropriate feature selection can alleviate the problem by eliminating irrelevant and redundant features, and thus improve prediction performance. Methods: We introduce a new feature subset selection method, FS-MLP, that selects relevant features using multi-layered perceptron (MLP) learning. The method includes MLP learning with a training dataset and then feature subset selection using decompositional approach to analyze the trained MLP. Our method is able to select a subset of relevant features in high dimensional, multi-variate and non-linear domains. Results: Using five artificial datasets that represent four data types, we verified the FS-MLP performance with seven other feature selection methods. Experimental results showed that the FS-MLP is superior at high dimensional, multi-variate and non-linear domains. In experiments with HIV-1 protease cleavage dataset, the FS-MLP selected a set of 14 highly relevant features among 160 original features. On a validation set of 131 test instances, classifiers that used the 14 features showed about 95% accuracy which outperformed other seven methods in terms of accuracy and the number of features. Conclusions: Our experimental results indicate that the FS-MLP is effective in analyzing multi-variate, non-linear and high dimensional datasets such as HIV-1 protease cleavage dataset. The 14 relevant features which were selected by the FS-MLP provide us with useful insights into the HIV-1 cleavage site domain as well. The FS-MLP is a useful method for computational sequence analysis in general.},
journal = {Artif. Intell. Med.},
month = feb,
pages = {83–89},
numpages = {7},
keywords = {Multi-layered perceptron, HIV-1 protease cleavage site prediction, Feature selection, Dimension reduction}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {performance, fourier transform, feature interactions, boolean functions},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106204,
author = {Luthmann, Lars and Stephan, Andreas and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106204},
doi = {10.1145/3106195.3106204},
abstract = {Real-time requirements are crucial for embedded software in many modern application domains of software product lines. Hence, techniques for modeling and analyzing time-critical software have to be lifted to software product line engineering, too. Existing approaches extend timed automata (TA) by feature constraints to so-called featured timed automata (FTA) facilitating efficient verification of real-time properties for entire product lines in a single run. In this paper, we propose a novel modeling formalism, called configurable parametric timed automata (CoPTA), extending expressiveness of FTA by supporting freely configurable and therefore a-priori unbounded timing intervals for real-time constraints, which are defined as feature attributes in extended feature models with potentially infinite configuration spaces. We further describe an efficient test-suite generation methodology for CoPTA models, achieving location coverage on every possible model configuration. Finally, we present evaluation results gained from applying our tool implementation to a collection of case studies, demonstrating efficiency improvements compared to a variant-by-variant analysis.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {104–113},
numpages = {10},
keywords = {Timed Automata, Software Product Lines, Real-Time Systems, Model-based Testing},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr\"{u}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.cor.2004.07.011,
author = {(Sundar) Balakrishnan, P. V. and Gupta, Rakesh and Jacob, Varghese S.},
title = {An investigation of mating and population maintenance strategies in hybrid genetic heuristics for product line designs},
year = {2006},
issue_date = {March 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {33},
number = {3},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2004.07.011},
doi = {10.1016/j.cor.2004.07.011},
abstract = {This research builds on prior work on developing near optimal solutions to the product line design problems within the conjoint analysis framework. In this research, we investigate and compare different genetic algorithm operators; in particular, we examine systematically the impact of employing alternative population maintenance strategies and mutation techniques within our problem context. Two alternative population maintenance methods, that we term ''Emigration'' and ''Malthusian'' strategies, are deployed to govern how individual product lines in one generation are carried over to the next generation. We also allow for two different types of reproduction methods termed ''Equal Opportunity'' in which the parents to be paired for mating are selected with equal opportunity and a second based on always choosing the best string in the current generation as one of the parents which is referred to as the ''Queen bee'', while the other parent is randomly selected from the set of parent strings. We also look at the impact of integrating the artificial intelligence approach with a traditional optimization approach by seeding the GA with solutions obtained from a Dynamic Programming heuristic proposed by others. A detailed statistical analysis is also carried out to determine the impact of various problem and technique aspects on multiple measures of performance through means of a Monte Carlo simulation study. Our results indicate that such proposed procedures are able to provide multiple ''good'' solutions. This provides more flexibility for the decision makers as they now have the opportunity to select from a number of very good product lines. The results obtained using our approaches are encouraging, with statistically significant improvements averaging 5% or more, when compared to the traditional benchmark of the heuristic dynamic programming technique.},
journal = {Comput. Oper. Res.},
month = mar,
pages = {639–659},
numpages = {21},
keywords = {Marketing, Malthusian, Hybrid heuristics, Genetic algorithms, Dynamic programming, Decision support, Conjoint analysis, Attribute importance}
}

@inproceedings{10.1145/568760.568800,
author = {Barber, K. Suzanne and Holt, Jim and Baker, Geoff},
title = {Performance evaluation of domain reference architectures},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568800},
doi = {10.1145/568760.568800},
abstract = {Architectures embody the requirements expressed by system stakeholders, and the type of architecture used to capture a given set of requirements dictates when evaluation can occur and what will be evaluated. This research aims to leverage requirements and a resulting architecture dictated by the problem domain and captured early in the lifecycle. Thus, the research goal is to provide early performance evaluation in an effort to convey the most accurate blueprint to system implementers specifically and system stakeholders in general. A new software architecture evaluation tool called Arcade, developed to support the Systems Engineering Process Activities (SEPA), automates early performance evaluation of software architectures using simulation. SEPA suggests a comprehensive approach to capture and represent different types of requirements as a multi-level software architecture. One SEPA architecture level, the Domain Reference Architecture (DRA) encompasses performance characteristics inherent to the domain in terms of what processes, data, and timing are required, rather than how a system should be implemented. Performance evaluation of a DRA can provide qualitative data to (1) aid in identification and validation of domain related performance concerns, and (2) provide system implementers with performance guidelines towards satisfying the performance constraints inherent to the domain. The range of performance statistics Arcade is capable of analyzing is demonstrated through a case study of a Motorola e-business project.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {225–232},
numpages = {8},
keywords = {software architecture, performance},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1145/2896377.2901505,
author = {Qureshi, Mubashir Adnan and Mahimkar, Ajay and Qiu, Lili and Ge, Zihui and Puthenpura, Sarat and Mir, Nabeel and Ahuja, Sanjeev},
title = {Automated Test Location Selection For Cellular Network Upgrades},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901505},
doi = {10.1145/2896377.2901505},
abstract = {Cellular networks are constantly evolving due to frequent changes in radio access and end user equipment technologies, applications, and traffic. Network upgrades should be performed with extreme caution since millions of users heavily depend on the cellular networks. Before upgrading the entire network, it is important to conduct field evaluation of upgrades.The choice and number of field test locations have significant impact on the time-to-market and confidence in how well various network upgrades will work out in the rest of the network. We propose a novel approach -- Reflection to automatically determine where to conduct the upgrade field tests to accurately identify important features that affect the upgrade and predict for the performance of untested locations. We demonstrate its effectiveness using real traces collected from a major US cellular network as well as synthetic traces.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {371–372},
numpages = {2},
keywords = {test planning, diagnosis, cellular network},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1007/s11042-010-0522-2,
author = {Ranathunga, Lochandaka and Zainuddin, Roziati and Abdullah, Nor Aniza},
title = {Performance evaluation of the combination of Compacted Dither Pattern Codes with Bhattacharyya classifier in video visual concept depiction},
year = {2011},
issue_date = {August    2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {2},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-010-0522-2},
doi = {10.1007/s11042-010-0522-2},
abstract = {High dimensionality and multi-feature combinations can have negative effect on visual concept classification. In our research, we formulated a new compacted form which is Compacted Dither Pattern Code (CDPC) as a chromatic syntactic feature for visual feature extraction. The effectiveness of CDPC with Bhattacharyya classifier for irregular shapes based visual concepts depiction is reported in this paper. The proposed technique can reduce feature space and computational complexity while maintaining visual data mining and retrieval accuracy in high standard. Our system was empowered with Bhattacharyya classifier which has improved efficiency by considering one numeric value which is the Bhattacharyya coefficient. Experiments were conducted on various combinations and compared with different visual descriptors and classifiers. The first experiment illustrates the comparison of the CDPC based results with well known feature space reduction classes. The second and third experiments demonstrate the effectiveness of our approach with multiple perspectives of performance measures including various concepts.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {263–289},
numpages = {27},
keywords = {Visual concept depiction, Video content analysis, Support Vector Machines, Compacted Dither Pattern Code (CDPC), Bhattacharyya measure, 3D colour histogram}
}

@inproceedings{10.1145/1403375.1403608,
author = {Mayer, Albrecht and Hellwig, Frank},
title = {System performance optimization methodology for Infineon's 32-bit automotive microcontroller architecture},
year = {2008},
isbn = {9783981080131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1403375.1403608},
doi = {10.1145/1403375.1403608},
abstract = {Microcontrollers are the core part of automotive Electronic Control Units (ECUs). A significant investment of the ECU manufacturers and even their customers is linked to the specified microcontroller family. To preserve this investment it is required to continuously design new generations of the microcontroller with hardware and software compatibility but higher system performance and/or lower cost. The challenge for the microcontroller manufacturer is to get the relevant inputs for improving the system performance, since a microcontroller is used by many customers in many different applications.For Infineon's latest TriCore® based 32-bit microcontroller product line, the required statistical data is gathered by using the trace features of the Emulation Device (ED). Infineon's customers use EDs in their unchanged target system and application environment. With an analytical methodology and based on this statistical data, the performance improvements of different SoC architecture and implementation options can be quantified. This allows an objective assessment of improvement options by comparing their performance cost ratios.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {962–966},
numpages = {5},
location = {Munich, Germany},
series = {DATE '08}
}

@article{10.1145/3491056,
author = {Nam, Yun Seong and Gao, Jianfei and Bothra, Chandan and Ghabashneh, Ehab and Rao, Sanjay and Ribeiro, Bruno and Zhan, Jibin and Zhang, Hui},
title = {Xatu: Richer Neural Network Based Prediction for Video Streaming},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3491056},
doi = {10.1145/3491056},
abstract = {The performance of Adaptive Bitrate (ABR) algorithms for video streaming depends on accurately predicting the download time of video chunks. Existing prediction approaches (i) assume chunk download times are dominated by network throughput; and (ii) apriori cluster sessions (e.g., based on ISP and CDN) and only learn from sessions in the same cluster. We make three contributions. First, through analysis of data from real-world video streaming sessions, we show (i) apriori clustering prevents learning from related clusters; and (ii) factors such as the Time to First Byte (TTFB) are key components of chunk download times but not easily incorporated into existing prediction approaches. Second, we propose Xatu, a new prediction approach that jointly learns a neural network sequence model with an interpretable automatic session clustering method. Xatu learns clustering rules across all sessions it deems relevant, and models sequences with multiple chunk-dependent features (e.g., TTFB) rather than just throughput. Third, evaluations using the above datasets and emulation experiments show that Xatu significantly improves prediction accuracies by 23.8% relative to CS2P (a state-of-the-art predictor). We show Xatu provides substantial performance benefits when integrated with multiple ABR algorithms including MPC (a well studied ABR algorithm), and FuguABR (a recent algorithm using stochastic control) relative to their default predictors (CS2P and a fully connected neural network respectively). Further, Xatu combined with MPC outperforms Pensieve, an ABR based on deep reinforcement learning.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {44},
numpages = {26},
keywords = {adaptive bitrate algorithms, neural networks, predictive models, video streaming}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@inproceedings{10.1145/3358960.3379127,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Transferring Pareto Frontiers across Heterogeneous Hardware Environments},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379127},
doi = {10.1145/3358960.3379127},
abstract = {Software systems provide user-relevant configuration options called features. Features affect functional and non-functional system properties, whereas selections of features represent system configurations. A subset of configuration space forms a Pareto frontier of optimal configurations in terms of multiple properties, from which a user can choose the best configuration for a particular scenario. However, when a well-studied system is redeployed on a different hardware, information about property value and the Pareto frontier might not apply. We investigate whether it is possible to transfer this information across heterogeneous hardware environments. We propose a methodology for approximating and transferring Pareto frontiers of configurable systems across different hardware environments. We approximate a Pareto frontier by training an individual predictor model for each system property, and by aggregating predictions of each property into an approximated frontier. We transfer the approximated frontier across hardware by training a transfer model for each property, by applying it to a respective predictor, and by combining transferred properties into a frontier. We evaluate our approach by modeling Pareto frontiers as binary classifiers that separate all system configurations into optimal and non-optimal ones. Thus we can assess quality of approximated and transferred frontiers using common statistical measures like sensitivity and specificity. We test our approach using five real-world software systems from the compression domain, while paying special attention to their performance. Evaluation results demonstrate that accuracy of approximated frontiers depends linearly on predictors' training sample sizes, whereas transferring introduces only minor additional error to a frontier even for small training sizes.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {12–23},
numpages = {12},
keywords = {regression trees, performance prediction, linear regression, configurable software, Pareto frontier transferring, Pareto frontier},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1007/s11257-007-9035-8,
author = {Sotiropoulos, Dionysios N. and Lampropoulos, Aristomenis S. and Tsihrintzis, George A.},
title = {MUSIPER: a system for modeling music similarity perception based on objective feature subset selection},
year = {2008},
issue_date = {September 2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {4},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-007-9035-8},
doi = {10.1007/s11257-007-9035-8},
abstract = {We explore the use of objective audio signal features to model the individualized (subjective) perception of similarity between music files. We present  MUSIPER , a content-based music retrieval system which constructs music similarity perception models of its users by associating different music similarity measures to different users. Specifically, a user-supplied relevance feedback procedure and related neural network-based incremental learning allows the system to determine which subset of a set of objective features approximates more accurately the subjective music similarity perception of a specific user. Our implementation and evaluation of MUSIPER verifies the relation between subsets of objective features and individualized music similarity perception and exhibits significant improvement in individualized perceived similarity in subsequent music retrievals.},
journal = {User Modeling and User-Adapted Interaction},
month = sep,
pages = {315–348},
numpages = {34},
keywords = {User model, User driven feature selection, Relevance feedback, Music similarity perception, Individualization, Content-based retrieval}
}

@inproceedings{10.1145/2832087.2832096,
author = {Natarajan, Chitra and Beckmann, Carl and Nguyen, Anthony and Araya-Polo, Mauricio and Fossum, Tryggve and Hohl, Detlef},
title = {Simulating stencil-based application on future Xeon Phi processor},
year = {2015},
isbn = {9781450340090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832087.2832096},
doi = {10.1145/2832087.2832096},
abstract = {An important application for hydrocarbon exploration is simulated on a performance model of a novel Intel architecture. The accuracy of the simulation models is demonstrated by correlating against an existing processor first and then against high-accuracy simulation of the new architecture. The results show that key architectural features of the coming Knights Landing processor will positively impact the performance of the application.},
booktitle = {Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems},
articleno = {7},
numpages = {10},
keywords = {stencil, simulation, performance, hydrocarbon exploration, KNL},
location = {Austin, Texas},
series = {PMBS '15}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Quality assurance, Machine learning, Software testing, Software variability, Configurable system, Software product line}
}

@inproceedings{10.1145/3417990.3419488,
author = {Schiedermeier, Maximilian},
title = {A concern-oriented software engineering methodology for micro-service architectures},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3419488},
doi = {10.1145/3417990.3419488},
abstract = {Component-Based Systems (CBS) allow for the construction of modular, highly scalable software. Decomposing a system into individually maintainable and deployable components enables a targeted replication of performance bottlenecks, and promotes code modularity. Over the last years, the Micro-Service Architecture (MSA) style has become a popular approach to maximize the benefits of CBS. However, MSA introduces new challenges, by imposing a conceptual and technological stack on adherent projects, which require new critical design choices. Throughout my PhD I want to investigate to which extent a systematic reuse of MSA solutions of various granularity can streamline MSA application development by guiding design decisions.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {28},
numpages = {5},
keywords = {representational state transfer, model-driven engineering, micro-service architectures, concern-oriented reuse},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1162/evco_a_00215,
author = {Kerschke, Pascal and Kotthoff, Lars and Bossek, Jakob and Hoos, Holger H. and Trautmann, Heike},
title = {Leveraging TSP Solver Complementarity through Machine Learning},
year = {2018},
issue_date = {Winter 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {26},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00215},
doi = {10.1162/evco_a_00215},
abstract = {The Travelling Salesperson Problem (TSP) is one of the best-studied NP-hard problems. Over the years, many different solution approaches and solvers have been developed. For the first time, we directly compare five state-of-the-art inexact solvers—namely, LKH, EAX, restart variants of those, and MAOS—on a large set of well-known benchmark instances and demonstrate complementary performance, in that different instances may be solved most effectively by different algorithms. We leverage this complementarity to build an algorithm selector, which selects the best TSP solver on a per-instance basis and thus achieves significantly improved performance compared to the single best solver, representing an advance in the state of the art in solving the Euclidean TSP. Our in-depth analysis of the selectors provides insight into what drives this performance improvement.},
journal = {Evol. Comput.},
month = dec,
pages = {597–620},
numpages = {24},
keywords = {machine learning., performance modeling, automated algorithm selection, Travelling Salesperson Problem}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@article{10.1007/s11257-011-9114-8,
author = {Wang, Yang and Kobsa, Alfred},
title = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-011-9114-8},
doi = {10.1007/s11257-011-9114-8},
abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data.},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {41–82},
numpages = {42},
keywords = {User modeling, User experiment, Product line architecture, Privacy preferences, Privacy laws, Performance evaluation, Disclosure behavior, Compliance}
}

@phdthesis{10.5555/AAI28156239,
author = {Iorio, Francesco and Demke, Brown, Angela and Bianca, Schroeder,},
advisor = {Cristiana, Amza, and Christopher, Beck, J.},
title = {Inductive Transfer Learning for Incremental Modeling and Optimization of Cloud Systems Performance},
year = {2021},
isbn = {9798522942144},
publisher = {University of Toronto (Canada)},
abstract = {Due to the cost of sampling system performance, it is expensive to obtain performance characteristics of a complex computer system in different configurations. As an alternative, in this dissertation, we propose to reuse existing partial and full performance models and explicitly model the effects of configuration variations, with a goal of substantially reducing the time and effort required to perform performance reasoning and optimization on cloud-based systems, applications and services.We introduce Model Mapping, a novel inductive transfer learning technique for incremental performance modeling of highly configurable systems. Model Mapping captures many explicit and latent types of dynamic system evolution, including configuration changes, scaling and hardware upgrades, by deriving and modeling these kinds of incremental transformations between system and/or application instances, over time. Modeling these transformations allows us to build accurate models for new configuration instances with just a few samples.We experimentally test our method on a variety of system performance modeling and optimization scenarios, using a carefully designed experimental testbed and realistic benchmarks, to obtain insight on the method's applicability in real-world cloud computing environments.Among other examples, we show how our method can be used to quickly derive an accurate resource allocation split that optimizes a given overall performance goal for co-hosted applications in a virtualized environment.Compared to using conventional direct and incremental modeling techniques, our method achieves higher accuracy by up to an order of magnitude when the sampling budget is extremely limited, in particular when samples are limited to between 0% to 5% of an exhaustive sampling budget.},
note = {AAI28156239}
}

@inproceedings{10.1145/3106237.3106238,
author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
title = {Using bad learners to find good configurations},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106238},
doi = {10.1145/3106237.3106238},
abstract = {Finding the optimally performing configuration of a software system for a given setting is often challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, building an accurate performance model can be very expensive (and is often infeasible in practice). The central insight of this paper is that exact performance values (e.g., the response time of a software system) are not required to rank configurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the difference between actual and predicted performance) can still be used rank configurations and hence find the optimal configuration. This novel rank-based approach allows us to significantly reduce the cost (in terms of number of measurements of sample configuration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 software systems and demonstrate that our approach is beneficial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Sampling, SBSE, Rank-based method, Performance Prediction},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1007/s10515-017-0225-2,
author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
title = {Faster discovery of faster system configurations with spectral learning},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0225-2},
doi = {10.1007/s10515-017-0225-2},
abstract = {Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors--less than 10% prediction error, with a standard deviation of less than 2%. When compared to the state of the art, WHAT (a) requires 2---10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance.},
journal = {Automated Software Engg.},
month = jun,
pages = {247–277},
numpages = {31},
keywords = {Spectral learning, Search-based software engineering, Sampling, Performance prediction, Decision trees}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {type checking, product-line analysis, fuji, feature-oriented programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1007/s00158-013-0891-z,
author = {Woodruff, Matthew J. and Reed, Patrick M. and Simpson, Timothy W.},
title = {Many objective visual analytics: rethinking the design of complex engineered systems},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {48},
number = {1},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-013-0891-z},
doi = {10.1007/s00158-013-0891-z},
abstract = {Many cognitive and computational challenges accompany the design of complex engineered systems. This study proposes the many-objective visual analytics (MOVA) framework as a new approach to the design of complex engineered systems. MOVA emphasizes learning through problem reformulation, enabled by visual analytics and many-objective search. This study demonstrates insights gained by evolving the formulation of a General Aviation Aircraft (GAA) product family design problem. This problem's considerable complexity and difficulty, along with a history encompassing several formulations, make it well-suited to demonstrate the MOVA framework. The MOVA framework results compare a single objective, a two objective, and a ten objective formulation for optimizing the GAA product family. Highly interactive visual analytics are exploited to demonstrate how decision biases can arise for lower dimensional, highly aggregated problem formulations.},
journal = {Struct. Multidiscip. Optim.},
month = jul,
pages = {201–219},
numpages = {19},
keywords = {Product family design, Multidimensional data visualization, Multi-objective optimization}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3368089.3409744,
author = {Baranov, Eduard and Legay, Axel and Meel, Kuldeep S.},
title = {Baital: an adaptive weighted sampling approach for improved t-wise coverage},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409744},
doi = {10.1145/3368089.3409744},
abstract = {The rise of highly configurable complex software and its widespread usage requires design of efficient testing methodology. t-wise coverage is a leading metric to measure the quality of the testing suite and the underlying test generation engine. While uniform sampling-based test generation is widely believed to be the state of the art approach to achieve t-wise coverage in presence of constraints on the set of configurations, such a scheme often fails to achieve high t-wise coverage in presence of complex constraints. In this work, we propose a novel approach Baital, based on adaptive weighted sampling using literal weighted functions, to generate test sets with high t-wise coverage. We demonstrate that our approach reaches significantly higher t-wise coverage than uniform sampling. The novel usage of literal weighted sampling leaves open several interesting directions, empirical as well as theoretical, for future research.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1114–1126},
numpages = {13},
keywords = {t-wise coverage, Weighted sampling, Configurable software},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {software product lines, searching configuration spaces, finding optimal configurations},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1186/s13677-017-0083-2,
author = {Armstrong, Django and Djemame, Karim and Kavanagh, Richard},
title = {Towards energy aware cloud computing application construction},
year = {2017},
issue_date = {December  2017},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {6},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-017-0083-2},
doi = {10.1186/s13677-017-0083-2},
abstract = {The energy consumption of cloud computing continues to be an area of significant concern as data center growth continues to increase. This paper reports on an energy efficient interoperable cloud architecture realised as a cloud toolbox that focuses on reducing the energy consumption of cloud applications holistically across all deployment models. The architecture supports energy efficiency at service construction, deployment and operation. We discuss our practical experience during implementation of an architectural component, the Virtual Machine Image Constructor (VMIC), required to facilitate construction of energy aware cloud applications. We carry out a performance evaluation of the component on a cloud testbed. The results show the performance of Virtual Machine construction, primarily limited by available I/O, to be adequate for agile, energy aware software development. We conclude that the implementation of the VMIC is feasible, incurs minimal performance overhead comparatively to the time taken by other aspects of the cloud application construction life-cycle, and make recommendations on enhancing its performance.},
journal = {J. Cloud Comput.},
month = dec,
articleno = {83},
numpages = {13},
keywords = {Virtualization, Performance evaluation, Energy efficiency, Cloud interoperability, Cloud engineering, Cloud computing, Cloud architectures}
}

@article{10.1162/evco_a_00194,
author = {Mu\~{n}oz, Mario A. and Smith-Miles, Kate A.},
title = {Performance analysis of continuous black-box optimization algorithms via footprints in instance space},
year = {2017},
issue_date = {Winter 2017},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00194},
doi = {10.1162/evco_a_00194},
abstract = {This article presents a method for the objective assessment of an algorithm's strengths and weaknesses. Instead of examining the performance of only one or more algorithms on a benchmark set, or generating custom problems that maximize the performance difference between two algorithms, our method quantifies both the nature of the test instances and the algorithm performance. Our aim is to gather information about possible phase transitions in performance, that is, the points in which a small change in problem structure produces algorithm failure. The method is based on the accurate estimation and characterization of the algorithm footprints, that is, the regions of instance space in which good or exceptional performance is expected from an algorithm. A footprint can be estimated for each algorithm and for the overall portfolio. Therefore, we select a set of features to generate a common instance space, which we validate by constructing a sufficiently accurate prediction model. We characterize the footprints by their area and density. Our method identifies complementary performance between algorithms, quantifies the common features of hard problems, and locates regions where a phase transition may lie.},
journal = {Evol. Comput.},
month = dec,
pages = {529–554},
numpages = {26},
keywords = {performance prediction, footprint analysis, exploratory landscape analysis, black-box continuous optimization, Algorithm selection}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@inproceedings{10.1145/2517208.2517209,
author = {Siegmund, Norbert and von Rhein, Alexander and Apel, Sven},
title = {Family-based performance measurement},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517209},
doi = {10.1145/2517208.2517209},
abstract = {Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {95–104},
numpages = {10},
keywords = {performance prediction, featurehouse, family-based analysis},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1145/2020976.2069288,
author = {Lilja, David J. and Mirandola, Raffaela and Sachs, Kai},
title = {Paper Abstracts of the 2nd International Conferernce on Performance Engineering (ICPE 2011)},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2069288},
doi = {10.1145/2020976.2069288},
abstract = {Foreword This issue of SEN contains the abstracts of the papers, which were presented on the Second Joint WOSP/SIPEW International Conference (ICPE 2011), held in Karlsruhe, Germany, March 14-16, 2011, now established as a regular event known as ACM/SPEC International Conference on Performance Engineering (ICPE). The primary goal of this conference series is to bridge the gap between theory and practice in the field of computer systems performance engineering by providing a forum for sharing ideas and experiences between industry and academia. This years conference brought together researchers and industry practitioners to share and present their experiences, discuss challenges, and report on both state-of-the-art research and work-in-progress on performance engineering of software and systems, including performance measurement, modeling, benchmark design, and run-time performance management. The ICPE gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of computer systems performance engineering. The call for papers attracted 63 research and 24 industrial paper submissions from Europe, Asia, Africa, and North America. The program committees accepted 19 full research papers and 7 short papers together with 13 industrial papers. These papers cover a variety of topics, including performance modeling and techniques and measurement and benchmarking strategies for adaptive systems, power management, virtualized environments, and large-scale and distributed systems. We are confident that you will find the abstracts stimulating and that they will provide you with many new ideas and insights. The full paper are available at the ACM Digital Library. David J. Lilja: Program Co-Chair - Research Track Raffaela Mirandola: Program Co-Chair - Research Track Kai Sachs: Program Co-Chair - Industrial Track},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {36–53},
numpages = {18}
}

@inproceedings{10.1145/3194554.3194576,
author = {Alsafrjalani, Mohamad Hammam and Adegbija, Tosiron},
title = {TaSaT: Thermal-Aware Scheduling and Tuning Algorithm for Heterogeneous and Configurable Embedded Systems},
year = {2018},
isbn = {9781450357241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194554.3194576},
doi = {10.1145/3194554.3194576},
abstract = {Heterogeneous and configurable systems (HaCS) have been widely used to meet stringent runtime performance and energy constraints in embedded systems. However, no prior work has addressed the emerging runtime thermal constraints in these systems. To leverage HaCS' capabilities to meet thermal constraints, in addition to performance and energy constraints, we propose TaSaT, a Thermal-aware Scheduling and Tuning algorithm for HaCS. TaSaT reduces HaCS temperature while meeting performance and energy constraints during runtime, without a priori knowledge of applications.},
booktitle = {Proceedings of the 2018 Great Lakes Symposium on VLSI},
pages = {75–80},
numpages = {6},
keywords = {configurable caches, dynamic optimization, heterogeneous and configurable hardware, thermal-aware scheduling, tuning},
location = {Chicago, IL, USA},
series = {GLSVLSI '18}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1016/j.jvcir.2017.11.002,
author = {Gao, Guangyu and Han, Cen and Ma, Kun and Liu, Chi Harold and Ding, Gangyi and Liu, Erwu},
title = {Optimal feature combination analysis for crowd saliency prediction},
year = {2018},
issue_date = {Jan 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {50},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2017.11.002},
doi = {10.1016/j.jvcir.2017.11.002},
journal = {J. Vis. Comun. Image Represent.},
month = jan,
pages = {1–8},
numpages = {8},
keywords = {Face detection, Visual attention, Random forest, Saliency, Crowd}
}

@inproceedings{10.1007/978-3-642-12239-2_39,
author = {Nemati, Shahla and Basiri, Mohammad Ehsan},
title = {Particle swarm optimization for feature selection in speaker verification},
year = {2010},
isbn = {3642122388},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12239-2_39},
doi = {10.1007/978-3-642-12239-2_39},
abstract = {The problem addressed in this paper concerns the feature subset selection for an automatic speaker verification system. An effective algorithm based on particle swarm optimization is proposed here for discovering the best feature combinations. After feature reduction phase, feature vectors are applied to a Gaussian mixture model which is a text-independent speaker verification model. The performance of proposed system is compared to the performance of a genetic algorithm-based system and the baseline algorithm. Experimentation is carried out, using TIMIT corpora. The results of experiments indicate that with the optimized feature subset, the performance of the system is improved. Moreover, the speed of verification is significantly increased since by use of PSO, number of features is reduced over 85% which consequently decrease the complexity of our ASV system.},
booktitle = {Proceedings of the 2010 International Conference on Applications of Evolutionary Computation - Volume Part I},
pages = {371–380},
numpages = {10},
keywords = {speaker verification, particle swarm optimization (PSO), genetic algorithm (GA), feature selection (FS), Gaussian mixture model (GMM)},
location = {Istanbul, Turkey},
series = {EvoApplicatons'10}
}

@inproceedings{10.1145/3405671.3405812,
author = {Lavinia, Yukhe and Durairajan, Ramakrishnan and Rejaie, Reza and Willinger, Walter},
title = {Challenges in Using ML for Networking Research: How to Label If You Must},
year = {2020},
isbn = {9781450380430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405671.3405812},
doi = {10.1145/3405671.3405812},
abstract = {Leveraging innovations in Machine Learning (ML) research is of great current interest to researchers across the sciences, including networking research. However, using ML for networking poses challenging new problems that have been responsible for slowing the pace of innovation and the adoption of ML in the networking domain. Among the main problems are a well-known lack of data in general and representative data in particular, an overall inability to label data at scale, unknown data quality due to differences in data collection strategies, and data privacy issues that are unique to network data. Motivated by these challenges, we describe the design of Emerge1, a novel framework to support efforts to dEmocratize the use of ML for nEtwoRkinG rEsearch. In particular, Emerge focuses on the problem of providing a low-cost, scalable, and high-quality methodology for labeling networking data. To illustrate the benefits of Emerge, we use publicly available network measurement datasets from Caida's Ark project and create and evaluate data labels for them in a programmable fashion.},
booktitle = {Proceedings of the Workshop on Network Meets AI &amp; ML},
pages = {21–27},
numpages = {7},
keywords = {Weak supervision, Labeling network data at scale},
location = {Virtual Event, USA},
series = {NetAI '20}
}

@inproceedings{10.1145/2961111.2962602,
author = {Han, Xue and Yu, Tingting},
title = {An Empirical Study on Performance Bugs for Highly Configurable Software Systems},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962602},
doi = {10.1145/2961111.2962602},
abstract = {Modern computer systems are highly-configurable, complicating the testing and debugging process. The sheer size of the configuration space makes the quality of software even harder to achieve. Performance is one of the key aspects of non-functional qualities, where performance bugs can cause significant performance degradation and lead to poor user experience. However, performance bugs are difficult to expose, primarily because detecting them requires specific inputs, as well as a specific execution environment (e.g., configurations). While researchers have developed techniques to analyze, quantify, detect, and fix performance bugs, we conjecture that many of these techniques may not be effective in highly-configurable systems. In this paper, we study the challenges that configurability creates for handling performance bugs. We study 113 real-world performance bugs, randomly sampled from three highly-configurable open-source projects: Apache, MySQL and Firefox. The findings of this study provide a set of lessons learned and guidance to aid practitioners and researchers to better handle performance bugs in highly-configurable software systems.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {23},
numpages = {10},
keywords = {Performance, Empirical Study, Configuration},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.5555/1937055.1937104,
author = {Guo, Qi and White, Ryen W. and Dumais, Susan T. and Wang, Jue and Anderson, Blake},
title = {Predicting query performance using query, result, and user interaction features},
year = {2010},
publisher = {LE CENTRE DE HAUTES ETUDES INTERNATIONALES D'INFORMATIQUE DOCUMENTAIRE},
address = {Paris, FRA},
abstract = {The high cost of search engine evaluation makes techniques for accurately predicting engine effectiveness valuable. In this paper we present a study in which we use features of the query, search results, and user interaction with the search results to predict query performance. We establish which features are most useful, study the effect of different classes of features, and examine the effect of query frequency on our predictions. Our findings show that performance predictions using result and interaction features are substantially better than those obtained using only query features. Such results can support automated search engine evaluation methods and new query processing capabilities.},
booktitle = {Adaptivity, Personalization and Fusion of Heterogeneous Information},
pages = {198–201},
numpages = {4},
keywords = {query performance prediction},
location = {Paris, France},
series = {RIAO '10}
}

@inproceedings{10.1145/3238147.3238175,
author = {Bao, Liang and Liu, Xin and Xu, Ziheng and Fang, Baoyin},
title = {AutoConfig: automatic configuration tuning for distributed message systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238175},
doi = {10.1145/3238147.3238175},
abstract = {Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {29–40},
numpages = {12},
keywords = {weighted Latin hypercube sampling, distributed message system, comparison-based model, automatic configuration tuning},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1016/j.jss.2013.01.038,
author = {Thurimella, Anil Kumar and Br\"{u}Gge, Bernd},
title = {A mixed-method approach for the empirical evaluation of the issue-based variability modeling},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.01.038},
doi = {10.1016/j.jss.2013.01.038},
abstract = {Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1831–1849},
numpages = {19},
keywords = {Variability, Software product lines, Requirements engineering, Rationale management, Mixed-methods, Empirical software engineering}
}

@article{10.1007/s00779-019-01285-2,
author = {Yu, Mingchao and Li, Gongfa and Jiang, Du and Jiang, Guozhang and Tao, Bo and Chen, Disi},
title = {Hand medical monitoring system based on machine learning and optimal EMG feature set},
year = {2019},
issue_date = {Dec 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {6},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01285-2},
doi = {10.1007/s00779-019-01285-2},
abstract = {Considering that serious hand function damage will greatly affect the daily life of patients, its recovery mainly depends on the regular inspection and manual training of medical staff, and medical monitoring based on bioelectric signals can largely replace manual re-examination as autonomous rehabilitation technology. So, for the rationality of feature selection and the diversity of classifier design in the gesture recognition process based on electromyography (EMG) signals, this paper proposes a hand medical monitoring system based on feature selection method of feature subset average recognition rate and optimal machine learning algorithm selection, which mainly depends on the prediction of hand movement. At the same time, since most experiments are conducted in different non-public proprietary databases, the comparison between various gesture recognition methods can only be analyzed to a certain extent. Therefore, this paper uses the DB1 dataset in the large publicly available NinaPro database and combines with presently well-known 11 time-domain (TD) features and 5 frequency domain (FD) features, then uses the support vector machine (SVM) classifier to comparative analysis total 136 feature combinations under various feature numbers. Under the premise of ensuring the overall recognition rate of electromyography gesture, this method will be able to reduce the number of features in feature set, according to the change of the average remove redundant features, and construct an optimal reduced EMG feature set. Finally, through the four common hand motion classifiers based on machine learning: SVM, back propagation neural network, linear discriminant analysis, and K-nearest neighbor, this paper tests and verifies the separability of the optimal reduced EMG feature set, and based on this, selects the optimal hand motion classifier to build the optimal hand motion recognition system, improve the hand medical monitoring system, and provide technical reference for the construction of real-time medical monitoring system.},
journal = {Personal Ubiquitous Comput.},
month = aug,
pages = {1991–2007},
numpages = {17},
keywords = {EMG signal, Feature extraction, Machine learning, EMG feature set, Gesture recognition, Medical monitoring}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Challenges, Multiple-case study, Software product lines, Variability management}
}

@inproceedings{10.1145/3427921.3450255,
author = {Han, Xue and Yu, Tingting and Pradel, Michael},
title = {ConfProf: White-Box Performance Profiling of Configuration Options},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450255},
doi = {10.1145/3427921.3450255},
abstract = {Modern software systems are highly customizable through configuration options. The sheer size of the configuration space makes it challenging to understand the performance influence of individual configuration options and their interactions under a specific usage scenario. Software with poor performance may lead to low system throughput and long response time. This paper presents ConfProf, a white-box performance profiling technique with a focus on configuration options. ConfProf helps developers understand how configuration options and their interactions influence the performance of a software system. The approach combines dynamic program analysis, machine learning, and feedback-directed configuration sampling to profile the program execution and analyze the performance influence of configuration options. Compared to existing approaches, ConfProf uses a white-box approach combined with machine learning to rank performance-influencing configuration options from execution traces. We evaluate the approach with 13 scenarios of four real-world, highly-configurable software systems. The results show that ConfProf ranks performance-influencing configuration options with high accuracy and outperform a state of the art technique.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {1–8},
numpages = {8},
keywords = {software performance, performance profiling},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3242102.3242128,
author = {Khokhar, Muhammad Jawad and Spetebroot, Thierry and Barakat, Chadi},
title = {A Methodology for Performance Benchmarking of Mobile Networks for Internet Video Streaming},
year = {2018},
isbn = {9781450359603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242102.3242128},
doi = {10.1145/3242102.3242128},
abstract = {Video streaming is a dominant contributor to the global Internet traffic. Consequently, gauging network performance w.r.t. the video Quality of Experience (QoE) is of paramount importance to both telecom operators and regulators. Modern video streaming systems, e.g. YouTube, have huge catalogs of billions of different videos that vary significantly in content type. Owing to this difference, the QoE of different videos as perceived by end users can vary for the same network Quality of Service (QoS). In this paper, we present a methodology for benchmarking performance of mobile operators w.r.t Internet video that considers this variation in QoE. We take a data-driven approach to build a predictive model using supervised machine learning (ML) that takes into account a wide range of videos and network conditions. To that end, we first build and analyze a large catalog of YouTube videos. We then propose and demonstrate a framework of controlled experimentation based on active learning to build the training data for the targeted ML model. Using this model, we then devise YouScore, an estimate of the percentage of YouTube videos that may play out smoothly under a given network condition. Finally, to demonstrate the benchmarking utility of YouScore, we apply it on an open dataset of real user mobile network measurements to compare performance of mobile operators for video streaming.},
booktitle = {Proceedings of the 21st ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {217–225},
numpages = {9},
keywords = {quality of experience, mobile network measurements, internet video, controlled experimentation, active learning},
location = {Montreal, QC, Canada},
series = {MSWIM '18}
}

@article{10.1007/s00779-017-1042-0,
author = {Wang, Wen-Fong and Yang, Ching-Yu and Wu, Yan-Fu},
title = {SVM-based classification method to identify alcohol consumption using ECG and PPG monitoring},
year = {2018},
issue_date = {April     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-017-1042-0},
doi = {10.1007/s00779-017-1042-0},
abstract = {Driving under the influence (DUI) of alcohol ("drunk driving") is dangerous and may cause serious harm to people and damage to property. To address this problem, this study developed a system for identifying excess alcohol consumption. Electrocardiogram (ECG) and photoplethysmography (PPG) sensors and intoxilyzers were used to acquire signals regarding the ECG, PPG, and alcohol consumption levels of participants before and after drinking. The signals were preprocessed, segmented, and subjected to feature extraction using specific algorithms to produce ECG and PPG training and test data. Based on the ECG, PPG, and alcohol consumption data we developed a fast and accurate identification scheme using the support vector machine (SVM) algorithm for identifying alcohol consumption. Optimized SVM classifiers were trained using the training data, and the test data were applied to verify the identification performance of the trained SVMs. The identification performance of the proposed classifiers achieved 95% on average. In this study, different feature combinations were tested to select the optimum technological configuration. Because the PPG and ECG features produce identical classification performance and the PPG features are more convenient to acquire, the technological configuration based on PPG is definitely preferable for developing smart and wearable devices for the identification of DUI.},
journal = {Personal Ubiquitous Comput.},
month = apr,
pages = {275–287},
numpages = {13},
keywords = {Wearable, SVM, PPG, ECG, Alcohol}
}

@inproceedings{10.1145/2934872.2934906,
author = {Liu, Zaoxing and Manousis, Antonis and Vorsanger, Gregory and Sekar, Vyas and Braverman, Vladimir},
title = {One Sketch to Rule Them All: Rethinking Network Flow Monitoring with UnivMon},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2934906},
doi = {10.1145/2934872.2934906},
abstract = {Network management requires accurate estimates of metrics for traffic engineering (e.g., heavy hitters), anomaly detection (e.g., entropy of source addresses), and security (e.g., DDoS detection). Obtaining accurate estimates given router CPU and memory constraints is a challenging problem. Existing approaches fall in one of two undesirable extremes: (1) low fidelity general-purpose approaches such as sampling, or (2) high fidelity but complex algorithms customized to specific application-level metrics. Ideally, a solution should be both general (i.e., supports many applications) and provide accuracy comparable to custom algorithms. This paper presents UnivMon, a framework for flow monitoring which leverages recent theoretical advances and demonstrates that it is possible to achieve both generality and high accuracy. UnivMon uses an application-agnostic data plane monitoring primitive; different (and possibly unforeseen) estimation algorithms run in the control plane, and use the statistics from the data plane to compute application-level metrics. We present a proof-of-concept implementation of UnivMon using P4 and develop simple coordination techniques to provide a ``one-big-switch'' abstraction for network-wide monitoring. We evaluate the effectiveness of UnivMon using a range of trace-driven evaluations and show that it offers comparable (and sometimes better) accuracy relative to custom sketching solutions.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {101–114},
numpages = {14},
keywords = {Streaming Algorithm, Sketching, Flow Monitoring},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

@article{10.1145/3393667,
author = {Bauer, Jan and Aschenbruck, Nils},
title = {Towards a Low-cost RSSI-based Crop Monitoring},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3393667},
doi = {10.1145/3393667},
abstract = {The continuous monitoring of crop growth is crucial for site-specific and sustainable farm management in the context of precision agriculture. With the help of precise in situ information, agricultural practices, such as irrigation, fertilization, and plant protection, can be dynamically adapted to the changing needs of individual sites, thereby supporting yield increases and resource optimization. Nowadays, IoT technology with networked sensors deployed in greenhouses and farmlands already contributes to in situ information. In addition to existing soil sensors for moisture or nutrient monitoring, there are also (mainly optical) sensors to assess growth developments and vital conditions of crops. This article presents a novel and complementary approach for a low-cost crop sensing that is based on temporal variations of the signal strength of low-power IoT radio communication. To this end, the relationship between crop growth, represented by the leaf area index (LAI), and the attenuation of signal propagation of low-cost radio transceivers is investigated. Real-world experiments in wheat fields show a significant correlation between LAI and received signal strength indicator (RSSI) time series. Moreover, influencing meteorological factors are identified and their effects are analyzed. Including these factors, a multiple linear model is finally developed that enables an RSSI-based LAI estimation with great potential.},
journal = {ACM Trans. Internet Things},
month = jun,
articleno = {21},
numpages = {26},
keywords = {wireless sensor network (WSN), vegetation, temperature, received signal strength indicator&nbsp;(RSSI), radio propagation, precision agriculture, link quality&nbsp;(LQ), leaf area index&nbsp;(LAI), humidity, Internet-of-Things (IoT)}
}

@article{10.1016/j.eswa.2017.04.017,
author = {Jiang, Shancheng and Chin, Kwai-Sang and Wang, Long and Qu, Gang and Tsui, Kwok L.},
title = {Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {82},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.04.017},
doi = {10.1016/j.eswa.2017.04.017},
abstract = {A deep learning approach to model the patients demand for different key resources.Normal GA is improved for feature selection tasks by redesigning its key operators.An SAE-based pre-training process is designed to improve the initialization of DNN.The case study is implemented with real data collected from a hospital.Proposed approach can be used as a universal model for demand forecasting in hospital. A well-performed demand forecasting can provide outpatient department (OPD) managers with essential information for staff scheduling and rostering, considering the non-reservation policy of OPD in China. Based on the results reported by relevant studies, most approaches have focused on forecasting the overall amount of patient flow and ignored the demand for other key resources in OPD or similar department. Moreover, few studies have conducted feature selection before training a forecast model, which is a significant pre-processing operation of data mining and widely applied for knowledge discovery in expert and intelligent system. This study develops a novel hybrid methodology to forecast the patients demand for different key resources in OPD, by combining a new feature selection method and a deep learning approach. A modified version of genetic algorithm (MGA) is proposed for feature selection. The key operators of normal genetic algorithm are redesigned to utilize useful information provided by filter-based feature selection and feature combinations. A feedforward deep neural network is introduced as the forecast model, and the initial parameter set is generated from a stacked autoencoder-based pre-training process to overcome the optimization challenges in constructing deep architectures. In order to evaluate the performance of our methodology, it is applied to an OPD located at Northeast China. The results are compared with those obtained from combinations of other feature selection methods and demand forecasting models. Compared with GA and PCA, MGA improves the quality and efficiency of feature selection, with less selected features to get higher forecast accuracy. Pre-trained DNN optimally strengthens the advantage of MGA, compared with MLR, ARIMAX and SANN. The combination of MGA and pre-trained DNN possesses strongest predictive power among all involved combinations. Furthermore, the results of proposed methodology are crucial prerequisites for staff scheduling and resource allocation in OPD. Elite features obtained by MGA can provide practical insights on potential association between manifold feature combinations and demand variance.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {216–230},
numpages = {15},
keywords = {Stacked autoencoder, Modified genetic algorithm, Feature selection, Demand forecasting in hospital, Deep learning}
}

@article{10.1145/3226111,
author = {Liu, Xiaobai and Xu, Qian and Mu, Yadong and Yang, Jiadi and Lin, Liang and Yan, Shuicheng},
title = {High-Precision Camera Localization in Scenes with Repetitive Patterns},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3226111},
doi = {10.1145/3226111},
abstract = {This article presents a high-precision multi-modal approach for localizing moving cameras with monocular videos, which has wide potentials in many intelligent applications, including robotics, autonomous vehicles, and so on. Existing visual odometry methods often suffer from symmetric or repetitive scene patterns, e.g., windows on buildings or parking stalls. To address this issue, we introduce a robust camera localization method that contributes in two aspects. First, we formulate feature tracking, the critical step of visual odometry, as a hierarchical min-cost network flow optimization task, and we regularize the formula with flow constraints, cross-scale consistencies, and motion heuristics. The proposed regularized formula is capable of adaptively selecting distinctive features or feature combinations, which is more effective than traditional methods that detect and group repetitive patterns in a separate step. Second, we develop a joint formula for integrating dense visual odometry and sparse GPS readings in a common reference coordinate. The fusion process is guided with high-order statistics knowledge to suppress the impacts of noises, clusters, and model drifting. We evaluate the proposed camera localization method on both public video datasets and a newly created dataset that includes scenes full of repetitive patterns. Results with comparisons show that our method can achieve comparable performance to state-of-the-art methods and is particularly effective for addressing repetitive pattern issues.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {66},
numpages = {21},
keywords = {flow optimization, feature matching, Visual odometry}
}

@article{10.1016/j.matcom.2018.11.007,
author = {Wang, Kung-Jeng and Lee, Kun-Shan and Liao, Jia-Hong},
title = {Technology cooperation modeling of multiple profit-centered business units: A system dynamics framework},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {0378-4754},
url = {https://doi.org/10.1016/j.matcom.2018.11.007},
doi = {10.1016/j.matcom.2018.11.007},
journal = {Math. Comput. Simul.},
month = aug,
pages = {195–220},
numpages = {26},
keywords = {System dynamics, Technology cooperation, Capital deployment}
}

@inproceedings{10.1007/978-3-030-68821-9_36,
author = {Wang, Xuesong and Liu, Yizhi and Liao, Zhuhua and Zhao, Yijiang},
title = {DeepFM-Based Taxi Pick-Up Area Recommendation},
year = {2021},
isbn = {978-3-030-68820-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68821-9_36},
doi = {10.1007/978-3-030-68821-9_36},
abstract = {Recommending accurately pick-up area with sparse GPS data is valuable and still challenging to increase taxi drivers’ profits and reduce fuel consumption. In recent years, the recommendation approach based on matrix factorization has been proposed to deal with sparsity. However, it is not accurate enough due to the regardless of the interaction effect between features. Therefore, this paper proposes DeepFM-based taxi pick-up area recommendation. Firstly, the research area is divided into grid area of equal size, the pick-up point information is extracted from the original GPS trajectory data, the pick-up point information and POI data are mapped to the grid area, the corresponding grid attributes are calculated and the grid feature matrix is constructed; Then, DeepFM is used to mine the combined relationship between the grid features, combining spatial information to recommend the most suitable grid area for drivers; Finally, the performance evaluation is carried out using DiDi's public data. The experimental results show that this method can significantly improve the quality of the recommended results and is superior to some existing recommended methods.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part V},
pages = {407–421},
numpages = {15},
keywords = {Feature interaction, DeepFM, Taxi pick-up area recommendation, Location-based service, Trajectory mining}
}

@inproceedings{10.1145/2785956.2787489,
author = {Biswas, Sanjit and Bicket, John and Wong, Edmund and Musaloiu-E, Raluca and Bhartia, Apurv and Aguayo, Dan},
title = {Large-scale Measurements of Wireless Network Behavior},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787489},
doi = {10.1145/2785956.2787489},
abstract = {Meraki is a cloud-based network management system which provides centralized configuration, monitoring, and network troubleshooting tools across hundreds of thousands of sites worldwide. As part of its architecture, the Meraki system has built a database of time-series measurements of wireless link, client, and application behavior for monitoring and debugging purposes. This paper studies an anonymized subset of measurements, containing data from approximately ten thousand radio access points, tens of thousands of links, and 5.6 million clients from one-week periods in January 2014 and January 2015 to provide a deeper understanding of real-world network behavior. This paper observes the following phenomena: wireless network usage continues to grow quickly, driven most by growth in the number of devices connecting to each network. Intermediate link delivery rates are common indoors across a wide range of deployment environments. Typical access points share spectrum with dozens of nearby networks, but the presence of a network on a channel does not predict channel utilization. Most access points see 2.4 GHz channel utilization of 20% or more, with the top decile seeing greater than 50%, and the majority of the channel use contains decodable 802.11 headers.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {153–165},
numpages = {13},
keywords = {network usage data, large-scale measurements, 802.11},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1109/ASE.2019.00065,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Accurate modeling of performance histories for evolving software systems},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00065},
doi = {10.1109/ASE.2019.00065},
abstract = {Learning from the history of a software system's performance behavior does not only help discovering and locating performance bugs, but also identifying evolutionary performance patterns and general trends, such as when technical debt accumulates. Exhaustive regression testing is usually impractical, because rigorous performance benchmarking requires executing a realistic workload per revision, which results in large execution times. In this paper, we propose a novel active revision sampling approach, which aims at tracking and understanding a system's performance history by approximating the performance behavior of a software system across all of its revisions. In a nutshell, we iteratively sample and measure the performance of specific revisions that help us building an exact performance-evolution model, and we use Gaussian Process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement. We have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real-world software systems. Our evaluation demonstrates that Gaussian Process models are able to accurately estimate the performance-evolution history of real-world software systems with only few measurements and to reveal interesting behaviors and trends.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {640–652},
numpages = {13},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.3233/AIC-170744,
author = {Sutcliffe, Geoff},
title = {The CADE-26 automated theorem proving system competition – CASC-26},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {30},
number = {6},
issn = {0921-7126},
url = {https://doi.org/10.3233/AIC-170744},
doi = {10.3233/AIC-170744},
abstract = {The CADE ATP System Competition (CASC) is the annual evaluation of fully automatic, classical logic Automated Theorem Proving (ATP) systems. CASC-26 was the twenty-second competition in the CASC series. Twenty-one ATP systems and system variants competed in the various competition divisions. An outline of the competition design, and a commentated summary of the results, are presented.},
journal = {AI Commun.},
month = jan,
pages = {419–432},
numpages = {14},
keywords = {competition, Automated theorem proving}
}

@article{10.1504/ijcvr.2021.116558,
author = {Mandal, Sunandan and Singh, Bikesh Kumar and Thakur, Kavita},
title = {Majority voting-based hybrid feature selection in machine learning paradigm for epilepsy detection using EEG},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2021.116558},
doi = {10.1504/ijcvr.2021.116558},
abstract = {This article presents a combination of statistical and discrete wavelet transform (DWT)-based features for the identification of epileptic seizures in electroencephalogram (EEG) signals. A total of 150 quantitative features are extracted from EEG signals. A multi-criteria hybrid feature selection is proposed by combining six feature ranking methods using the majority voting technique to identify the most relevant EEG markers. Kernel-based support vector machine is used to evaluate the proposed approach along with a hybrid classifier namely support vector neural network (SVNN) which is a combination of support vector machine (SVM) and artificial neural network (ANN). For performance evaluation of the proposed method, a benchmarked database is used. A comparative study of various types of SVM and SVNN with ten-fold and hold-out cross-validation techniques is conducted. The highest classification accuracy (CA) of 98.18% and 100% sensitivity is achieved with a fine Gaussian SVM classifier with hold-out data division protocol.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {385–400},
numpages = {15},
keywords = {classification, multi-criteria feature selection, wavelet transform, epilepsy, EEG quantitative features}
}

@article{10.1016/j.infsof.2019.03.007,
author = {Pradhan, Dipesh and Wang, Shuai and Yue, Tao and Ali, Shaukat and Liaaen, Marius},
title = {Search-based test case implantation for testing untested configurations},
year = {2019},
issue_date = {Jul 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {111},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.007},
doi = {10.1016/j.infsof.2019.03.007},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {22–36},
numpages = {15},
keywords = {Test case implantation, Genetic algorithms, Multi-objective optimization, Search}
}

@inproceedings{10.1109/SEAMS.2017.11,
author = {Jamshidi, Pooyan and Velez, Miguel and K\"{a}stner, Christian and Siegmund, Norbert and Kawthekar, Prasad},
title = {Transfer learning for improving model predictions in highly configurable software},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.11},
doi = {10.1109/SEAMS.2017.11},
abstract = {Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {31–41},
numpages = {11},
keywords = {transfer learning, model prediction, model learning, machine learning, highly configurable software},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@article{10.1145/2670533,
author = {Bhardwaj, Amit and Chaudhuri, Subhasis and Dabeer, Onkar},
title = {Design and Analysis of Predictive Sampling of Haptic Signals},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/2670533},
doi = {10.1145/2670533},
abstract = {In this article, we identify adaptive sampling strategies for haptic signals. Our approach relies on experiments wherein we record the response of several users to haptic stimuli. We then learn different classifiers to predict the user response based on a variety of causal signal features. The classifiers that have good prediction accuracy serve as candidates to be used in adaptive sampling. We compare the resultant adaptive samplers based on their rate-distortion tradeoff using synthetic as well as natural data. For our experiments, we use a haptic device with a maximum force level of 3 N and 10 users. Each user is subjected to several piecewise constant haptic signals and is required to click a button whenever he perceives a change in the signal. For classification, we not only use classifiers based on level crossings and Weber’s law but also random forests using a variety of causal signal features. The random forest typically yields the best prediction accuracy and a study of the importance of variables suggests that the level crossings and Weber’s classifier features are most dominant. The classifiers based on level crossings and Weber’s law have good accuracy (more than 90%) and are only marginally inferior to random forests. The level crossings classifier consistently outperforms the one based on Weber’s law even though the gap is small. Given their simple parametric form, the level crossings and Weber’s law--based classifiers are good candidates to be used for adaptive sampling. We study their rate-distortion performance and find that the level crossing sampler is superior. For example, for haptic signals obtained while exploring various rendered objects, for an average sampling rate of 10 samples per second, the level crossings adaptive sampler has a mean square error about 3dB less than the Weber sampler.},
journal = {ACM Trans. Appl. Percept.},
month = dec,
articleno = {16},
numpages = {20},
keywords = {rate-distortion curve, random forest, linear regression, level crossings, decision tree, Weber’s law, Adaptive sampling}
}

@article{10.1007/s10586-018-1901-0,
author = {Muthusankar, D. and Kalaavathi, B. and Kaladevi, P.},
title = {High performance feature selection algorithms using filter method for cloud-based recommendation system},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1901-0},
doi = {10.1007/s10586-018-1901-0},
abstract = {In cloud-based recommendation system, the feature selection is implemented to reduce the large dimension of the cloud data. The feature selection increases the performance of the recommendation system without affecting the accuracy of the system. In this paper two filter model based algorithms SFS and MSFS are proposed to extract the necessary features for the recommendation system. The state of the art Naive bayes classification algorithm is used to evaluate the performance of the feature selection algorithm. The bench mark datasets Newsgroups, WebKB and Book Crossing are used for performance evaluation. The experimental results show that the proposed algorithm is superior to the existing feature selection algorithms T-Score, Information Gain and Chi squared.},
journal = {Cluster Computing},
month = jan,
pages = {311–322},
numpages = {12},
keywords = {Selective feature selection, Filter method, Feature selection}
}

@inproceedings{10.1145/3194718.3194722,
author = {Choudhary, Ankur and Agrawal, Arun Prakash and Kaur, Arvinder},
title = {An effective approach for regression test case selection using pareto based multi-objective harmony search},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194722},
doi = {10.1145/3194718.3194722},
abstract = {Regression testing is a way of catching bugs in new builds and releases to avoid the product risks. Corrective, progressive, retest all and selective regression testing are strategies to perform regression testing. Retesting all existing test cases is one of the most reliable approaches but it is costly in terms of time and effort. This limitation opened a scope to optimize regression testing cost by selecting only a subset of test cases that can detect faults in optimal time and effort. This paper proposes Pareto based Multi-Objective Harmony Search approach for regression test case selection from an existing test suite to achieve some test adequacy criteria. Fault coverage, unique faults covered and algorithm execution time are utilised as performance measures to achieve optimization criteria. The performance evaluation of proposed approach is performed against Bat Search and Cuckoo Search optimization. The results of statistical tests indicate significant improvement over existing approaches.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {13–20},
numpages = {8},
keywords = {test case selection, software testing, regression testing, optimization, harmony search, cuckoo search optimization, bat search optimization},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@article{10.1109/TNET.2013.2269837,
author = {Cunha, \'{I}talo and Teixeira, Renata and Veitch, Darryl and Diot, Christophe},
title = {DTRACK: a system to predict and track internet path changes},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2269837},
doi = {10.1109/TNET.2013.2269837},
abstract = {In this paper, we implement and evaluate a system that predicts and tracks Internet path changes to maintain an up-to-date network topology. Based on empirical observations, we claim that monitors can enhance probing according to the likelihood of path changes. We design a simple predictor of path changes and show that it can be used to enhance probe targeting. Our path tracking system, called DTRACK, focuses probes on unstable paths and spreads probes over time to minimize the chances of missing path changes. Our evaluations of DTRACK with trace-driven simulations and with a prototype show that DTRACK can detect up to three times more path changes than traditional traceroute-based topology mapping techniques.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1025–1038},
numpages = {14},
keywords = {tracking, topology mapping, path changes}
}

@book{10.5555/2671152,
author = {Goransson, Paul and Black, Chuck},
title = {Software Defined Networks: A Comprehensive Approach},
year = {2014},
isbn = {012416675X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Defined Networks discusses the historical networking environment that gave rise to SDN, as well as the latest advances in SDN technology. The book gives you the state of the art knowledge needed for successful deployment of an SDN, including: How to explain to the non-technical business decision makers in your organization the potential benefits, as well as the risks, in shifting parts of a network to the SDN model How to make intelligent decisions about when to integrate SDN technologies in a network How to decide if your organization should be developing its own SDN applications or looking to acquire these from an outside vendor How to accelerate the ability to develop your own SDN application, be it entirely novel or a more efficient approach to a long-standing problem Discusses the evolution of the switch platforms that enable SDN Addresses when to integrate SDN technologies in a network Provides an overview of sample SDN applications relevant to different industries Includes practical examples of how to write SDN applications}
}

@article{10.1109/TCBB.2020.3004970,
author = {Halder, Anup Kumar and Bandyopadhyay, Soumyendu Sekhar and Chatterjee, Piyali and Nasipuri, Mita and Plewczynski, Dariusz and Basu, Subhadip},
title = {&lt;italic&gt;JUPPI&lt;/italic&gt;: A Multi-Level Feature Based Method for PPI Prediction and a Refined Strategy for Performance Assessment},
year = {2020},
issue_date = {Jan.-Feb. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3004970},
doi = {10.1109/TCBB.2020.3004970},
abstract = {Over the years, several methods have been proposed for the computational PPI prediction with different performance evaluation strategies. While attempting to benchmark performance scores, most of these methods often suffer with ill-treated cross-validation strategies, adhoc selection of positive/negative samples etc. To address these issues, in our proposed multi-level feature based PPI prediction approach (&lt;italic&gt;JUPPI&lt;/italic&gt;), using sequence, domain and GO information as features, a refined evaluation strategy has been introduced. During the evaluation process, we first extract high quality negative data using three-stage filtering, and then introduce a pair-input based cross validation strategy with three difficulty levels for test-set predictions. Our proposed evaluation strategy reduces the component-level overlapping issue in test sets. Performance of &lt;italic&gt;JUPPI&lt;/italic&gt; is compared with those of the &lt;italic&gt;state-of-the-art&lt;/italic&gt; approaches in this domain and tested on six independent PPI datasets. In almost all the datasets, &lt;italic&gt;JUPPI&lt;/italic&gt; outperforms the &lt;italic&gt;state-of-the-art&lt;/italic&gt; not only at human proteome level for PPI prediction, but also for prediction of interactors for intrinsic disordered human proteins. &lt;uri&gt;https://figshare.com/projects/JUPPI_A_Multi-level_Feature_Based_Method_for_PPI_Prediction_and_a_Refined_Strategy_for_Performance_Assessment/81656&lt;/uri&gt; &lt;italic&gt;JUPPI&lt;/italic&gt; tool and the developed datasets (&lt;italic&gt;JUPPId&lt;/italic&gt;) are available in public domain for academic use along with supplementary materials, which can be found on the Computer Society Digital Library at &lt;uri&gt;http://doi.ieeecomputersociety.org/10.1109/TCBB.2020.3004970&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {531–542},
numpages = {12}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.1007/978-3-030-59762-7_10,
author = {Sinha, Urjoshi and Cashman, Mikaela and Cohen, Myra B.},
title = {Using a Genetic Algorithm to Optimize Configurations in a Data-Driven Application},
year = {2020},
isbn = {978-3-030-59761-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59762-7_10},
doi = {10.1007/978-3-030-59762-7_10},
abstract = {Users of highly-configurable software systems often want to optimize a particular objective such as improving a functional outcome or increasing system performance. One approach is to use an evolutionary algorithm. However, many applications today are data-driven, meaning they depend on inputs or data which can be complex and varied. Hence, a search needs to be run (and re-run) for all inputs, making optimization a heavy-weight and potentially impractical process. In this paper, we explore this issue on a data-driven highly-configurable scientific application. We build an exhaustive database containing 3,000 configurations and 10,000 inputs, leading to almost 100 million records as our oracle, and then run a genetic algorithm individually on each of the 10,000 inputs. We ask if (1) a genetic algorithm can find configurations to improve functional objectives; (2) whether patterns of best configurations over all input data emerge; and (3) if we can we use sampling to approximate the results. We find that the original (default) configuration is best only 34% of the time, while clear patterns emerge of other best configurations. Out of 3,000 possible configurations, only 112 distinct configurations achieve the optimal result at least once across all 10,000 inputs, suggesting the potential for lighter weight optimization approaches. We show that sampling of the input data finds similar patterns at a lower cost.},
booktitle = {Search-Based Software Engineering: 12th International Symposium, SSBSE 2020, Bari, Italy, October 7–8, 2020, Proceedings},
pages = {137–152},
numpages = {16},
keywords = {SSBSE, Data-driven, Genetic algorithm},
location = {Bari, Italy}
}

@article{10.1147/JRD.2017.2648298,
author = {Wang, J. and Li, C. and Han, S. and Sarkar, S. and Zhou, X.},
title = {Predictive maintenance based on event-log analysis: a case study},
year = {2017},
issue_date = {January/February 2017},
publisher = {IBM Corp.},
address = {USA},
volume = {61},
number = {1},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2017.2648298},
doi = {10.1147/JRD.2017.2648298},
abstract = {Predictive maintenance techniques are designed to help anticipate equipment failures to allow for advance scheduling of corrective maintenance, thereby preventing unexpected equipment downtime, improving service quality for customers, and also reducing the additional cost caused by over-maintenance in preventative maintenance policies. Many types of equipment--e.g., automated teller machines (ATMs), information technology equipment, medical devices, etc.--track run-time status by generating system messages, error events, and log files, which can be used to predict impending failures. Aiming at these types of equipment, we present a general classification-based failure prediction method. In our parameterized model, we systematically defined four categories of features to try to cover all possibly useful features, and then used feature selection to identify the most important features for model construction. The general solution is sufficiently flexible and complex to address failure prediction for target equipment types. We chose ATMs as the example equipment and used real ATM run-time event logs and maintenance records as experimental data to evaluate our method on the feasibility and effectiveness. In this paper, we also share insights on how to optimize the model parameters, select the most effective features, and tune classifiers to build a high-performance prediction model.},
journal = {IBM J. Res. Dev.},
month = jan,
pages = {11:121–11:132},
numpages = {12}
}

@article{10.1016/j.infsof.2015.01.002,
author = {Varnell-Sarjeant, Julia and Amschler Andrews, Anneliese and Lucente, Joe and Stefik, Andreas},
title = {Comparing development approaches and reuse strategies},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.002},
doi = {10.1016/j.infsof.2015.01.002},
abstract = {ContextThere is a debate in the aerospace industry whether lessons from reuse successes and failures in nonembedded software can be applied to embedded software. Reuse supposedly reduces development time and errors. The aerospace industry was an early advocate of reuse, but in Aerospace, not all reuse experiences have been as successful as expected. Some major projects experienced large overruns in time, budget, as well as inferior performance, at least in part, due to the gap between reuse expectations and reuse outcomes. This seemed to be especially the case for embedded systems. ObjectiveOur goal is to discover software reuse practices in the aerospace industry. In particular, we wish to learn whether practitioners who develop embedded systems use the same development approaches and artifacts as software practitioners who develop nonembedded systems. We wish to learn whether reuse influences selection of development approaches and artifacts and whether outcomes are impacted. MethodWe developed a survey given to software practitioners in a major Aerospace Corporation developing either embedded or nonembedded systems. The survey probed to identify development methods used, artifacts reused and outcomes resulting from the reuse. We used qualitative and quantitative methods such as descriptive statistics, MANOVA, Principle Component Analysis and an analysis of freeform comments to compare reuse practices between embedded systems and nonembedded systems development. ResultsWe found that embedded systems were more likely to include component based development, product line development and model based development in their development approach, whereas nonembedded systems were more likely to include Ad Hoc and COTS/GOTS in their development approach. Embedded systems developers tended to reuse more and different reuse artifacts. ConclusionWe found that, while outcomes were nearly identical, the development approaches and artifacts used did, in fact, differ. In particular, the tight coupling between code and the platform in embedded systems often dictated the development approach and reuse artifacts and identified some of the reasons.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {71–92},
numpages = {22},
keywords = {Software reuse, Nonembedded systems, Empirical study, Embedded systems}
}

@inproceedings{10.1145/3459930.3469547,
author = {Golmaei, Sara Nouri and Luo, Xiao},
title = {DeepNote-GNN: predicting hospital readmission using clinical notes and patient network},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469547},
doi = {10.1145/3459930.3469547},
abstract = {With the increasing availability of Electronic Health Records (EHRs) and advances in deep learning techniques, developing deep predictive models that use EHR data to solve healthcare problems has gained momentum in recent years. The majority of clinical predictive models benefit from structured data in EHR (e.g., lab measurements and medications). Still, learning clinical outcomes from all possible information sources is one of the main challenges when building predictive models. This work focuses on two sources of information that have been underused by researchers; unstructured data (e.g., clinical notes) and a patient network. We propose a novel hybrid deep learning model, DeepNote-GNN, that integrates clinical notes information and patient network topological structure to improve 30-day hospital readmission prediction. DeepNote-GNN is a robust deep learning framework consisting of two modules: DeepNote and patient network. DeepNote extracts deep representations of clinical notes using a feature aggregation unit on top of a state-of-the-art Natural Language Processing (NLP) technique - BERT. By exploiting these deep representations, a patient network is built, and Graph Neural Network (GNN) is used to train the network for hospital readmission predictions. Performance evaluation on the MIMIC-III dataset demonstrates that DeepNote-GNN achieves superior results compared to the state-of-the-art baselines on the 30-day hospital readmission task. We extensively analyze the DeepNote-GNN model to illustrate the effectiveness and contribution of each component of it. The model analysis shows that patient network has a significant contribution to the overall performance, and DeepNote-GNN is robust and can consistently perform well on the 30-day readmission prediction task.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {19},
numpages = {9},
keywords = {clinical notes, deep learning, electronic health records, feature aggregation, graph neural networks, natural language processing},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.5555/2492708.2492926,
author = {Nirmaier, Thomas and zu Bexten, Volker Meyer and Tristl, Markus and Harrant, Manuel and Kunze, Matthias and Rafaila, Monica and Lau, Julia and Pelz, Georg},
title = {Measuring and improving the robustness of automotive smart power microelectronics},
year = {2012},
isbn = {9783981080186},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Automotive power micro-electronic devices in the past were low pin-count, low complexity devices. Robustness could be assessed by stressing the few operating conditions and by manual analysis of the simple analog circuitry. Nowadays complexity of Automotive Smart Power Devices is driven by the demands for energy efficiency and safety, which adds the need for additional monitoring circuitry, redundancy, power-modes, leading even to complex System-on-chips with embedded uC cores, embedded memory, sensors and other elements. Assessing the application robustness of this type of microelectronic devices goes hand-in-hand with exploring their verification space inside and to certain extends outside of the specification. While there are well established methods for standard functional verification, methods for application oriented robust verification are not yet available. In this paper we present promising directions and first results, to explore and assess device robustness through various pre- and post-Si verification and design exploration strategies, focusing on metamodeling, constrained-random verification and hardware-in-the-loop experiments, for exploration of the operating space.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {872–873},
numpages = {2},
keywords = {robustness, metamodeling, constrained-random-verification, component, automotive smart power IC},
location = {Dresden, Germany},
series = {DATE '12}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Software architecture, Reliability, Performance, Patterns, Elastic computing, Distributed and concurrent architecture, Autonomic computing}
}

@inproceedings{10.5555/3042094.3042491,
author = {Rai, Ajit and Valenzuela, Rene C. and Tuffin, Bruno and Rubino, Gerardo and Dersin, Pierre},
title = {Approximate zero-variance importance sampling for static network reliability estimation with node failures and application to rail systems},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {To accurately estimate the reliability of highly reliable rail systems and comply with contractual obligations, rail system suppliers such as ALSTOM require efficient reliability estimation techniques. Standard Monte-Carlo methods in their crude form are inefficient in estimating static network reliability of highly reliable systems. Importance Sampling techniques are an advanced class of variance reduction techniques used for rare-event analysis. In static network reliability estimation, the graph models often deal with failing links. In this paper, we propose an adaptation of an approximate Zero-Variance Importance Sampling method to evaluate the reliability of real transport systems where nodes are the failing components. This is more representative of railway telecommunication system behavior. Robustness measures of the accuracy of the estimates, bounded or vanishing relative error properties, are discussed and results from a real network (Data Communication System used in automated train control system) showing bounded relative error property, are presented.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {3201–3212},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@article{10.1145/378893.378896,
author = {Glew, Andy},
title = {An empirical investigation of OR indexing},
year = {1990},
issue_date = {Jan. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/378893.378896},
doi = {10.1145/378893.378896},
abstract = {This paper considers OR indexing as a substitute for, or an optimization of, addition in an addressing mode for a high speed processor. OR indexing is evaluated in the context of existing address streams, using time based sampling, and through compiler modifications.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jan,
pages = {41–49},
numpages = {9}
}

@article{10.1007/PL00013358,
author = {Mellentien, Christoph and Trautmann, Norbert},
title = {Resource allocation with project management software},
year = {2001},
issue_date = {August    2001},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/PL00013358},
doi = {10.1007/PL00013358},
abstract = {We present results of a benchmark test evaluating the resource allocation capabilities of the project management software packages Acos Plus.1 8.2, CA SuperProject 5.0a, CS Project Professional 3.0, MS Project 2000, and Scitor Project Scheduler 8.0.1. The tests are based on 1560 instances of precedence--- and resource---constrained project scheduling problems. For different complexity scenarios, we analyze the deviation of the makespan obtained by the software packages from the best feasible makespan known. Among the tested software packages, Acos Plus.1 and Project Scheduler show the best resource allocation performance. Moreover, our numerical analysis reveals a considerable performance gap between the implemented methods and state---of---the---art project scheduling algorithms, especially for large---sized problems. Thus, there is still a significant potential for improving solutions to resource allocation problems in practice.},
journal = {OR Spectr.},
month = aug,
pages = {383–394},
numpages = {12},
keywords = {Key words: Project management software --- Resource allocation --- Performance evaluation}
}

@article{10.1016/j.compbiomed.2021.104413,
author = {Chen, Shoukun and Xu, Kaili and Yao, Xiwen and Zhu, Siyi and Zhang, Bohan and Zhou, Haodong and Guo, Xin and Zhao, Bingfeng},
title = {Psychophysiological data-driven multi-feature information fusion and recognition of miner fatigue in high-altitude and cold areas},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104413},
doi = {10.1016/j.compbiomed.2021.104413},
journal = {Comput. Biol. Med.},
month = jun,
numpages = {14},
keywords = {Fatigue recognition, Electrocardiography (ECG), Electromyography (EMG), Psychophysiological, Information fusion, Machine learning}
}

@inproceedings{10.1145/2528265.2528267,
author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Garvin, Brady},
title = {Exploring feature interactions in the wild: the new feature-interaction challenge},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528267},
doi = {10.1145/2528265.2528267},
abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature-oriented software development, feature-interaction problem, feature modularity, feature interactions},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@article{10.1016/j.specom.2019.04.001,
author = {Ahmed, Ahmed Isam and Chiverton, John P. and Ndzi, David L. and Becerra, Victor M.},
title = {Speaker recognition using PCA-based feature transformation},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {110},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.04.001},
doi = {10.1016/j.specom.2019.04.001},
journal = {Speech Commun.},
month = jul,
pages = {33–46},
numpages = {14},
keywords = {I-vector system, Feature fusion, Weighted principal component analysis}
}

@article{10.1162/evco_a_00262,
author = {Mu\~{n}oz, Mario A. and Smith-Miles, Kate},
title = {Generating New Space-Filling Test Instances for Continuous Black-Box Optimization},
year = {2020},
issue_date = {Fall 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {28},
number = {3},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00262},
doi = {10.1162/evco_a_00262},
abstract = {This article presents a method to generate diverse and challenging new test instances for continuous black-box optimization. Each instance is represented as a feature vector of exploratory landscape analysis measures. By projecting the features into a two-dimensional instance space, the location of existing test instances can be visualized, and their similarities and differences revealed. New instances are generated through genetic programming which evolves functions with controllable characteristics. Convergence to selected target points in the instance space is used to drive the evolutionary process, such that the new instances span the entire space more comprehensively. We demonstrate the method by generating two-dimensional functions to visualize its success, and ten-dimensional functions to test its scalability. We show that the method can recreate existing test functions when target points are co-located with existing functions, and can generate new functions with entirely different characteristics when target points are located in empty regions of the instance space. Moreover, we test the effectiveness of three state-of-the-art algorithms on the new set of instances. The results demonstrate that the new set is not only more diverse than a well-known benchmark set, but also more challenging for the tested algorithms. Hence, the method opens up a new avenue for developing test instances with controllable characteristics, necessary to expose the strengths and weaknesses of algorithms, and drive algorithm development.},
journal = {Evol. Comput.},
month = sep,
pages = {379–404},
numpages = {26},
keywords = {instance generator., exploratory landscape analysis, black-box continuous optimization, benchmarking, Algorithm selection}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1007/s11042-020-09150-8,
author = {Li, Ruixiang and Li, Hui and Shi, Weibin},
title = {Human activity recognition based on LPA},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {41–42},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09150-8},
doi = {10.1007/s11042-020-09150-8},
abstract = {Human activity recognition and fall detection have been popular research topics because of its wide area of application. Traditional activity recognition methods have complex feature extraction steps. We propose a new feature extraction method based on linear prediction analysis(LPA) to reduce computational complexity involved with engineering features. The feature extraction method we propose establishes a link between human activity and the signal system and regards acceleration signals as the output of the human activity. Using the relationship between the human activity and the output signal, linear predictive analysis can isolate information about human activity and transform it into a compact representation through linear prediction coefficients (LPC). In order to verify the effectiveness of the method, we design an activity recognition system based on linear prediction analysis and feature extraction. At the same time, we study the performance of the combination of linear prediction coefficients and time domain features. We use data from the public dataset SCUT-NAA, which contains ten different activities, and another public dataset, which records people falling. A random forest classification algorithm based on ensemble learning is used for activity recognition and fall detection. The results show that the combined vector of linear prediction coefficient and time domain activity amplitude feature obtained a 93% accuracy rate and the system evaluation index F1 of 0.92 on the SCUT-NAA dataset. Additionally, we achieved an accuracy rate of 97% in fall detection.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {31069–31086},
numpages = {18},
keywords = {Feature extraction, Fall detection, Human activity recognition, Linear prediction coefficient}
}

@inproceedings{10.1145/2591062.2591173,
author = {Bird, Christian and Ranganath, Venkatesh-Prasad and Zimmermann, Thomas and Nagappan, Nachiappan and Zeller, Andreas},
title = {Extrinsic influence factors in software reliability: a study of 200,000 windows machines},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591173},
doi = {10.1145/2591062.2591173},
abstract = {Reliability of software depends not only on intrinsic factors such as its code properties, but also on extrinsic factors—that is, the properties of the environment it operates in. In an empirical study of more than 200,000 Windows users, we found that the reliability of individual applications is related to whether and which other applications are in-stalled: While games and file-sharing applications tend to decrease the reliability of other applications, security applications tend to increase it. Furthermore, application reliability is related to the usage profiles of these applications; generally, the more an application is used, the more likely it is to have negative impact on reliability of others. As a conse-quence, software testers must be careful to investigate and control these factors.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {205–214},
numpages = {10},
keywords = {Windows},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@article{10.1016/j.procs.2020.03.228,
author = {Don, S.},
title = {Random Subset Feature Selection and Classification of Lung Sound},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.228},
doi = {10.1016/j.procs.2020.03.228},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {313–322},
numpages = {10},
keywords = {Feature Selection, Fractal Dimension, RSFS, SFS, Random Sampling, Classification}
}

@inproceedings{10.1007/978-3-030-84529-2_57,
author = {Meng, Tong and Chen, Yuehui and Bao, Wenzheng and Cao, Yi},
title = {Mal_PCASVM: Malonylation Residues Classification with Principal Component Analysis Support Vector Machine},
year = {2021},
isbn = {978-3-030-84528-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-84529-2_57},
doi = {10.1007/978-3-030-84529-2_57},
abstract = {Post-translational modification (PTM) is considered a significant biological process with a tremendous impact on the function of proteins in both eukaryotes, and prokaryotes cells. Malonylation of lysine is a newly discovered post-translational modification, which is associated with many diseases, such as type 2 diabetes and different types of cancer. In addition, compared with the experimental identification of propionylation sites, the calculation method can save time and reduce cost. In this paper, we combine principal component analysis with support vector machine (SVM) to propose a new computational model - Mal-PCASVM (malonylation prediction). Firstly, the one-hot encoding, physicochemical properties and the composition of k-spacer acid pairs were used to extract sequence features. Secondly, we preprocess the data, select the best feature subset by principal component analysis (PCA), and predict the malonylation sites by SVM. And then, we do a five-fold cross validation, and the results show that compared with other methods, Mal-PCASVM can get better prediction performance. In the 10-fold cross validation of independent data sets, AUC (area under receiver operating characteristic curve) analysis has reached 96.39%. Mal-PCASVM is used to identify the malonylation sites in the protein sequence, which is a computationally reliable method. It is superior to the existing prediction tools that found in the literature and can be used as a useful tool for identifying and discovering novel malonylation sites in human proteins.},
booktitle = {Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Shenzhen, China, August 12–15, 2021, Proceedings, Part II},
pages = {681–695},
numpages = {15},
keywords = {Post-translational modification, Malonylation, One-hot encoding, Principal component analysis, Support vector machine},
location = {Shenzhen, China}
}

@inproceedings{10.1007/978-3-319-26844-6_29,
author = {Hendriks, Martijn and Verriet, Jacques and Basten, Twan and Brass\'{e}, Marco and Dankers, Reinier and Laan, Ren\'{e} and Lint, Alexander and Moneva, Hristina and Somers, Lou and Willekens, Marc},
title = {Performance Engineering for Industrial Embedded Data-Processing Systems},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_29},
doi = {10.1007/978-3-319-26844-6_29},
abstract = {Performance is a key aspect of many embedded systems, embedded data processing systems in particular. System performance can typically only be measured in the later stages of system development. To avoid expensive re-work in the final stages of development, it is essential to have accurate performance estimations in the early stages. For this purpose, we present a model-based approach to performance engineering that is integrated with the well-known V-model for system development. Our approach emphasizes model accuracy and is demonstrated using five embedded data-processing cases from the digital printing domain. We show how lightweight models can be used in the early stages of system development to estimate the influence of design changes on system performance.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {399–414},
numpages = {16},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.1145/3167132.3167317,
author = {Rafiq, Rahat Ibn and Hosseinmardi, Homa and Han, Richard and Lv, Qin and Mishra, Shivakant},
title = {Scalable and timely detection of cyberbullying in online social networks},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167317},
doi = {10.1145/3167132.3167317},
abstract = {Cyberbullying in Online Social Networks (OSNs) has grown to be a serious problem among teenagers. While a considerable amount of research has been conducted focusing on designing highly accurate classifiers to automatically detect cyberbullying instances in OSNs, two key practical issues remain to be worked upon, namely scalability of a cyberbullying detection system and timeliness of raising alerts whenever cyberbullying occurs. These two issues form the motivation of our work. We propose a multi-stage cyberbullying detection solution that drastically reduces the classification time and the time to raise alerts. The proposed system is highly scalable without sacrificing accuracy and highly responsive in raising alerts. The design is comprised of two novel components, a dynamic priority scheduler and an incremental classification mechanism. We have implemented this solution, and using data obtained from Vine, we conducted a thorough performance evaluation to demonstrate the utility and scalability of each of these components. We show that our complete solution is significantly more scalable and responsive than the current state of the art.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1738–1747},
numpages = {10},
keywords = {social networks, scalable systems, cyberbullying},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1186/s13677-020-00195-6,
author = {Koo, Jahwan and Faseeh Qureshi, Nawab Muhammad and Siddiqui, Isma Farah and Abbas, Asad and Bashir, Ali Kashif},
title = {IoT-enabled directed acyclic graph in spark cluster},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {9},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-020-00195-6},
doi = {10.1186/s13677-020-00195-6},
abstract = {Real-time data streaming fetches live sensory segments of the dataset in the heterogeneous distributed computing environment. This process assembles data chunks at a rapid encapsulation rate through a streaming technique that bundles sensor segments into multiple micro-batches and extracts into a repository, respectively. Recently, the acquisition process is enhanced with an additional feature of exchanging IoT devices’ dataset comprised of two components: (i) sensory data and (ii) metadata. The body of sensory data includes record information, and the metadata part consists of logs, heterogeneous events, and routing path tables to transmit micro-batch streams into the repository. Real-time acquisition procedure uses the Directed Acyclic Graph (DAG) to extract live query outcomes from in-place micro-batches through MapReduce stages and returns a result set. However, few bottlenecks affect the performance during the execution process, such as (i) homogeneous micro-batches formation only, (ii) complexity of dataset diversification, (iii) heterogeneous data tuples processing, and (iv) linear DAG workflow only. As a result, it produces huge processing latency and the additional cost of extracting event-enabled IoT datasets. Thus, the Spark cluster that processes Resilient Distributed Dataset (RDD) in a fast-pace using Random access memory (RAM) defies expected robustness in processing IoT streams in the distributed computing environment. This paper presents an IoT-enabled Directed Acyclic Graph (I-DAG) technique that labels micro-batches at the stage of building a stream event and arranges stream elements with event labels. In the next step, heterogeneous stream events are processed through the I-DAG workflow, which has non-linear DAG operation for extracting queries’ results in a Spark cluster. The performance evaluation shows that I-DAG resolves homogeneous IoT-enabled stream event issues and provides an effective stream event heterogeneous solution for IoT-enabled datasets in spark clusters.},
journal = {J. Cloud Comput.},
month = sep,
numpages = {15},
keywords = {Micro-batch stream, MapReduce, Directed acyclic graph, Internet of Things (IoT), Apache spark}
}

@article{10.1007/s10115-018-1265-z,
author = {Yadav, Shweta and Ekbal, Asif and Saha, Sriparna},
title = {Information theoretic-PSO-based feature selection: an application in biomedical entity extraction},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {60},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1265-z},
doi = {10.1007/s10115-018-1265-z},
abstract = {Named entity recognition is a vital task for various applications related to biomedical natural language processing. It aims at extracting different biomedical entities from the text and classifying them into some predefined categories. The types could vary depending upon the genre and domain, such as gene versus non-gene in a coarse-grained scenario, or protein, DNA, RNA, cell line, and cell-type in a fine-grained scenario. In this paper, we present a novel filter-based feature selection technique utilizing the search capability of particle swarm optimization (PSO) for determining the most optimal feature combination. The technique yields in the most optimized feature set, that when used for classifiers learning, enhance the system performance. The proposed approach is assessed over four popular biomedical corpora, namely GENIA, GENETAG, AIMed, and Biocreative-II Gene Mention Recognition (BC-II). Our proposed model obtains the F score values of $$74.49%$$, $$91.11%$$, $$90.47%$$, $$88.64%$$ on GENIA, GENETAG, AIMed, and BC-II dataset, respectively. The efficiency of feature pruning through PSO is evident with significant performance gains, even with a much reduced set of features.},
journal = {Knowl. Inf. Syst.},
month = sep,
pages = {1453–1478},
numpages = {26},
keywords = {Particle swarm optimization, Normalized mutual information, Mutual information, Correlation, Binary PSO, Feature selection, Named entity recognition}
}

@inproceedings{10.1007/978-3-642-53956-5_12,
author = {Mooij, Arjan J. and Hooman, Jozef and Albers, Rob},
title = {Early Fault Detection Using Design Models for Collision Prevention in Medical Equipment},
year = {2013},
isbn = {9783642539558},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-53956-5_12},
doi = {10.1007/978-3-642-53956-5_12},
abstract = {In the medical domain there is a tension between the requested speed of innovation and the time needed to deliver a certifiable system. To ensure the required safety, usually a long test and integration phase is needed. To shorten this phase and to avoid late bug fixing, the aim is to detect faults if any much earlier in the development process. This can be achieved by combining a number of model-based techniques such as 1 architecture validation by simulating executable models, 2 development of a Domain-Specific Language DSL to combine precision with higher levels of abstraction, and 3 transformations from DSLs to analysis models for performance evaluation and formal verification. We illustrate such techniques using an industrial study project on a new architecture for movement control including collision prevention.},
booktitle = {Revised Selected Papers of the Third International Symposium on Foundations of Health Information Engineering and Systems - Volume 8315},
pages = {170–187},
numpages = {18},
location = {Macau, China},
series = {FHIES 2013}
}

@inproceedings{10.1145/3324884.3416573,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Identifying software performance changes across variants and versions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416573},
doi = {10.1145/3324884.3416573},
abstract = {We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {active learning, configurable software systems, machine learning, software evolution, software performance},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3340531.3412683,
author = {Xu, Kuan and Fu, Chilin and Zhang, Xiaolu and Chen, Cen and Zhang, Ya-Lin and Rong, Wenge and Wen, Zujie and Zhou, Jun and Li, Xiaolong and Qiao, Yu},
title = {aDMSCN: A Novel Perspective for User Intent Prediction in Customer Service Bots},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412683},
doi = {10.1145/3340531.3412683},
abstract = {As one of the core components of customer service bot, User Intent Prediction (UIP) aims at predicting users? intents (usually represented as predefined user questions) before they ask, and has been widely applied in real applications. However, when developing a machine learning system for this problem, two critical issues, i.e., the problem of feature drift and class imbalance, may emerge and seriously deprave the system performance. Moreover, various scenarios may arise due to business demands, making the aforementioned problems much more severe. To address these two problems, we propose an attention-based Deep Multi-instance Sequential Cross Network (aDMSCN) to deal with the UIP task. On the one hand,the UIP task can be subtly formalized as multi-instance learning(MIL) task with an attention-based method proposed to alleviate the influences of feature drift. To the best of our knowledge, this is the first attempt to model the problem from a MIL perspective.On the other hand, a ratio-sensitive loss is also developed in our model, which can mitigate the negative impact of class imbalance. Extensive experiments on both offline real-world datasets and on-line A/B testing show that our proposed framework significantly out performs other state-of-art methods for the UIP task.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2853–2860},
numpages = {8},
keywords = {user intent prediction, recommender system, multiple instancelearning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1016/j.cie.2021.107782,
author = {Chien, Chen-Fu and Lan, Yu-Bin},
title = {Agent-based approach integrating deep reinforcement learning and hybrid genetic algorithm for dynamic scheduling for Industry 3.5 smart production},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107782},
doi = {10.1016/j.cie.2021.107782},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {11},
keywords = {Industry 3.5, Semiconductor manufacturing, Hybrid genetic algorithm, Dynamic scheduling, Deep reinforcement learning}
}

@article{10.1016/j.patcog.2015.09.015,
author = {Manivannan, Siyamalan and Li, Wenqi and Akbar, Shazia and Wang, Ruixuan and Zhang, Jianguo and McKenna, Stephen J.},
title = {An automated pattern recognition system for classifying indirect immunofluorescence images of HEp-2 cells and specimens},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {51},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.09.015},
doi = {10.1016/j.patcog.2015.09.015},
abstract = {Immunofluorescence antinuclear antibody tests are important for diagnosis and management of autoimmune conditions; a key step that would benefit from reliable automation is the recognition of subcellular patterns suggestive of different diseases. We present a system to recognize such patterns, at cellular and specimen levels, in images of HEp-2 cells. Ensembles of SVMs were trained to classify cells into six classes based on sparse encoding of texture features with cell pyramids, capturing spatial, multi-scale structure. A similar approach was used to classify specimens into seven classes. Software implementations were submitted to an international contest hosted by ICPR 2014 (Performance Evaluation of Indirect Immunofluorescence Image Analysis Systems). Mean class accuracies obtained on heldout test data sets were 87.1% and 88.5% for cell and specimen classification respectively. These were the highest achieved in the competition, suggesting that our methods are state-of-the-art. We provide detailed descriptions and extensive experiments with various features and encoding methods. HighlightsWe propose systems for classifying immunofluorescence images of HEp-2 cells.Images are classified at both the cell level and the specimen level.Ensemble SVM classification based on sparse coding of texture features was effective.Cell pyramids and artificial dataset augmentation increased mean class accuracy.The proposed systems came first in the I3A contest associated with ICPR 2014.},
journal = {Pattern Recogn.},
month = mar,
pages = {12–26},
numpages = {15},
keywords = {Subcellular fluorescence patterns, Multi-resolution local patterns, HEp-2 cells, Ensemble SVM, Cell classification, Anti-nuclear antibody test}
}

@inproceedings{10.1145/3459637.3482234,
author = {Wei, Zhikun and Wang, Xin and Zhu, Wenwu},
title = {AutoIAS: Automatic Integrated Architecture Searcher for Click-Trough Rate Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482234},
doi = {10.1145/3459637.3482234},
abstract = {Automating architecture design for recommendation tasks becomes a trending topic because expert efforts are saved, and better performance is expected. Neural Architecture Search (NAS) is introduced to discover powerful CTR prediction model architectures in recent works. CTR prediction model usually consists of three components: embedding layer, interaction layer, and deep neural network. However, existing automation works focus on searching single component and leaving other components hand-crafted. The isolated searching will cause incompatibility among components and lead to weak generalization ability. Moreover, there is not a unified framework for integrated CTR prediction model architecture searching. This paper presents Automatic Integrated Architecture Searcher (AutoIAS), a framework that provides a practical and general method to find optimal CTR prediction model architecture in an automatic manner. In AutoIAS, we unify existing interaction-based CTR prediction model architectures and propose an integrated search space for a complete CTR prediction model. We utilize a supernet to predict the performance of sub-architectures, and the supernet is trained with Knowledge Distillation(KD) to enhance consistency among sub-architectures. To efficiently explore the search space, we design an architecture generator network that explicitly models the architecture dependencies among components and generates conditioned architectures distribution for each component. Experiments on public datasets show the outstanding performance and generalization ability of AutoIAS. Ablation study shows the effectiveness of the KD-based supernet training method and the Architecture Generator Network.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2101–2110},
numpages = {10},
keywords = {unified search space, neural architecture search, click-trough rate prediction, architecture generator network},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3472456.3472521,
author = {Lehr, Jan-Patrick and Bischof, Christian and Dewald, Florian and Mantel, Heiko and Norouzi, Mohammad and Wolf, Felix},
title = {Tool-Supported Mini-App Extraction to Facilitate Program Analysis and Parallelization},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472521},
doi = {10.1145/3472456.3472521},
abstract = {The size and complexity of high-performance computing applications present a serious challenge to manual reasoning about program behavior. The vastness and diversity of code bases often break automatic analysis tools, which could otherwise be used. As a consequence, developers resort to mini-apps, i.e., trimmed-down proxies of the original programs that retain key performance characteristics. Unfortunately, their construction is difficult and time consuming and prevents their mass production. In this paper, we propose a systematic and tool-supported approach to extract mini-apps from large-scale applications that reduces the manual effort needed to create them. Our approach covers the stages kernel identification, data capture, code extraction and representativeness validation. We demonstrate it using an astrophysics simulation with ≈ 8.5 million lines of code and extract a mini-app with only ≈ 1, 100 lines of code. For the mini-app, we evaluate the reduction of code complexity and execution similarity, and show how it enables the tool-supported discovery of unexploited parallelization opportunities, reducing the simulation’s runtime significantly.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {35},
numpages = {10},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@article{10.5555/2372179.2372185,
author = {Puuronen, Seppo and Tsymbal, Alexey},
title = {Local Feature Selection with Dynamic Integration of Classifiers},
year = {2001},
issue_date = {January 2001},
publisher = {IOS Press},
address = {NLD},
volume = {47},
number = {1–2},
issn = {0169-2968},
abstract = {Multidimensional data is often feature space heterogeneous so that individual features have unequal importance in different sub areas of the feature space. This motivates to search for a technique that provides a strategic splitting of the instance space being able to identify the best subset of features for each instance to be classified. Our technique applies the wrapper approach where a classification algorithm is used as an evaluation function to differentiate between different feature subsets. In order to make the feature selection local, we apply the recent technique for dynamic integration of classifiers. This allows to determine which classifier and which feature subset should be used for each new instance. Decision trees are used to help to restrict the number of feature combinations analyzed. For each new instance we consider only those feature combinations that include the features present in the path taken by the new instance in the decision tree built on the whole feature set. We evaluate our technique on data sets from the UCI machine learning repository. In our experiments, we use the C4.5 algorithm as the learning algorithm for base classifiers and for the decision trees that guide the local feature selection. The experiments show some advantages of the local feature selection with dynamic integration of classifiers in comparison with the selection of one feature subset for the whole space.},
journal = {Fundam. Inf.},
month = jan,
pages = {91–117},
numpages = {27},
keywords = {machine learning, ensemble of classifiers, dynamic integration, data mining, Feature selection}
}

@article{10.1109/TCBB.2020.3017386,
author = {Li, Menglu and Wang, Yanan and Li, Fuyi and Zhao, Yun and Liu, Mengya and Zhang, Sijia and Bin, Yannan and Smith, A. Ian and Webb, Geoffrey I. and Li, Jian and Song, Jiangning and Xia, Junfeng},
title = {A Deep Learning-Based Method for Identification of Bacteriophage-Host Interaction},
year = {2020},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3017386},
doi = {10.1109/TCBB.2020.3017386},
abstract = {Multi-drug resistance (MDR) has become one of the greatest threats to human health worldwide, and novel treatment methods of infections caused by MDR bacteria are urgently needed. Phage therapy is a promising alternative to solve this problem, to which the key is correctly matching target pathogenic bacteria with the corresponding therapeutic phage. Deep learning is powerful for mining complex patterns to generate accurate predictions. In this study, we develop PredPHI (&lt;underline&gt;Pred&lt;/underline&gt;icting &lt;underline&gt;P&lt;/underline&gt;hage-&lt;underline&gt;H&lt;/underline&gt;ost &lt;underline&gt;I&lt;/underline&gt;nteractions), a deep learning-based tool capable of predicting the host of phages from sequence data. We collect &gt;3000 phage-host pairs along with their protein sequences from PhagesDB and GenBank databases and extract a set of features. Then we select high-quality negative samples based on the K-Means clustering method and construct a balanced training set. Finally, we employ a deep convolutional neural network to build the predictive model. The results indicate that PredPHI can achieve a predictive performance of 81 percent in terms of the area under the receiver operating characteristic curve on the test set, and the clustering-based method is significantly more robust than that based on randomly selecting negative samples. These results highlight that PredPHI is a useful and accurate tool for identifying phage-host interactions from sequence data.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = aug,
pages = {1801–1810},
numpages = {10}
}

@inproceedings{10.1145/3172944.3172998,
author = {Zhang, Yunfeng and Liao, Q. Vera and Srivastava, Biplav},
title = {Towards an Optimal Dialog Strategy for Information Retrieval Using Both Open- and Close-ended Questions},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172998},
doi = {10.1145/3172944.3172998},
abstract = {The emerging paradigm of dialogue interfaces for information retrieval systems opens new opportunities for interactively narrowing down users' information query and improving search results. Prior research has largely focused on methods that use a set of close-ended questions, such as decision tree, to learn about the user's search target. However, when there is a myriad of documents or items to search, solely relying on close-ended questions can lead to long and undesirable dialogues. We propose an adaptive dialogue strategy framework that incorporates open-ended questions at the optimal timing to reduce the length of the dialogue. We propose a method to estimate the information gain of open-ended questions, and in each dialog turn, we compare it with that of close-ended questions to decide which question to ask. We present experiments using several synthetic datasets designed to explore the behavior of such an adaptive dialogue strategy under different environments, and compare the system's performance with that of a close-ended-questions-only strategy.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces},
pages = {365–369},
numpages = {5},
keywords = {decision-tree induction, dialog systems, interactive information retrieval, question asking},
location = {Tokyo, Japan},
series = {IUI '18}
}

@article{10.1504/ijbic.2021.117425,
author = {Wang, Yan-li and Qu, Bo-yang and Liang, Jing and Hu, Yi and Wei, Yun-peng},
title = {Research on the ensemble feature selection algorithm based on multimodal optimisation techniques},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {1},
issn = {1758-0366},
url = {https://doi.org/10.1504/ijbic.2021.117425},
doi = {10.1504/ijbic.2021.117425},
abstract = {Feature selection is essentially a high-dimensional combinatorial optimisation problem. To find representative feature subsets, the selection method needs powerful exploration ability. In addition, if alternative feature subsets could be provided, the final prediction accuracy can be improved by ensembling these subsets. Multimodal optimisation (MO) methods with high exploration power can find multiple suitable solutions in one single run. Therefore, this paper presents an ensemble feature selection algorithm based on multimodal optimisation techniques. Differential evolution based on fitness Euclidean-distance ratio (FERDE) algorithm is utilised to search for multiple diverse feature subsets in the huge feature space. A set of diverse-based classifiers are built based on these subsets and ensemble to improve the final classification performance. Compared with several existing classical algorithms and ensemble feature selection methods, the proposed method can achieve higher predictive accuracy.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {49–58},
numpages = {9},
keywords = {classification, multimodal optimisation, ensemble learning, feature selection}
}

@inproceedings{10.1145/2503210.2503216,
author = {Liu, Mingliang and Jin, Ye and Zhai, Jidong and Zhai, Yan and Shi, Qianqian and Ma, Xiaosong and Chen, Wenguang},
title = {ACIC: automatic cloud I/O configurator for HPC applications},
year = {2013},
isbn = {9781450323789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503210.2503216},
doi = {10.1145/2503210.2503216},
abstract = {The cloud has become a promising alternative to traditional HPC centers or in-house clusters. This new environment highlights the I/O bottleneck problem, typically with top-of-the-line compute instances but sub-par communication and I/O facilities. It has been observed that changing cloud I/O system configurations leads to significant variation in the performance and cost efficiency of I/O intensive HPC applications. However, storage system configuration is tedious and error-prone to do manually, even for experts.This paper proposes ACIC, which takes a given application running on a given cloud platform, and automatically searches for optimized I/O system configurations. ACIC utilizes machine learning models to perform black-box performance/cost predictions. To tackle the high-dimensional parameter exploration space unique to cloud platforms, we enable affordable, reusable, and incremental training guided by Plackett and Burman Matrices. Results with four representative applications indicate that ACIC consistently identifies near-optimal configurations among a large group of candidate settings.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {38},
numpages = {12},
keywords = {storage, performance, modeling, cloud computing},
location = {Denver, Colorado},
series = {SC '13}
}

@article{10.1145/3466691,
author = {Chen, Mu-Yen and Fan, Min-Hsuan and Huang, Li-Xiang},
title = {AI-Based Vehicular Network toward 6G and IoT: Deep Learning Approaches},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3466691},
doi = {10.1145/3466691},
abstract = {In recent years, vehicular networks have become increasingly large, heterogeneous, and dynamic, making it difficult to meet strict requirements of ultralow latency, high reliability, high security, and massive connections for next generation (6G) networks. Recently, deep learning (DL) has emerged as a powerful artificial intelligence (AI) technique to optimize the efficiency and adaptability of vehicle and wireless communication. However, rapidly increasing absolute numbers of vehicles on the roads are leading to increased automobile accidents, many of which are attributable to drivers interacting with their mobile phones. To address potentially dangerous driver behavior, this study applies deep learning approaches to image recognition to develop an AI-based detection system that can detect potentially dangerous driving behavior. Multiple convolutional neural network (CNN)-based techniques including VGG16, VGG19, Densenet, and Openpose were compared in terms of their ability to detect and identify problematic driving.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {6},
numpages = {12},
keywords = {internet of things, 6G, vehicular network, deep learning, Convolutional neural network}
}

@article{10.1023/A:1013349419545,
author = {Khoshgoftaar, Taghi M. and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
title = {Data Mining of Software Development Databases},
year = {2001},
issue_date = {November 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1023/A:1013349419545},
doi = {10.1023/A:1013349419545},
abstract = {Software quality models can predict which modules will have high risk, enabling developers to target enhancement activities to the most problematic modules. However, many find collection of the underlying software product and process metrics a daunting task.Many software development organizations routinely use very large databases for project management, configuration management, and problem reporting which record data on events during development. These large databases can be an unintrusive source of data for software quality modeling. However, multiplied by many releases of a legacy system or a broad product line, the amount of data can overwhelm manual analysis. The field of data mining is developing ways to find valuable bits of information in very large databases. This aptly describes our software quality modeling situation.This paper presents a case study that applied data mining techniques to software quality modeling of a very large legacy telecommunications software system's configuration management and problem reporting databases. The case study illustrates how useful models can be built and applied without interfering with development.},
journal = {Software Quality Journal},
month = nov,
pages = {161–176},
numpages = {16},
keywords = {software quality modeling, software metrics, knowledge discovery, data mining, classification trees}
}

@article{10.1155/2015/470274,
author = {Wu, Jiyun and Chen, Zhide},
title = {An implicit identity authentication system considering changes of gesture based on keystroke behaviors},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1550-1329},
url = {https://doi.org/10.1155/2015/470274},
doi = {10.1155/2015/470274},
abstract = {Smartphones have become ubiquitous personal devices so that much of sensitive and private information will be saved in the phone, and users have their own unique behavioral characteristics when using smartphones, so, to prevent private information from falling into the hands of impostors, there is a kind of identity authentication system based on user's behavioral features while the user is unlocking. However, due to the impact of environmental factors, changes of gesture will introduce bias into the feature data, which results in a diminishment of the system performance. To solve this problem, we propose an implicit identity authentication system based on keystroke behaviors, and it is the first attempt to consider the changes of a user's gesture. This system collects five keystroke features in the background and analyzes to identify different users without additional hardware supporting. We present our work with an experimental study, and our experiments show that the accuracy of identity authentication system we proposed is up to 99.1329%. Comparing with the identity authentication system without considering the impact of gesture changes, the EER of the system considering the impact of gesture changes is decreased by 1.2514%.},
journal = {Int. J. Distrib. Sen. Netw.},
month = jan,
articleno = {110},
numpages = {1}
}

@inproceedings{10.1145/1878431.1878439,
author = {Taysi, Z. Cihan and Guvensan, M. Amac and Melodia, Tommaso},
title = {TinyEARS: spying on house appliances with audio sensor nodes},
year = {2010},
isbn = {9781450304580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878431.1878439},
doi = {10.1145/1878431.1878439},
abstract = {Fine-grained awareness on how and where energy is spent is being increasingly recognized as the key to conserve energy. While several solutions to monitor the energy consumption patterns for commercial and industrial users exist, energy reporting systems currently available to residential users require time-consuming and intrusive installation procedures, or are otherwise unable to provide device-level reports on energy consumption. To fill this gap, this paper discusses the design and performance evaluation of the Tiny Energy Accounting and Reporting System (TinyEARS), a fine-grained energy monitoring system that generates devicelevel power consumption reports primarily based on the acoustic signatures of household appliances. Experiments demonstrate that TinyEARS is able to report the power consumption of individual household appliances within a 10% error margin.},
booktitle = {Proceedings of the 2nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building},
pages = {31–36},
numpages = {6},
keywords = {wireless audio sensor networks, house appliances, energy monitoring, audio data classification},
location = {Zurich, Switzerland},
series = {BuildSys '10}
}

@inproceedings{10.1145/2884781.2884794,
author = {Chen, Bihuan and Liu, Yang and Le, Wei},
title = {Generating performance distributions via probabilistic symbolic execution},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884794},
doi = {10.1145/2884781.2884794},
abstract = {Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are very important software engineering tasks. There have been model-based and program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are typically coarse-grained and could be imprecise. Program analysis-based approaches collect program profiles to identify performance bottlenecks, which often fail to capture the overall program performance. In this paper, we propose a performance analysis framework PerfPlotter. It takes the program source code and usage profile as inputs and generates a performance distribution that captures the input probability distribution over execution times for the program. It heuristically explores high-probability and low-probability paths through probabilistic symbolic execution. Once a path is explored, it generates and runs a set of test inputs to model the performance of the path. Finally, it constructs the performance distribution for the program. We have implemented PerfPlotter based on the Symbolic PathFinder infrastructure, and experimentally demonstrated that PerfPlotter could accurately capture the best-case, worst-case and distribution of program execution times. We also show that performance distributions can be applied to various important tasks such as performance understanding, bug validation, and algorithm selection.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {49–60},
numpages = {12},
keywords = {symbolic execution, performance analysis},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.4018/irmj.2012100101,
author = {Ishfaq, Rafay and Raja, Uzma},
title = {Task-Resource Capability Alignment: Discerning Staffing and Service Issues in Software Maintenance},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {25},
number = {4},
issn = {1040-1628},
url = {https://doi.org/10.4018/irmj.2012100101},
doi = {10.4018/irmj.2012100101},
abstract = {The effective management of software maintenance processes involves decisions about workforce levels, skill and expertise mix of developers, assignment of defect resolution tasks, and monitoring key system performance measures. This research uses a queuing based simulation approach to study these managerial issues. Using the data archives of a large global software organization, an empirical study of the historical defect reports and management decisions is conducted. A task-resource capability alignment scheme is developed that captures the defect complexity and skill/experience capabilities of software maintainers. The results of the empirical-computational study show that the defect arrival/reporting process affects the resource utilization and the time a defect spends in the system. The results also highlight the role of dedicated and shared resources on the system performance and indicate that replacing an experienced and skilled developer requires a significant order of magnitude increase in the maintenance workforce.},
journal = {Inf. Resour. Manage. J.},
month = oct,
pages = {1–25},
numpages = {25},
keywords = {Task Assignment, Software Maintenance, Simulation, Resource Allocation, Defect Resolution}
}

@inproceedings{10.1145/3340531.3412032,
author = {Chakraborty, Anirban and Ganguly, Debasis and Conlan, Owen},
title = {Retrievability based Document Selection for Relevance Feedback with Automatically Generated Query Variants},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412032},
doi = {10.1145/3340531.3412032},
abstract = {To mitigate the problem of over-dependence of a pseudo-relevance feedback algorithm on the top-M document set, we make use of a set of equivalence classes of queries rather than one single query. These query equivalents are automatically constructed either from a) a knowledge base of prior distributions of terms with respect to the given query terms, or b) iteratively generated from a relevance model of term distributions in the absence of such priors. These query variants are then used to estimate the retrievability of each document with the hypothesis that documents that are more likely to be retrieved at top-ranks for a larger number of these query variants are more likely to be effective for relevance feedback. Results of our experiments show that our proposed method is able to achieve substantially better precision at top-ranks (e.g. higher nDCG@5 and P@5 values) for ad-hoc IR and points-of-interest (POI) recommendation tasks.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {125–134},
numpages = {10},
keywords = {retrievability, query variants, pseudo-relevance feedback},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1504/ijcat.2021.117285,
author = {Mohanty, Monalisa and Subudhi, Asit and Mohanty, Mihir Narayan},
title = {Detection of supraventricular tachycardia using decision tree model},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {65},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2021.117285},
doi = {10.1504/ijcat.2021.117285},
abstract = {Supra Ventricular Tachycardia (SVT) refers to an abnormally fast heartbeat that arises because of the improper electrical activity in the upper chamber of the heart. In this paper, authors have attempted to detect the SVT of human subjects. The ECG recordings have been collected from the MIT-BIH supraventricular arrhythmia database (SVDB) of the Physionet repository. Using Gain Ratio Attribute Evaluation method features are extracted. The evaluated features are then ranked according to their weightage value using the Ranker Search algorithm. The set of features are extracted for ST, N and VF. Machine learning-based classifiers such as Multi-Layer Perceptron (MLP) and Logistic Model Tree (LMT) are utilised to classify the ECG signals from the feature set. It is found that the proposed LMT model outperforms the MLP model and provides 99.23% accuracy. Also, the performance measures are done with sensitivity, specificity and precision as exhibited in the result section.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {378–388},
numpages = {10},
keywords = {LMT, MLP, feature extraction, SVT, tachycardia}
}

@inproceedings{10.5555/3108244.3108256,
author = {Bapty, Theodore A. and Scott, Jason and Neema, Sandeep and Owens, Robert},
title = {Integrated modeling and simulation for cyberphysical systems extending multi-domain M&amp;S to the design community},
year = {2017},
isbn = {9781510838260},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Modeling and Simulation is increasingly important in managing design of complex cyber-physical systems. The diversity of tools, each with unique syntax and semantics, necessitates creation of multiple independent models in order to fully analyze a system. In addition to increasing the cost and time required to fully leverage computational tools, issues with model synchronization present a hazard. This is only compounded when multiple design options must be considered. In this paper, we discuss work on the OpenMETA framework (Sztipanovits et al 2015), which uses a single integrated model, coupled with automated design composition to transform the model into the specific syntax and semantics of multiple analysis tools for computing system performance. Design space specification primitives, and exploration tools allow an entire design family to be explored, across multiple physical domains and levels of abstraction. A simple design study is reviewed to demonstrate the process.},
booktitle = {Proceedings of the Symposium on Model-Driven Approaches for Simulation Engineering},
articleno = {12},
numpages = {12},
keywords = {simulation composition, model-based, design space},
location = {Virginia Beach, Virginia},
series = {Mod4Sim '17}
}

@inproceedings{10.1007/978-3-030-22244-4_1,
author = {Zhai, Jiahe and Zhu, Zhengzhou and Li, Deqi and Huang, Nanxiong and Zhang, Kaiyue and Huang, Yuqi},
title = {A Learning Early-Warning Model Based on Knowledge Points},
year = {2019},
isbn = {978-3-030-22243-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22244-4_1},
doi = {10.1007/978-3-030-22244-4_1},
abstract = {Learning early-warning is one of the important ways to realize adaptive learning. Aiming at the problem of too large prediction granularity in learning early-warning, we divide student’s characters into three dimensions (knowledge, behavior and emotion). Secondly, we predict the student’s master degree of knowledge, based on the knowledge point. And then we realized learning early-warning model. In the model, we take 60 points as the learning early-warning standard, and take RF and GDBT as base classifiers, and give the strategy of selecting the basic model. The experiment shows that the prediction of knowledge mastery of the model and the real data Pearson correlation coefficient can reach 0.904279, and the prediction accuracy of the model below the early-warning line can reach 76%.},
booktitle = {Intelligent Tutoring Systems: 15th International Conference, ITS 2019, Kingston, Jamaica, June 3–7, 2019, Proceedings},
pages = {1–6},
numpages = {6},
keywords = {Learning early-warning, Emotion, Type of question, Knowledge points},
location = {Kingston, Jamaica}
}

@article{10.1016/j.compeleceng.2021.107215,
author = {Ardagna, Claudio A. and Bellandi, Valerio and Damiani, Ernesto and Bezzi, Michele and Hebert, Cedric},
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107215},
doi = {10.1016/j.compeleceng.2021.107215},
journal = {Comput. Electr. Eng.},
month = jul,
numpages = {10},
keywords = {Security and privacy, Machine learning, Big Data Analytics, Artificial intelligence}
}

@inproceedings{10.1145/3411408.3411433,
author = {Korovesis, Konstantinos and Alexandridis, Georgios and Caridakis, George and Polydoras, Pavlos and Tsantilas, Panagiotis},
title = {Leveraging aspect-based sentiment prediction with textual features and document metadata},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411433},
doi = {10.1145/3411408.3411433},
abstract = {Aspect-based sentiment prediction is a specific area of sentiment analysis that models the sentiment of a text excerpt as a multi-dimensional quantity pertaining to various interpretations, rather than a scalar one, that admits a single explanation. Extending earlier work, the said task is examined as a part of a unified architecture that collects, analyzes and stores documents from various online sources, including blogs &amp; social network posts. The obtained data are processed at various levels; initially, a hybrid, attention-based bi-directional long short-term memory network, coupled with convolutional layers, is used to extract the textual features of the document. Following, an additional number of document metadata are also examined, such as the number of repetitions, the existence, type and frequency of emoji ideograms and, especially, the presence of keywords, assigned either manually (e.g. in the form of hashtags) or automatically. All of the aforementioned features are subsequently provided as input to a fully-connected, multi-layered, feed-forward artificial neural network that performs the final prediction task. The overall approach is tested on a large corpus of documents, with encouraging results.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {168–174},
numpages = {7},
keywords = {deep learning, convolutional neural networks, bi-directional long short-term memory units, attention mechanism, aspect-based sentiment prediction},
location = {Athens, Greece},
series = {SETN 2020}
}

@article{10.1016/j.compeleceng.2019.03.015,
author = {Naeem, Hamad and Guo, Bing and Naeem, Muhammad Rashid and Ullah, Farhan and Aldabbas, Hamza and Javed, Muhammad Sufyan},
title = {Identification of malicious code variants based on image visualization},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2019.03.015},
doi = {10.1016/j.compeleceng.2019.03.015},
journal = {Comput. Electr. Eng.},
month = jun,
pages = {225–237},
numpages = {13},
keywords = {Machine learning, Malware variants, Malware detection, LGMP, Image visualization, Grayscale image, Feature extraction and selection, Cyber security}
}

@article{10.1007/s10664-013-9266-8,
author = {Itkonen, Juha and M\"{a}ntyl\"{a}, Mika V.},
title = {Are test cases needed? Replicated comparison between exploratory and test-case-based software testing},
year = {2014},
issue_date = {April     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9266-8},
doi = {10.1007/s10664-013-9266-8},
abstract = {Manual software testing is a widely practiced verification and validation method that is unlikely to fade away despite the advances in test automation. In the domain of manual testing, many practitioners advocate exploratory testing (ET), i.e., creative, experience-based testing without predesigned test cases, and they claim that it is more efficient than testing with detailed test cases. This paper reports a replicated experiment comparing effectiveness, efficiency, and perceived differences between ET and test-case-based testing (TCT) using 51 students as subjects, who performed manual functional testing on the jEdit text editor. Our results confirm the findings of the original study: 1) there is no difference in the defect detection effectiveness between ET and TCT, 2) ET is more efficient by requiring less design effort, and 3) TCT produces more false-positive defect reports than ET. Based on the small differences in the experimental design, we also put forward a hypothesis that the effectiveness of the TCT approach would suffer more than ET from time pressure. We also found that both approaches had distinctive issues: in TCT, the problems were related to correct abstraction levels of test cases, and the problems in ET were related to test design and logging of the test execution and results. Finally, we recognize that TCT has other benefits over ET in managing and controlling testing in large organizations.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {303–342},
numpages = {40},
keywords = {Test cases, Software testing, Manual testing, Exploratory testing, Experiment, Efficiency, Effectiveness}
}

@article{10.1016/j.compbiolchem.2021.107583,
author = {Zhang, Shengli and Shi, Hongyan},
title = {iR5hmcSC: Identifying RNA 5-hydroxymethylcytosine with multiple features based on stacking learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {1476-9271},
url = {https://doi.org/10.1016/j.compbiolchem.2021.107583},
doi = {10.1016/j.compbiolchem.2021.107583},
journal = {Comput. Biol. Chem.},
month = dec,
numpages = {8},
keywords = {Stacking, One-hot encoding, Pseudo structure status composition, K-mer, RNA 5-hydroxymethylcytosine}
}

@article{10.1007/s00500-021-05850-x,
author = {Bai, Anita and Hira, Swati},
title = {An intelligent hybrid deep belief network model for predicting students employability},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {14},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05850-x},
doi = {10.1007/s00500-021-05850-x},
abstract = {In recent times, the question of employability has become a critical concern not only for degree holders but for educational organizations. Hence, employability prediction models play an important role in analyzing the student’s capability to get employment. In this paper, a hybrid model of deep belief network and soft max regression (DBN-SR) is proposed for student employability prediction. Initially, pre-processing is performed on student’s data for removing irrelevant attributes to achieve data consistency. Further, to enhance the accuracy of the prediction model, the crow search algorithm-based feature selection model is employed to select the optimal subset of features from original features. Then, the selected subset of features is taken as the input of the deep belief network (DBN) for intrinsic feature learning to obtain high-level feature representation. Finally, the soft max regression (SR) is used to predict the class of students as employed or unemployed. The proposed employability prediction model achieves above 98% of accuracy which is comparatively 2.5%, 5% higher than the deep autoencoder and deep neural network-based models. The performance outcomes proved that the proposed DBN-SR model has been well suitable for predicting student’s employability.},
journal = {Soft Comput.},
month = jul,
pages = {9241–9254},
numpages = {14},
keywords = {Data mining, Deep belief network-softmax regression, Crow search algorithm, Employability prediction}
}

@article{10.1007/s10772-018-09572-8,
author = {Lalitha, S. and Tripathi, Shikha and Gupta, Deepa},
title = {Enhanced speech emotion detection using deep neural networks},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-018-09572-8},
doi = {10.1007/s10772-018-09572-8},
abstract = {This paper focusses on investigation of the effective performance of perceptual based speech features on emotion detection. Mel frequency cepstral coefficients (MFCC’s), perceptual linear predictive cepstrum (PLPC), Mel frequency perceptual linear prediction cepstrum (MFPLPC), bark frequency cepstral coefficients (BFCC), revised perceptual linear prediction coefficient’s (RPLP) and inverted Mel frequency cepstral coefficients (IMFCC) are the perception features considered. The algorithm using these auditory cues is evaluated with deep neural networks (DNN). The novelty of the work involves analysis of the perceptual features to identify predominant features that contain significant emotional information about the speaker. The validity of the algorithm is analysed on publicly available Berlin database with seven emotions in 1-dimensional space termed categorical and 2-dimensional continuous space consisting of emotions in valence and arousal dimensions. Comparative analysis reveals that considerable improvement in the performance of emotion recognition is obtained using DNN with the identified combination of perceptual features.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {497–510},
numpages = {14},
keywords = {Valence, Recognition accuracy, Perceptual features, Emotion detection, DNN, Cepstrum, BFCC, Arousal}
}

@article{10.1007/s11390-021-0782-5,
author = {Cao, Yang-Jie and Wu, Shuang and Liu, Chang and Lin, Nan and Wang, Yuan and Yang, Cong and Li, Jie},
title = {Seg-CapNet: A Capsule-Based Neural Network for the Segmentation of Left Ventricle from Cardiac Magnetic Resonance Imaging},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {2},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-021-0782-5},
doi = {10.1007/s11390-021-0782-5},
abstract = {Deep neural networks (DNNs) have been extensively studied in medical image segmentation. However, existing DNNs often need to train shape models for each object to be segmented, which may yield results that violate cardiac anatomical structure when segmenting cardiac magnetic resonance imaging (MRI). In this paper, we propose a capsule-based neural network, named Seg-CapNet, to model multiple regions simultaneously within a single training process. The Seg-CapNet model consists of the encoder and the decoder. The encoder transforms the input image into feature vectors that represent objects to be segmented by convolutional layers, capsule layers, and fully-connected layers. And the decoder transforms the feature vectors into segmentation masks by up-sampling. Feature maps of each down-sampling layer in the encoder are connected to the corresponding up-sampling layers, which are conducive to the backpropagation of the model. The output vectors of Seg-CapNet contain low-level image features such as grayscale and texture, as well as semantic features including the position and size of the objects, which is beneficial for improving the segmentation accuracy. The proposed model is validated on the open dataset of the Automated Cardiac Diagnosis Challenge 2017 (ACDC 2017) and the Sunnybrook Cardiac Magnetic Resonance Imaging (MRI) segmentation challenge. Experimental results show that the mean Dice coefficient of Seg-CapNet is increased by 4.7% and the average Hausdorff distance is reduced by 22%. The proposed model also reduces the model parameters and improves the training speed while obtaining the accurate segmentation of multiple regions.},
journal = {J. Comput. Sci. Technol.},
month = apr,
pages = {323–333},
numpages = {11},
keywords = {capsule neural network, image segmentation, left ventricle segmentation, cardiac magnetic resonance imaging}
}

@article{10.1007/s11277-019-06540-6,
author = {Naeem, Hamad},
title = {Detection of Malicious Activities in Internet of Things Environment Based on Binary Visualization and Machine Intelligence},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06540-6},
doi = {10.1007/s11277-019-06540-6},
abstract = {Internet of Things (IoT) devices are increasingly deployed for different purposes such as data sensing, collecting and controlling. IoT improves user experiences by allowing a large number of smart devices to connect and share information. Many existing malware attacks, targeted at traditional computers connected to the Internet, may also be directed at IoT devices. Therefore, efficient protection at IoT devices could save millions of internet users from malicious activities. However, existing malware detection approaches suffer from high computational complexity. In this study, we propose a more accurate and fast model for detecting malware in the IoT environment. We introduce a Malware Threat Hunting System (MTHS) in the proposed model. MTHS first converts malware binary into a color image and then conducts the machine or deep learning analysis for efficient malware detection. We finally prepare a baseline to compare the performance of MTHS with traditional state-of-the-art malware detection approaches. We conduct experiments on two public datasets of Windows and Android software. The experimental results indicate that the response time and the detection accuracy of MTHS are better than those of previous machine learning and deep learning approaches.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2609–2629},
numpages = {21},
keywords = {Visualization, Machine learning, MTHS, Malware detection, Internet of things, Deep learning, Color image, Cyber security}
}

@article{10.1016/j.eswa.2014.11.031,
author = {Lee, Namyeon and Kwon, Ohbyung},
title = {A privacy-aware feature selection method for solving the personalization-privacy paradox in mobile wellness healthcare services},
year = {2015},
issue_date = {April 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.11.031},
doi = {10.1016/j.eswa.2014.11.031},
abstract = {The essence of personalization in mobile wellness healthcare (MWH) services is feature selection.A novel method of privacy-aware feature selection for individualized MWH is presented.An optimal set of features is automatically found to solve the personalization-privacy paradox.The results successfully show that the proposed method outperforms conventional methods. Despite the vast number of available opportunities, customers' privacy concerns can inhibit their acceptance of mobile wellness healthcare services. Personalization of these services may go some way toward alleviating these concerns. The essence of personalization in mobile wellness healthcare services is feature selection because users may be concerned about disclosing private information, although it may be useful in provision of personalized services. Therefore, an optimal feature selection method is needed which considers both these privacy concerns and the quality of personalization. Such safe and accurate data collection would facilitate understanding of customers' preferences while safeguarding their privacy. In this study, a privacy-aware feature selection method is proposed based on the personalization-privacy paradox, and this paradox is explored in the context of wellness healthcare services with consideration of the personal characteristics of customers using these services.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2764–2771},
numpages = {8},
keywords = {Privacy decision-making, Privacy calculus, Personalization-privacy paradox, Mobile wellness healthcare services, Feature selection}
}

@article{10.1007/s11042-013-1824-y,
author = {Nakayama, Minoru and Fujimoto, Masashi},
title = {Features of Oculo-motors and their chronological changes in response to varying sizes of visual stimuli},
year = {2015},
issue_date = {April     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {8},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-013-1824-y},
doi = {10.1007/s11042-013-1824-y},
abstract = {The chronological process of eye accommodation used when viewing objects was analysed to predict the size of viewed objects. Viewer's visual evoked potentials (VEP), eye movements and pupil oscillations were measured while six sizes of Landolt circles were presented for 1s each. The significant features of the metrics were extracted to illustrate the perception process and the resulting sizes of visual stimuli. The stimulus sizes affect P1 and N1 delay times of VEP, means of eye movement lengths and pupil diameters and oscillations during the second 500 milliseconds of observation. The feasibility of prediction of stimulus sizes viewed was confirmed using the features of eye behaviour and support vector machines. Both indices of eye movements and pupil diameters were determined to be significant features for making predictions. The influences of the combination of features and viewer's individual factors were also confirmed. This result suggests that features of oculo-motor indices reflect the size of objects being viewed and that these viewing sizes can be predicted using the features.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {2841–2859},
numpages = {19},
keywords = {Visual object size viewed, Visual Evoked Potential (VEP), Pupil, Eye movement, Accommodation}
}

@article{10.1145/3424308,
author = {Chen, Zhenpeng and Cao, Yanbin and Yao, Huihan and Lu, Xuan and Peng, Xin and Mei, Hong and Liu, Xuanzhe},
title = {Emoji-powered Sentiment and Emotion Detection from Software Developers’ Communication Data},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3424308},
doi = {10.1145/3424308},
abstract = {Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {18},
numpages = {48},
keywords = {software engineering, sentiment, emotion, Emoji}
}

@article{10.1007/s11704-020-8426-4,
author = {Hu, Yu and Nie, Tiezheng and Shen, Derong and Kou, Yue and Yu, Ge},
title = {An integrated pipeline model for biomedical entity alignment},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {3},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-8426-4},
doi = {10.1007/s11704-020-8426-4},
abstract = {Biomedical entity alignment, composed of two sub-tasks: entity identification and entity-concept mapping, is of great research value in biomedical text mining while these techniques are widely used for name entity standardization, information retrieval, knowledge acquisition and ontology construction.Previous works made many efforts on feature engineering to employ feature-based models for entity identification and alignment. However, the models depended on subjective feature selection may suffer error propagation and are not able to utilize the hidden information. With rapid development in health-related research, researchers need an effective method to explore the large amount of available biomedical literatures.Therefore, we propose a two-stage entity alignment process, biomedical entity exploring model, to identify biomedical entities and align them to the knowledge base interactively. The model aims to automatically obtain semantic information for extracting biomedical entities and mining semantic relations through the standard biomedical knowledge base. The experiments show that the proposed method achieves better performance on entity alignment. The proposed model dramatically improves the F1 scores of the task by about 4.5% in entity identification and 2.5% in entity-concept mapping.},
journal = {Front. Comput. Sci.},
month = jun,
numpages = {15},
keywords = {neural network model, biomedical text mining, entity alignment}
}

@article{10.1023/A:1008297832410,
author = {Klein, Hans W.},
title = {The EPAC Architecture: An Expert Cell Approach to Field Programmable Analog Devices},
year = {1998},
issue_date = {Sept. 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {1–2},
issn = {0925-1030},
url = {https://doi.org/10.1023/A:1008297832410},
doi = {10.1023/A:1008297832410},
abstract = {This paper describes the architectural configuration and various design trade-offs of the Electrically Programmable Analog Circuit (EPAC ^TM ), an expert-cell approach to meeting the market needs for an analog counterpart to the digital FPGA. The paper provides an overview of the technology, discusses architectural issues, and describes the internal operation of the first commercial EPAC devices. The paper concludes with various application examples and performance measurements.},
journal = {Analog Integr. Circuits Signal Process.},
month = sep,
pages = {91–103},
numpages = {13},
keywords = {in-system programming, high-performance cells, field programmable analog IC}
}

@inproceedings{10.1145/3485730.3485948,
author = {Ye, Hanting and Wang, Qing},
title = {SpiderWeb: Enabling Through-Screen Visible Light Communication},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485948},
doi = {10.1145/3485730.3485948},
abstract = {We are now witnessing a trend of realizing full-screen on electronic devices such as smartphones to maximize their screen-to-body ratio for a better user experience. Thus the bezel/narrow-bezel on today's devices to host various line-of-sight sensors would disappear. This trend not only is forcing sensors like the front cameras to be placed under the screen of devices, but also will challenge the deployment of the emerging Visible Light Communication (VLC) technology, a paradigm for the next-generation wireless communication.In this work, we propose the concept of through-screen VLC with photosensors placed under Organic Light-Emitting Diode (OLED) screen. Though being transparent, an OLED screen greatly attenuates the intensity of passing-through light, degrading the efficiency of intensity-based VLC systems. In this paper, we instead exploit the color domain to build SpiderWeb, a through-screen VLC system. For the first time, we observe that an OLED screen introduces a color-pulling effect at photosensors, affecting the decoding of color-based VLC signals. Motivated by this observation and by the structure of spider's web, we design the SWebCSK Color-Shift Keying modulation scheme and a slope-based demodulation method, which can eliminate the color-pulling effect. We prototype SpiderWeb with off-the-shelf hardware and evaluate its performance thoroughly under various scenarios. The results show that compared to existing solutions, our solutions can reduce the bit error rate by two orders of magnitude and can achieve a 3.4x data rate.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {316–328},
numpages = {13},
keywords = {transparent OLED screen, color-pulling effect, Through-screen VLC},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1016/j.knosys.2021.107525,
author = {Liu, Cong and Xu, Xiaolong},
title = {AMFF: A new attention-based multi-feature fusion method for intention recognition},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107525},
doi = {10.1016/j.knosys.2021.107525},
journal = {Know.-Based Syst.},
month = dec,
numpages = {8},
keywords = {Classification, Short text, Multi-feature fusion, Intention recognition}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@article{10.1016/j.jvcir.2019.04.004,
author = {Liu, Tsung-Jung and Liu, Kuan-Hsien and Shen, Kuan-Hung},
title = {Learning based no-reference metric for assessing quality of experience of stereoscopic images},
year = {2019},
issue_date = {May 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2019.04.004},
doi = {10.1016/j.jvcir.2019.04.004},
journal = {J. Vis. Comun. Image Represent.},
month = may,
pages = {272–283},
numpages = {12},
keywords = {Visual discomfort, Stereoscopic image, Quality of experience, No reference, Disparity-depth map}
}

@article{10.1007/s00034-019-01125-x,
author = {Bhukya, Ramesh K. and Prasanna, S. R. Mahadeva and Sarma, Biswajit Dev},
title = {Robust Methods for Text-Dependent Speaker Verification},
year = {2019},
issue_date = {Nov 2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {11},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-019-01125-x},
doi = {10.1007/s00034-019-01125-x},
abstract = {In this work, we explore various noise robust techniques at different stages of a Text-Dependent Speaker Verification (TDSV) system. A speech-specific knowledge-based robust end points detection technique is used for noise compensation at signal level. Feature-level compensation is done by using robust features extracted from Hilbert Spectrum (HS) of the Intrinsic Mode Functions obtained from Modified Empirical Mode Decomposition of speech. We also explored a combined temporal and spectral speech enhancement technique prior to the end points detection for enhancing speech regions embedded in noise. All experimental studies are conducted using two databases, namely the RSR2015 and the IITG database. It is found that the use of robust end points detection improves the performance of the TDSV system compared to the energy-based end points detection in both clean and degraded speech conditions. Use of noise robust HS features augmented with Mel-frequency cepstral coefficients further improves the performance of the system. It is also found that the use of speech enhancement prior to signal and feature-level compensation results in further improvement in performance for the low SNR cases. The final combined system obtained by using three robust methods provides a relative improvement from 6 to 25% in terms of the EER, on the RSR2015 database corrupted with Babble noise of varying strength and by around from 30 to 45% relative improvement on the IITG database.},
journal = {Circuits Syst. Signal Process.},
month = nov,
pages = {5253–5288},
numpages = {36},
keywords = {DTW, TDSV, MFCCs, Hilbert spectrum, IMFs, MEMD, Foreground speech segmentation, Glottal activity detection, Dominant resonant frequency, VLRs, End point detection}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@article{10.1016/j.eswa.2021.114756,
author = {Chen, Shui-xia and Wang, Xiao-kang and Zhang, Hong-yu and Wang, Jian-qiang},
title = {Customer purchase prediction from the perspective of imbalanced data: A machine learning framework based on factorization machine},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114756},
doi = {10.1016/j.eswa.2021.114756},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Machine learning, Imbalanced data, Factorization machine, Customer purchase prediction}
}

@inproceedings{10.1145/3238147.3238204,
author = {Han, Xue and Yu, Tingting and Lo, David},
title = {PerfLearner: learning from bug reports to understand and generate performance test frames},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238204},
doi = {10.1145/3238147.3238204},
abstract = {Software performance is important for ensuring the quality of software products. Performance bugs, defined as programming errors that cause significant performance degradation, can lead to slow systems and poor user experience. While there has been some research on automated performance testing such as test case generation, the main idea is to select workload values to increase the program execution times. These techniques often assume the initial test cases have the right combination of input parameters and focus on evolving values of certain input parameters. However, such an assumption may not hold for highly configurable real-word applications, in which the combinations of input parameters can be very large. In this paper, we manually analyze 300 bug reports from three large open source projects - Apache HTTP Server, MySQL, and Mozilla Firefox. We found that 1) exposing performance bugs often requires combinations of multiple input parameters, and 2) certain input parameters are frequently involved in exposing performance bugs. Guided by these findings, we designed and evaluated an automated approach, PerfLearner, to extract execution commands and input parameters from descriptions of performance bug reports and use them to generate test frames for guiding actual performance test case generation.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {17–28},
numpages = {12},
keywords = {Software Testing, Software Mining, Performance Bugs},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.5555/1812740.1812798,
author = {Abreu, M\'{a}rjory and Fairhurst, Michael},
title = {Improving identity prediction in signature-based unimodal systems using soft biometrics},
year = {2009},
isbn = {3642043909},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {System optimisation, where even small individual system performance gains can often have a significant impact on applicability and viability of biometric solutions, is an important practical issue. This paper analyses two different techniques for using soft biometric information (which is often already available or easily obtainable in many applications) to improve identity prediction accuracy of signature-based tasks. It is shown that such a strategy can improve performance of unimodal systems, supporting high usability profiles and low-cost processing.},
booktitle = {Proceedings of the 2009 Joint COST 2101 and 2102 International Conference on Biometric ID Management and Multimodal Communication},
pages = {348–356},
numpages = {9},
location = {Madrid, Spain},
series = {BioID_MultiComm'09}
}

@inproceedings{10.5555/306792.306845,
author = {Taheri, H. Reza and Askins, Bradley J.},
title = {Simulating the performance of a multiprocessor operating system},
year = {1991},
isbn = {0818621699},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the 24th Annual Symposium on Simulation},
pages = {81–90},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {ANSS '91}
}

@inproceedings{10.5555/3327345.3327396,
author = {Zhang, Xin and Solar-Lezama, Armando and Singh, Rishabh},
title = {Interpreting neural network judgments via minimal, stable, and symbolic corrections},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4879–4890},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1007/s00138-021-01167-9,
author = {Jia, Dongyao and Zhang, Chuanwang and Zhang, Bing},
title = {Crowd density classification method based on pixels and texture features},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-021-01167-9},
doi = {10.1007/s00138-021-01167-9},
abstract = {Crowd density classification has been a challenging task in the field of computer vision, which has various applications in public and commercial domains. Many researches on the classification and recognition method of the crowd density have been introduced in the past, while there still exists the problems of inaccuracy, poor robustness and inefficiency. An adaptive crowd density classification method based on pixels and texture features is proposed in this paper. Core part of the method is to adopt different processing methods according to the corresponding crowd density. The method based on pixel regression method is used for the sparse crowd condition, while the texture features are applied in the dense crowd. Variety of texture features like local binary pattern (LBP), gray-level co-occurrence matrix (GLCM), Gabor, Haar-like and Wavelet group are used on the WorldExpo’10 dataset to obtain an optimum combination of these features, which is proposed to extract the texture features of the crowd images. Then the SVM classifier model based on Bayesian estimation is adopted to train the model which can filter the abnormal sample data to improve the accuracy and generalization performance of the algorithm. Meanwhile, a K-means clustering iterative training method based on optimized sorting samples is designed to improve the training speed in the training process. Extensive experiments from various aspects including parameter optimization, feature selection and model evaluation were conducted. The performance of the model is tested based on mean absolute error (MAE), mean squared error (MSE) and classification rate (CR) in dataset UCSD, Shanghai Tech_A and UCF_CC_50. The experimental results show that CR of the proposed method can reach to 98.2%, whose indexes of MAE and MSE also outperform the most existing methods. In general, the proposed approach in this paper has obvious advantages and great application value.},
journal = {Mach. Vision Appl.},
month = mar,
numpages = {22},
keywords = {Iterative training, Support vector machine (SVM), Bias estimation, Crowd density}
}

@article{10.1016/j.dss.2015.04.006,
author = {Throckmorton, Chandra S. and Mayew, William J. and Venkatachalam, Mohan and Collins, Leslie M.},
title = {Financial fraud detection using vocal, linguistic and financial cues},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2015.04.006},
doi = {10.1016/j.dss.2015.04.006},
abstract = {Corporate financial fraud has a severe negative impact on investors and the capital market in general. The current resources committed to financial fraud detection (FFD), however, are insufficient to identify all occurrences in a timely fashion. Methods for automating FFD have mainly relied on financial statistics, although some recent research has suggested that linguistic or vocal cues may also be useful indicators of deception. Tools based on financial numbers, linguistic behavior, and non-verbal vocal cues have each demonstrated the potential for detecting financial fraud. However, the performance of these tools continues to be poorer than desired, limiting their use on a stand-alone basis to help identify companies for further investigation. The hypothesis investigated in this study is that an improved tool could be developed if specific attributes from these feature categories were analyzed concurrently. Combining features across categories provided better fraud detection than was achieved by any of the feature categories alone. However, performance improvements were only observed if feature selection was used suggesting that it is important to discard non-informative features. We investigate whether a prediction model for corporate financial fraud that jointly considers numeric financial information as well as both linguistic and vocalic aspects of corporate executive speech improves predictive accuracy.Optimization results reveal that only a subset of the complete set of numeric, linguistic and vocalic predictors enhance overall predictive accuracy.These results should assist investors, financial analysts and regulators in identifying the most effective markers of corporate fraud.},
journal = {Decis. Support Syst.},
month = jun,
pages = {78–87},
numpages = {10},
keywords = {Financial fraud, Corporate executive speech, Automated deception detection}
}

@article{10.3233/JIFS-189090,
author = {Algin, Ramazan and Alkaya, Ali Fuat and Agaoglu, Mustafa and Kahraman, Cengiz},
title = {Feature selection via computational intelligence techniques},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189090},
doi = {10.3233/JIFS-189090},
abstract = {Feature selection (FS) has become an essential task in overcoming high dimensional and complex machine learning problems. FS is a process used for reducing the size of the dataset by separating or extracting unnecessary and unrelated properties from it. This process improves the performance of classification algorithms and reduces the evaluation time by enabling the use of small sized datasets with useful features during the classification process. FS aims to gain a minimal feature subset in a problem domain while retaining the accuracy of the original data. In this study, four computational intelligence techniques, namely, migrating birds optimization (MBO), simulated annealing (SA), differential evolution (DE) and particle swarm optimization (PSO) are implemented for the FS problem as search algorithms and compared on the 17 well-known datasets taken from UCI machine learning repository where the dimension of the tackled datasets vary from 4 to 500. This is the first time that MBO is applied for solving the FS problem. In order to judge the quality of the subsets generated by the search algorithms, two different subset evaluation methods are implemented in this study. These methods are probabilistic consistency-based FS (PCFS) and correlation-based FS (CFS). Performance comparison of the algorithms is done by using three well-known classifiers; k-nearest neighbor, naive bayes and decision tree (C4.5). As a benchmark, the accuracy values found by classifiers using the datasets with all features are used. Results of the experiments show that our MBO-based filter approach outperforms the other three approaches in terms of accuracy values. In the experiments, it is also observed that as a subset evaluator CFS outperforms PCFS and as a classifier C4.5 gets better results when compared to k-nearest neighbor and naive bayes.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6205–6216},
numpages = {12},
keywords = {subset evaluators, classification algorithms, meta-heuristics, dimensionality reduction, computational intelligence, Feature selection}
}

@article{10.1016/j.asoc.2021.107507,
author = {Xueshuo, Xie and Jiming, Wang and Junyi, Ye and Yaozheng, Fang and Ye, Lu and Tao, Li and Guiling, Wang},
title = {AWAP: Adaptive weighted attribute propagation enhanced community detection model for bitcoin de-anonymization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {109},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107507},
doi = {10.1016/j.asoc.2021.107507},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {15},
keywords = {Feature engineering, Attribute propagation, Community detection, Bitcoin anonymity}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Systematic mapping study, Software component, Component-based software engineering, Component-based software development}
}

@article{10.1016/j.comnet.2019.106969,
author = {Pan, Lujia and Zhang, Jianfeng and Lee, Patrick P.C. and Kalander, Marcus and Ye, Junjian and Wang, Pinghui},
title = {Proactive microwave link anomaly detection in cellular data networks},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.106969},
doi = {10.1016/j.comnet.2019.106969},
journal = {Comput. Netw.},
month = feb,
numpages = {12},
keywords = {Active learning, Network embedding, Anomaly detection}
}

@article{10.1007/s00778-018-0512-y,
author = {Breβ, Sebastian and K\"{o}cher, Bastian and Funke, Henning and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Generating custom code for efficient query execution on heterogeneous processors},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0512-y},
doi = {10.1007/s00778-018-0512-y},
abstract = {Processor manufacturers build increasingly specialized processors to mitigate the effects of the power wall in order to deliver improved performance. Currently, database engines have to be manually optimized for each processor which is a costly and error- prone process. In this paper, we propose concepts to adapt to and to exploit the performance enhancements of modern processors automatically. Our core idea is to create processor-specific code variants and to learn a well-performing code variant for each processor. These code variants leverage various parallelization strategies and apply both generic- and processor-specific code transformations. Our experimental results show that the performance of code variants may diverge up to two orders of magnitude. In order to achieve peak performance, we generate custom code for each processor. We show that our approach finds an efficient custom code variant for multi-core CPUs, GPUs, and MICs.},
journal = {The VLDB Journal},
month = dec,
pages = {797–822},
numpages = {26},
keywords = {Variant optimization, Query compilation, MIC, Heterogeneous processors, GPU, Database systems, Database query processing, Code variants, Code generation, CPU}
}

@article{10.1007/s11265-016-1200-z,
author = {Ondusko, Russell and Marbach, Matthew and Ramachandran, Ravi P. and Head, Linda M.},
title = {Blind Signal-to-Noise Ratio Estimation of Speech Based on Vector Quantizer Classifiers and Decision Level Fusion},
year = {2017},
issue_date = {Nov 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {89},
number = {2},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-016-1200-z},
doi = {10.1007/s11265-016-1200-z},
abstract = {A blind approach for estimating the signal to noise ratio (SNR) of a speech signal corrupted by additive noise is proposed. The method is based on a pattern recognition paradigm using various linear predictive based features, a vector quantizer classifier and estimation combination. Blind SNR estimation is very useful in speaker identification systems in which a confidence metric is determined along with the speaker identity. The confidence metric is partially based on the mismatch between the training and testing conditions of the speaker identification system and SNR estimation is very important in evaluating the degree of this mismatch. The aim is to correctly estimate SNR values from 0 to 30 dB, a range that is both practical and crucial for speaker identification systems. Experiments consider (1) artificially generated additive white Gaussian noise, pink noise and bandpass noise and (2) fifteen noise types from the NOISEX database. Four features are combined to get the best results. The average SNR estimation error depends on the type of noise in that a relatively low error results for pink noise and jet cockpit noise and a high error results for destroyer operations room noise and military vehicle noise. For both artificially generated noise and the NOISEX data, the error is lower than what is achieved by the IMCRA method that uses SNR estimation for speech enhancement. Combining the four features with IMCRA lowers the error for 8 of the 15 noise types from NOISEX.},
journal = {J. Signal Process. Syst.},
month = nov,
pages = {335–345},
numpages = {11},
keywords = {Vector quantizer classifier, Overall average absolute error, Linear predictive features, Estimation combination, Blind estimation}
}

@article{10.1007/s10619-020-07288-w,
author = {Kossmann, Jan and Schlosser, Rainer},
title = {Self-driving database systems: a conceptual approach},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {38},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-020-07288-w},
doi = {10.1007/s10619-020-07288-w},
abstract = {Challenges for self-driving database systems, which tune their physical design and configuration autonomously, are manifold: Such systems have to anticipate future workloads, find robust configurations efficiently, and incorporate knowledge gained by previous actions into later decisions. We present a component-based framework for self-driving database systems that enables database integration and development of self-managing functionality with low overhead by relying on separation of concerns. By keeping the components of the framework reusable and exchangeable, experiments are simplified, which promotes further research in that area. Moreover, to optimize multiple mutually dependent features, e.g., index selection and compression configurations, we propose a linear programming (LP) based algorithm to derive an efficient tuning order automatically. Afterwards, we demonstrate the applicability and scalability of our approach with reproducible examples.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {795–817},
numpages = {23},
keywords = {Robustness, Workload prediction, Recursive tuning, Self-driving, Database systems}
}

@article{10.1016/j.patcog.2019.107059,
author = {Ma, Jizhou and Li, Shuai and Qin, Hong and Hao, Aimin},
title = {Adaptive appearance modeling via hierarchical entropy analysis over multi-type features},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107059},
doi = {10.1016/j.patcog.2019.107059},
journal = {Pattern Recogn.},
month = feb,
numpages = {14},
keywords = {Image classification, Hierarchical maximum entropy model, Random forest, Adaptive feature selection, Description model}
}

@inproceedings{10.1145/3340555.3353743,
author = {Li, Sixia and Okada, Shogo and Dang, Jianwu},
title = {Interaction Process Label Recognition in Group Discussion},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353743},
doi = {10.1145/3340555.3353743},
abstract = {In qualifying and analyzing the performance of group interaction, interaction processing analysis (IPA) defined by Bale is considered a useful approach. IPA is a system for labeling a total of 12 interaction categories for the interaction process. Automatic IPA can manually encompass the gap in spending manpower and can efficiently qualify group performance. In this paper, we present computational interaction processing analysis by developing a model to recognize categories of IPA. We extract both verbal features and nonverbal features for IPA category recognition modeling with SVM, RF, DNN and LSTM machine learning algorithms and analyze the contribution of multimodal features and unimodal features for the total data and each label. We also investigate the effect of context information by training sequences with different lengths with an LSTM and evaluating them. The results show that multimodal features achieve the best performance with an F1 score of 0.601 for the recognition of 12 IPA categories using the total data. Multimodal features are better than the unimodal features for the total data and most labels. The results of investigating context information show that a suitable length of sequence enables a longer sequence to achieve the best F1 score of 0.602 and a better performance for recognition.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {426–434},
numpages = {9},
keywords = {Multimodal, Machine Learning, Interaction Process Analysis, Group Analysis},
location = {Suzhou, China},
series = {ICMI '19}
}

@article{10.1016/j.compbiomed.2016.11.011,
author = {Araki, Tadashi and Jain, Pankaj K. and Suri, Harman S. and Londhe, Narendra D. and Ikeda, Nobutaka and El-Baz, Ayman and Shrivastava, Vimal K. and Saba, Luca and Nicolaides, Andrew and Shafique, Shoaib and Laird, John R. and Gupta, Ajay and Suri, Jasjit S.},
title = {Stroke Risk Stratification and its Validation using Ultrasonic Echolucent Carotid Wall Plaque Morphology},
year = {2017},
issue_date = {January 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {80},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.11.011},
doi = {10.1016/j.compbiomed.2016.11.011},
abstract = {Stroke risk stratification based on grayscale morphology of the ultrasound carotid wall has recently been shown to have a promise in classification of high risk versus low risk plaque or symptomatic versus asymptomatic plaques. In previous studies, this stratification has been mainly based on analysis of the far wall of the carotid artery. Due to the multifocal nature of atherosclerotic disease, the plaque growth is not restricted to the far wall alone. This paper presents a new approach for stroke risk assessment by integrating assessment of both the near and far walls of the carotid artery using grayscale morphology of the plaque. Further, this paper presents a scientific validation system for stroke risk assessment. Both these innovations have never been presented before.The methodology consists of an automated segmentation system of the near wall and far wall regions in grayscale carotid B-mode ultrasound scans. Sixteen grayscale texture features are computed, and fed into the machine learning system. The training system utilizes the lumen diameter to create ground truth labels for the stratification of stroke risk. The cross-validation procedure is adapted in order to obtain the machine learning testing classification accuracy through the use of three sets of partition protocols: (5, 10, and Jack Knife).The mean classification accuracy over all the sets of partition protocols for the automated system in the far and near walls is 95.08% and 93.47%, respectively. The corresponding accuracies for the manual system are 94.06% and 92.02%, respectively. The precision of merit of the automated machine learning system when compared against manual risk assessment system are 98.05% and 97.53% for the far and near walls, respectively. The ROC of the risk assessment system for the far and near walls is close to 1.0 demonstrating high accuracy.},
journal = {Comput. Biol. Med.},
month = jan,
pages = {77–96},
numpages = {20},
keywords = {Ultrasound, Stroke, Segmentation, ROC, Precision of merit, Near, Machine learning, Far, Carotid wall}
}

@inproceedings{10.1145/2623330.2623347,
author = {Zheng, Li and Zeng, Chunqiu and Li, Lei and Jiang, Yexi and Xue, Wei and Li, Jingxuan and Shen, Chao and Zhou, Wubai and Li, Hongtai and Tang, Liang and Li, Tao and Duan, Bing and Lei, Ming and Wang, Pengnian},
title = {Applying data mining techniques to address critical process optimization needs in advanced manufacturing},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623347},
doi = {10.1145/2623330.2623347},
abstract = {Advanced manufacturing such as aerospace, semi-conductor, and flat display device often involves complex production processes, and generates large volume of production data. In general, the production data comes from products with different levels of quality, assembly line with complex flows and equipments, and processing craft with massive controlling parameters. The scale and complexity of data is beyond the analytic power of traditional IT infrastructures. To achieve better manufacturing performance, it is imperative to explore the underlying dependencies of the production data and exploit analytic insights to improve the production process. However, few research and industrial efforts have been reported on providing manufacturers with integrated data analytical solutions to reveal potentials and optimize the production process from data-driven perspectives.In this paper, we design, implement and deploy an integrated solution, named PDP-Miner, which is a data analytics platform customized for process optimization in Plasma Display Panel (PDP) manufacturing. The system utilizes the latest advances in data mining technologies and Big Data infrastructures to create a complete analytical solution. Besides, our proposed system is capable of supporting automatically configuring and scheduling analysis tasks, and balancing heterogeneous computing resources. The system and the analytic strategies can be applied to other advanced manufacturing fields to enable complex data analysis tasks. Since 2013, PDP-Miner has been deployed as the data analysis platform of ChangHong COC. By taking the advantages of our system, the overall PDP yield rate has increased from 91% to 94%. The monthly production is boosted by 10,000 panels, which brings more than 117 million RMB of revenue improvement per year.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1739–1748},
numpages = {10},
keywords = {process optimization, data mining platform, big data, advanced manufacturing},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1016/j.jss.2019.06.100,
author = {Han, Xue and Carroll, Daniel and Yu, Tingting},
title = {Reproducing performance bug reports in server applications: The researchers’ experiences},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.100},
doi = {10.1016/j.jss.2019.06.100},
journal = {J. Syst. Softw.},
month = oct,
pages = {268–282},
numpages = {15},
keywords = {Experience report, Bug characteristics study, Performance bug reproduction}
}

@inproceedings{10.1007/978-3-319-91764-1_14,
author = {Pratama, Azkario Rizky and Widyawan and Lazovik, Alexander and Aiello, Marco},
title = {Power-Based Device Recognition for Occupancy Detection},
year = {2017},
isbn = {978-3-319-91763-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91764-1_14},
doi = {10.1007/978-3-319-91764-1_14},
abstract = {Each person using electrical devices leaves electricity fingerprints in the form of power consumption. These can be very useful for understanding the context of that person in, for instance, a smart office. A device that is highly correlated with the presence of a person in an office is the computer monitor; the correlation is in the range 83–96%. Therefore, it is useful to recognize from an aggregated power load the portion that is due to computer monitors. In this paper, we propose an event-based device recognition approach. After studying several predictors and features for device classification, we build a prototype for the classification. We evaluate the approach with actual power measurement of seven office monitors used by four workers in an office environment. Our experiments show that the approach is feasible and the per-day accuracy ranges in the range 69–80% for seven and five physical devices, respectively.},
booktitle = {Service-Oriented Computing – ICSOC 2017 Workshops: ASOCA, ISyCC, WESOACS, and Satellite Events, M\'{a}laga, Spain, November 13–16, 2017, Revised Selected Papers},
pages = {174–187},
numpages = {14},
keywords = {Device recognition, Load disaggregation, Occupancy detection, Appliance recognition},
location = {Malaga, Spain}
}

@inproceedings{10.1007/978-3-030-66843-3_28,
author = {Yousaf, Sobia and Anwar, Syed Muhammad and RaviPrakash, Harish and Bagci, Ulas},
title = {Brain Tumor Survival Prediction Using Radiomics Features},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_28},
doi = {10.1007/978-3-030-66843-3_28},
abstract = {Surgery planning in patients diagnosed with brain tumor is dependent on their survival prognosis. A poor prognosis might demand for a more aggressive treatment and therapy plan, while a favorable prognosis might enable a less risky surgery plan. Thus, accurate survival prognosis is an important step in treatment planning. Recently, deep learning approaches have been used extensively for brain tumor segmentation followed by the use of deep features for prognosis. However, radiomics-based studies have shown more promise using engineered/hand-crafted features. In this paper, we propose a three-step approach for multi-class survival prognosis. In the first stage, we extract image slices corresponding to tumor regions from multiple magnetic resonance image modalities. We then extract radiomic features from these 2D slices. Finally, we train machine learning classifiers to perform the classification. We evaluate our proposed approach on the publicly available BraTS 2019 data and achieve an accuracy of 76.5% and precision of 74.3% using the random forest classifier, which to the best of our knowledge are the highest reported results yet. Further, we identify the most important features that contribute in improving the prediction.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {284–293},
numpages = {10},
location = {Lima, Peru}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.patrec.2017.10.026,
author = {Yu, Xianguo and Yu, Qifeng and Shang, Yang and Zhang, Hongliang},
title = {Dense structural learning for infrared object tracking at 200+ Frames per Second},
year = {2017},
issue_date = {December 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2017.10.026},
doi = {10.1016/j.patrec.2017.10.026},
abstract = {Optimized algorithm for high speed tracking with dense structural learning.Effective feature representation for infrared object tracking.Superior performance on VOT-TIR2016. Infrared object tracking is a key technology in many surveillance applications. General visual tracking algorithms designed for color images can not handle infrared targets very well due to their relatively low resolutions and blurred edges. This paper presents a new tracking by detection method based on online structural learning. We show how to train the classifier efficiently with dense samples through Fourier techniques and careful implementation. Furthermore, we introduce an effective feature representation for infrared objects. Finally, we demonstrate the performance of the proposed tracker on public infrared sequences with top accuracy and robustness. Meanwhile, our single thread C++ implementation of the algorithm achieves an average tracking speed of 215 FPS on a modern cpu.},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {152–159},
numpages = {8},
keywords = {Structural learning, Infrared object tracking, High speed, Dense sampling, 65D17, 65D05, 41A10, 41A05}
}

@article{10.1016/j.eswa.2015.03.014,
author = {Shrivastava, Vimal K. and Londhe, Narendra D. and Sonawane, R.S. and Suri, Jasjit S.},
title = {Reliable and accurate psoriasis disease classification in dermatology images using comprehensive feature space in machine learning paradigm},
year = {2015},
issue_date = {September 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {15},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.03.014},
doi = {10.1016/j.eswa.2015.03.014},
abstract = {CADx system to classify psoriatic lesion and healthy skin in dermatology images.Integration of grayscale, color, redness and chaotic feature space.Accurate system design with classification accuracy of 99.81%.Reliable system design. Classification reliability and accuracy are important components for any computer-aided diagnostic system. This paper presents a dermatology CADx system to automatically classify dermatology images into psoriatic lesion and healthy skin using an online system. The novelty of the system is an exploration of the unique and comprehensive feature space combined with classification in support vector machine (SVM) paradigm. The unique feature space consists of grayscale space, color space and aggressiveness of psoriatic disease such as redness and chaoticness.The proposed CADx framework is conventional in paradigm that it has offline and online components. The offline system trains using unique integrated feature space and apriori dermatologist derived ground truth. This training system yields machine learning parameters. The online system is applied on the incoming test images which get transformed by an online classifier utilizing the offline machine learning parameters. The accuracy of the system is evaluated using cross-validation procedure depending upon one of the three partition protocols such as (5-fold, 10-fold and Jack Knife).The proposed CADx system shows the classification accuracy of 99.53%, 99.66% and 99.81% for 5-fold, 10-fold and Jack Knife protocols respectively for 15 optimal features. Further, our results show that, we can demonstrate the reliability and consistency factor by showing the monotonously rising accuracy with increase in data size. Our system is benchmarked against previous reported systems and outstands besides being unique and novel.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {6184–6195},
numpages = {12},
keywords = {Reliability, Psoriasis, Feature selection, Dermatology, Classification, Accuracy}
}

@inproceedings{10.1145/2910896.2910899,
author = {Bornand, Nicolas J. and Balakireva, Lyudmila and Van de Sompel, Herbert},
title = {Routing Memento Requests Using Binary Classifiers},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2910899},
doi = {10.1145/2910896.2910899},
abstract = {The Memento protocol provides a uniform approach to query individual web archives. Soon after its emergence, Memento Aggregator infrastructure was introduced that supports querying across multiple archives simultaneously. An Aggregator generates a response by issuing the respective Memento request against each of the distributed archives it covers. As the number of archives grows, it becomes increasingly challenging to deliver aggregate responses while keeping response times and computational costs under control. Ad-hoc heuristic approaches have been introduced to address this challenge and research has been conducted aimed at optimizing query routing based on archive profiles. In this paper, we explore the use of binary, archive-specific classifiers generated on the basis of the content cached by an Aggregator, to determine whether or not to query an archive for a given URI. Our results turn out to be readily applicable and can help to significantly decrease both the number of requests and the overall response times without compromising on recall. We find, among others, that classifiers can reduce the average number of requests by 77% compared to a brute force approach on all archives, and the overall response time by 42% while maintaining a recall of 0.847.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {63–72},
numpages = {10},
keywords = {web archiving, request routing, memento, machine learning},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@article{10.1016/j.ins.2021.05.035,
author = {Hammad, Mohamed and Kandala, Rajesh N.V.P.S. and Abdelatey, Amira and Abdar, Moloud and Zomorodi‐Moghadam, Mariam and Tan, Ru San and Acharya, U. Rajendra and P\l{}awiak, Joanna and Tadeusiewicz, Ryszard and Makarenkov, Vladimir and Sarrafzadegan, Nizal and Khosravi, Abbas and Nahavandi, Saeid and EL-Latif, Ahmed A. Abd and P\l{}awiak, Pawe\l{}},
title = {Automated detection of shockable ECG signals: A review},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {571},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.035},
doi = {10.1016/j.ins.2021.05.035},
journal = {Inf. Sci.},
month = sep,
pages = {580–604},
numpages = {25},
keywords = {Evolutionary computation, Optimization, Feature selection, Feature extraction, Ensemble learning, Deep learning, Machine learning, Signal processing, Computer-aided arrhythmia classification (CAAC), Arrhythmia, Electrocardiogram (ECG)}
}

@article{10.1016/j.neucom.2021.08.082,
author = {Yao, Zhaojian and Wang, Luping},
title = {Multi-pathway feature integration network for salient object detection},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {461},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.08.082},
doi = {10.1016/j.neucom.2021.08.082},
journal = {Neurocomput.},
month = oct,
pages = {462–478},
numpages = {17},
keywords = {Boundary information enhancement, Cross-feature hierarchical fusion, Multi-pathway feature fusion, Convolutional neural networks, Saliency detection}
}

@article{10.1016/j.compbiomed.2021.104520,
author = {Pezoulas, Vasileios C. and Grigoriadis, Grigoris I. and Gkois, George and Tachos, Nikolaos S. and Smole, Tim and Bosni\'{c}, Zoran and Pi\v{c}ulin, Matej and Olivotto, Iacopo and Barlocco, Fausto and Robnik-\v{S}ikonja, Marko and Jakovljevic, Djordje G. and Goules, Andreas and Tzioufas, Athanasios G. and Fotiadis, Dimitrios I.},
title = {A computational pipeline for data augmentation towards the improvement of disease classification and risk stratification models: A case study in two clinical domains},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104520},
doi = {10.1016/j.compbiomed.2021.104520},
journal = {Comput. Biol. Med.},
month = jul,
numpages = {12},
keywords = {HCM risk stratification, Lymphoma classification, Virtual population generation, Data augmentation, Artificial intelligence}
}

@article{10.1016/j.knosys.2013.02.008,
author = {Ekbal, Asif and Saha, Sriparna},
title = {Stacked ensemble coupled with feature selection for biomedical entity extraction},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.02.008},
doi = {10.1016/j.knosys.2013.02.008},
abstract = {Entity extraction is one of the most fundamental and important tasks in biomedical information extraction. In this paper we propose a two-stage algorithm for the extraction of biomedical entities in the forms of genes and gene product mentions in text. Several different approaches have emerged but most of these state-of-the-art approaches suggest that individual system may not cover entity representations with arbitrary set of features and cannot achieve best performance. We identify and implement a diverse set of features which are relevant for the identification of biomedical entities and classification of them into some predefined categories. One most important criterion of these features is that these are identified and selected largely without using any domain knowledge. In the first stage we use a genetic algorithm (GA) based feature selection technique to determine the most relevant set of features for Support Vector Machine (SVM) and Conditional Random Field (CRF) classifiers. The GA based feature selection algorithm produces best population that can be used to generate different classification models based on CRF and SVM. In the second stage we develop a stacked based ensemble to combine the classifiers selected in the first stage. The proposed approach is evaluated on two benchmark datasets, namely JNLPBA 2004 shared task and GENETAG. The proposed approach yields the overall F-measure values of 75.17% and 94.70% for JNLPBA 2004 and GENETAG data sets, respectively.},
journal = {Know.-Based Syst.},
month = jul,
pages = {22–32},
numpages = {11},
keywords = {Support Vector Machine (SVM), Stack based Ensemble, GA based feature selection, Conditional Random Field (CRF), Biomedical entity extraction}
}

@inproceedings{10.1109/FOSE.2007.32,
author = {Woodside, Murray and Franks, Greg and Petriu, Dorina C.},
title = {The Future of Software Performance Engineering},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.32},
doi = {10.1109/FOSE.2007.32},
abstract = {Performance is a pervasive quality of software systems; everything affects it, from the software itself to all underlying layers, such as operating system, middleware, hardware, communication networks, etc. Software Performance Engineering encompasses efforts to describe and improve performance, with two distinct approaches: an early-cycle predictive modelbased approach, and a late-cycle measurement-based approach. Current progress and future trends within these two approaches are described, with a tendency (and a need) for them to converge, in order to cover the entire development cycle.},
booktitle = {2007 Future of Software Engineering},
pages = {171–187},
numpages = {17},
series = {FOSE '07}
}

@article{10.1007/s13748-021-00250-6,
author = {Mohanty, Monalisa and Dash, Manasa and Biswal, Pradyut and Sabut, Sukanta},
title = {Classification of ventricular arrhythmias using empirical mode decomposition and machine learning algorithms},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {4},
url = {https://doi.org/10.1007/s13748-021-00250-6},
doi = {10.1007/s13748-021-00250-6},
abstract = {Ventricular arrhythmias such as ventricular tachycardia (VT) and ventricular fibrillation (VF) are the main life-threatening arrhythmias which have to be detected accurately by designing automated system. In this work, we propose a novel method based on ensemble empirical mode decomposition to decompose the ECG signal and classified with decision tree classifier and support vector machine (SVM) for discriminating the VT/VF conditions using informative ranked features. Total fifty-seven records of ECG signals from Creighton University Ventricular Tachyarrhythmia Database (CUDB) and the MIT-BIH Malignant Ventricular Arrhythmia Database (VFDB) database of PhysioNet were taken for evaluation. We obtained the sensitivity of 97.74%, specificity of 99% and accuracy of 98.69% in C4.5 classifier, whereas the accuracy of 90.52% was achieved with SVM classifier. These results indicate that the C4.5 algorithm is a superior approach for identification of cardiac arrhythmia class. The proposed system is an effective method that may be used to assist in decision support system in clinical practice for accurate recognition of ventricular arrhythmias.},
journal = {Prog. in Artif. Intell.},
month = dec,
pages = {489–504},
numpages = {16},
keywords = {Decision tree, EEMD, ECG signals, Ventricular tachycardia, Ventricular arrhythmias, Ventricular arrhythmia}
}

@article{10.1016/j.cie.2014.09.026,
author = {Claypool, Erin and Norman, Bryan A. and Needy, Kim LaScola},
title = {Modeling risk in a Design for Supply Chain problem},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.09.026},
doi = {10.1016/j.cie.2014.09.026},
abstract = {Developed a Design for Supply Chain (DFSC) and risk model.Model analyzes product design, supply chain design and risk concurrently.New Product Development risks in model: time-to-market, demand risk.Supply chain risks: supplier reliability, strategic exposure, supplier capacity.Analysis shows importance of concurrently considering risk. The objective of Design for Supply Chain (DFSC) is to design a supply chain in parallel to designing a new product. Risk is an inherent element of this process. Although supply chain risk models and product development risk models are available, there are few models that consider the combined effect of risk to product development and the supply chain. This gap is filled by the development of a DFSC and risk model that looks at design, supply chain and risk concurrently. The model consists of two components. First, a Mixed Integer Programming (MIP) model makes the DFSC decisions while simultaneously considering time-to-market risk, supplier reliability risk and strategic exposure risk. The results from the MIP are then used in the second model component which is a discrete event simulation. The simulation tests the robustness of the MIP solution for supplier capacity risk and demand risk. When a decision maker is potentially facing either of these risks the simulation shows whether it is best to use an alternative solution or proceed with the MIP solution. The model provides analytical results, but also allows decision makers to use their own judgment to select the best option for overall profitability. In conclusion, testing shows that risk mitigation strategies can and should be determined from the DFSC and risk model, but that they will be dependent on the specific design problem being solved.},
journal = {Comput. Ind. Eng.},
month = dec,
pages = {44–54},
numpages = {11},
keywords = {Supplier Selection, Simulation, Risk, New Product Development, MIP, Design for Supply Chain}
}

@article{10.1007/s10664-021-10007-3,
author = {Revoredo, Kate and Djurica, Djordje and Mendling, Jan},
title = {A study into the practice of reporting software engineering experiments},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10007-3},
doi = {10.1007/s10664-021-10007-3},
abstract = {It has been argued that reporting software engineering experiments in a standardized way helps researchers find relevant information, understand how experiments were conducted and assess the validity of their results. Various guidelines have been proposed specifically for software engineering experiments. The benefits of such guidelines have often been emphasized, but the actual uptake and practice of reporting have not yet been investigated since the introduction of many of the more recent guidelines. In this research, we utilize a mixed-method study design including sequence analysis techniques for evaluating to which extent papers follow such guidelines. Our study focuses on the four most prominent software engineering journals and the time period from 2000 to 2020. Our results show that many experimental papers miss information suggested by guidelines, that no de facto standard sequence for reporting exists, and that many papers do not cite any guidelines. We discuss these findings and implications for the discipline of experimental software engineering focusing on the review process and the potential to refine and extend guidelines, among others, to account for theory explicitly.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {50},
keywords = {Method mining, Process mining, Controlled experiments, Guideline for software engineering experiments}
}

@article{10.1016/j.eswa.2021.115774,
author = {Ilhan, Hamza Osman and Yuzkat, Mecit and Aydin, Nizamettin},
title = {Sperm Motility Analysis by using Recursive Kalman Filters with the smartphone based data acquisition and reporting approach},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115774},
doi = {10.1016/j.eswa.2021.115774},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {12},
keywords = {Sperm Motility Analysis, Trajectory clustering, Video stabilization, Recursive Kalman Filter tracking, Videomicroscopy analysis, Biomedical video processing}
}

@inproceedings{10.1145/337180.337232,
author = {Leszak, Marek and Perry, Dewayne E. and Stoll, Dieter},
title = {A case study in root cause defect analysis},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337232},
doi = {10.1145/337180.337232},
abstract = {There are three interdependent factors that drive our software development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction.We report a retrospective root cause defect analysis study of the defect Modification Requests (MRs) discovered while building, testing, and deploying a release of a transmission network element product. We subsequently introduced this analysis methodology into new development projects as an in-process measurement collection requirement for each major defect MR.We present the experimental design of our case study discussing the novel approach we have taken to defect and root cause classification and the mechanisms we have used for randomly selecting the MRs to analyze and collecting the analyses via a web interface. We then present the results of our analyses of the MRs and describe the defects and root causes that we found, and delineate the countermeasures created to either prevent those defects and their root causes or detect them at the earliest possible point in the development process.We conclude with lessons learned from the case study and resulting ongoing improvement activities.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {428–437},
numpages = {10},
keywords = {root cause analyis, quality assurance, process improvement, modification management, defect prevention},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@article{10.1016/j.future.2015.06.013,
author = {Budgaga, Walid and Malensek, Matthew and Pallickara, Sangmi and Harvey, Neil and Breidt, F. Jay and Pallickara, Shrideep},
title = {Predictive analytics using statistical, learning, and ensemble methods to support real-time exploration of discrete event simulations},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.06.013},
doi = {10.1016/j.future.2015.06.013},
abstract = {Discrete event simulations (DES) provide a powerful means for modeling complex systems and analyzing their behavior. DES capture all possible interactions between the entities they manage, which makes them highly expressive but also compute-intensive. These computational requirements often impose limitations on the breadth and/or depth of research that can be conducted with a discrete event simulation.This work describes our approach for leveraging the vast quantity of computing and storage resources available in both private organizations and public clouds to enable real-time exploration of discrete event simulations. Rather than directly targeting simulation execution speeds, we autonomously generate and execute novel scenario variants to explore a representative subset of the simulation parameter space. The corresponding outputs from this process are analyzed and used by our framework to produce models that accurately forecast simulation outcomes in real time, providing interactive feedback and facilitating exploratory research.Our framework distributes the workloads associated with generating and executing scenario variants across a range of commodity hardware, including public and private cloud resources. Once the models have been created, we evaluate their performance and improve prediction accuracy by employing dimensionality reduction techniques and ensemble methods. To make these models highly accessible, we provide a user-friendly interface that allows modelers and epidemiologists to modify simulation parameters and see projected outcomes in real time. Our approach enables fast, accurate forecasts of discrete event simulations.The framework copes with high dimensionality and voluminous datasets.We facilitate simulation execution with cycle scavenging and cloud resources.We create and evaluate several predictive models, including ensemble methods.Our framework is made accessible to end users through an interactive web interface.},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {360–374},
numpages = {15},
keywords = {Latin Hypercube Sampling, Distributed execution, Discrete event simulation, Cloud infrastructure}
}

@article{10.1016/j.cmpb.2016.08.021,
author = {Abdul-Kadir, Nurul Ashikin and Mat Safri, Norlaili and Othman, Mohd Afzan},
title = {Dynamic ECG features for atrial fibrillation recognition},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.08.021},
doi = {10.1016/j.cmpb.2016.08.021},
abstract = {The characterization of atrial fibrillation using second order dynamic system.The appropriate windowing length for ECG signal processing during normal sinus rhythm and atrial fibrillation.The AF recognition used the pattern recognition machine learning methods (the ANN and SVM) with k-fold cross validation.The study proposed a method based on dynamic system, which achieved high accuracy of 95%.Our major results provide novel method in detection and classification of atrial fibrillation. BackgroundAtrial fibrillation (AF) can cause the formation of blood clots in the heart. The clots may move to the brain and cause a stroke. Therefore, this study analyzed the ECG features of AF and normal sinus rhythm signals for AF recognition which were extracted by using a second-order dynamic system (SODS) concept. ObjectiveTo find the appropriate windowing length for feature extraction based on SODS and to determine a machine learning method that could provide higher accuracy in recognizing AF. MethodECG features were extracted based on a dynamic system (DS) that uses a second-order differential equation to describe the short-term behavior of ECG signals according to the natural frequency (ω), damping coefficient, (\'{z}), and forcing input (u). The extracted features were windowed into 2, 3, 4, 6, 8, and 10 second episodes to find the appropriate windowing size for AF signal processing. ANOVA and t-tests were used to determine the significant features. In addition, pattern recognition machine learning methods (an artificial neural network (ANN) and a support vector machine (SVM)) with k-fold cross validation (k-CV) were used to develop the ECG recognition system. ResultsSignificant differences (p\'{z}&lt;\'{z}0.0001) were observed among all ECG groups (NSR, N, AF) using 2, 3, 4 and 6 second episodes for the features ω and u/ω; 4, 6 and 8 second episodes for features ω and u; 4 and 6 second episodes for features ω, u and u/ω, and; 10 second episodes for the feature \'{z}. The highest accuracy for AF recognition (AF, NSR) using ANN with k-CV was 95.3% using combination of features (ω and u; ω, u and u/ω) and SVM with k-CV was 95.0% using a combination of features ω, u and u/ω. ConclusionThis study found that 4\'{z}s is the most appropriate windowing length, using two features (ω and u) for AF detection with an accuracy of 95.3%. Moreover, the pattern recognition learning machine uses an ANN with 10-fold cross validation based on DS.},
journal = {Comput. Methods Prog. Biomed.},
month = nov,
pages = {143–150},
numpages = {8},
keywords = {k-fold cross validation, Support vector machine, Pattern recognition, Dynamic system, Atrial fibrillation, Artificial neural network}
}

@inproceedings{10.5555/3540261.3540604,
author = {Zhao, Yue and Rossi, Ryan A. and Akoglu, Leman},
title = {Automatic unsupervised outlier model selection},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given an unsupervised outlier detection task on a new dataset, how can we automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model)? In this work, we tackle the unsupervised outlier model selection (UOMS) problem, and propose METAOD, a principled, data-driven approach to UOMS based on meta-learning. The UOMS problem is notoriously challenging, as compared to model selection for classification and clustering, since (i) model evaluation is infeasible due to the lack of hold-out data with labels, and (ii) model comparison is infeasible due to the lack of a universal objective function. METAOD capitalizes on the performances of a large body of detection models on historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity within our meta-learning framework, we introduce specialized meta-features that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta-learning techniques that we tailored for UOMS. Moreover upon (meta-)training, METAOD is extremely efficient at test time; selecting from a large pool of 300+ models takes less than 1 second for a new task. We open-source1 METAOD and our meta-learning database for practical use and to foster further research on the UOMS problem.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {343},
numpages = {14},
series = {NIPS '21}
}

@article{10.1145/3447556.3447567,
author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
title = {Techniques for Automated Machine Learning},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3447556.3447567},
doi = {10.1145/3447556.3447567},
abstract = {Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a problem description, its task type, and datasets. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we portray AutoML as a bi-level optimization problem, where one problem is nested within another to search the optimum in the search space, and review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter tuning (AutoMHT), and automated deep learning (AutoDL). Stateof- the-art techniques in the three categories are presented. The iterative solver is proposed to generalize AutoML techniques. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {35–50},
numpages = {16}
}

@inproceedings{10.1007/978-3-030-68799-1_19,
author = {Khan, Hanif and Asghar, Muhammad Usama and Asghar, Muhammad Zubair and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy},
title = {Fake Review Classification Using Supervised Machine Learning},
year = {2021},
isbn = {978-3-030-68798-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68799-1_19},
doi = {10.1007/978-3-030-68799-1_19},
abstract = {The revolution of social media has propelled the online community to take advantage of online reviews for not only posting feedback about the products, services, and other issues but also assists individuals to analyze user’s feedback for making purchase decisions, and companies for improving the quality of manufactured goods. However, the propagation of fake reviews has become an alarming issue, as it deceives online users while purchasing and promotes or demotes the reputation of competing brands. In this work, we propose a supervised learning-based technique for the detection of fake reviews from the online textual content. The study employs machine learning classifiers for bifurcating fake and genuine reviews. Experimental results are evaluated against different evaluation measures and the performance of the proposed system is compared with baseline works.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part IV},
pages = {269–288},
numpages = {20},
keywords = {Machine learning, Fake reviews, Social networks}
}

@article{10.1016/j.neucom.2021.05.032,
author = {Lin, Zhongqi and Jia, Jingdun and Huang, Feng and Gao, Wanlin},
title = {A coarse-to-fine capsule network for fine-grained image categorization},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {456},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.032},
doi = {10.1016/j.neucom.2021.05.032},
journal = {Neurocomput.},
month = oct,
pages = {200–219},
numpages = {20},
keywords = {Increasingly specialized perception, Coarse-to-fine attention, Fine-grained image classification, Capsule network (CapsNet)}
}

@inproceedings{10.1145/2744769.2744911,
author = {Sarma, Santanu and Muck, T. and Bathen, Luis A. D. and Dutt, N. and Nicolau, A.},
title = {SmartBalance: a sensing-driven linux load balancer for energy efficiency of heterogeneous MPSoCs},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744911},
doi = {10.1145/2744769.2744911},
abstract = {Due to increased demand for higher performance and better energy efficiency, MPSoCs are deploying heterogeneous architectures with architecturally differentiated core types. However, the traditional Linux-based operating system is unable to exploit this heterogeneity since existing kernel load balancing and scheduling approaches lack support for aggressively heterogeneous architectural configurations (e.g. beyond two core types). In this paper we present SmartBalance: a sensing-driven closed-loop load balancer for aggressively heterogeneous MPSoCs that performs load balancing using a sense-predict-balance paradigm. SmartBalance can efficiently manage the chip resources while opportunistically exploiting the workload variations and performance-power trade-offs of different core types. When compared to the standard vanilla Linux kernel load balancer, our per-thread and per-core performance-power-aware scheme shows an improvement in energy efficiency (throughput/Watt) of over 50% for benchmarks from the PARSEC benchmark suite executing on a heterogeneous MPSoC with 4 different core types and over 20% w.r.t. state-of-the-art ARM's global task scheduling (GTS) scheme for octa-core big.Little architecture.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {109},
numpages = {6},
keywords = {task allocation &amp; scheduling, system-on-chip (SoC), power-aware systems, operating system, load balancing, linux scheduler, heterogeneous multiprocessors (HMP), energy efficiency, embedded software, OS kernel, MPSoC},
location = {San Francisco, California},
series = {DAC '15}
}

@article{10.1145/2414416.2414424,
author = {Sch\"{a}fer, Christian},
title = {Particle Algorithms for Optimization on Binary Spaces},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/2414416.2414424},
doi = {10.1145/2414416.2414424},
abstract = {We discuss a unified approach to stochastic optimization of pseudo-Boolean objective functions based on particle methods, including the cross-entropy method and simulated annealing as special cases. We point out the need for auxiliary sampling distributions, meaning parametric families on binary spaces, which are able to reproduce complex dependency structures, and illustrate their usefulness in our numerical experiments. We provide numerical evidence that particle-driven optimization algorithms based on parametric families yield superior results on strongly multimodal optimization problems while local search heuristics outperform them on easier problems.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jan,
articleno = {8},
numpages = {25},
keywords = {simulated annealing, sequential Monte Carlo, pseudo-Boolean optimization, cross-entropy method, Binary parametric families}
}

@article{10.1145/268403.268413,
author = {Devroye, Luc},
title = {Random variate generation for multivariate unimodal densities},
year = {1997},
issue_date = {Oct. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/268403.268413},
doi = {10.1145/268403.268413},
abstract = {A probability density on a finite-dimensional Euclidean space is orthounimodal with a given mode if within each orthant (quadrant) defined by the mode, the density is a monotone function of each of its arguments individually. Up to a linear transformation, most of the commonly used random vectors possess orthounimodal densities. To generate a random vector from a given orthounimodal density, several general-purpose algorithms are presented; and an experimental performance evaluation illustrates the potential efficiency increases that can be achieved by these algorithms versus naive rejection.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
pages = {447–477},
numpages = {31},
keywords = {unimodality, random variate generation, nonparametric classes, multivariate densities}
}

@article{10.1007/s10270-013-0394-9,
author = {Szvetits, Michael and Zdun, Uwe},
title = {Systematic literature review of the objectives, techniques, kinds, and architectures of models at runtime},
year = {2016},
issue_date = {February  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0394-9},
doi = {10.1007/s10270-013-0394-9},
abstract = {In the context of software development, models provide an abstract representation of a software system or a part of it. In the software development process, they are primarily used for documentation and communication purposes in analysis, design, and implementation activities. Model-Driven Engineering (MDE) further increases the importance of models, as in MDE models are not only used for documentation and communication, but as central artefacts of the software development process. Various recent research approaches take the idea of using models as central artefacts one step further by using models at runtime to cope with dynamic aspects of ever-changing software and its environment. In this article, we analyze the usage of models at runtime in the existing research literature using the Systematic Literature Review (SLR) research method. The main goals of our SLR are building a common classification and surveying the existing approaches in terms of objectives, techniques, architectures, and kinds of models used in these approaches. The contribution of this article is to provide an overview and classification of current research approaches using models at runtime and to identify research areas not covered by models at runtime so far.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {31–69},
numpages = {39},
keywords = {Runtime, Models, Literature review}
}

@article{10.1016/j.imavis.2016.04.003,
author = {Dai, Shuanglu and Man, Hong},
title = {Statistical adaptive metric learning in visual action feature set recognition},
year = {2016},
issue_date = {November 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {P2},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.04.003},
doi = {10.1016/j.imavis.2016.04.003},
abstract = {Great variances in visual features often present significant challenges in human action recognitions. To address this common problem, this paper proposes a statistical adaptive metric learning (SAML) method by exploring various selections and combinations of multiple statistics in a unified metric learning framework. Most statistics have certain advantages in specific controlled environments, and systematic selections and combinations can adapt them to more realistic "in the wild" scenarios. In the proposed method, multiple statistics, include means, covariance matrices and Gaussian distributions, are explicitly mapped or generated in the Riemannian manifolds. Typically, d-dimensional mean vectors in Rd are mapped to a Rd d space of symmetric positive definite (SPD) matrices S y m d + . Subsequently, by embedding the heterogeneous manifolds in their tangent Hilbert space, subspace combination with minimal deviation is selected from multiple statistics. Then Mahalanobis metrics are introduced to map them back into the Euclidean space. Unified optimizations are finally performed based on the Euclidean distances. In the proposed method, subspaces with smaller deviations are selected before metric learning. Therefore, by exploring different metric combinations, the final learning is more representative and effective than exhaustively learning from all the hybrid metrics. Experimental evaluations are conducted on human action recognitions in both static and dynamic scenarios. Promising results demonstrate that the proposed method performs effectively for human action recognitions in the wild. Display Omitted A statistical adaptive metric learning (SAML) is proposed to classify action features.SAML explores multiple statistic combinations for feature sets in different scales.Discriminative statistic subspace is learned by a unified metric learning framework.High competitive performances are achieved by SAML on five benchmark databases.},
journal = {Image Vision Comput.},
month = nov,
pages = {138–148},
numpages = {11},
keywords = {Metric learning, Manifold selection, Hybrid statistic modeling, Feature set classification}
}

@article{10.1007/s11045-019-00651-w,
author = {Mohanty, Monalisa and Biswal, Pradyut and Sabut, Sukanta},
title = {Machine learning approach to recognize ventricular arrhythmias using VMD based features},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0923-6082},
url = {https://doi.org/10.1007/s11045-019-00651-w},
doi = {10.1007/s11045-019-00651-w},
abstract = {The occurrence of life-threatening ventricular arrhythmias (VAs) such as Ventricular tachycardia (VT) and Ventricular fibrillation (VF) leads to sudden cardiac death which requires detection at an early stage. The main aim of this work is to develop an automated system using machine learning tool for accurate prediction of VAs that may reduce the mortality rate. In this paper, a novel method using variational mode decomposition (VMD) based features and C4.5 classifier for detection of ventricular arrhythmias is presented. The VMD model was used to decompose the electrocardiography (ECG) signals to extract useful informative features. The method was tested for ECG signals obtained from PhysioNet database. Two standard databases i.e. CUDB (Creighton University Ventricular Tachyarrhythmia Database) and VFDB (MIT-BIH Malignant Ventricular Ectopy Database) were considered for this work. A set of time–frequency features were extracted and ranked by the gain ratio attribute evaluation method. The ranked features are subjected to support vector machine (SVM) and C4.5 classifier for classification of normal, VT and VF classes. The best detection was obtained with sensitivity of 97.97%, specificity of 99.15%, and accuracy of 99.18% for C4.5 classifier with a 5&nbsp;s data analysis window. These results were better than SVM classifier result having an average accuracy of 86.87%. Hence, the proposed method demonstrates the efficiency in detecting the life-threatening VAs and can serve as an assistive tool to clinicians in the diagnosis process.},
journal = {Multidimensional Syst. Signal Process.},
month = jan,
pages = {49–71},
numpages = {23},
keywords = {Classification, Machine learning, Features, VMD, ECG, Ventricular arrhythmias}
}

@article{10.1007/s11276-021-02742-8,
author = {Zhang, Runzhou and Ning, Lei and Li, Mengkun and Wang, Chengcai and Li, Wei and Wang, Yongjian},
title = {Feature extraction of trajectories for mobility modeling in 5G NB-IoT networks},
year = {2021},
issue_date = {Jul 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02742-8},
doi = {10.1007/s11276-021-02742-8},
abstract = {Industry applications that support massive mobile nodes are an important part of 5G Narrow Band Internet of Things (NB-IoT). The mobile node is abstracted as a terminal scene with many shapes and complex behaviors in the cellular internet of things, which makes the network need a better radio resource management strategy to meet more mobility, larger connection requests and higher access rate services. Therefore, the feature extraction of the node trajectories will greatly facilitate the development of optimal algorithms for radio resource management in the large mobile scene of 5G NB-IoT networks. This paper presents a feature mining of the real trajectories from the urban operating vehicles in the city of Shenzhen, China. Meanwhile, the generated trajectories by four common mobility models are also handled as a comparison. The self-similarity, hot-spots, long-tails and travel time are evaluated due to the widely recognized four features of human traces. Mining results show that the vehicles to serve the daily trip of human in the city always take a short travel and activate in several hot-spots with randomly roaming, while the vehicles to serve the goods are showing the opposite characteristics. Moreover, the trajectory generated by the models does not have the characteristics of short travel and roaming in multiple hot spots, nor does it have different movement trends in various time periods.},
journal = {Wirel. Netw.},
month = aug,
pages = {3781–3793},
numpages = {13},
keywords = {Trajectory mining, Vehicle mobility, Mobility model, 5G NB-IoT networks}
}

@article{10.1145/1823838.1823840,
author = {Thomasian, Alexander},
title = {Storage research in industry and universities},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/1823838.1823840},
doi = {10.1145/1823838.1823840},
abstract = {We review activities at universities and industrial research centers in the storage area, but also briefly mention topics such as processor design, operating systems, databases, and performance analysis. Our starting point is the Berkeley RAID proposal and the associated taxonomy two decades ago. Important research groups are listed and key researchers are identified. We pay special attention to faculty/student relationships, listing PhD theses and articles related to storage. We also describe innovative storage products and the companies behind them. This paper complements author's "Publications in Storage and Systems", ACM CAN, Sept. 2009.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {1–48},
numpages = {48}
}

@article{10.5555/3215716.3215720,
author = {Gallien, J\'{e}r\'{e}mie and Mersereau, Adam J. and Garro, Andres and Mora, Alberte Dapena and Vidal, Mart\'{\i}n N\'{o}voa},
title = {Initial Shipment Decisions for New Products at Zara},
year = {2015},
issue_date = {April 2015},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {63},
number = {2},
issn = {0030-364X},
abstract = {Given uncertain popularity of new products by location, fast fashion retailer Zara faces a trade-off. Large initial shipments to stores reduce lost sales in the critical first days of the product life cycle, but maintaining stock at the warehouse allows restocking flexibility once initial sales are observed. In collaboration with Zara, we develop and test a decision support system featuring a data-driven model of forecast updating and a dynamic optimization formulation for allocating limited stock by location over time. A controlled field experiment run worldwide with 34 articles during the 2012 season showed an increase in total average season sales by approximately 2% and a reduction in the number of unsold units at the end of the regular selling season by approximately 4%.},
journal = {Oper. Res.},
month = apr,
pages = {269–286},
numpages = {18},
keywords = {retailing, inventory control, field experiment, demand learning, apparel industry}
}

@inproceedings{10.1007/978-3-642-11301-7_50,
author = {Shen, Jialie and Pang, HweeHwa and Tao, Dacheng and Li, Xuelong},
title = {Dual phase learning for large scale video gait recognition},
year = {2010},
isbn = {3642113001},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-11301-7_50},
doi = {10.1007/978-3-642-11301-7_50},
abstract = {Accurate gait recognition from video is a complex process involving heterogenous features, and is still being developed actively. This article introduces a novel framework, called GC2F, for effective and efficient gait recognition and classification. Adopting a ”refinement-and-classification” principle, the framework comprises two components: 1) a classifier to generate advanced probabilistic features from low level gait parameters; and 2) a hidden classifier layer (based on multilayer perceptron neural network) to model the statistical properties of different subject classes. To validate our framework, we have conducted comprehensive experiments with a large test collection, and observed significant improvements in identification accuracy relative to other state-of-the-art approaches.},
booktitle = {Proceedings of the 16th International Conference on Advances in Multimedia Modeling},
pages = {500–510},
numpages = {11},
location = {Chongqing, China},
series = {MMM'10}
}

@article{10.5555/3215696.3215700,
author = {Gallien, J\'{e}r\'{e}mie and Mersereau, Adam J. and Garro, Andres and Mora, Alberte Dapena and Vidal, Mart\'{\i}n N\'{o}voa},
title = {Initial Shipment Decisions for New Products at Zara},
year = {2015},
issue_date = {April 2015},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {63},
number = {2},
issn = {0030-364X},
abstract = {Given uncertain popularity of new products by location, fast fashion retailer Zara faces a trade-off. Large initial shipments to stores reduce lost sales in the critical first days of the product life cycle, but maintaining stock at the warehouse allows restocking flexibility once initial sales are observed. In collaboration with Zara, we develop and test a decision support system featuring a data-driven model of forecast updating and a dynamic optimization formulation for allocating limited stock by location over time. A controlled field experiment run worldwide with 34 articles during the 2012 season showed an increase in total average season sales by approximately 2% and a reduction in the number of unsold units at the end of the regular selling season by approximately 4%.},
journal = {Oper. Res.},
month = apr,
pages = {269–286},
numpages = {18},
keywords = {retailing, inventory control, field experiment, demand learning, apparel industry}
}

@inproceedings{10.1145/3375462.3375479,
author = {Tuti, Timothy and Paton, Chris and Winters, Niall},
title = {Learning to represent healthcare providers knowledge of neonatal emergency care: findings from a smartphone-based learning intervention targeting clinicians from LMICs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375479},
doi = {10.1145/3375462.3375479},
abstract = {Modelling healthcare providers' knowledge while they are gaining new concepts is an important step towards supporting self-regulated personalised learning at scale. This is especially important if we are to address health workforce skills development and enhance the subsequent quality of care patients receive in the Global South, where a huge skills gap exists. Rich data about healthcare providers' learning can be captured by their responses to close-ended problems within conjunctive solution space -such as clinical training scenarios for emergency care delivery- on smartphone-based learning interventions which are being proposed as a solution for reducing the healthcare skills gap in this context. Together with sequential data detailing a learner's progress while they are solving a learning task, this provides useful insights into their learning behaviour. Predicting learning or forgetting curves from representations of healthcare providers knowledge is a difficult task, but recent promising machine learning advances have produced techniques capable of learning knowledge representations and overcoming this challenge. In this study, we train a Long Short-Term Memory neural network for predicting learners' future performance and forgetting curves by feeding it sequence embeddings of learning task attempts from healthcare providers from Global South. From this training, the model captures nuanced representations of a healthcare provider's clinical knowledge and their patterns of learning behaviours, predicting their future performance with high accuracy. More significantly, by differentiating reduced performance based on spaced learning, the model can help provide timely warning that helps support healthcare providers to reinforce their self-regulated learning while providing a basis for personalised instructional support to aid improved clinical outcomes from their professional practices.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {320–329},
numpages = {10},
keywords = {smartphones, neonatal care, global health, forgetting curves, emergency care, deep knowledge tracing, clinical training},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1007/978-3-030-28577-7_26,
author = {Kelly, Liadh and Suominen, Hanna and Goeuriot, Lorraine and Neves, Mariana and Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene and Zuccon, Guido and Scells, Harrisen and Palotti, Jo\~{a}o},
title = {Overview of the CLEF eHealth Evaluation Lab 2019},
year = {2019},
isbn = {978-3-030-28576-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-28577-7_26},
doi = {10.1007/978-3-030-28577-7_26},
abstract = {In this paper, we provide an overview of the seventh annual edition of the CLEF eHealth evaluation lab. CLEF eHealth 2019 continues our evaluation resource building efforts around the easing and support of patients, their next-of-kins, clinical staff, and health scientists in understanding, accessing, and authoring electronic health information in a multilingual setting. This year’s lab advertised three tasks: Task 1 on indexing non-technical summaries of German animal experiments with International Classification of Diseases, Version 10 codes; Task 2 on technology assisted reviews in empirical medicine building on 2017 and 2018 tasks in English; and Task 3 on consumer health search in mono- and multilingual settings that builds on the 2013–18 Information Retrieval tasks. In total nine teams took part in these tasks (six in Task 1 and three in Task 2). Herein, we describe the resources created for these tasks and evaluation methodology adopted. We also provide a brief summary of participants of this year’s challenges and results obtained. As in previous years, the organizers have made data and tools associated with the lab tasks available for future research and&nbsp;development.},
booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction: 10th International Conference of the CLEF Association, CLEF 2019, Lugano, Switzerland, September 9–12, 2019, Proceedings},
pages = {322–339},
numpages = {18},
keywords = {Text segmentation, Text classification, Test-set generation, Systematic reviews, Self-diagnosis, Medical informatics, Information extraction, High recall, Health records, Information retrieval, Entity linking, Evaluation},
location = {Lugano, Switzerland}
}

@article{10.5555/3215661.3215663,
author = {Gallien, J\'{e}r\'{e}mie and Mersereau, Adam J. and Garro, Andres and Mora, Alberte Dapena and Vidal, Mart\'{\i}n N\'{o}voa},
title = {Initial Shipment Decisions for New Products at Zara},
year = {2015},
issue_date = {April 2015},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {62},
number = {2},
issn = {0030-364X},
abstract = {Given uncertain popularity of new products by location, fast fashion retailer Zara faces a trade-off. Large initial shipments to stores reduce lost sales in the critical first days of the product life cycle, but maintaining stock at the warehouse allows restocking flexibility once initial sales are observed. In collaboration with Zara, we develop and test a decision support system featuring a data-driven model of forecast updating and a dynamic optimization formulation for allocating limited stock by location over time. A controlled field experiment run worldwide with 34 articles during the 2012 season showed an increase in total average season sales by approximately 2% and a reduction in the number of unsold units at the end of the regular selling season by approximately 4%.},
journal = {Oper. Res.},
month = apr,
pages = {269–286},
numpages = {18},
keywords = {retailing, inventory control, field experiment, demand learning, apparel industry}
}

@inproceedings{10.1145/2110363.2110467,
author = {Zhang, Rui and Pakhomov, Serguei and Melton, Genevieve B.},
title = {Automated identification of relevant new information in clinical narrative},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110467},
doi = {10.1145/2110363.2110467},
abstract = {The ability to explore and visualize clinical information is important for clinicians when reviewing and cognitively synthesizing electronic clinical documents for new patients contained in electronic health record (EHR) systems. In this study, we explore the use of language models for detecting new and potentially relevant information within an individual patient's collection of clinical documents using an expert-based reference standard for evaluation. We achieved good accuracy with a heterogeneous system based on a modified n-gram language model with statistically-derived and classic stop word removal and lexical normalization, as well as heuristic rules. This technique also identified relevant new information not identified with the expert-derived reference standard. These methods appear promising for providing an automated means to improve the use of electronic documents by clinicians.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {837–842},
numpages = {6},
keywords = {natural language processing, n-gram model, information retrieval, information redundancy, electronic health record},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@article{10.1145/3191744,
author = {Huang, Chenyu and Chen, Huangxun and Yang, Lin and Zhang, Qian},
title = {BreathLive: Liveness Detection for Heart Sound Authentication with Deep Breathing},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191744},
doi = {10.1145/3191744},
abstract = {Nowadays, considerable number of devices have been proposed to monitor cardiovascular health. To protect medical data on these devices from unauthorized access, researchers have proposed ECG-based and heart sound-based authentication methods. However, their vulnerabilities to replay attacks have recently been revealed. In this paper, we leverage liveness detection to enhance heart sound-based authentication against replay attacks. We utilize the inherent correlation between sounds and chest motion caused by deep breathing to realize a reliable liveness detection system, BreathLive. To be specific, BreathLive captures breathing sounds and chest motion simultaneously, and then eliminates signal delay caused by any imperfections of device components. Next, it extracts a set of features to characterize the correlation between sounds and motion signals, and uses them to train the classifier. We implement and evaluated BreathLive under different attacking scenarios and contexts. The results show that BreathLive achieves an equal error rate of 4.0%, 6.4% and 8.3% for random impersonation attacks, advanced impersonation attacks and advanced replay attacks respectively, which indicates its effectiveness in defending against different attacks. Also the extensive experiments prove the system can be robust to different contexts with a small training set.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {12},
numpages = {25},
keywords = {Gyroscope, Liveness Detection, Microphone, Wearable Computing}
}

@article{10.1007/s11241-007-9014-5,
author = {Shankaran, Nishanth and Koutsoukos, Xenofon D. and Schmidt, Douglas C. and Xue, Yuan and Lu, Chenyang},
title = {Hierarchical control of multiple resources in distributed real-time and embedded systems},
year = {2008},
issue_date = {August    2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {39},
number = {1–3},
issn = {0922-6443},
url = {https://doi.org/10.1007/s11241-007-9014-5},
doi = {10.1007/s11241-007-9014-5},
abstract = {Real-time and embedded systems have traditionally been designed for closed environments where operating conditions, input workloads, and resource availability are known a priori, and are subject to little or no change at runtime. There is increasing demand, however, for adaptive capabilities in distributed real-time and embedded (DRE) systems that execute in open environments where system operational conditions, input workload, and resource availability cannot be characterized accurately a priori. A challenging problem faced by researchers and developers of such systems is devising effective adaptive resource management strategies that can meet end-to-end quality of service (QoS) requirements of applications. To address key resource management challenges of open DRE systems, this paper presents the Hierarchical Distributed Resource-management Architecture (HiDRA), which provides adaptive resource management using control techniques that adapt to workload fluctuations and resource availability for both bandwidth and processor utilization simultaneously.

This paper presents three contributions to research in adaptive resource management for DRE systems. First, we describe the structure and functionality of HiDRA. Second, we present an analytical model of HiDRA that formalizes its control-theoretic behavior and presents analytical assurance of system performance. Third, we evaluate the performance of HiDRA via experiments on a representative DRE system that performs real-time distributed target tracking. Our analytical and empirical results indicate that HiDRA yields predictable, stable, and efficient system performance, even in the face of changing workload and resource availability.},
journal = {Real-Time Syst.},
month = aug,
pages = {237–282},
numpages = {46},
keywords = {Real-time systems, Quality of service, Hierarchical control, Embedded systems, Distributed systems, Adaptive systems}
}

@article{10.1007/s10844-021-00648-7,
author = {Mavropoulos, Thanassis and Symeonidis, Spyridon and Tsanousa, Athina and Giannakeris, Panagiotis and Rousi, Maria and Kamateri, Eleni and Meditskos, Georgios and Ioannidis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Smart integration of sensors, computer vision and knowledge representation for intelligent monitoring and verbal human-computer interaction},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {2},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-021-00648-7},
doi = {10.1007/s10844-021-00648-7},
abstract = {The details presented in this article revolve around a sophisticated monitoring framework equipped with knowledge representation and computer vision capabilities, that aims to provide innovative solutions and support services in the healthcare sector, with a focus on clinical and non-clinical rehabilitation and care environments for people with mobility problems. In contemporary pervasive systems most modern virtual agents have specific reactions when interacting with humans and usually lack extended dialogue and cognitive competences. The presented tool aims to provide natural human-computer multi-modal interaction via exploitation of state-of-the-art technologies in computer vision, speech recognition and synthesis, knowledge representation, sensor data analysis, and by leveraging prior clinical knowledge and patient history through an intelligent, ontology-driven, dialogue manager with reasoning capabilities, which can also access a web search and retrieval engine module. The framework’s main contribution lies in its versatility to combine different technologies, while its inherent capability to monitor patient behaviour allows doctors and caregivers to spend less time collecting patient-related information and focus on healthcare. Moreover, by capitalising on voice, sensor and camera data, it may bolster patients’ confidence levels and encourage them to naturally interact with the virtual agent, drastically improving their moral during a recuperation process.},
journal = {J. Intell. Inf. Syst.},
month = oct,
pages = {321–345},
numpages = {25},
keywords = {Human-computer interaction, Sensors, Natural language processing, Pervasive systems, Computer vision, Healthcare}
}

@article{10.5555/3215679.3215683,
author = {Gallien, J\'{e}r\'{e}mie and Mersereau, Adam J. and Garro, Andres and Mora, Alberte Dapena and Vidal, Mart\'{\i}n N\'{o}voa},
title = {Initial Shipment Decisions for New Products at Zara},
year = {2015},
issue_date = {April 2015},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {63},
number = {2},
issn = {0030-364X},
abstract = {Given uncertain popularity of new products by location, fast fashion retailer Zara faces a trade-off. Large initial shipments to stores reduce lost sales in the critical first days of the product life cycle, but maintaining stock at the warehouse allows restocking flexibility once initial sales are observed. In collaboration with Zara, we develop and test a decision support system featuring a data-driven model of forecast updating and a dynamic optimization formulation for allocating limited stock by location over time. A controlled field experiment run worldwide with 34 articles during the 2012 season showed an increase in total average season sales by approximately 2% and a reduction in the number of unsold units at the end of the regular selling season by approximately 4%.},
journal = {Oper. Res.},
month = apr,
pages = {269–286},
numpages = {18},
keywords = {retailing, inventory control, field experiment, demand learning, apparel industry}
}

@article{10.1016/j.artmed.2015.06.001,
author = {Kotu, Lasya Priya and Engan, Kjersti and Borhani, Reza and Katsaggelos, Aggelos K. and \O{}rn, Stein and Woie, Leik and Eftest\o{}l, Trygve},
title = {Cardiac magnetic resonance image-based classification of the risk of arrhythmias in post-myocardial infarction patients},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {64},
number = {3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2015.06.001},
doi = {10.1016/j.artmed.2015.06.001},
abstract = {HighlightsCardiac magnetic resonance image-based features were used to distinguish post-myocardial infarction patients into high and low arrhythmic risk groups.Seventeen features describing the size, location and texture of the scarred myocardium were used in different classifiers.In Experiment 1, a systematic testing of features and their combinations was done.SMOTE, wrapper based feature selection, and nested cross-validation were used in Experiment 2.Experiments 1 and 2 gave an accuracy of 94.4% (AUC=0.965) and 92.6% (AUC=0.921), respectively. IntroductionPatients surviving myocardial infarction (MI) can be divided into high and low arrhythmic risk groups. Distinguishing between these two groups is of crucial importance since the high-risk group has been shown to benefit from implantable cardioverter defibrillator insertion; a costly surgical procedure with potential complications and no proven advantages for the low-risk group. Currently, markers such as left ventricular ejection fraction and myocardial scar size are used to evaluate arrhythmic risk. MethodsIn this paper, we propose quantitative discriminative features extracted from late gadolinium enhanced cardiac magnetic resonance images of post-MI patients, to distinguish between 20 high-risk and 34 low-risk patients. These features include size, location, and textural information concerning the scarred myocardium. To evaluate the discriminative power of the proposed features, we used several built-in classification schemes from matrix laboratory (MATLAB) and Waikato environment for knowledge analysis (WEKA) software, including k-nearest neighbor (k-NN), support vector machine (SVM), decision tree, and random forest. ResultsIn Experiment 1, the leave-one-out cross-validation scheme is implemented in MATLAB to classify high- and low-risk groups with a classification accuracy of 94.44%, and an AUC of 0.965 for a feature combination that captures size, location and heterogeneity of the scar. In Experiment 2 with the help of WEKA, nested cross-validation is performed with k-NN, SVM, adjusting decision tree and random forest classifiers to differentiate high-risk and low-risk patients. SVM classifier provided average accuracy of 92.6%, and AUC of 0.921 for a feature combination capturing location and heterogeneity of the scar. Experiment 1 and Experiment 2 show that textural features from the scar are important for classification and that localization features provide an additional benefit. ConclusionThese promising results suggest that the discriminative features introduced in this paper can be used by medical professionals, or in automatic decision support systems, along with the recognized risk markers, to improve arrhythmic risk stratification in post-MI patients.},
journal = {Artif. Intell. Med.},
month = jul,
pages = {205–215},
numpages = {11},
keywords = {k-Nearest neighbor classifier, Support vector machine classifier, Sobel filter, Local binary pattern, High and low arrhythmic risk, Cardiac magnetic resonance image}
}

@article{10.1016/j.jss.2015.05.006,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
title = {Feature extraction approaches from natural language requirements for reuse in software product lines},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.05.006},
doi = {10.1016/j.jss.2015.05.006},
abstract = {Hybrid NLP approaches were more common for extracting textual requirements.There is a mixture of automated and semi-automated approaches from IR and data mining.Support tools were not made available to the public.Not all studies use software metrics in conjunction with experiments and case studies.Reconfirm practitioners guidelines' absence from selected studies (Alves et al., 2010). Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
journal = {J. Syst. Softw.},
month = aug,
pages = {132–149},
numpages = {18},
keywords = {Systematic literature review, Software product lines, Requirements reuse, Natural language requirements, Feature extractions}
}

@article{10.1016/j.comcom.2005.07.017,
author = {Negru, D. and Mehaoua, A. and Hadjadj-aoul, Y. and Berthelot, C.},
title = {Dynamic bandwidth allocation for efficient support of concurrent digital TV and IP multicast services in DVB-T networks},
year = {2006},
issue_date = {March, 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {6},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2005.07.017},
doi = {10.1016/j.comcom.2005.07.017},
abstract = {Convergence of digital video broadcasting networks, from one hand, and IP wireless networks, from the other hand, represents undoubtedly the main evolution in next generation of services combining digital television and interactive Internet applications. This cross-industry synergy is jointly driven by the growing number of mobile devices willing to access the Internet and the large-scale and low-cost deployment of broadband terrestrial digital broadcasting infrastructures (DVB-T). In this paper, we describe a novel dynamic bandwidth allocation algorithm, called iDBMS, for the provisioning of concurrent IP multicast and Digital TV services over a new type of broadband wireless metropolitan area networks that utilises the DVB-T stream in regenerative configurations for creating a multi-service capable infrastructure in the UHF/VHF band. This fast DVB-T metropolitan backbone will permit the seamless inter-connection of multi-technological access networks (i.e. WiFi, xDSL, PSTN, ...) at a regional level by means of a shared broadband DVB-T downlink and point-to-point physical return channels. IP over DVB optimization, asymmetric wireless channels and resource allocation are addressed in this paper. Implementation and performance evaluation of the proposed resource management algorithm iDBMS is also presented in the large-scale context of the IST European project ATHENA.},
journal = {Comput. Commun.},
month = mar,
pages = {741–756},
numpages = {16},
keywords = {Resource management, QoS, IP, DVB}
}

@article{10.1016/j.cie.2018.05.040,
author = {Alemany, MME and Ortiz, A. and Fuertes-Miquel, Vicente S.},
title = {A decision support tool for the order promising process with product homogeneity requirements in hybrid Make-To-Stock and Make-To-Order environments. Application to a ceramic tile company},
year = {2018},
issue_date = {Aug 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2018.05.040},
doi = {10.1016/j.cie.2018.05.040},
journal = {Comput. Ind. Eng.},
month = aug,
pages = {219–234},
numpages = {16},
keywords = {Mixed integer linear programming, Hybrid MTS-MTO, Customer homogeneity requirement, Order promising process, Capable-to-promise, Available-to-promise}
}

@article{10.1016/j.cmpb.2015.10.022,
author = {Araki, Tadashi and Ikeda, Nobutaka and Shukla, Devarshi and Londhe, Narendra D. and Shrivastava, Vimal K. and Banchhor, Sumit K. and Saba, Luca and Nicolaides, Andrew and Shafique, Shoaib and Laird, John R. and Suri, Jasjit S.},
title = {A new method for IVUS-based coronary artery disease risk stratification},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {124},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2015.10.022},
doi = {10.1016/j.cmpb.2015.10.022},
abstract = {Coronary risk stratification.Machine learning to link two arteries.56 grayscale features.Coronary tissue characterization.Classification accuracy 94.95% and AUC 0.95. Interventional cardiologists have a deep interest in risk stratification prior to stenting and percutaneous coronary intervention (PCI) procedures. Intravascular ultrasound (IVUS) is most commonly adapted for screening, but current tools lack the ability for risk stratification based on grayscale plaque morphology. Our hypothesis is based on the genetic makeup of the atherosclerosis disease, that there is evidence of a link between coronary atherosclerosis disease and carotid plaque built up. This novel idea is explored in this study for coronary risk assessment and its classification of patients between high risk and low risk.This paper presents a strategy for coronary risk assessment by combining the IVUS grayscale plaque morphology and carotid B-mode ultrasound carotid intima-media thickness (cIMT) - a marker of subclinical atherosclerosis. Support vector machine (SVM) learning paradigm is adapted for risk stratification, where both the learning and testing phases use tissue characteristics derived from six feature combinational spaces, which are then used by the SVM classifier with five different kernels sets. These six feature combinational spaces are designed using 56 novel feature sets. K-fold cross validation protocol with 10 trials per fold is used for optimization of best SVM-kernel and best feature combination set.IRB approved coronary IVUS and carotid B-mode ultrasound were jointly collected on 15 patients (2 days apart) via: (a) 40MHz catheter utilizing iMap (Boston Scientific, Marlborough, MA, USA) with 2865 frames per patient (42,975 frames) and (b) linear probe B-mode carotid ultrasound (Toshiba scanner, Japan). Using the above protocol, the system shows the classification accuracy of 94.95% and AUC of 0.95 using optimized feature combination. This is the first system of its kind for risk stratification as a screening tool to prevent excessive cost burden and better patients' cardiovascular disease management, while validating our two hypotheses.},
journal = {Comput. Methods Prog. Biomed.},
month = feb,
pages = {161–179},
numpages = {19},
keywords = {Risk assessment, Machine learning, IVUS, Coronary artery, Carotid disease, B-mode ultrasound}
}

@article{10.1147/JRD.2014.2304864,
author = {Arnold, W. C. and Arroyo, D. J. and Segmuller, W. and Spreitzer, M. and Steinder, M. and Tantawi, A. N.},
title = {Workload orchestration and optimization for software defined environments},
year = {2014},
issue_date = {March/May 2014},
publisher = {IBM Corp.},
address = {USA},
volume = {58},
number = {2–3},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2014.2304864},
doi = {10.1147/JRD.2014.2304864},
abstract = {The software defined environment (SDE) provides a powerful programmable interface to a cloud infrastructure through an abstraction of compute, network, and storage resources. A workload refers to the application to be deployed in such an infrastructure. To take advantage of the SDE interface, the workload is described using a declarative workload definition language and is then deployed in the infrastructure through an automated workload orchestration and optimization layer. This paper describes the architecture and algorithms that make up this layer. Given a definition of the workload, including the virtual components of the application and their resource needs, as well as other meta-information relating to factors such as performance, availability, and privacy, the function of the workload orchestration and optimization layer is to map virtual resources to physical resources and realize such a mapping in the infrastructure. This mapping, known as placement, is optimized so that the infrastructure is efficiently utilized, and the workload requirements are satisfied. We present the overall architecture of the workload orchestration and optimization runtime. We focus on the workload placement problem and describe our optimization framework. Then, we consider a real application, IBM Connections, as a use-case to demonstrate the orchestration and optimization functionalities.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {11},
numpages = {1}
}

@article{10.1007/s11036-019-01503-4,
author = {Zhang, Jian and Gao, Cheng and Gong, Liangyi and Gu, Zhaojun and Man, Dapeng and Yang, Wu and Li, Wenzhen},
title = {Malware Detection Based on Multi-level and Dynamic Multi-feature Using Ensemble Learning at Hypervisor},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {1383-469X},
url = {https://doi.org/10.1007/s11036-019-01503-4},
doi = {10.1007/s11036-019-01503-4},
abstract = {As more and more applications migrate to clouds, the type and amount of malware attack against virtualized environments are increasing, which is a key factor that restricts the widespread deployment and application of cloud platforms. Traditional in-VM-based security software is not effective against malware attacks, as the security software itself becomes the target of malware attacks and can easily be tampered with or even subverted. In this paper, we propose a new malware detection method to improve virtual machine security performance and ensure the security of the entire cloud platform. This paper uses the virtual machine introspection(VMI) combined with the memory forensics analysis(MFA) technology to extract multiple types of dynamic features from the virtual machine memory, the hypervisor layer and the hardware layer. Furthermore, this paper proposes an adaptive feature selection method. By combining three different search strategies, three types of features are compared and analyzed from three aspects: effectiveness, system load and security. By adjusting the weight of each feature, it meets the detection requirements of different malware in the cloud environment as expected. Finally, the detection method improves the detection accuracy and generalization ability of the overall classifier using the AdaBoost ensemble learning method with Voting’s combination strategy. The experiment used a large number of real malicious samples, and achieved an accuracy of 0.999 (AUC), with a maximum performance overhead of 5.6%.},
journal = {Mob. Netw. Appl.},
month = aug,
pages = {1668–1685},
numpages = {18},
keywords = {Ensemble learning, Hardware features, Feature extraction, Memory forensics analysis, Virtual machine introspection, Cloud security, Malware detection}
}

@article{10.1016/j.cie.2014.09.017,
author = {Dev, Navin K. and Shankar, Ravi and Dey, Prasanta K. and Gunasekaran, Angappa},
title = {Holonic supply chain},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.09.017},
doi = {10.1016/j.cie.2014.09.017},
abstract = {We analyze the impact of inventory system on family dispatching rule.Appropriate family dispatching rule have mitigation effect on demand amplification.The information is conceptualized under the paradigm of Koestler's holonic theory.For an industrial case, analysis of dispatching rules is performed from total supply chain perspective.Family dispatching rules are compared for Average Flow Time performance. In the contemporary business environment, to adhere to the need of the customers, caused the shift from mass production to mass-customization. This necessitates the supply chain (SC) to be effective flexible. The purpose of this paper is to seek flexibility through adoption of family-based dispatching rules under the influence of inventory system implemented at downstream echelons of an industrial supply chain network. We compared the family-based dispatching rules in existing literature under the purview of inventory system and information sharing within a supply chain network. The dispatching rules are compared for Average Flow Time performance, which is averaged over the three product families. The performance is measured using extensive discrete event simulation process. Given the various inventory related operational factors at downstream echelons, the present paper highlights the importance of strategically adopting appropriate family-based dispatching rule at the manufacturing end. In the environment of mass customization, it becomes imperative to adopt the family-based dispatching rule from the system wide SC perspective. This warrants the application of intra as well as inter-echelon information coordination. The holonic paradigm emerges in this research stream, amidst the holistic approach and the vital systemic approach. The present research shows its novelty in triplet. Firstly, it provides leverage to manager to strategically adopting a dispatching rule from the inventory system perspective. Secondly, the findings provide direction for the attenuation of adverse impact accruing from demand amplification (bullwhip effect) in the form of inventory levels by appropriately adopting family-based dispatching rule. Thirdly, the information environment is conceptualized under the paradigm of Koestler's holonic theory.},
journal = {Comput. Ind. Eng.},
month = dec,
pages = {1–11},
numpages = {11},
keywords = {Supply chain coordination, Holonic paradigm, Family-based dispatching, Discrete event simulation, Average Flow Time}
}

@article{10.1155/ASP/2006/29104,
author = {Anliker, U. and Randall, J. F. and Tr\"{o}ster, G.},
title = {Speaker separation and tracking system},
year = {2006},
issue_date = {01 January},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2006},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP/2006/29104},
doi = {10.1155/ASP/2006/29104},
abstract = {Replicating human hearing in electronics under the constraints of using only two microphones (even with more than two speakers) and the user carrying the device at all times (i.e., mobile device weighing less than 100 g) is nontrivial. Our novel contribution in this area is a two-microphone system that incorporates both blind source separation and speaker tracking. This system handles more than two speakers and overlapping speech in a mobile environment. The system also supports the case in which a feedback loop from the speaker tracking step to the blind source separation can improve performance. In order to develop and optimize this system, we have established a novel benchmark that we here with present. Using the introduced complexity metrics, we present the tradeoffs between system performance and computational load. Our results prove that in our case, source separation was significantly more dependent on frame duration than on sampling frequency.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {171},
numpages = {1}
}

@article{10.1007/s10514-019-09883-y,
author = {Yan, Zhi and Duckett, Tom and Bellotto, Nicola},
title = {Online learning for 3D&nbsp;LiDAR-based human detection: experimental analysis of point cloud clustering and classification methods},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {44},
number = {2},
issn = {0929-5593},
url = {https://doi.org/10.1007/s10514-019-09883-y},
doi = {10.1007/s10514-019-09883-y},
abstract = {This paper presents a system for online learning of human classifiers by mobile service robots using 3D&nbsp;LiDAR sensors, and its experimental evaluation in a large indoor public space. The learning framework requires a minimal set of labelled samples (e.g. one or several samples) to initialise a classifier. The classifier is then retrained iteratively during operation of the robot. New training samples are generated automatically using multi-target tracking and a pair of “experts” to estimate false negatives and false positives. Both classification and tracking utilise an efficient real-time clustering algorithm for segmentation of 3D point cloud data. We also introduce a new feature to improve human classification in sparse, long-range point clouds. We provide an extensive evaluation of our the framework using a 3D&nbsp;LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments demonstrate the influence of the system components and improved classification of humans compared to the state-of-the-art.},
journal = {Auton. Robots},
month = jan,
pages = {147–164},
numpages = {18},
keywords = {93C85, 68T40, Dataset, 3D&nbsp;LiDAR-based tracking, Point cloud segmentation, Human detection, Online learning}
}

@article{10.1109/TNET.2017.2711642,
author = {Nikkhah, Mehdi and Mangal, Aman and Dovrolis, Constantine and Guerin, Roch},
title = {A Statistical Exploration of Protocol Adoption},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2711642},
doi = {10.1109/TNET.2017.2711642},
abstract = {The development and adoption of new protocols or of extensions to existing protocols is arguably central to the Internet's evolution. However, and in spite of over 40 years of experience with this process, we have limited understanding of what factors may contribute to a protocol's success. A sound technical design and a well-grounded purpose are obviously important, but we have many examples of failures that met those two criteria. What other factors affect a protocol's likelihood of success, and under what circumstances? We investigate this question through a statistical approach, based on analyzing a set of about 250 Internet standard documents, Internet engineering task force request for comments RFCs. We characterize these RFCs using a number of key features, which we then seek to associate with positive or negative odds when it comes to success. Our high-level results are intuitive, e.g., protocols that call for Internet-wide adoption face greater challenges. Focusing on more targeted subsets of protocols reveals more subtle and possibly more interesting differences between areas of the Internet landscape. We also apply our prediction framework to IPv6, and use different “what-if” scenarios to explore what might have affected its deployment.},
journal = {IEEE/ACM Trans. Netw.},
month = oct,
pages = {2858–2871},
numpages = {14}
}

@inproceedings{10.1145/3152494.3152513,
author = {Mohapatra, Prateeti and Ray, Anupama and Dasgupta, Gargi},
title = {FuTSe: a fuzzy taxonomy service to facilitate product search},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3152513},
doi = {10.1145/3152494.3152513},
abstract = {The number of people who buy services and products on-line have drastically accelerated over the last couple of years. Service and product vendors like IBM and Amazon claim that 50% of their customers begin their journey on the respective on-line sites be it for technical support, maintenance services or new product purchase. Enabling customers to get to the service/product they are looking for in the most efficient manner is an important problem to solve for superior customer experience. In the specific industry of Technical support services, product sites get several thousand search queries every day. These queries vary across a range of technical issues, maintenance requests or new purchases for a suite of hardware and software products. The key to responding satisfactorily to these queries is in understanding the products or the services being talked about. Our analysis shows that majority of the time users either do not specify the product or the service name in the search query, or they use a non-standard, colloquial version of the name. Thus, finding the right product/service by either implicitly using context or fuzzily using the search terms is both a crucial problem to solve for on-line service providers as well as for the customers. Faceted search and keyword-based search does not work when there are partial or incomplete queries, and typographical mistakes or absence of product information. In this paper, we build a fuzzy-based taxonomy service that (a) extracts the keywords from a user query, (b) handles partial queries by auto-completing in context, (c) utilizes user browsing context and then (d) uses a fuzzy-based similarity metric to retrieve relevant products/services and their associated information from the product taxonomy file. We show on real user queries that by using the service we are able to get 90% of the product names accurately for a technical service provider, when they are either not mentioned or incomplete.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {220–229},
numpages = {10},
keywords = {product taxonomy, partial queries, fuzzy matching},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@article{10.1145/3357495.3357502,
author = {Preum, Sarah and Shu, Sile and Hotaki, Mustafa and Williams, Ronald and Stankovic, John and Alemzadeh, Homa},
title = {CognitiveEMS: a cognitive assistant system for emergency medical services},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
url = {https://doi.org/10.1145/3357495.3357502},
doi = {10.1145/3357495.3357502},
abstract = {This paper presents our preliminary results on development of a Cognitive assistant system for Emergency Medical Services (CognitiveEMS) that aims to improve situational awareness and safety of first responders. CognitiveEMS integrates a suite of smart wearable sensors, devices, and analytics for real-time collection and analysis of in-situ data from incident scene and delivering dynamic data-driven insights to responders on the most effective response actions to take. We present the overall architecture of CognitiveEMS pipeline for processing information collected from the responder, which includes stages for converting speech to text, extracting medical and EMS protocol specific concepts, and modeling and execution of an EMS protocol. The performance of the pipeline is evaluated in both noise-free and noisy incident environments. The experiments are conducted using two types of publicly-available real EMS data: short radio calls and post-incident patient care reports. Three different noise profiles are considered for simulating the noisy environments: cafeteria, people talking, and emergency sirens. Noise was artificially added at 3 intensity levels of low, medium, and high to pre-recorded audio data. The results show that the i) state-of-the-art speech recognition tools such as Google Speech API are quite robust to low and medium noise intensities; ii) in the presence of high noise levels, the overall recall rate in medical concept annotation is reduced; and iii) the effect of noise often propagates to the final decision making stage and results in generating misleading feedback to responders.},
journal = {SIGBED Rev.},
month = aug,
pages = {51–60},
numpages = {10},
keywords = {speech recognition, natural language processing, medical emergency, cognitive assistant system, EMS}
}

@article{10.1007/s00521-014-1652-7,
author = {Chen, Chia-Chen},
title = {RFID-based intelligent shopping environment: a comprehensive evaluation framework with neural computing approach},
year = {2014},
issue_date = {Dec 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {7–8},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-014-1652-7},
doi = {10.1007/s00521-014-1652-7},
abstract = {This research proposes a radio frequency identification (RFID)-based intelligent shopping environment and its distributed reading capability to raise quality of service through improving the automation of product presentation, inventory monitor, billing procedures, manpower logistics, and customer lifetime value prediction. This research also uses RFID to successfully create a smart-shelf-enabled system as an advanced decision-making mechanism for managers. A case study based on a well-known fashion retailing company is used to demonstrate how the proposed system can significantly improve daily business operations. In addition, this research also used artificial neural network to predict the VIP member classification and customer retention rate. The experimental results figure out that the artificial intelligence approach would be outperformed the statistical and decision tree methods. Finally, a questionnaire was administered to 120 customers and investigated their degree of RFID usage willingness and purchase intention based on the Unified Theory of Acceptance and Use of Technology model. The empirical results of our study present the easy-to-use and social influence factors that would be most influenced the customers' usage willing and purchase intention with RFID technology.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {1685–1697},
numpages = {13},
keywords = {Smart-shelf-enabled system, Smart space, Radio frequency identification, Fashion industry, Artificial neural network}
}

@proceedings{10.1145/2984043,
title = {SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.14778/3476249.3476307,
author = {Jacob, Vincent and Song, Fei and Stiegler, Arnaud and Rad, Bijan and Diao, Yanlei and Tatbul, Nesime},
title = {Exathlon: a benchmark for explainable anomaly detection over time series},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476307},
doi = {10.14778/3476249.3476307},
abstract = {Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present Exathlon, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. Exathlon has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an Apache Spark cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of Exathlon's dataset, evaluation methodology, and end-to-end data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2613–2626},
numpages = {14}
}

@article{10.1155/2018/8740989,
author = {Ding, Yu and Wang, Fei and Wang, Zhen-ya and Zhang, Wen-jin and Song, Gangbing},
title = {Fault Diagnosis for Hydraulic Servo System Using Compressed Random Subspace Based ReliefF},
year = {2018},
issue_date = {2018},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2018},
issn = {1076-2787},
url = {https://doi.org/10.1155/2018/8740989},
doi = {10.1155/2018/8740989},
abstract = {Playing an important role in electromechanical systems, hydraulic servo system is crucial to mechanical systems like engineering machinery, metallurgical machinery, ships, and other equipment. Fault diagnosis based on monitoring and sensory signals plays an important role in avoiding catastrophic accidents and enormous economic losses. This study presents a fault diagnosis scheme for hydraulic servo system using compressed random subspace based ReliefF (CRSR) method. From the point of view of feature selection, the scheme utilizes CRSR method to determine the most stable feature combination that contains the most adequate information simultaneously. Based on the feature selection structure of ReliefF, CRSR employs feature integration rules in the compressed domain. Meanwhile, CRSR substitutes information entropy and fuzzy membership for traditional distance measurement index. The proposed CRSR method is able to enhance the robustness of the feature information against interference while selecting the feature combination with balanced information expressing ability. To demonstrate the effectiveness of the proposed CRSR method, a hydraulic servo system joint simulation model is constructed by HyPneu and Simulink, and three fault modes are injected to generate the validation data.},
journal = {Complex.},
month = jan,
numpages = {14}
}

@article{10.1007/s00500-012-0885-6,
author = {Ekbal, Asif and Saha, Sriparna},
title = {Combining feature selection and classifier ensemble using a multiobjective simulated annealing approach: application to named entity recognition},
year = {2013},
issue_date = {January   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-012-0885-6},
doi = {10.1007/s00500-012-0885-6},
abstract = {In this paper, we propose a two-stage multiobjective-simulated annealing (MOSA)-based technique for named entity recognition (NER). At first, MOSA is used for feature selection under two statistical classifiers, viz. conditional random field (CRF) and support vector machine (SVM). Each solution on the final Pareto optimal front provides a different classifier. These classifiers are then combined together by using a new classifier ensemble technique based on MOSA. Several different versions of the objective functions are exploited. We hypothesize that the reliability of prediction of each classifier differs among the various output classes. Thus, in an ensemble system, it is necessary to find out the appropriate weight of vote for each output class in each classifier. We propose a MOSA-based technique to determine the weights for votes automatically. The proposed two-stage technique is evaluated for NER in Bengali, a resource-poor language, as well as for English. Evaluation results yield the highest recall, precision and F-measure values of 93.95, 95.15 and 94.55 %, respectively for Bengali and 89.01, 89.35 and 89.18 %, respectively for English. Experiments also suggest that the classifier ensemble identified by the proposed MOO-based approach optimizing the  F-measure values of named entity (NE) boundary detection outperforms all the individual classifiers and four conventional baseline models.},
journal = {Soft Comput.},
month = jan,
pages = {1–16},
numpages = {16},
keywords = {Weighted voting, Support vector machine (SVM), Simulated annealing (SA), Natural language processing, Named entity recognition, Multiobjective optimization (MOO), Maximum entropy (ME), Conditional random field (CRF), Classifier ensemble}
}

@article{10.14778/3415478.3415559,
author = {Suri, Sahaana and Chanda, Raghuveer and Bulut, Neslihan and Narayana, Pradyumna and Zeng, Yemao and Bailis, Peter and Basu, Sugato and Narlikar, Girija and R\'{e}, Christopher and Sethi, Abishek},
title = {Leveraging organizational resources to adapt models to new data modalities},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415559},
doi = {10.14778/3415478.3415559},
abstract = {As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utilize the domain expertise and data they have cultivated from developing tasks for existing modalities. We demonstrate how organizational resources, in the form of aggregate statistics, knowledge bases, and existing services that operate over related tasks, enable teams to construct a common feature space that connects new and existing data modalities. This allows teams to apply methods for data curation (e.g., weak supervision and label propagation) and model training (e.g., forms of multi-modal learning) across these different data modalities. We study how this use of organizational resources composes at production scale in over 5 classification tasks at Google, and demonstrate how it reduces the time needed to develop models for new modalities from months to weeks or days.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3396–3410},
numpages = {15}
}

@article{10.1109/TCBB.2021.3098126,
author = {Zhang, Yuan and Ye, Fei and Gao, Xieping},
title = {MCA-Net: Multi-Feature Coding and Attention Convolutional Neural Network for Predicting lncRNA-Disease Association},
year = {2021},
issue_date = {Sept.-Oct. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3098126},
doi = {10.1109/TCBB.2021.3098126},
abstract = {With the advent of the era of big data, it is troublesome to accurately predict the associations between lncRNAs and diseases based on traditional biological experiments due to its time-consuming and subjective. In this paper, we propose a novel deep learning method for predicting lncRNA-disease associations using multi-feature coding and attention convolutional neural network (MCA-Net). We first calculate six similarity features to extract different types of lncRNA and disease feature information. Second, a multi-feature coding method is proposed to construct the feature vectors of lncRNA-disease association samples by integrating the six similarity features. Furthermore, an attention convolutional neural network is developed to identify lncRNA-disease associations under 10-fold cross-validation. Finally, we evaluate the performance of MCA-Net from different perspectives including the effects of the model parameters, distinct deep learning models, and the necessity of attention mechanism. We also compare MCA-Net with several state-of-the-art methods on three publicly available datasets, i.e., LncRNADisease, Lnc2Cancer, and LncRNADisease2.0. The results show that our MCA-Net outperforms the state-of-the-art methods on all three dataset. Besides, case studies on breast cancer and lung cancer further verify that MCA-Net is effective and accurate for the lncRNA-disease association prediction.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {2907–2919},
numpages = {13}
}

@article{10.1007/s10916-019-1388-0,
author = {Haider, Nishi Shahnaj and Singh, Bikesh Kumar and Periyasamy, R. and Behera, Ajoy K.},
title = {Respiratory Sound Based Classification of Chronic Obstructive Pulmonary Disease: a Risk Stratification Approach in Machine Learning Paradigm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {8},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1388-0},
doi = {10.1007/s10916-019-1388-0},
abstract = {This article investigates the classification of normal and COPD subjects on the basis of respiratory sound analysis using machine learning techniques. Thirty COPD and 25 healthy subject data are recorded. Total of 39 lung sound features and 3 spirometry features are extracted and evaluated. Various parametric and nonparametric tests are conducted to evaluate the relevance of extracted features. Classifiers such as support vector machine (SVM), k-nearest neighbor (KNN), logistic regression (LR), decision tree and discriminant analysis (DA) are used to categorize normal and COPD breath sounds. Classification based on spirometry parameters as well as respiratory sound parameters are assessed. Maximum classification accuracy of 83.6% is achieved by the SVM classifier while using the most relevant lung sound parameters i.e. median frequency and linear predictive coefficients. Further, SVM classifier and LR classifier achieved classification accuracy of 100% when relevant lung sound parameters, i.e. median frequency and linear predictive coefficient are combined with the spirometry parameters, i.e. forced vital capacity (FVC) and forced expiratory volume in 1 s (FEV1). It is concluded that combining lung sound based features with spirometry data can improve the accuracy of COPD diagnosis and hence the clinician's performance in routine clinical practice. The proposed approach is of great significance in a clinical scenario wherein it can be used to assist clinicians for automated COPD diagnosis. A complete handheld medical system can be developed in the future incorporating lung sounds for COPD diagnosis using machine learning techniques.},
journal = {J. Med. Syst.},
month = aug,
pages = {1–13},
numpages = {13},
keywords = {Spirometry, Risk stratification, Machine learning, Lung sound, Feature extraction, Chronic obstructive pulmonary disease diagnosis}
}

@article{10.1287/isre.2020.0921,
author = {Lee, Gene Moo and He, Shu and Lee, Joowon and Whinston, Andrew B.},
title = {Matching Mobile Applications for Cross-Promotion},
year = {2020},
issue_date = {September 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {3},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.2020.0921},
doi = {10.1287/isre.2020.0921},
abstract = {As the mobile app market grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. Similar to the case of recommender systems, the challenge is how apps can be recommended to the right users and how consumers can find the right apps. This paper studies a new mobile app ad framework, cross-promotion (CP), which is to promote new “target” apps within other “source” apps. With unique random matching experiment data, we empirically test the important determinants of ad effectiveness. We then propose a machine-learning-based framework to optimally match source apps to target apps to improve ad effectiveness in terms of app downloads and postdownload usages. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of CP campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. The paper has important managerial implications because it provides direct guidance to better utilize CP for app developers and to leverage data analytics and machine-learning models for platform managers. It also provides policy implications on the trade-off between utility and privacy in the growing data economy.The mobile applications (apps) market is one of the most successful software markets. As the platform grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. The challenge is how app developers can target the right users with their apps and how consumers can find the apps that fit their needs. Cross-promotion, advertising a mobile app (target app) in another app (source app), is introduced as a new app-promotion framework to alleviate the issue of search costs. In this paper, we model source app user behaviors (downloads and postdownload usages) with respect to different target apps in cross-promotion campaigns. We construct a novel app similarity measure using latent Dirichlet allocation topic modeling on apps’ production descriptions and then analyze how the similarity between the source and target apps influences users’ app download and usage decisions. To estimate the model, we use a unique data set from a large-scale random matching experiment conducted by a major mobile advertising company in Korea. The empirical results show that consumers prefer more diversified apps when they are making download decisions compared with their usage decisions, which is supported by the psychology literature on people’s variety-seeking behavior. Lastly, we propose an app-matching system based on machine-learning models (on app download and usage prediction) and generalized deferred acceptance algorithms. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of cross-promotion campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. This paper has implications on the trade-off between utility and privacy in the growing mobile economy.},
journal = {Info. Sys. Research},
month = sep,
pages = {865–891},
numpages = {27},
keywords = {mobile analytics, algorithm, deferred acceptance, machine learning, topic modeling, two-sided platform, search cost, matching, cross-promotion, mobile applications}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {effort-aware indicator, balancing distribution, transfer learning, cross-project defect prediction}
}

@inproceedings{10.1145/3194133.3194138,
author = {Duarte, Francisco and Gil, Richard and Romano, Paolo and Lopes, Ant\'{o}nia and Rodrigues, Lu\'{\i}s},
title = {Learning non-deterministic impact models for adaptation},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194138},
doi = {10.1145/3194133.3194138},
abstract = {Many adaptive systems react to variations in their environment by changing their configuration. Often, they make the adaptation decisions based on some knowledge about how the reconfiguration actions impact the key performance indicators. However, the outcome of these actions is typically affected by uncertainty. Adaptation actions have non-deterministic impacts, potentially leading to multiple outcomes. When this uncertainty is not captured explicitly in the models that guide adaptation, decisions may turn out ineffective or even harmful to the system. Also critical is the need for these models to be interpretable to the human operators that are accountable for the system. However, accurate impact models for actions that result in non-deterministic outcomes are very difficult to obtain and existing techniques that support the automatic generation of these models, mainly based on machine learning, are limited in the way they learn non-determinism.In this paper, we propose a method to learn human-readable models that capture non-deterministic impacts explicitly. Additionally, we discuss how to exploit expert's knowledge to bootstrap the adaptation process as well as how to use the learned impacts to revise models defined offline. We motivate our work on the adaptation of applications in the cloud, typically affected by hardware heterogeneity and resource contention. To validate our approach we use a prototype based on the RUBiS auction application.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {196–205},
numpages = {10},
keywords = {adaptive systems, machine learning, runtime models, uncertainty},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1561/2200000087,
author = {Liu, Jiani and Zhu, Ce and Long, Zhen and Liu, Yipeng},
title = {Tensor Regression},
year = {2021},
issue_date = {Sep 2021},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {14},
number = {4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000087},
doi = {10.1561/2200000087},
abstract = {The presence of multidirectional correlations in emerging
    multidimensional data poses a challenge to traditional regression
    modeling methods. Traditional modeling methods
    based on matrix or vector, for example, not only overlook
    the data’s multidimensional information and lower model
    performance, but also add additional computations and storage
    requirements. Driven by the recent advances in applied
    mathematics, tensor regression has been widely used and
    proven effective in many fields, such as sociology, climatology,
    geography, economics, computer vision, chemometrics, and
    neuroscience. Tensor regression can explore multidirectional
    relatedness, reduce the number of model parameters and
    improve model robustness and efficiency. It is timely and
    valuable to summarize the developments of tensor regression
    in recent years and discuss promising future directions,
    which will help accelerate the research process of tensor
    regression, broaden the research direction, and provide tutorials
    for researchers interested in high dimensional regression
    tasks.The fundamentals, motivations, popular algorithms, related
    applications, available datasets, and software resources for
    tensor regression are all covered in this monograph. The first
    part focuses on the key concepts for tensor regression, mainly
    analyzing existing tensor regression algorithms from the perspective
    of regression families. Meanwhile, the adopted low
    rank tensor representations and optimization frameworks
    are also summarized. In addition, several extensions in online
    learning and sketching are described. The second part
    covers related applications, widely used public datasets and
    software resources, as well as some real-world examples, such
    as multitask learning, spatiotemporal learning, human motion
    analysis, facial image analysis, neuroimaging analysis
    (disease diagnosis, neuron decoding, brain activation, and
    connectivity analysis) and chemometrics. This survey can be
    used as a basic reference in tensor-regression-related fields
    and assist readers in efficiently dealing with high dimensional
    regression tasks.},
journal = {Found. Trends Mach. Learn.},
month = sep,
pages = {379–565},
numpages = {190}
}

@inproceedings{10.1145/3281411.3281443,
author = {H\o{}iland-J\o{}rgensen, Toke and Brouer, Jesper Dangaard and Borkmann, Daniel and Fastabend, John and Herbert, Tom and Ahern, David and Miller, David},
title = {The eXpress data path: fast programmable packet processing in the operating system kernel},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281443},
doi = {10.1145/3281411.3281443},
abstract = {Programmable packet processing is increasingly implemented using kernel bypass techniques, where a userspace application takes complete control of the networking hardware to avoid expensive context switches between kernel and userspace. However, as the operating system is bypassed, so are its application isolation and security mechanisms; and well-tested configuration, deployment and management tools cease to function.To overcome this limitation, we present the design of a novel approach to programmable packet processing, called the eXpress Data Path (XDP). In XDP, the operating system kernel itself provides a safe execution environment for custom packet processing applications, executed in device driver context. XDP is part of the mainline Linux kernel and provides a fully integrated solution working in concert with the kernel's networking stack. Applications are written in higher level languages such as C and compiled into custom byte code which the kernel statically analyses for safety, and translates into native instructions.We show that XDP achieves single-core packet processing performance as high as 24 million packets per second, and illustrate the flexibility of the programming model through three example use cases: layer-3 routing, inline DDoS protection and layer-4 load balancing.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {54–66},
numpages = {13},
keywords = {programmable networking, XDP, DPDK, BPF},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.5555/3091125.3091236,
author = {Porteous, Julie and Charles, Fred and Smith, Cameron and Cavazza, Marc and Mouw, Jolien and van den Broek, Paul},
title = {Using Virtual Narratives to Explore Children's Story Understanding},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Interactive Narratives are systems that use automated narrative generation techniques to create multiple story variants which can be shown to an audience, as virtual narratives, using cinematic staging techniques. Previous research in this area has focused on assessment of aspects such as the quality of the automatically generated narratives and their acceptance by the audience. However in our work we deviate from this to explore the use of interactive narratives to support cognitive psychology experiments in story understanding. We hypothesized that the use of virtual narratives would enable narrative comprehension to be studied independently of linguistic phenomena. To assess this we developed a demonstration interactive narrative featuring a virtual environment (Unity3D engine) based on a pre-existing children's story which allows for the generation of variants of the original story that can be "told" via visualization in the 3D world. In the paper we introduce a narrative generation mechanism that provides control over insertion of cues facilitating story understanding, whilst also ensuring that the plot itself is unaffected. An intuitive user interface allows experimenters to insert and order cues and specific events while the narrative generation techniques ensure these requests are effected in a consistent fashion. We also report the results of a field experiment with children (age 9-10) that demonstrates the potential for the use of virtual narratives in story understanding experiments. Our results demonstrated acceptance of virtual narratives, the usability of the system and the impact of cue insertion on inference and story understanding.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {773–781},
numpages = {9},
keywords = {virtual agents, planning, narrative modeling, interactive storytelling, game-based education},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@article{10.1007/s11192-014-1455-8,
author = {Daud, Ali and Ahmad, Muhammad and Malik, M. S. and Che, Dunren},
title = {Using machine learning techniques for rising star prediction in co-author network},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {2},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-014-1455-8},
doi = {10.1007/s11192-014-1455-8},
abstract = {Online bibliographic databases are powerful resources for research in data mining and social network analysis especially co-author networks. Predicting future rising stars is to find brilliant scholars/researchers in co-author networks. In this paper, we propose a solution for rising star prediction by applying machine learning techniques. For classification task, discriminative and generative modeling techniques are considered and two algorithms are chosen for each category. The author, co-authorship and venue based information are incorporated, resulting in eleven features with their mathematical formulations. Extensive experiments are performed to analyze the impact of individual feature, category wise and their combination w.r.t classification accuracy. Then, two ranking lists for top 30 scholars are presented from predicted rising stars. In addition, this concept is demonstrated for prediction of rising stars in database domain. Data from DBLP and Arnetminer databases (1996---2000 for wide disciplines) are used for algorithms' experimental analysis.},
journal = {Scientometrics},
month = feb,
pages = {1687–1711},
numpages = {25},
keywords = {Rising star, Prediction, MEMM, Group leader, Classification, CART}
}

@article{10.1016/j.cor.2009.03.012,
author = {Aghezzaf, El-Houssaine and Sitompul, Carles and Najid, Najib M.},
title = {Models for robust tactical planning in multi-stage production systems with uncertain demands},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {37},
number = {5},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2009.03.012},
doi = {10.1016/j.cor.2009.03.012},
abstract = {We consider the problem of designing robust tactical production plans, in a multi-stage production system, when the periodic demands of the finished products are uncertain. First, we discuss the concept of robustness in tactical production planning and how we intend to approach it. We then present and discuss three models to generate robust tactical plans when the finished-product demands are stochastic with known distributions. In particular, we discuss plans produced, respectively, by a two-stage stochastic planning model, by a robust stochastic optimization planning model, and by an equivalent deterministic planning model which integrates the variability of the finished-product demands. The third model uses finished-product average demands as minimal requirements to satisfy, and seeks to offset the effect of demand variability through the use of planned capacity cushion levels at each stage of the production system. An experimental study is carried out to compare the performances of the plans produced by the three models to determine how each one achieves robustness. The main result is that the proposed robust deterministic model produces plans that achieve better trade-offs between minimum average cost and minimum cost variability. Moreover, the required computational time and space are by far less important in the proposed robust deterministic model compared to the two others.},
journal = {Comput. Oper. Res.},
month = may,
pages = {880–889},
numpages = {10},
keywords = {Stochastic and robust optimization, Robust tactical planning, Capacity cushion levels}
}

@article{10.1016/j.cie.2005.05.001,
author = {Tesfamariam, Daniel and Lindberg, Bengt},
title = {Aggregate analysis of manufacturing systems using system dynamics and ANP},
year = {2005},
issue_date = {August 2005},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {49},
number = {1},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2005.05.001},
doi = {10.1016/j.cie.2005.05.001},
abstract = {Aggregate analysis in manufacturing system design is a useful approach to relegate the non-feasible alternatives at earlier stages. A reusable System Dynamics model and the Analytic Network Process is proposed for a rapid and strategically consistent decision-making. The SD model captures the causal relationships and interdependence of the factors that can be simulated while the ANP provides the preferences towards the performance objectives consistent to the strategic objectives. The basis for the SD and ANP is the Causal Loop Diagram (CLD) that shows the relevant relationships and feedbacks among the model parameters. The approach is exemplified via a case to select the best among competing system configurations.},
journal = {Comput. Ind. Eng.},
month = aug,
pages = {98–117},
numpages = {20},
keywords = {System dynamics, Production layout, Manufacturing system design, Aggregate analysis, ANP}
}

@inproceedings{10.1145/1026711.1026744,
author = {Rautiainen, Mika and Ojala, Timo and Sepp\"{a}nen, Tapio},
title = {Analysing the performance of visual, concept and text features in content-based video retrieval},
year = {2004},
isbn = {1581139403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1026711.1026744},
doi = {10.1145/1026711.1026744},
abstract = {This paper describes revised content-based search experiments in the context of TRECVID 2003 benchmark. Experiments focus on measuring content-based video retrieval performance with following search cues: visual features, semantic concepts and text. The fusion of features uses weights and similarity ranks. Visual similarity is computed using Temporal Gradient Correlogram and Temporal Color Correlogram features that are extracted from the dynamic content of a video shot. Automatic speech recognition transcripts and concept detectors enable higher-level semantic searching. 60 hours of news videos from TRECVID 2003 search task were used in the experiments. System performance was evaluated with 25 pre-defined search topics using average precision. In visual search, multiple examples improved the results over single example search. Weighted fusion of text, concept and visual features improved the performance over text search baseline. Expanded query term list of text queries gave also notable increase in performance over the baseline text search},
booktitle = {Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval},
pages = {197–204},
numpages = {8},
keywords = {semantic concepts, feature fusion, content-based video retrieval, Borda count},
location = {New York, NY, USA},
series = {MIR '04}
}

@article{10.5555/2698250.2698421,
author = {Yin, Peipei and Sun, Fuchun and Wang, Chao and Liu, Huaping},
title = {An adaptive feature fusion framework for multi-class classification based on SVM},
year = {2008},
issue_date = {May       2008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {7},
issn = {1432-7643},
abstract = {An adaptive feature fusion framework is proposed for multi-class classification based on SVM. In a similar manner of one-versus-all (OVA), one of the multi-class SVM schemes, the proposed approach decomposes a multi-class classification into several binary classifications. The main difference lies in that each classifier is created with the most suitable feature vectors to discriminate one class from all the other classes. The feature vectors of the unknown samples are selected by each classifier adaptively such that recognition is fulfilled accordingly. In addition, novel evaluation criterions are defined to deal with the frequent small-number sample problems. A writer recognition experiment is carried out to accomplish this framework with three kinds of feature vectors: texture, structure and morphological features. Finally, the performance of the proposed approach is illustrated as compared with the OVA by applying the same feature vectors for all classes.},
journal = {Soft Comput.},
month = may,
pages = {685–691},
numpages = {7},
keywords = {Writer recognition, SVM, OVA, Multi-class classification, Feature fusion}
}

@inproceedings{10.1007/11763864_20,
author = {Kakarontzas, George and Stamelos, Ioannis},
title = {A tactic-driven process for developing reusable components},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_20},
doi = {10.1007/11763864_20},
abstract = {True reusability of components assumes that they not only offer the functionality prescribed by their APIs, but also that they conform to a well-defined set of quality attributes so that we know if a component can be successfully reused in a new software product. One of the problems with quality attributes however is that it is hard to identify the characteristics of components that contribute to their emergence. End-user quality attributes are versatile and difficult to predict but their occurrence is not of an accidental nature. In this paper we propose a methodology for the exploration of candidate architectural tactics during component analysis and design for the achievement of desirable quality effects. Our approach is based on executable specifications of components that are augmented with the required tactic-related parameters to form a testbed for quality-driven experimentation. We believe that the proposed approach delivers both reusable components as well as reusable models.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {273–286},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@article{10.1016/j.cosrev.2019.100221,
author = {Zhang, Ji and Tan, Leonard and Tao, Xiaohui and Pham, Thuan and Chen, Bing},
title = {Relational intelligence recognition in online social networks — A survey},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2019.100221},
doi = {10.1016/j.cosrev.2019.100221},
journal = {Comput. Sci. Rev.},
month = feb,
numpages = {33},
keywords = {Ensembles, Pattern recognition, Deep learning, Artificial intelligence, Social networks}
}

@article{10.1016/j.specom.2021.09.004,
author = {Ngo, Thuanvan and Kubo, Rieko and Akagi, Masato},
title = {Increasing speech intelligibility and naturalness in noise based on concepts of modulation spectrum and modulation transfer function},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {135},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.09.004},
doi = {10.1016/j.specom.2021.09.004},
journal = {Speech Commun.},
month = dec,
pages = {11–24},
numpages = {14},
keywords = {Intelligibility, Modulation spectrum, Modulation transfer function}
}

@article{10.1016/j.compbiomed.2021.104799,
author = {Al-Qazzaz, Noor Kamal and Alyasseri, Zaid Abdi Alkareem and Abdulkareem, Karrar Hameed and Ali, Nabeel Salih and Al-Mhiqani, Mohammed Nasser and Guger, Christoph},
title = {EEG feature fusion for motor imagery: A new robust framework towards stroke patients rehabilitation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104799},
doi = {10.1016/j.compbiomed.2021.104799},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {15},
keywords = {Stroke rehabilitation, Motor imagery, Feature fusion, Feature extraction, Brain-computer interfaces, EEG}
}

@inproceedings{10.1145/3343031.3350957,
author = {Zhang, Xiao and Zhuang, Fuzhen and Li, Wenzhong and Ying, Haochao and Xiong, Hui and Lu, Sanglu},
title = {Inferring Mood Instability via Smartphone Sensing: A Multi-View Learning Approach},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350957},
doi = {10.1145/3343031.3350957},
abstract = {A high correlation between mood instability (MI), the rapid and constant fluctuation in mood, and mental health has been demonstrated. However, conventional approaches to measure MI are limited owing to the high manpower and time cost required. In this paper, we propose a smartphone-based MI detection that can automatically and passively detect MI with minimal human involvement. The proposed method trains a multi-view learning classification model using features extracted from the smartphone sensing data of volunteers and their self-reported moods. The trained classifier is then used to detect the MI of unseen users efficiently, thereby reducing the human involvement and time cost significantly. Based on extensive experiments conducted with the dataset collected from 68 volunteers, we demonstrate that the proposed multi-view learning model outperforms the baseline classifiers.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1401–1409},
numpages = {9},
keywords = {smartphone sensing, multi-view learning, mood instability detection, attention},
location = {Nice, France},
series = {MM '19}
}

@article{10.1016/j.eswa.2019.113087,
author = {Nam, Hyoju and Yun, Unil and Yoon, Eunchul and Lin, Jerry Chun-Wei},
title = {Efficient approach for incremental weighted erasable pattern mining with list structure},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113087},
doi = {10.1016/j.eswa.2019.113087},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {22},
keywords = {Pruning techniques, Weighted conditions, Incremental mining, Erasable patterns, Data mining}
}

@article{10.1007/s00521-014-1764-0,
author = {Wang, Zhiqiong and Qu, Qixun and Yu, Ge and Kang, Yan},
title = {Breast tumor detection in double views mammography based on extreme learning machine},
year = {2016},
issue_date = {January   2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-014-1764-0},
doi = {10.1007/s00521-014-1764-0},
abstract = {Mammography is one of the most important methods for breast tumor detection, while existing computer-aided diagnosis (CAD) technology based on single-view mammograms ignores the contrastive feature between medio-lateral oblique (MLO) and cranio-caudal (CC) views, and CAD technology based on double-view overlooks features of single views. But in clinical environment, radiologists not only read both CC view images and MLO view images individually, but also contrast these two types of views to diagnose each case. Therefore, to simulate diagnosis process of radiologists, in this paper, a fused feature model which blends features of single views with contrastive features of double views is proposed. The fused feature model is optimized by means of feature selection methods. Then, a CAD detection method based on extreme learning machine, a classifier with wonderful universal approximation capability, is proposed to improve the effectiveness of breast tumor detection by applying the optimum fused feature. The effectiveness of proposed method is verified by 222 pairs of mammograms from 222 women in Northeast China through the complete experiment.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {227–240},
numpages = {14},
keywords = {Mammograms, Image processing, Feature selection, Extreme learning machine (ELM), Computer-aided diagnosis (CAD)}
}

@inproceedings{10.1145/1247660.1247676,
author = {Volgyesi, Peter and Balogh, Gyorgy and Nadas, Andras and Nash, Christopher B. and Ledeczi, Akos},
title = {Shooter localization and weapon classification with soldier-wearable networked sensors},
year = {2007},
isbn = {9781595936141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247660.1247676},
doi = {10.1145/1247660.1247676},
abstract = {The paper presents a wireless sensor network-based mobilecountersniper system. A sensor node consists of a helmetmountedmicrophone array, a COTS MICAz mote for internodecommunication and a custom sensorboard that implementsthe acoustic detection and Time of Arrival (ToA) estimationalgorithms on an FPGA. A 3-axis compass providesself orientation and Bluetooth is used for communicationwith the soldier's PDA running the data fusion and the userinterface. The heterogeneous sensor fusion algorithm canwork with data from a single sensor or it can fuse ToA orAngle of Arrival (AoA) observations of muzzle blasts andballistic shockwaves from multiple sensors. The system estimatesthe trajectory, the range, the caliber and the weapontype. The paper presents the system design and the resultsfrom an independent evaluation at the US Army AberdeenTest Center. The system performance is characterized by 1-degree trajectory precision and over 95% caliber estimationaccuracy for all shots, and close to 100% weapon estimationaccuracy for 4 out of 6 guns tested.},
booktitle = {Proceedings of the 5th International Conference on Mobile Systems, Applications and Services},
pages = {113–126},
numpages = {14},
keywords = {weapon classification, sensor networks, data fusion, caliber estimation, acoustic source localization},
location = {San Juan, Puerto Rico},
series = {MobiSys '07}
}

@article{10.1007/s11042-020-08898-3,
author = {Chahal, Prabhjot Kaur and Pandey, Shreelekha and Goel, Shivani},
title = {A survey on brain tumor detection techniques for MR images},
year = {2020},
issue_date = {Aug 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {29–30},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-08898-3},
doi = {10.1007/s11042-020-08898-3},
abstract = {One of the most crucial tasks in any brain tumor detection system is the isolation of abnormal tissues from normal brain tissues. Interestingly, domain of brain tumor analysis has effectively utilized the concepts of medical image processing, particularly on MR images, to automate the core steps, i.e. extraction, segmentation, classification for proximate detection of tumor. Research is more inclined towards MR for its non-invasive imaging properties. Computer aided diagnosis or detection systems are becoming challenging and are still an open problem due to variability in shapes, areas, and sizes of tumor. The past works of many researchers under medical image processing and soft computing have made noteworthy review analysis on automatic brain tumor detection techniques focusing segmentation as well as classification and their combinations. In the manuscript, various brain tumor detection techniques for MR images are reviewed along with the strengths and difficulties encountered in each to detect various brain tumor types. The current segmentation, classification and detection techniques are also conferred emphasizing on the pros and cons of the medical imaging approaches in each modality. The survey presented here aims to help the researchers to derive the essential characteristics of brain tumor types and identifies various segmentation/classification techniques which are successful for detection of a range of brain diseases. The manuscript covers most relevant strategies, methods, their working rules, preferences, constraints, and their future snags on MR image brain tumor detection. An attempt to summarize the current state-of-art with respect to different tumor types would help researchers in exploring future directions.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21771–21814},
numpages = {44},
keywords = {Classification, Segmentation, Magnetic resonance images, Medical imaging, Computer-aided diagnosis, Brain tumor detection systems}
}

@inproceedings{10.1145/3485730.3485938,
author = {Ling, Neiwen and Wang, Kai and He, Yuze and Xing, Guoliang and Xie, Daqi},
title = {RT-mDL: Supporting Real-Time Mixed Deep Learning Tasks on Edge Platforms},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485938},
doi = {10.1145/3485730.3485938},
abstract = {Recent years have witnessed an emerging class of real-time applications, e.g., autonomous driving, in which resource-constrained edge platforms need to execute a set of real-time mixed Deep Learning (DL) tasks concurrently. Such an application paradigm poses major challenges due to the huge compute workload of deep neural network models, diverse performance requirements of different tasks, and the lack of real-time support from existing DL frameworks. In this paper, we present RT-mDL, a novel framework to support mixed real-time DL tasks on edge platform with heterogeneous CPU and GPU resource. RT-mDL aims to optimize the mixed DL task execution to meet their diverse real-time/accuracy requirements by exploiting unique compute characteristics of DL tasks. RT-mDL employs a novel storage-bounded model scaling method to generate a series of model variants, and systematically optimizes the DL task execution by joint model variants selection and task priority assignment. To improve the CPU/GPU utilization of mixed DL tasks, RT-mDL also includes a new priority-based scheduler which employs a GPU packing mechanism and executes the CPU/GPU tasks independently. Our implementation on an F1/10 autonomous driving testbed shows that, RT-mDL can enable multiple concurrent DL tasks to achieve satisfactory real-time performance in traffic light detection and sign recognition. Moreover, compared to state-of-the-art baselines, RT-mDL can reduce deadline missing rate by 40.12% while only sacrificing 1.7% model accuracy.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {1–14},
numpages = {14},
keywords = {Real-time Scheduling, Real-time Deep Learning, Edge Computing, Deep Learning System},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1145/3242180,
author = {Roy, Dwaipayan and Mitra, Mandar and Ganguly, Debasis},
title = {To Clean or Not to Clean: Document Preprocessing and Reproducibility},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3242180},
doi = {10.1145/3242180},
abstract = {Web document collections such as WT10G, GOV2, and ClueWeb are widely used for text retrieval experiments. Documents in these collections contain a fair amount of non-content-related markup in the form of tags, hyperlinks, and so on. Published articles that use these corpora generally do not provide specific details about how this markup information is handled during indexing. However, this question turns out to be important: Through experiments, we find that including or excluding metadata in the index can produce significantly different results with standard IR models. More importantly, the effect varies across models and collections. For example, metadata filtering is found to be generally beneficial when using BM25, or language modeling with Dirichlet smoothing, but can significantly reduce retrieval effectiveness if language modeling is used with Jelinek-Mercer smoothing. We also observe that, in general, the performance differences become more noticeable as the amount of metadata in the test collections increase. Given this variability, we believe that the details of document preprocessing are significant from the point of view of reproducibility. In a second set of experiments, we also study the effect of preprocessing on query expansion using RM3. In this case, once again, we find that it is generally better to remove markup before using documents for query expansion.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {18},
numpages = {25},
keywords = {web data, selecting indexable content, relevance feedback, noise, metadata preprocessing, Reproducibility}
}

@article{10.1145/1883612.1883618,
author = {Nie, Changhai and Leung, Hareton},
title = {A survey of combinatorial testing},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/1883612.1883618},
doi = {10.1145/1883612.1883618},
abstract = {Combinatorial Testing (CT) can detect failures triggered by interactions of parameters in the Software Under Test (SUT) with a covering array test suite generated by some sampling mechanisms. It has been an active field of research in the last twenty years. This article aims to review previous work on CT, highlights the evolution of CT, and identifies important issues, methods, and applications of CT, with the goal of supporting and directing future practice and research in this area. First, we present the basic concepts and notations of CT. Second, we classify the research on CT into the following categories: modeling for CT, test suite generation, constraints, failure diagnosis, prioritization, metric, evaluation, testing procedure and the application of CT. For each of the categories, we survey the motivation, key issues, solutions, and the current state of research. Then, we review the contribution from different research groups, and present the growing trend of CT research. Finally, we recommend directions for future CT research, including: (1) modeling for CT, (2) improving the existing test suite generation algorithm, (3) improving analysis of testing result, (4) exploring the application of CT to different levels of testing and additional types of systems, (5) conducting more empirical studies to fully understand limitations and strengths of CT, and (6) combining CT with other testing techniques.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {11},
numpages = {29},
keywords = {test case generation, covering array, combinatorial testing (CT), Software testing}
}

@article{10.1007/s10270-017-0652-3,
author = {Zayan, Dina and Sarkar, Atrisha and Antkiewicz, Micha\l{} and Maciel, Rita Suzana and Czarnecki, Krzysztof},
title = {Example-driven modeling: on effects of using examples on structural model comprehension, what makes them useful, and how to create them},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0652-3},
doi = {10.1007/s10270-017-0652-3},
abstract = {We present a controlled experiment for the empirical evaluation of example-driven modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate (Masters and Ph.D. level) and undergraduate (Bachelor level) students from electrical and computer engineering, computer science, and software engineering programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples on model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model abstractions only, with an improvement of 39% for diagram completeness, 33% for questions completeness, 71% for efficiency, and a reduction in the number of mistakes by 80%. We provide qualitative results showing that participants receiving model abstractions augmented with examples experienced lower perceived difficulty in performing the comprehension tasks, higher perceived confidence in their tasks' solutions, and asked 90% fewer clarifying domain questions. We also present participants' feedback regarding the usefulness of the provided examples, their number and types, as well as the use of partial examples. We present a taxonomy of the different types of examples, explain their significance, and propose guidelines for manual and automatic creation of useful examples.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2213–2239},
numpages = {27},
keywords = {Structural modeling, Software engineering, Example-driven modeling, Empirical study}
}

@inproceedings{10.1007/978-3-642-12116-6_9,
author = {dos Santos, C\'{\i}cero N. and Milidi\'{u}, Ruy L. and Crestana, Carlos E. M. and Fernandes, Eraldo R.},
title = {ETL ensembles for chunking, NER and SRL},
year = {2010},
isbn = {3642121152},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12116-6_9},
doi = {10.1007/978-3-642-12116-6_9},
abstract = {We present a new ensemble method that uses Entropy Guided Transformation Learning (ETL) as the base learner. The proposed approach, ETL Committee, combines the main ideas of Bagging and Random Subspaces. We also propose a strategy to include redundancy in transformation-based models. To evaluate the effectiveness of the ensemble method, we apply it to three Natural Language Processing tasks: Text Chunking, Named Entity Recognition and Semantic Role Labeling. Our experimental findings indicate that ETL Committee significantly outperforms single ETL models, achieving state-of-the-art competitive results. Some positive characteristics of the proposed ensemble strategy are worth to mention. First, it improves the ETL effectiveness without any additional human effort. Second, it is particularly useful when dealing with very complex tasks that use large feature sets. And finally, the resulting training and classification processes are very easy to parallelize.},
booktitle = {Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing},
pages = {100–112},
numpages = {13},
keywords = {text chunking, named entity recognition.semantic role labeling, entropy guided transformation learning, ensemble methods},
location = {Ia\c{s}i, Romania},
series = {CICLing'10}
}

@article{10.1016/j.jvcir.2015.04.005,
author = {Liu, Maofu and Liu, Ya and Hu, Huijun and Nie, Liqiang},
title = {Genetic algorithm and mathematical morphology based binarization method for strip steel defect image with non-uniform illumination},
year = {2016},
issue_date = {May 2016},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {37},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2015.04.005},
doi = {10.1016/j.jvcir.2015.04.005},
abstract = {We proposed EOBMM to reduce the non-uniform illumination in strip steel defect image.We put forward BMBGA to decide the threshold value of defect image binarization.We combined EOBMM and BMBGA to present strip steel defect image binarization method.The proposed binarization method is effective and efficiency in real-time environment. In order to precisely extract the image shape feature for the defect detection and classification, the strip steel image needs to firstly be binarized effectively. In this paper, the intelligent information processing, including mathematical morphology and genetic algorithm, is introduced to the strip steel defect image binarization. In order to eliminate the effect of non-uniform illumination and enhance the detailed information of the strip steel defect image, an enhancement operator based on mathematical morphology (EOBMM) is proposed firstly. And then, the binarization method based on genetic algorithm (BMBGA) is applied to the binarization of the strip steel defect image processed by EOBMM. The experiment results show that our method is effective and efficiency in the strip steel defect image binarization and outperforms the traditional image binarization methods, Otsu and Bernsen.},
journal = {J. Vis. Comun. Image Represent.},
month = may,
pages = {70–77},
numpages = {8},
keywords = {Top-hat transformation, Strip steel defect image, Non-uniform illumination, Mathematical morphology, Image binarization, Genetic operations, Genetic algorithm, Fitness function, EOBMM}
}

@inproceedings{10.1145/3077136.3080819,
author = {Chen, Ruey-Cheng and Gallagher, Luke and Blanco, Roi and Culpepper, J. Shane},
title = {Efficient Cost-Aware Cascade Ranking in Multi-Stage Retrieval},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080819},
doi = {10.1145/3077136.3080819},
abstract = {Complex machine learning models are now an integral part of modern, large-scale retrieval systems. However, collection size growth continues to outpace advances in efficiency improvements in the learning models which achieve the highest effectiveness. In this paper, we re-examine the importance of tightly integrating feature costs into multi-stage learning-to-rank (LTR) IR systems. We present a novel approach to optimizing cascaded ranking models which can directly leverage a variety of different state-of-the-art LTR rankers such as LambdaMART and Gradient Boosted Decision Trees. Using our cascade model, we conclusively show that feature costs and the number of documents being re-ranked in each stage of the cascade can be balanced to maximize both efficiency and effectiveness. Finally, we also demonstrate that our cascade model can easily be deployed on commonly used collections to achieve state-of-the-art effectiveness results while only using a subset of the features required by the full model.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {445–454},
numpages = {10},
keywords = {learning to rank, efficiency-effectiveness tradeoffs, cascade ranking model},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1609/aaai.v33i01.33015541,
author = {Xu, Hongzuo and Wang, Yongjun and Wu, Zhiyue and Wang, Yijie},
title = {Embedding-based complex feature value coupling learning for detecting outliers in non-IID categorical data},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015541},
doi = {10.1609/aaai.v33i01.33015541},
abstract = {Non-IID categorical data is ubiquitous and common in real-world applications. Learning various kinds of couplings has been proved to be a reliable measure when detecting outliers in such non-IID data. However, it is a critical yet challenging problem to model, represent, and utilise high-order complex value couplings. Existing outlier detection methods normally only focus on pairwise primary value couplings and fail to uncover real relations that hide in complex couplings, resulting in suboptimal and unstable performance. This paper introduces a novel unsupervised embedding-based complex value coupling learning framework EMAC and its instance SCAN to address these issues. SCAN first models primary value couplings. Then, coupling bias is defined to capture complex value couplings with different granularities and highlight the essence of outliers. An embedding method is performed on the value network constructed via biased value couplings, which further learns high-order complex value couplings and embeds these couplings into a value representation matrix. Bidirectional selective value coupling learning is proposed to show how to estimate value and object outlierness through value couplings. Substantial experiments show that SCAN (i) significantly outperforms five state-of-the-art outlier detection methods on thirteen real-world datasets; and (ii) has much better resilience to noise than its competitors.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {679},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1145/3473337,
author = {Pan, Yaoxin and Liang, Shangsong and Ren, Jiaxin and Meng, Zaiqiao and Zhang, Qiang},
title = {Personalized, Sequential, Attentive, Metric-Aware Product Search},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3473337},
doi = {10.1145/3473337},
abstract = {The task of personalized product search aims at retrieving a ranked list of products given a user’s input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {36},
numpages = {29},
keywords = {metric learning, LSTM, neural networks, personalized web search, Product search}
}

@article{10.1007/s10618-021-00750-y,
author = {Pang, Guansong and Cao, Longbing and Chen, Ling},
title = {Homophily outlier detection in non-IID categorical data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00750-y},
doi = {10.1007/s10618-021-00750-y},
abstract = {Most of existing outlier detection methods assume that the outlier factors (i.e., outlierness scoring measures) of data entities (e.g., feature values and data objects) are Independent and Identically Distributed (IID). This assumption does not hold in real-world applications where the outlierness of different entities is dependent on each other and/or taken from different probability distributions (non-IID). This may lead to the failure of detecting important outliers that are too subtle to be identified without considering the non-IID nature. The issue is even intensified in more challenging contexts, e.g., high-dimensional data with many noisy features. This work introduces a novel outlier detection framework and its two instances to identify outliers in categorical data by capturing non-IID outlier factors. Our approach first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. It then models an outlierness propagation process in the value graph to learn the outlierness of feature values. The learned value outlierness allows for either direct outlier detection or outlying feature selection. The graph representation and mining approach is employed here to well capture the rich non-IID characteristics. Our empirical results on 15 real-world data sets with different levels of data complexities show that (i) the proposed outlier detection methods significantly outperform five state-of-the-art methods at the 95%/99% confidence level, achieving 10–28% AUC improvement on the 10 most complex data sets; and (ii) the proposed feature selection methods significantly outperform three competing methods in enabling subsequent outlier detection of two different existing detectors.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1163–1224},
numpages = {62},
keywords = {Coupling learning, Random walk, Homophily relation, Categorical data, Non-IID learning, Feature selection, Outlier detection}
}

@article{10.1007/s10032-018-0298-x,
author = {Elanwar, Randa and Qin, Wenda and Betke, Margrit},
title = {Making scanned Arabic documents machine accessible using an ensemble of SVM classifiers},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1–2},
issn = {1433-2833},
url = {https://doi.org/10.1007/s10032-018-0298-x},
doi = {10.1007/s10032-018-0298-x},
abstract = {Raster-image PDF files originating from scanning or photographing paper documents are inaccessible to both text search engines and screen readers that people with visual impairments use. We here focus on the relatively less-researched problem of converting raster-image files with Arabic script into machine-accessible documents. Our method, called ECDP for "Ensemble-based classification of document patches," segments the physical layout of the document, classifies image patches as containing text or graphics, assembles homogeneous document regions, and passes the text to an optical character recognition engine to convert into natural language. Classification is based on the majority voting of an ensemble of support vector machines. When tested on the dataset BCE-Arabic [Saad et al. in: ACM 9th annual international conference on pervasive technologies related to assistive environments (PETRA'16), Corfu, 2016], ECDP yielded an average patch classification accuracy of 97.3% and average $$F_1$$F1 score of 95.26% for text patches and efficiently extracted text zones in both paragraphs and text-embedded graphics, even if the text is rotated by $$90^{circ }$$90\'{z} or is in English. ECDP outperforms a classical layout analysis method (RLSA) and a state-of-the-art commercial product (RDI-CleverPage) on this dataset and maintains a relatively high level of performance on document images drawn from two other datasets (Hesham et al. in Pattern Anal Appl 20:1275---1287, 2017; Proprietary Dataset of 109 Arabic Documents. http://www.rdi-eg.com). The results suggest that the proposed method has the potential to generalize well to the analysis of documents with a broad range of content.},
journal = {Int. J. Doc. Anal. Recognit.},
month = jun,
pages = {59–75},
numpages = {17},
keywords = {Screen readers, Physical layout analysis, Page zone classification, Page layout analysis, Optical character recognition (OCR), Creation of structured meta data, Classifier ensemble, Arabic document analysis}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {question quality, latent semantics, classification, association rules, Community-based question answering}
}

@inproceedings{10.1145/2661136.2661143,
author = {Walkingshaw, Eric and K\"{a}stner, Christian and Erwig, Martin and Apel, Sven and Bodden, Eric},
title = {Variational Data Structures: Exploring Tradeoffs in Computing with Variability},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661143},
doi = {10.1145/2661136.2661143},
abstract = {Variation is everywhere, and in the construction and analysis of customizable software it is paramount. In this context, there arises a need for variational data structures for efficiently representing and computing with related variants of an underlying data type. So far, variational data structures have been explored and developed ad hoc. This paper is a first attempt and a call to action for systematic and foundational research in this area. Research on variational data structures will benefit not only customizable software, but many other application domains that must cope with variability. In this paper, we show how support for variation can be understood as a general and orthogonal property of data types, data structures, and algorithms. We begin a systematic exploration of basic variational data structures, exploring the tradeoffs among different implementations. Finally, we retrospectively analyze the design decisions in our own previous work where we have independently encountered problems requiring variational data structures.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {213–226},
numpages = {14},
keywords = {variation, variability-aware analyses, software product lines, data structures, configurable software},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@inproceedings{10.1145/3132847.3132852,
author = {Liu, Bang and Niu, Di and Lai, Kunfeng and Kong, Linglong and Xu, Yu},
title = {Growing Story Forest Online from Massive Breaking News},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132852},
doi = {10.1145/3132847.3132852},
abstract = {We describe our experience of implementing a news content organization system at Tencent that discovers events from vast streams of breaking news and evolves news story structures in an online fashion. Our real-world system has distinct requirements in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we 1) need to accurately and quickly extract distinguishable events from massive streams of long text documents that cover diverse topics and contain highly redundant information, and 2) must develop the structures of event stories in an online manner, without repeatedly restructuring previously formed stories, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest, a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. We conducted extensive evaluation based on 60 GB of real-world Chinese news data, although our ideas are not language-dependent and can easily be extended to other languages, through detailed pilot user experience studies. The results demonstrate the superior capability of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers, compared to multiple existing algorithm frameworks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {777–785},
numpages = {9},
keywords = {text clustering, online story tree, information retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3383202,
author = {Sarwar, Raheem and Rutherford, Attapol T. and Hassan, Saeed-Ul and Rakthanmanon, Thanawin and Nutanong, Sarana},
title = {Native Language Identification of Fluent and Advanced Non-Native Writers},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3383202},
doi = {10.1145/3383202},
abstract = {Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top-k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors’ classifier on the identified top-k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French, and German. Our experimental studies show that our solution outperforms competitive methods and reports more than 80% accuracy across languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {55},
numpages = {19},
keywords = {text classification, stylometry, native language identification, forensic investigation, Author profiling}
}

@article{10.1007/s00158-019-02288-6,
author = {Oh, Hyunseok and Choi, Hwanoh and Jung, Joon Ha and Youn, Byeng D.},
title = {A robust and convex metric for unconstrained optimization in statistical model calibration—probability residual (PR)},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {60},
number = {3},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-019-02288-6},
doi = {10.1007/s00158-019-02288-6},
abstract = {Statistical model calibration is a practical tool for computational model development processes. However, in optimization-based model calibration, the quality of the calibrated model is often unsatisfactory due to inefficiency and/or inaccuracy of calibration metrics. This paper proposes a new calibration metric, namely, probability residual (PR). PR quantifies the degree of agreement or disagreement between the computational response and experimental results. The PR metric is defined as the sum of the product of a scale factor and the squared residual. First, the scale factor defines the shape of the squared residual to maintain consistent sensitivity during the optimization process. Thus, the number of function evaluations can be reduced. Second, the mathematical form of the squared residuals is used to make convex optimization feasible. Therefore, the existence of a global minimum is guaranteed. To evaluate the performance of the proposed metric, numerical examples are shown in a case study. Various system functions—including linear, non-linear, and elliptical—are incorporated into the statistical model calibration. A case study that examines journal bearing rotor systems is presented to demonstrate the application of the proposed calibration metric to a real-world engineered system.},
journal = {Struct. Multidiscip. Optim.},
month = sep,
pages = {1171–1187},
numpages = {17},
keywords = {Journal bearing rotor system, Validity check, Calibration metric, Statistical model calibration, Computational model}
}

@article{10.14778/3484224.3484226,
author = {Wang, Huayi and Meng, Jingfan and Gong, Long and Xu, Jun and Ogihara, Mitsunori},
title = {MP-RW-LSH: an efficient multi-probe LSH solution to ANNS-L1},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484226},
doi = {10.14778/3484224.3484226},
abstract = {Approximate Nearest Neighbor Search (ANNS) is a fundamental algorithmic problem, with numerous applications in many areas of computer science. Locality-Sensitive Hashing (LSH) is one of the most popular solution approaches for ANNS. A common shortcoming of many LSH schemes is that since they probe only a single bucket in a hash table, they need to use a large number of hash tables to achieve a high query accuracy. For ANNS-L2, a multi-probe scheme was proposed to overcome this drawback by strategically probing multiple buckets in a hash table. In this work, we propose MP-RW-LSH, the first and so far only multi-probe LSH solution to ANNS in L1 distance, and show that it achieves a better tradeoff between scalability and query efficiency than all existing LSH-based solutions. We also explain why a state-of-the-art ANNS-L1 solution called Cauchy projection LSH (CP-LSH) is fundamentally not suitable for multi-probe extension. Finally, as a use case, we construct, using MP-RW-LSH as the underlying "ANNS-L1 engine", a new ANNS-E (E for edit distance) solution that beats the state of the art.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {3267–3280},
numpages = {14}
}

@article{10.1016/j.knosys.2017.03.020,
author = {Akhtar, Md Shad and Gupta, Deepak and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {Feature selection and ensemble construction},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.020},
doi = {10.1016/j.knosys.2017.03.020},
abstract = {In this paper we present a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO) for aspect based sentiment analysis. Aspect based sentiment analysis is performed in two steps, viz. aspect term extraction and sentiment classification. The pruned, compact set of features performs better compared to the baseline model that makes use of the complete set of features for aspect term extraction and sentiment classification. We further construct an ensemble based on PSO, and put it in cascade after the feature selection module. We use the features that are identified based on the properties of different classifiers and domains. As base learning algorithms we use three classifiers, namely Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). Experiments for aspect term extraction and sentiment analysis on two different kinds of domains show the effectiveness of our proposed approach.},
journal = {Know.-Based Syst.},
month = jun,
pages = {116–135},
numpages = {20},
keywords = {Support vector machine, Sentiment analysis, Particle swarm optimization, Maximum entropy, Feature selection, Ensemble, Conditional random field, Aspect term extraction}
}

@article{10.1016/j.future.2019.05.056,
author = {Bagozi, Ada and Bianchini, Devis and De Antonellis, Valeria and Garda, Massimiliano and Marini, Alessandro},
title = {A Relevance-based approach for Big Data Exploration},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.05.056},
doi = {10.1016/j.future.2019.05.056},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {51–69},
numpages = {19},
keywords = {Cyber Physical Systems, Industry 4.0, Human-In-the-Loop Data Analysis, Multi-dimensional data modelling, Big data, Data exploration}
}

@article{10.1109/TASLP.2015.2430815,
author = {Dimitriadis, Dimitrios and Bocchieri, Enrico},
title = {Use of micro-modulation features in large vocabulary continuous speech recognition tasks},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2430815},
doi = {10.1109/TASLP.2015.2430815},
abstract = {Most of the state-of-the-art ASR systems take as input a single type of acoustic features, dominated by the traditional feature schemes, i.e., MFCCs or PLPs. However, these features cannot model rapid, intra-frame phenomena present in the actual speech signals. On the other hand, micro-modulation components, inspired by the AM-FM speech model, can capture these important characteristics of spoken speech, resulting in significant performance improvements, as previously shown in small-vocabulary ASR tasks. Yet, they have limited use in large vocabulary ASR applications, where feature post-processing schemes are usually employed. To enable the successful application of these frequency measures in real-life tasks, we investigate their combination with the traditional Cepstral features when employing linear, e.g., HDA, and nonlinear, i.e., bottleneck neural net (BN), feature transforms. This feature combination is investigated in the context of the hybrid DNN-HMM framework, as well. The experimental results reveal that the integration of micro-modulation and Cepstral features, using neural nets, can greatly improve the ASR performance with respect to using the Cepstral features alone. We apply this novel feature extraction approach on different tasks, i.e., a clean speech task (DARPA-WSJ), the Aurora- 4 task and a real-life, open-vocabulary, mobile search task, the Speak4it, always reporting improved performance, while the obtained relative word error reduction ranges between 7%-21% depending on the task, e.g., a relative WER improvement of 18% for the Speak4it task, and similar improvements, up to 21%, for the WSJ task are reported.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1348–1357},
numpages = {10},
keywords = {speech recognition, speech processing, robustness, neural networks, feature extraction}
}

@inproceedings{10.5555/2964398.2964461,
author = {Mathe, Stefan and Sminchisescu, Cristian},
title = {Dynamic Eye Movement Datasets and Learnt Saliency Models for Visual Action Recognition},
year = {2012},
isbn = {9783642337086},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Systems based on bag-of-words models operating on image features collected at maxima of sparse interest point operators have been extremely successful for both computer-based visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in "saccade and fixate" regimes, the knowledge, methodology, and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large-scale dynamic computer vision datasets like Hollywood-2[1] and UCF Sports[2] with human eye movements collected under the ecological constraints of the visual action recognition task. To our knowledge these are the first massive human eye tracking datasets of significant size to be collected for video 497,107 frames, each viewed by 16 subjects, unique in terms of their a large scale and computer vision relevance, b dynamic, video stimuli, c task control, as opposed to free-viewing. Second, we introduce novel dynamic consistency and alignment models, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the massive amounts of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the most advanced computer vision practice, can lead to state of the art results.},
booktitle = {Proceedings, Part II, of the 12th European Conference on Computer Vision --- ECCV 2012 - Volume 7573},
pages = {842–856},
numpages = {15}
}

@inbook{10.1145/3107990.3107997,
author = {Schuller, Bj\"{o}rn},
title = {Multimodal user state and trait recognition: an overview},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3107990.3107997},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
pages = {129–165},
numpages = {37}
}

@article{10.1016/j.artmed.2011.03.002,
author = {Lee, Joon and Steele, Catriona M. and Chau, Tom},
title = {Classification of healthy and abnormal swallows based on accelerometry and nasal airflow signals},
year = {2011},
issue_date = {May, 2011},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {52},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2011.03.002},
doi = {10.1016/j.artmed.2011.03.002},
abstract = {Background: Dysphagia assessment involves diagnosis of individual swallows in terms of the depth of airway invasion and degree of bolus clearance. The videofluoroscopic swallowing study is the current gold standard for dysphagia assessment but is time-consuming and costly. An ideal alternative would be an automated abnormal swallow detection methodology based on non-invasive signals. Objective: Building upon promising results from single-axis cervical accelerometry, the objective of this study was to investigate the combination of dual-axis accelerometry and nasal airflow for classification of healthy and abnormal swallows in a patient population with dysphagia. Methods: Signals were acquired from 24 adult patients with dysphagia (17.8+/-8.8 swallows per patient). The abnormality of each swallow was quantified using 4-point videofluoroscopic rating scales for its depth of airway invasion, bolus clearance from the valleculae, and bolus clearance from the pyriform sinuses. For each scale, we endeavored to automatically discriminate between the 2 extreme ratings, yielding 3 separate binary classification problems. Various time, frequency, and time-frequency domain features were extracted. A genetic algorithm was deployed for feature selection. Smoothed bootstrapping was utilized to balance the two classes and provide sufficient training data for a multidimensional feature space. Results: A Euclidean linear discriminant classifier resulted in a mean adjusted accuracy of 74.7% for the depth of airway invasion rating, whereas Mahalanobis linear discriminant classifiers yielded mean adjusted accuracies of 83.7% and 84.2% for bolus clearance from the valleculae and pyriform sinuses, respectively. The bolus clearance from the valleculae problem required the lowest feature space dimensionality. Wavelet features were found to be most discriminatory. Conclusions: This exploratory study confirms that dual-axis accelerometry and nasal airflow signals can be used to discriminate healthy and abnormal swallows from patients with dysphagia. The fact that features from all signal channels contributed discriminatory information suggests that multi-sensor fusion is promising in abnormal swallow detection.},
journal = {Artif. Intell. Med.},
month = may,
pages = {17–25},
numpages = {9},
keywords = {Swallowing, Nasal airflow, Dysphagia, Deglutition, Accelerometry, Abnormal swallow detection}
}

@article{10.1016/j.jss.2011.06.004,
author = {Ahmed, Bestoun S. and Zamli, Kamal Z.},
title = {A variable strength interaction test suites generation strategy using Particle Swarm Optimization},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.004},
doi = {10.1016/j.jss.2011.06.004},
abstract = {This paper highlights a novel strategy for generating variable-strength (VS) interaction test suites, called VS Particle Swarm Test Generator (VS-PSTG). As the name suggests, VS-PSTG adopts Particle Swarm Optimization to ensure optimal test size reduction. To determine its efficiency in terms of the size of the generated test suite, VS-PSTG was subjected to well-known benchmark configurations. Comparative results indicate that VS-PSTG gives competitive results as compared to existing strategies. An empirical case study was conducted on a non-trivial software system to show the applicability of the strategy and to determine the effectiveness of the generated test suites to detect faults.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2171–2185},
numpages = {15},
keywords = {Variable-strength interaction, Software testing, Search-based Software Testing, Particle Swarm Optimization, Artificial intelligence}
}

@article{10.1007/s00450-014-0282-8,
author = {Bomhard, Thomas and W\"{o}rner, Dominic and R\"{o}schlin, Marc},
title = {Towards smart individual-room heating for residential buildings},
year = {2016},
issue_date = {August    2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {3},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0282-8},
doi = {10.1007/s00450-014-0282-8},
abstract = {In many homes, residents keep their heating system always turned on although they are out or only occupy certain rooms, and thereby large amounts of energy are wasted. With our work, we aim to build an individual-room heating system that automatically detects occupancy, predicts a schedule based on that, and controls the heaters accordingly. First, we present our technical prototype for individual-room heating control. Second, we show that binary occupancy can be estimated using room climate sensors. We collected room climate data and occupancy data for three rooms over several days. We identified the relevant features and applied a Hidden Markov Model in a supervised and unsupervised way. We achieve a F1-score up to 85 % for both variants in rooms which are occupied for longer periods. Third, we describe how a well-known occupancy prediction approach should be integrated into our heating control for optimal performance.},
journal = {Comput. Sci.},
month = aug,
pages = {127–134},
numpages = {8},
keywords = {Occupancy detection, Individual room heating, Hidden markov model, Heating control}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@inproceedings{10.1145/3208903.3208941,
author = {Mammen, Priyanka Mary and Kumar, Hareesh and Ramamritham, Krithi and Rashid, Haroon},
title = {Want to Reduce Energy Consumption, Whom should we call?},
year = {2018},
isbn = {9781450357678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208903.3208941},
doi = {10.1145/3208903.3208941},
abstract = {Power shortage is a serious issue in developing nations. During periods of high demand, utilities need to motivate the consumers to curtail their consumption for maintaining grid stability and avoiding blackouts or brownouts. Identification of suitable candidates is essential for such events, as the budget set aside by utilities for Demand Response (DR) events for providing incentives to the consumers should not exceed the added production cost due to peaks. Similarly, from the consumers' point of view, participation comes with the compromise to their convenience. Hence, the selection criteria should be such that it minimizes the peaking cost to the utility without affecting consumer comfort.In this paper, we present SmarDeR, a smart DR consumer selection strategy which considers several factors and consolidates them into a single function which can work in different modes to strategically choose the candidates for the DR event based on the goals specified by the utility. We evaluate different policies and metrics for approaching the right consumers for participating in the DR events. Thereby, we can maintain a fair distribution of requests among the most relevant and reliable users. Experiments with smart-meter data from apartments in our campus demonstrates the effectiveness of our SmarDeR approach.},
booktitle = {Proceedings of the Ninth International Conference on Future Energy Systems},
pages = {12–20},
numpages = {9},
keywords = {Smart Meter data, Selection Optimization, Demand Response, Consumer Analysis},
location = {Karlsruhe, Germany},
series = {e-Energy '18}
}

@article{10.1007/s00138-012-0459-8,
author = {Zhang, Yungang and Zhang, Bailing and Coenen, Frans and Lu, Wenjin},
title = {Breast cancer diagnosis from biopsy images with highly reliable random subspace classifier ensembles},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {7},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-012-0459-8},
doi = {10.1007/s00138-012-0459-8},
abstract = {Accurate and reliable classification of microscopic biopsy images is an important issue in computer assisted breast cancer diagnosis. In this paper, a new cascade Random Subspace ensembles scheme with reject options is proposed for microscopic biopsy image classification. The classification system is built as a serial fusion of two different Random Subspace classifier ensembles with rejection options to enhance the classification reliability. The first ensemble consists of a set of Support Vector Machine classifiers that converts the original  $$K$$ -class classification problem into a number of  $$K$$  2-class problems. The second ensemble consists of a Multi-Layer Perceptron ensemble, that focuses on the rejected samples from the first ensemble. For both of the ensembles, the reject option is implemented by relating the consensus degree from majority voting to a confidence measure, and abstaining to classify ambiguous samples if the consensus degree is lower than some threshold. We also investigated the effectiveness of a feature description approach by combining Local Binary Pattern (LBP) texture analysis, statistics derived using the Gray Level Co-occurrence Matrix (GLCM) and the Curvelet Transform. While the LBP analysis efficiently describes local texture properties and the GLCM reflects global texture statistics, the Curvelet Transform is particularly appropriate for the representation of piece-wise smooth images with rich edge information. The combined feature description thus provides a comprehensive biopsy image characterization by taking advantages of their complementary strengths. Using a benchmark microscopic biopsy image dataset, obtained from the Israel Institute of Technology, a high classification accuracy of  $$99.25 %$$  was obtained (with a rejection rate of  $$1.94 %$$ ) using the proposed system.},
journal = {Mach. Vision Appl.},
month = oct,
pages = {1405–1420},
numpages = {16},
keywords = {Reject option, Random subspace ensemble, Combined feature, Breast cancer diagnosis, Biopsy image}
}

@inproceedings{10.1145/2786805.2786809,
author = {Lo, David and Nagappan, Nachiappan and Zimmermann, Thomas},
title = {How practitioners perceive the relevance of software engineering research},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786809},
doi = {10.1145/2786805.2786809},
abstract = {The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {415–425},
numpages = {11},
keywords = {Survey, Software Engineering Research, Industry},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2911451.2911517,
author = {Liu, Yiqun and Liu, Zeyang and Zhou, Ke and Wang, Meng and Luan, Huanbo and Wang, Chao and Zhang, Min and Ma, Shaoping},
title = {Predicting Search User Examination with Visual Saliency},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911517},
doi = {10.1145/2911451.2911517},
abstract = {Predicting users' examination of search results is one of the key concerns in Web search related studies. With more and more heterogeneous components federated into search engine result pages (SERPs), it becomes difficult for traditional position-based models to accurately predict users' actual examination patterns. Therefore, a number of prior works investigate the connection between examination and users' explicit interaction behaviors (e.g.~click-through, mouse movement). Although these works gain much success in predicting users' examination behavior on SERPs, they require the collection of large scale user behavior data, which makes it impossible to predict examination behavior on newly-generated SERPs. To predict user examination on SERPs containing heterogenous components without user interaction information, we propose a new prediction model based on visual saliency map and page content features. Visual saliency, which is designed to measure the likelihood of a given area to attract human visual attention, is used to predict users' attention distribution on heterogenous search components. With an experimental search engine, we carefully design a user study in which users' examination behavior (eye movement) is recorded. Examination prediction results based on this collected data set demonstrate that visual saliency features significantly improve the performance of examination model in heterogeneous search environments. We also found that saliency features help predict internal examination behavior within vertical results.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {619–628},
numpages = {10},
keywords = {eye tracking, user behavior analysis, visual saliency},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1016/j.cad.2012.08.006,
author = {Chandrasegaran, Senthil K. and Ramani, Karthik and Sriram, Ram D. and Horv\'{a}Th, Imr\'{e} and Bernard, Alain and Harik, Ramy F. and Gao, Wei},
title = {The evolution, challenges, and future of knowledge representation in product design systems},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {45},
number = {2},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2012.08.006},
doi = {10.1016/j.cad.2012.08.006},
abstract = {Product design is a highly involved, often ill-defined, complex and iterative process, and the needs and specifications of the required artifact get more refined only as the design process moves toward its goal. An effective computer support tool that helps the designer make better-informed decisions requires efficient knowledge representation schemes. In today's world, there is a virtual explosion in the amount of raw data available to the designer, and knowledge representation is critical in order to sift through this data and make sense of it. In addition, the need to stay competitive has shrunk product development time through the use of simultaneous and collaborative design processes, which depend on effective transfer of knowledge between teams. Finally, the awareness that decisions made early in the design process have a higher impact in terms of energy, cost, and sustainability, has resulted in the need to project knowledge typically required in the later stages of design to the earlier stages. Research in design rationale systems, product families, systems engineering, and ontology engineering has sought to capture knowledge from earlier product design decisions, from the breakdown of product functions and associated physical features, and from customer requirements and feedback reports. VR (Virtual reality) systems and multidisciplinary modeling have enabled the simulation of scenarios in the manufacture, assembly, and use of the product. This has helped capture vital knowledge from these stages of the product life and use it in design validation and testing. While there have been considerable and significant developments in knowledge capture and representation in product design, it is useful to sometimes review our position in the area, study the evolution of research in product design, and from past and current trends, try and foresee future developments. The goal of this paper is thus to review both our understanding of the field and the support tools that exist for the purpose, and identify the trends and possible directions research can evolve in the future.},
journal = {Comput. Aided Des.},
month = feb,
pages = {204–228},
numpages = {25},
keywords = {Virtual reality, Systems engineering, Simulation, Product design, Ontology, Multidisciplinary modeling, Knowledge representation, Knowledge management, Knowledge capture, Design rationale, Computational tools, Collaborative engineering}
}

@article{10.1016/j.asoc.2016.12.022,
author = {Sahoo, Anita and Chandra, Satish},
title = {Multi-objective Grey Wolf Optimizer for improved cervix lesion classification},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.12.022},
doi = {10.1016/j.asoc.2016.12.022},
abstract = {This is a novel effort towards effective characterization of cervix lesions from CECT images.Two different approaches have been adopted for designing multi-objective binary GWO algorithms.For the utilized cases, Non-dominated Sorting based GWO dominates the other meta-heuristics based methods compared with.Cervix lesions are up to 91.1% accurately classified as benign and malignant with only five features selected by NSGWO.Efficiency of NSGWO is further verified on high-dimensional microarray gene expression datasets available online. Cervical cancer is one of the vital and most frequent cancers, but can be cured effectively if diagnosed in the early stage. This is a novel effort towards effective characterization of cervix lesions from contrast enhanced CT-Scan images to provide a reliable and objective discrimination between benign and malignant lesions. Performance of such classification models mostly depends on features used to represent samples in a training dataset. Selection of optimal feature subset here is NP-hard; where, randomized algorithms do better. In this paper, Grey Wolf Optimizer (GWO), which is a population based meta-heuristic inspired by the leadership hierarchy and hunting mechanism of grey wolves has been utilized for feature selection. The traditional GWO is applicable for continuous single objective optimization problems. Since, feature selection is inherently multi-objective; this paper proposes two different approaches for multi-objective binary GWO algorithms. One is a scalarized approach to multi-objective GWO (MOGWO) and the other is a Non-dominated Sorting based GWO (NSGWO). These are used for wrapper based feature selection that selects optimal textural feature subset for improved classification of cervix lesions. For experiments, contrast enhanced CT-Scan (CECT) images of 62 patients have been used, where all lesions had been recommended for surgical biopsy by specialist. Gray-level co-occurrence matrix based texture features are extracted from two-level decomposition of wavelet coefficients of cervix regions extracted from CECT images. The results of proposed approaches are compared with mostly used meta-heuristics such as genetic algorithm (GA) and firefly algorithm (FA) for multi-objective optimization. With better diversification and intensification, GWO obtains Pareto solutions, which dominate the solutions obtained by GA and FA when assessed on the utilized cervix lesion cases. Cervix lesions are up to 91% accurately classified as benign and malignant with only five features selected by NSGWO. A two-tailed t-test was conducted by hypothesizing the mean F-score obtained by the proposed NSGWO method at significance level=0.05. This confirms that NSGWO performs significantly better than other methods for the real cervix lesion dataset in hand. Further experiments were conducted on high dimensional microarray gene expression datasets collected online. The results demonstrate that the proposed method performs significantly better than other methods selecting relevant genes for high-dimensional, multi-category cancer diagnosis with an average of 12.82% improvement in F-score value.},
journal = {Appl. Soft Comput.},
month = mar,
pages = {64–80},
numpages = {17},
keywords = {Wrapper based feature selection, Wavelet features, Textural features, Multi-objective optimization, Grey Wolf Optimizer, Feature selection, Classification, Cervix lesion characterization}
}

@article{10.1145/3362063,
author = {Zhang, Rui and Stanley, Kevin G. and Fuller, Daniel and Bell, Scott},
title = {Differentiating Population Spatial Behavior Using Representative Features of Geospatial Mobility (ReFGeM)},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3362063},
doi = {10.1145/3362063},
abstract = {Understanding how humans use and consume space by comparing stratified groups, either through observation or controlled study, is key to designing better spaces, cities, and policies. GPS data traces provide detailed movement patterns of individuals but can be difficult to interpret due to the scale and scope of the data collected. For actionable insights, GPS traces are usually reduced to one or more features that express the spatial phenomenon of interest. However, it is not always clear which spatial features should be employed, and substantial effort can be invested into designing features that may or may not provide insight. In this article, we present an alternative approach: a standardized feature set with actionable interpretations that can be efficiently run against many datasets. We show that these features can distinguish between disparate human mobility patterns, although no single feature can distinguish them alone.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = feb,
articleno = {2},
numpages = {25},
keywords = {spatial behavior, fractal dimension, features, entropy rate, activity space, Datasets}
}

@article{10.1016/j.compbiomed.2016.02.012,
author = {Ding, Hui and Liang, Zhi-Yong and Guo, Feng-Biao and Huang, Jian and Chen, Wei and Lin, Hao},
title = {Predicting bacteriophage proteins located in host cell with feature selection technique},
year = {2016},
issue_date = {April 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {71},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.02.012},
doi = {10.1016/j.compbiomed.2016.02.012},
abstract = {A bacteriophage is a virus that can infect a bacterium. The fate of an infected bacterium is determined by the bacteriophage proteins located in the host cell. Thus, reliably identifying bacteriophage proteins located in the host cell is extremely important to understand their functions and discover potential anti-bacterial drugs. Thus, in this paper, a computational method was developed to recognize bacteriophage proteins located in host cells based only on their amino acid sequences. The analysis of variance (ANOVA) combined with incremental feature selection (IFS) was proposed to optimize the feature set. Using a jackknife cross-validation, our method can discriminate between bacteriophage proteins located in a host cell and the bacteriophage proteins not located in a host cell with a maximum overall accuracy of 84.2%, and can further classify bacteriophage proteins located in host cell cytoplasm and in host cell membranes with a maximum overall accuracy of 92.4%. To enhance the value of the practical applications of the method, we built a web server called PHPred ({http://lin.uestc.edu.cn/server/PHPred}). We believe that the PHPred will become a powerful tool to study bacteriophage proteins located in host cells and to guide related drug discovery. Display Omitted Novel analytical method is developed to predict the phage proteins located in host cell.A significant feature selection technique is proposed and used to optimize features of proteinsA powerful predictor is constructed to identify phage proteins distribution in host cell.},
journal = {Comput. Biol. Med.},
month = apr,
pages = {156–161},
numpages = {6},
keywords = {g-gap dipeptide, Feature analysis, Bacteriophage proteins, Analysis of variance}
}

@article{10.1016/j.artmed.2010.04.004,
author = {Iakovidis, Dimitris K. and Keramidas, Eystratios G. and Maroulis, Dimitris},
title = {Fusion of fuzzy statistical distributions for classification of thyroid ultrasound patterns},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {50},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2010.04.004},
doi = {10.1016/j.artmed.2010.04.004},
abstract = {Objective: This paper proposes a novel approach for thyroid ultrasound pattern representation. Considering that texture and echogenicity are correlated with thyroid malignancy, the proposed approach encodes these sonographic features via a noise-resistant representation. This representation is suitable for the discrimination of nodules of high malignancy risk from normal thyroid parenchyma. Materials and methods: The material used in this study includes a total of 250 thyroid ultrasound patterns obtained from 75 patients in Greece. The patterns are represented by fused vectors of fuzzy features. Ultrasound texture is represented by fuzzy local binary patterns, whereas echogenicity is represented by fuzzy intensity histograms. The encoded thyroid ultrasound patterns are discriminated by support vector classifiers. Results: The proposed approach was comprehensively evaluated using receiver operating characteristics (ROCs). The results show that the proposed fusion scheme outperforms previous thyroid ultrasound pattern representation methods proposed in the literature. The best classification accuracy was obtained with a polynomial kernel support vector machine, and reached 97.5% as estimated by the area under the ROC curve. Conclusions: The fusion of fuzzy local binary patterns and fuzzy grey-level histogram features is more effective than the state of the art approaches for the representation of thyroid ultrasound patterns and can be effectively utilized for the detection of nodules of high malignancy risk in the context of an intelligent medical system.},
journal = {Artif. Intell. Med.},
month = sep,
pages = {33–41},
numpages = {9},
keywords = {Ultrasound imaging, Thyroid nodules, Support vector classification, Fuzzy feature extraction}
}

@inproceedings{10.1145/3134263.3134264,
author = {Peng, Yao and Ye, Hao and Lin, Yining and Bao, Yixin and Zhao, Zhijian and Qiu, Haonan and Lu, Yao and Wang, Li and Zheng, Yingbin},
title = {Large-Scale Video Classification with Elastic Streaming Sequential Data Processing System},
year = {2017},
isbn = {9781450355377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134263.3134264},
doi = {10.1145/3134263.3134264},
abstract = {Videos are dominant on the Internet. Current systems to process large-scale videos are suboptimal due to the following reasons: (1) machine learning modules such as feature extractors and classifiers generate huge intermediate data and place heavy burden to the storage and network, and (2) task scheduling is explicit; manually configuring the machine learning modules on the cluster is tedious and inefficient. In this work, we propose Elastic Streaming Sequential data Processing system (ESSP) that supports automatic task scheduling; multiple machine learning components are automatically parallelized. Further, our system prevents extensive disc I/O by applying the in-memory dataflow scheme. Evaluation on real-world video classification datasets shows many-fold improvements.},
booktitle = {Proceedings of the Workshop on Large-Scale Video Classification Challenge},
pages = {1–7},
numpages = {7},
keywords = {two-stream network, large-scale video classification, elastic streaming sequential data processing system},
location = {Mountain View, California, USA},
series = {LSVC '17}
}

@inproceedings{10.5555/1770770.1770778,
author = {Kanan, Hamidreza Rashidy and Faez, Karim and Taheri, Sayyed Mostafa},
title = {Feature selection using ant colony optimization (ACO): a new method and comparative study in the application of face recognition system},
year = {2007},
isbn = {9783540734345},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature Selection (FS) and reduction of pattern dimensionality is a most important step in pattern recognition systems. One approach in the feature selection area is employing population-based optimization algorithms such as Genetic Algorithm (GA)-based method and Ant Colony Optimization (ACO)- based method. This paper presents a novel feature selection method that is based on Ant Colony Optimization (ACO). ACO algorithm is inspired of ant's social behavior in their search for the shortest paths to food sources. Most common techniques for ACO-Based feature selection use the priori information of features. However, in the proposed algorithm, classifier performance and the length of selected feature vector are adopted as heuristic information for ACO. So, we can select the optimal feature subset without the priori information of features. This approach is easily implemented and because of using one simple classifier in it, its computational complexity is very low. Simulation results on face recognition system and ORL database show the superiority of the proposed algorithm.},
booktitle = {Proceedings of the 7th Industrial Conference on Advances in Data Mining: Theoretical Aspects and Applications},
pages = {63–76},
numpages = {14},
keywords = {genetic algorithm, feature selection, face recognition, ant colony optimization (ACO)},
location = {Leipzig, Germany},
series = {ICDM'07}
}

@article{10.1145/3478129,
author = {Kakaraparthi, Vimal and Shao, Qijia and Carver, Charles J. and Pham, Tien and Bui, Nam and Nguyen, Phuc and Zhou, Xia and Vu, Tam},
title = {FaceSense: Sensing Face Touch with an Ear-worn System},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478129},
doi = {10.1145/3478129},
abstract = {Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one's face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {110},
numpages = {27},
keywords = {thermo-physiological sensing, multimodal deep learning, face touch detection}
}

@inproceedings{10.1145/3266302.3266304,
author = {Huang, Jian and Li, Ya and Tao, Jianhua and Lian, Zheng and Niu, Mingyue and Yang, Minghao},
title = {Multimodal Continuous Emotion Recognition with Data Augmentation Using Recurrent Neural Networks},
year = {2018},
isbn = {9781450359832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266302.3266304},
doi = {10.1145/3266302.3266304},
abstract = {This paper presents our effects for Cross-cultural Emotion Sub-challenge in the Audio/Visual Emotion Challenge (AVEC) 2018, whose goal is to predict the level of three emotional dimensions time-continuously in a cross-cultural setup. We extract the emotional features from audio, visual and textual modalities. The state of art regressor for continuous emotion recognition, long short term memory recurrent neural network (LSTM-RNN) is utilized. We augment the training data by replacing the original training samples with shorter overlapping samples extracted from them, thus multiplying the number of training samples and also beneficial to train emotional temporal model with LSTM-RNN. In addition, two strategies are explored to decrease the interlocutor influence to improve the performance. We also compare the performance of feature level fusion and decision level fusion. The experimental results show the efficiency of the proposed method and competitive results are obtained.},
booktitle = {Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop},
pages = {57–64},
numpages = {8},
keywords = {multimodal fusion, lstm-rnn, interlocutor influence, data augmentation, continuous emotion recognition},
location = {Seoul, Republic of Korea},
series = {AVEC'18}
}

@article{10.1016/j.neucom.2016.03.084,
title = {On interactive learning-to-rank for IR},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {208},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.03.084},
doi = {10.1016/j.neucom.2016.03.084},
abstract = {With the amount and variety of information available on digital repositories, answering complex user needs and personalizing information access became a hard task. Putting the user in the retrieval loop has emerged as a reasonable alternative to enhance search effectiveness and consequently the user experience. Due to the great advances on machine learning techniques, optimizing search engines according to user preferences has attracted great attention from the research and industry communities. Interactively learning-to-rank has greatly evolved over the last decade but it still faces great theoretical and practical obstacles. This paper describes basic concepts and reviews state-of-the-art methods on the several research fields that complementarily support the creation of interactive information retrieval (IIR) systems. By revisiting ground concepts and gathering recent advances, this article also intends to foster new research activities on IIR by highlighting great challenges and promising directions. The aggregated knowledge provided here is intended to work as a comprehensive introduction to those interested in IIR development, while also providing important insights on the vast opportunities of novel research.},
journal = {Neurocomput.},
month = oct,
pages = {3–24},
numpages = {22}
}

@inproceedings{10.1145/2093256.2093271,
author = {Puschmann, Andr\'{e} and Kalil, Mohamed A. and Mitschele-Thiel, Andreas},
title = {Implementation and evaluation of a practical SDR testbed},
year = {2011},
isbn = {9781450309127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093256.2093271},
doi = {10.1145/2093256.2093271},
abstract = {Software defined radios provide a perfect playground for researchers to experiment with current and future wireless communication systems. However, the benefits, especially for communication protocols, largely depend on non-functional requirements (i.e. temporal behavior) that have to be met by the system architecture. In this paper, we share our experience with a flexible SDR framework named Iris. We present an implementation of a Send-and-Wait Automatic Repeat reQuest (ARQ) protocol as well as a virtual network driver component which, in conjunction with Iris, form the core of our software defined radio networking testbed. To verify the practical suitability of the testbed and to evaluate the performance of the protocol, we are conducting end-to-end throughput and delay benchmarks.},
booktitle = {Proceedings of the 4th International Conference on Cognitive Radio and Advanced Spectrum Management},
articleno = {15},
numpages = {5},
keywords = {software radio, latency, cognitive radio, benchmark, USRP2, Iris},
location = {Barcelona, Spain},
series = {CogART '11}
}

@article{10.3233/FI-2018-1685,
author = {Sandhu, Jasminder Kaur and Verma, Anil Kumar and Rana, Prashant Singh},
title = {A Novel Framework for Reliable Network Prediction of Small Scale Wireless Sensor Networks (SSWSNs)},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {160},
number = {3},
issn = {0169-2968},
url = {https://doi.org/10.3233/FI-2018-1685},
doi = {10.3233/FI-2018-1685},
abstract = {In Small Scale Wireless Sensor Networks (SSWSNs), reliability is defined as the capability of a network to perform its intended task under certain conditions for a stated time span. There are many tools for modeling and analyzing the reliability of a network. As the intricacy of various networks is increasing, there is a need for many sophisticated methods for reliability analysis. The term reliability is used as an umbrella term to capture various attributes such as safety, availability, security, and ease of use. The existing methods have many shortcomings which include inadequacy of a novel framework and inefficacy to handle scalable networks. This paper presents a novel framework which predicts the overall reliability of the SSWSNs in terms of performance metrics such as, sent packets, received packets, packets forfeit, packet delivery ratio and throughput. This framework includes various phases starting with scenario generation, construction of a dataset, applying ensemble based machine learning techniques to predict the parameters which cannot be calculated. The ensemble model predicts with an optimum accuracy of 99.9% for data flow, 99.9% for the protocol used and 97.6% for the number of nodes. Finally, to check the robustness of the ensemble model 10-fold cross-validation is used. The dataset used in this work is available as a supplement at .},
journal = {Fundam. Inf.},
month = jan,
pages = {303–341},
numpages = {39},
keywords = {Ensemble, Network Prediction, Machine Learning, Reliability, Small Scale Wireless Sensor Networks}
}

@article{10.1007/s11042-016-3618-5,
author = {Acar, Esra and Hopfgartner, Frank and Albayrak, Sahin},
title = {A comprehensive study on mid-level representation and ensemble learning for emotional analysis of video material},
year = {2017},
issue_date = {May       2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {9},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-016-3618-5},
doi = {10.1007/s11042-016-3618-5},
abstract = {In today's society where audio-visual content such as professionally edited and user-generated videos is ubiquitous, automatic analysis of this content is a decisive functionality. Within this context, there is an extensive ongoing research about understanding the semantics (i.e., facts) such as objects or events in videos. However, little research has been devoted to understanding the emotional content of the videos. In this paper, we address this issue and introduce a system that performs emotional content analysis of professionally edited and user-generated videos. We concentrate both on the representation and modeling aspects. Videos are represented using mid-level audio-visual features. More specifically, audio and static visual representations are automatically learned from raw data using convolutional neural networks (CNNs). In addition, dense trajectory based motion and SentiBank domain-specific features are incorporated. By means of ensemble learning and fusion mechanisms, videos are classified into one of predefined emotion categories. Results obtained on the VideoEmotion dataset and a subset of the DEAP dataset show that (1) higher level representations perform better than low-level features, (2) among audio features, mid-level learned representations perform better than mid-level handcrafted ones, (3) incorporating motion and domain-specific information leads to a notable performance gain, and (4) ensemble learning is superior to multi-class support vector machines (SVMs) for video affective content analysis.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {11809–11837},
numpages = {29},
keywords = {Video affective content analysis, SentiBank, MFCC, Ensemble learning, Dense trajectories, Deep learning, Color}
}

@article{10.1016/j.patcog.2015.03.010,
author = {Lee, Min-Ho and Fazli, Siamac and Mehnert, Jan and Lee, Seong-Whan},
title = {Subject-dependent classification for robust idle state detection using multi-modal neuroimaging and data-fusion techniques in BCI},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {8},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.03.010},
doi = {10.1016/j.patcog.2015.03.010},
abstract = {Brain-computer interfaces (BCIs) allow users to control external devices by their intentions. Currently, most BCI systems are synchronous. They rely on cues or tasks to which a subject has to react. In order to design an asynchronous BCI one needs to be able to robustly detect an idle class. In this study, we examine whether multi-modal neuroimaging, based on simultaneous EEG and near-infrared spectroscopy (NIRS) measurements, can assist in the robust detection of the idle class within a sensory motor rhythm-based BCI paradigm. We propose two types of subject-dependent classification strategies to combine the information of both modalities. Our results demonstrate that not only idle-state decoding can be significantly improved by exploiting the complementary information of multi-modal recordings, but also it is possible to minimize the delay of the system, caused by the slow inherent hemodynamic response of the NIRS signal. HighlightsEEG-NIRS measurements can robustly detect the idle class.An activation function to reduce hemodynamic response delays in NIRS is proposed.A novel hybrid classification strategy combining EEG-NIRS classifiers is proposed.Superior performance by complementary information of multimodal recordings.},
journal = {Pattern Recogn.},
month = aug,
pages = {2725–2737},
numpages = {13},
keywords = {Subject-dependent classification, Hybrid brain-computer interfacing, Combined EEG-NIRS, Classifier combination}
}

@article{10.1287/msom.2019.0855,
author = {Zhang, Yiling and Lu, Mengshi and Shen, Siqian},
title = {On the Values of Vehicle-to-Grid Electricity Selling in Electric Vehicle Sharing},
year = {2021},
issue_date = {March–April 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {23},
number = {2},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2019.0855},
doi = {10.1287/msom.2019.0855},
abstract = {Problem definition: We study electric vehicle (EV) sharing systems and explore the opportunity for incorporating vehicle-to-grid (V2G) electricity selling in EV sharing. Academic/practical relevance: The problem involves complex planning and operational decisions, as well as multiple sources of uncertainties. The related optimization models impose significant computational challenges. The potential value of V2G integration may have far-reaching impacts on EV sharing and sustainability. Methodology: We formulate the problem as a two-stage stochastic integer linear program. In the first stage, we optimize decisions related to service planning, the capacity of parking and charging facilities, EV battery capacities, and EV allocation in each zone under uncertain time-dependent trip demand and electricity prices. In the second stage, for a realized demand–price scenario, we construct a time-and-charging-status expanded transportation network and optimize operations of the shared vehicle fleet, EV battery charging, and V2G selling. We develop Benders decomposition and scenario decomposition approaches to improve computational efficiency. A linear-decision-rule-based approximation approach is also provided to model dynamic operations. Results: Via testing instances based on real-world and synthetic data, we demonstrate the computational efficacy of our approaches and study the benefits of integrating V2G in EV sharing from the service provider, consumer, and socioenvironmental aspects. Managerial implications: V2G integration can significantly increase the profitability of EV sharing and the quality of service. It results in the preference of larger EV fleets and battery capacities, which further leads to various socioenvironmental benefits. The benefit of V2G can still prevail, even with more severe battery degradation and can be more significant when combined with (i) more stringent service levels, (ii) more traffic congestion, or (iii) urban spatial structures with concentrated business/residential areas. V2G integration (complemented by fast charging technology) can also benefit carshare users through improvement in the quality of service.},
journal = {Manufacturing &amp; Service Operations Management},
month = mar,
pages = {488–507},
numpages = {20},
keywords = {decomposition algorithms, stochastic integer programming, vehicle-to-grid, electric vehicle sharing, energy systems, sustainable transportation}
}

@article{10.1016/j.dsp.2017.12.012,
author = {Waldekar, Shefali and Saha, Goutam},
title = {Classification of audio scenes with novel features in a fused system framework},
year = {2018},
issue_date = {Apr 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {75},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2017.12.012},
doi = {10.1016/j.dsp.2017.12.012},
journal = {Digit. Signal Process.},
month = apr,
pages = {71–82},
numpages = {12},
keywords = {SCFC, Machine listening, Fusion, Environmental sounds, CQCC, Block-based MFCC}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Quality assurance, Test artifact quality, Test suite quality, Test case quality, Software testing}
}

@article{10.1007/s11263-016-0961-y,
author = {Hughes, Benjamin and Burghardt, Tilo},
title = {Automated Visual Fin Identification of Individual Great White Sharks},
year = {2017},
issue_date = {May       2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {122},
number = {3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-016-0961-y},
doi = {10.1007/s11263-016-0961-y},
abstract = {This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {542–557},
numpages = {16},
keywords = {Textureless object recognition, Shape analysis, Animal biometrics}
}

@article{10.1016/j.jss.2009.06.032,
author = {Fung, Kam Hay and Low, Graham Cedric},
title = {Methodology evaluation framework for dynamic evolution in composition-based distributed applications},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.032},
doi = {10.1016/j.jss.2009.06.032},
abstract = {Dynamic evolution can be used to upgrade distributed applications without shutdown and restart as a way of improving service levels while minimising the loss of business revenue caused by the downtime. An evaluation framework assessing the level of support offered by existing methodologies in composition-based application (e.g. component-based and service-oriented) development is proposed. It was developed by an analysis of the literature and existing methodologies together with a refinement based on a survey of experienced practitioners and researchers. The use of the framework is demonstrated by applying it to twelve methodologies to assess their support for dynamic evolution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1950–1965},
numpages = {16},
keywords = {Service-oriented computing, Method engineering, Feature analysis, Evaluation framework, Dynamic evolution, Composition-based applications}
}

@inproceedings{10.1145/3133944.3133946,
author = {Huang, Jian and Li, Ya and Tao, Jianhua and Lian, Zheng and Wen, Zhengqi and Yang, Minghao and Yi, Jiangyan},
title = {Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network},
year = {2017},
isbn = {9781450355025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133944.3133946},
doi = {10.1145/3133944.3133946},
abstract = {The continuous dimensional emotion can depict subtlety and complexity of emotional change, which is an inherently challenging problem with growing attention. This paper presents our automatic prediction of dimensional emotional state for Audio-Visual Emotion Challenge (AVEC 2017), which uses multi-features and fusion across all available modalities. Besides the baseline features provided by the organizers, we also extract other acoustic audio feature sets, appearance features and deep visual features as complementary features. Each type of feature is trained using Long Short-Term Memory Recurrent Neutral Network (LSTM-RNN) for every dimensional emotion prediction separately considering annotation delay and temporal pooling. To overcome overfitting problem, robust models are chosen carefully for individual model. Finally, multimodal emotion fusion is achieved by utilizing Support Vector Regression (SVR) with the estimates from different feature sets in decision level fusion. The experimental results indicate that our extracted features are beneficial to performance improvement and our system design achieves very promising results with Concordant Correlation Coefficient (CCC), which outperform the baseline system on the testing set for arousal of 0.599 vs 0.375 (baseline) and for valence of 0.721 vs 0.466 and for liking 0.295 vs 0.246.},
booktitle = {Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge},
pages = {11–18},
numpages = {8},
keywords = {temporal pooling, overfitting, multimodal fusion, lstm-rnn, dimensional emotion recognition, delay},
location = {Mountain View, California, USA},
series = {AVEC '17}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Digital library, Replication, Threats to validity, Software engineering, Systematic literature review, Tertiary study}
}

@article{10.1016/j.knosys.2019.105114,
author = {Denham, Benjamin and Pears, Russel and Naeem, M. Asif},
title = {HDSM: A distributed data mining approach to classifying vertically distributed data streams},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {189},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105114},
doi = {10.1016/j.knosys.2019.105114},
journal = {Know.-Based Syst.},
month = feb,
numpages = {19},
keywords = {Online classification, Vertically-distributed data, Distributed data stream mining}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1145/3241383,
author = {Avrahami, Daniel and Patel, Mitesh and Yamaura, Yusuke and Kratz, Sven and Cooper, Matthew},
title = {Unobtrusive Activity Recognition and Position Estimation for Work Surfaces Using RF-Radar Sensing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3241383},
doi = {10.1145/3241383},
abstract = {Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users’ privacy concerns associated with cameras and is useful for a wide range of intelligent systems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {11},
numpages = {28},
keywords = {sensing, retail, radio frequency radar sensor, deskwork, IMU, Activity recognition}
}

@article{10.5555/2772708.2772719,
author = {Chakrabarti, Abhirup and Mitchell, Will},
title = {The Persistent Effect of Geographic Distance in Acquisition Target Selection},
year = {2013},
issue_date = {December 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {24},
number = {6},
issn = {1526-5455},
abstract = {Valuable resources often exist at distant points from a firm's current locations, with the result that strategic decisions such as growth have a spatial dimension in which firms seek information and choose between geographically distributed alternatives. Studies show that geographic proximity facilitates the flow of resources, but there is limited understanding of factors that exacerbate or ease the impact of geographic distance when firms seek new resources. This paper argues that the difficulty of search increases with distance, particularly when search involves greater information processing, but that firms can partially overcome the constraints of distance with direct, contextual, and vicarious learning. We study 2,070 domestic acquisition announcements by U.S. chemical manufacturers founded after 1979. The results demonstrate the persistent effect of spatial geography on organizational search processes.},
journal = {Organization Science},
month = dec,
pages = {1805–1826},
numpages = {22},
keywords = {target selection, mergers and acquisitions, geographic distance, business relatedness}
}

@article{10.1007/s10115-020-01467-y,
author = {Chelly&nbsp;Dagdia, Zaineb and Zarges, Christine and Beck, Ga\"{e}l and Lebbah, Mustapha},
title = {A scalable and effective rough set theory-based approach for big data pre-processing},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {8},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01467-y},
doi = {10.1007/s10115-020-01467-y},
abstract = {A big challenge in the knowledge discovery process is to perform data pre-processing, specifically feature selection, on a large amount of data and high dimensional attribute set. A variety of techniques have been proposed in the literature to deal with this challenge with different degrees of success as most of these techniques need further information about the given input data for thresholding, need to specify noise levels or use some feature ranking procedures. To overcome these limitations, rough set theory (RST) can be used to discover the dependency within the data and reduce the number of attributes enclosed in an input data set while using the data alone and requiring no supplementary information. However, when it comes to massive data sets, RST reaches its limits as it is highly computationally expensive. In this paper, we propose a scalable and effective rough set theory-based approach for large-scale data pre-processing, specifically for feature selection, under the Spark framework. In our detailed experiments, data sets with up to 10,000 attributes have been considered, revealing that our proposed solution achieves a good speedup and performs its feature selection task well without sacrificing performance. Thus, making it relevant to big data.},
journal = {Knowl. Inf. Syst.},
month = aug,
pages = {3321–3386},
numpages = {66},
keywords = {High-performance computing, Scalability, Distributed processing, Rough set theory, Data pre-processing, Big data}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@article{10.1007/s10590-019-09234-9,
author = {Azpeitia, Andoni and Etchegoyhen, Thierry},
title = {Efficient document alignment across scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {3},
issn = {0922-6567},
url = {https://doi.org/10.1007/s10590-019-09234-9},
doi = {10.1007/s10590-019-09234-9},
abstract = {We present and evaluate an approach to document alignment meant for efficiency and portability, as it relies on automatically extracted lexical translations and simple set-theoretic operations for the computation of document-level similarity. We compare our approach to the state of the art on a variety of alignment scenarios, showing that it outperforms alternative document-alignment methods in the vast majority of cases, on both parallel and comparable corpora. We also explore several forms of simple component optimisation to evaluate the potential for improvement of the core method, and describe several successful optimisation paths that lead to significant improvements over strong baselines. The proposed approach constitutes an effective and easy to deploy method to perform accurate document alignment across scenarios, with the potential to improve the creation of parallel corpora.},
journal = {Machine Translation},
month = sep,
pages = {205–237},
numpages = {33},
keywords = {Parallel corpora, Comparable corpora, Document alignment}
}

@inproceedings{10.5555/2063016.2063046,
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S\"{o}ren and Ngomo, Axel-Cyrille Ngonga},
title = {DBpedia SPARQL benchmark: performance assessment with real queries on real data},
year = {2011},
isbn = {9783642250729},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Triple stores are the backbone of increasingly many Data Web applications. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in general. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been converted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more useful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
booktitle = {Proceedings of the 10th International Conference on The Semantic Web - Volume Part I},
pages = {454–469},
numpages = {16},
location = {Bonn, Germany},
series = {ISWC'11}
}

@inproceedings{10.1145/2362499.2362510,
author = {Yu, Yang and Dang, Jiangbo},
title = {Semantic mining on customer survey},
year = {2012},
isbn = {9781450311120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362499.2362510},
doi = {10.1145/2362499.2362510},
abstract = {Business intelligence aims to support better business decision-making. Customer survey is priceless asset for intelligent business decision-making. However, business analysts usually have to read hundreds of textual comments and tabular data in survey to manually dig out the necessary information to feed business intelligence models and tools. This paper introduces a business intelligence system to solve this problem by extensively utilizing Semantic Web technologies. Ontology based knowledge extraction is the key to extract interesting terms and understand the logic concept of them. All knowledge extracted forms a semantic knowledge base. Flexible user queries and intelligent analysis can be easily issued to the system over the semantic data store through standard protocol. Besides resolving problems in theory, we designed a flexible, intuitive user interaction interface to explain and present the analysis result for business analysts. Through the real usage of this system, it is validated that our system gives good solution for semantic mining on customer survey for business intelligence.},
booktitle = {Proceedings of the 8th International Conference on Semantic Systems},
pages = {72–79},
numpages = {8},
keywords = {visualization, user interaction, sentiment analysis, ontology based text mining, ontology based knowledge extraction},
location = {Graz, Austria},
series = {I-SEMANTICS '12}
}

@inproceedings{10.1145/1865841.1865860,
author = {Luo, Xiongfei and Teng, Dongxing and Liu, Wei and Tian, Feng and Dai, Guozhong and Wang, Hongan},
title = {A developing framework for interactive temporal data visualization},
year = {2010},
isbn = {9781450304368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1865841.1865860},
doi = {10.1145/1865841.1865860},
abstract = {This article presents UCFM, a user-centered developing framework for interactive temporal data visualization. UCFM is comprised mainly by software architecture and software development methods. The software architecture describes modules in interactive temporal data visualization system and their relationships. And based on the software architecture and development practice, the software development methods summarize these specific steps to design and develop interactive temporal data visualization system. To demonstrate UCFM's validity, the development process of an interactive temporal data visualization application is illustrated.},
booktitle = {Proceedings of the 3rd International Symposium on Visual Information Communication},
articleno = {14},
numpages = {10},
keywords = {temporal data, interactive information visualization, developing framework},
location = {Beijing, China},
series = {VINCI '10}
}

@article{10.1016/j.jksuci.2014.06.011,
author = {Marton, Yuval and Zitouni, Imed},
title = {Transliteration normalization for Information Extraction and Machine Translation},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {26},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2014.06.011},
doi = {10.1016/j.jksuci.2014.06.011},
abstract = {Foreign name transliterations typically include multiple spelling variants. These variants cause data sparseness and inconsistency problems, increase the Out-of-Vocabulary (OOV) rate, and present challenges for Machine Translation, Information Extraction and other natural language processing (NLP) tasks. This work aims to identify and cluster name spelling variants using a Statistical Machine Translation method: word alignment. The variants are identified by being aligned to the same "pivot" name in another language (the source-language in Machine Translation settings). Based on word-to-word translation and transliteration probabilities, as well as the string edit distance metric, names with similar spellings in the target language are clustered and then normalized to a canonical form. With this approach, tens of thousands of high-precision name transliteration spelling variants are extracted from sentence-aligned bilingual corpora in Arabic and English (in both languages). When these normalized name spelling variants are applied to Information Extraction tasks, improvements over strong baseline systems are observed. When applied to Machine Translation tasks, a large improvement potential is shown.},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = dec,
pages = {379–387},
numpages = {9},
keywords = {Transliteration, Named Entity Recognition, Name normalization, Machine Translation, Information Extraction, Arabic}
}

@article{10.1017/S0269888910000299,
title = {From the journals???},
year = {2011},
issue_date = {February 2011},
publisher = {Cambridge University Press},
address = {USA},
volume = {26},
number = {1},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888910000299},
doi = {10.1017/S0269888910000299},
journal = {Knowl. Eng. Rev.},
month = feb,
pages = {73–97},
numpages = {25}
}

@article{10.1016/j.infsof.2019.07.009,
author = {Gomes, Luiz Alberto Ferreira and Torres, Ricardo da Silva and C\^{o}rtes, Mario L\'{u}cio},
title = {Bug report severity level prediction in open source software: A survey and research opportunities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.009},
doi = {10.1016/j.infsof.2019.07.009},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {58–78},
numpages = {21},
keywords = {Machine learning, Systematic mapping, Software repositories, Severity level prediction, Bug reports, Bug tracking systems, Software maintenance}
}

@inproceedings{10.1007/11527886_61,
author = {Peng, Xiaoyan and Silver, Daniel L.},
title = {User control over user adaptation: a case study},
year = {2005},
isbn = {3540278850},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527886_61},
doi = {10.1007/11527886_61},
abstract = {The A theory of user expectation of system interaction is introduced in the context of User Adapted Interfaces. The usability of an intelligent email client that learns to filter spam emails is tested under three variants of adaptation: no user modeling, user modeling with fixed (optimal) spam cut-offs, and user modeling with user adjustable spam cut-offs. The results supported our hypothesis that user control over adaptation is preferred because the user can maintain the system's interaction state within a region of user expectation. This remains true even when performance of the system (accuracy of spam filtering) degrades because of errors in user control (adjustment of spam cut-offs).},
booktitle = {Proceedings of the 10th International Conference on User Modeling},
pages = {443–447},
numpages = {5},
location = {Edinburgh, UK},
series = {UM'05}
}

@article{10.1177/0278364917727062,
author = {Valada, Abhinav and Burgard, Wolfram},
title = {Deep spatiotemporal models for robust proprioceptive terrain classification},
year = {2017},
issue_date = {12 2017},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {36},
number = {13–14},
issn = {0278-3649},
url = {https://doi.org/10.1177/0278364917727062},
doi = {10.1177/0278364917727062},
abstract = {Terrain classification is a critical component of any autonomous mobile robot system operating in unknown real-world environments. Over the years, several proprioceptive terrain classification techniques have been introduced to increase robustness or act as a fallback for traditional vision based approaches. However, they lack widespread adaptation due to various factors that include inadequate accuracy, robustness and slow run-times. In this paper, we use vehicle-terrain interaction sounds as a proprioceptive modality and propose a deep long-short term memory based recurrent model that captures both the spatial and temporal dynamics of such a problem, thereby overcoming these past limitations. Our model consists of a new convolution neural network architecture that learns deep spatial features, complemented with long-short term memory units that learn complex temporal dynamics. Experiments on two extensive datasets collected with different microphones on various indoor and outdoor terrains demonstrate state-of-the-art performance compared to existing techniques. We additionally evaluate the performance in adverse acoustic conditions with high-ambient noise and propose a noise-aware training scheme that enables learning of more generalizable models that are essential for robust real-world deployments.},
journal = {Int. J. Rob. Res.},
month = dec,
pages = {1521–1539},
numpages = {19},
keywords = {terrain classification, temporal classification, recurrent neural networks, long-short term memory networks, acoustics, Convolutional neural networks}
}

@inproceedings{10.1145/3037697.3037740,
author = {Li, Kaiwei and Chen, Jianfei and Chen, Wenguang and Zhu, Jun},
title = {SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037740},
doi = {10.1145/3037697.3037740},
abstract = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics.In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {497–509},
numpages = {13},
keywords = {topic model, palellel computing, lda, gpu},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1016/j.engappai.2017.08.013,
author = {Stani, Ivo and Musi, Josip and Gruji, Tamara},
title = {Gesture recognition system for real-time mobile robot control based on inertial sensors and motion strings},
year = {2017},
issue_date = {November 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2017.08.013},
doi = {10.1016/j.engappai.2017.08.013},
abstract = {Navigating and controlling a mobile robot in an indoor or outdoor environment by using a range of body-worn sensors is becoming an increasingly interesting research area in the robotics community. In such scenarios, hand gestures offer some unique capabilities for humanrobot interaction inherent to nonverbal communication with features and application scenarios not possible with the currently predominant vision-based systems. Therefore, in this paper, we propose and develop an effective inertial-sensor-based system, worn by the user, along with a microprocessor and wireless module for communication with the robot at distances of up to 250 m. Possible features describing hand-gesture dynamics are introduced and their feasibility is demonstrated in an off-line scenario by using several classification methods (e.g., random forests and artificial neural networks). Refined motion features are then used in K-means unsupervised clustering for motion primitive extraction, which forms the motion strings used for real-time classification. The system demonstrated an F1 score of 90.05% with the possibility of gesture spotting and null class classification (e.g., undefined gestures were discarded from the analysis). Finally, to demonstrate the feasibility of the proposed algorithm, it was implemented in an Arduino-based 8-bit ATmega2560 microcontroller for control of a mobile, tracked robot platform.},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {33–48},
numpages = {16},
keywords = {Real-time classification, Mobile robot control, Machine learning, Inertial sensors, Humanrobot interaction, Hand gestures}
}

@article{10.1145/2168752.2168755,
author = {Ewerth, Ralph and M\"{u}hling, Markus and Freisleben, Bernd},
title = {Robust Video Content Analysis via Transductive Learning},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168755},
doi = {10.1145/2168752.2168755},
abstract = {Reliable video content analysis is an essential prerequisite for effective video search. An important current research question is how to develop robust video content analysis methods that produce satisfactory results for a large variety of video sources, distribution platforms, genres, and content. The work presented in this article exploits the observation that the appearance of objects and events is often related to a particular video sequence, episode, program, or broadcast. This motivates our idea of considering the content analysis task for a single video or episode as a transductive setting: the final classification model must be optimal for the given video only, and not in general, as expected for inductive learning. For this purpose, the unlabeled video test data have to be used in the learning process. In this article, a transductive learning framework for robust video content analysis based on feature selection and ensemble classification is presented. In contrast to related transductive approaches for video analysis (e.g., for concept detection), the framework is designed in a general manner and not only for a single task. The proposed framework is applied to the following video analysis tasks: shot boundary detection, face recognition, semantic video retrieval, and semantic indexing of computer game sequences. Experimental results for diverse video analysis tasks and large test sets demonstrate that the proposed transductive framework improves the robustness of the underlying state-of-the-art approaches, whereas transductive support vector machines do not solve particular tasks in a satisfactory manner.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {41},
numpages = {26},
keywords = {self-supervised learning, robustness, robust video content analysis, face recognition, ensemble classification, cut detection, concept detection, Transductive learning}
}

@inproceedings{10.1145/3106237.3106251,
author = {Siegmund, Norbert and Sobernig, Stefan and Apel, Sven},
title = {Attributed variability models: outside the comfort zone},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106251},
doi = {10.1145/3106237.3106251},
abstract = {Variability models are often enriched with attributes, such as performance, that encode the influence of features on the respective attribute. In spite of their importance, there are only few attributed variability models available that have attribute values obtained from empirical, real-world observations and that cover interactions between features. But, what does it mean for research and practice when staying in the comfort zone of developing algorithms and tools in a setting where artificial attribute values are used and where interactions are neglected? This is the central question that we want to answer here. To leave the comfort zone, we use a combination of kernel density estimation and a genetic algorithm to rescale a given (real-world) attribute-value profile to a given variability model. To demonstrate the influence and relevance of realistic attribute values and interactions, we present a replication of a widely recognized, third-party study, into which we introduce realistic attribute values and interactions. We found statistically significant differences between the original study and the replication. We infer lessons learned to conduct experiments that involve attributed variability models. We also provide the accompanying tool Thor for generating attribute values including interactions. Our solution is shown to be agnostic about the given input distribution and to scale to large variability models.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {268–278},
numpages = {11},
keywords = {attributed variability models, Variability modelling, Thor},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1007/s10916-016-0441-5,
author = {Tripathy, R. K. and Sharma, L. N. and Dandapat, S.},
title = {Detection of Shockable Ventricular Arrhythmia using Variational Mode Decomposition},
year = {2016},
issue_date = {April     2016},
publisher = {Plenum Press},
address = {USA},
volume = {40},
number = {4},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-016-0441-5},
doi = {10.1007/s10916-016-0441-5},
abstract = {Ventricular tachycardia (VT) and ventricular fibrillation (VF) are shockable ventricular cardiac ailments. Detection of VT/VF is one of the important step in both automated external defibrillator (AED) and implantable cardioverter defibrillator (ICD) therapy. In this paper, we propose a new method for detection and classification of shockable ventricular arrhythmia (VT/VF) and non-shockable ventricular arrhythmia (normal sinus rhythm, ventricular bigeminy, ventricular ectopic beats, and ventricular escape rhythm) episodes from Electrocardiogram (ECG) signal. The variational mode decomposition (VMD) is used to decompose the ECG signal into number of modes or sub-signals. The energy, the renyi entropy and the permutation entropy of first three modes are evaluated and these values are used as diagnostic features. The mutual information based feature scoring is employed to select optimal set of diagnostic features. The performance of the diagnostic features is evaluated using random forest (RF) classifier. Experimental results reveal that, the feature subset derived from mutual information based scoring and the RF classifier produces accuracy, sensitivity and specificity values of 97.23 %, 96.54 %, and 97.97 %, respectively. The proposed method is compared with some of the existing techniques for detection of shockable ventricular arrhythmia episodes from ECG.},
journal = {J. Med. Syst.},
month = apr,
pages = {1–13},
numpages = {13},
keywords = {Variational mode decomposition, Shockable ventricular arrhythmia, Sensitivity, Renyi entropy, Random forest, Permutation entropy, Mutual information, Energy, Accuracy}
}

@article{10.1016/j.patcog.2016.09.030,
author = {Mahapatra, Dwarikanath},
title = {Semi-supervised learning and graph cuts for consensus based medical image segmentation},
year = {2017},
issue_date = {Mar 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.09.030},
doi = {10.1016/j.patcog.2016.09.030},
journal = {Pattern Recogn.},
month = mar,
pages = {700–709},
numpages = {10},
keywords = {Graph cuts, Semi supervised learning, Self-consistency, Retina, Crohn's disease, Segmentation, Multiple experts}
}

@inproceedings{10.1145/2602087.2602102,
author = {Darabseh, Alaa and Namin, Akbar Siami},
title = {The accuracy of user authentication through keystroke features using the most frequent words},
year = {2014},
isbn = {9781450328128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602087.2602102},
doi = {10.1145/2602087.2602102},
abstract = {We study the performance and influence of various keystroke features on keystroke dynamics authentication systems. In particular, we investigate the performance of keystroke features on a subset of most frequently used English words. Four features including key duration, flight time latency, diagraph time latency, and word total time duration were analyzed using non-parametric Wilcoxon Man-Whitney statistical test technique. Two experiments were performed to measure the performance of each feature individually as well as the results from the combinations of these features. The results of the experiments were evaluated with eight users. The results show that while the combination of hold time and word total duration produces best performance among all four keystroke features combinations, hold time of the key press offers the optimal result if used independently.},
booktitle = {Proceedings of the 9th Annual Cyber and Information Security Research Conference},
pages = {85–88},
numpages = {4},
keywords = {security, keystroke feature, keystroke dynamics, biometrics, authentication},
location = {Oak Ridge, Tennessee, USA},
series = {CISR '14}
}

@article{10.1016/j.ins.2016.05.042,
author = {Yu, Haiyan and Shen, Jiang and Xu, Man},
title = {Temporal case matching with information value maximization for predicting physiological states},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {367},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.05.042},
doi = {10.1016/j.ins.2016.05.042},
abstract = {A novel temporal case-matching framework is proposed to align multi-granularity modeling with decision making.The temporal classification system based on an inference engine leverages multi-scale features for interpretable reasoning.A bilevel mixed integer optimization model is proposed to select the optimal combination of multi-scale features.Belief integrating rules are adapted to identify the decision state of queries.The method propagating the case matching to predict modeling exhibits a satisfactory performance. With the rapid growth in volume of temporal medical data, predicting physiological states plays an important role in classifying medical cases. In this study, we propose a novel temporal classification framework that aligns multi-granularity modeling with decision-making. Particularly, we present a method for propagating case matching to predict unlabeled temporal cases and optimize the information value, which is the amount gained by the feature reconstruction module in answering queries. The proposed method facilitates the execution of multi-granularity case matching with temporal similarity and provide practitioners with a useful method of understanding temporal case-based reasoning. In the proposed method, the objective functions in a bilevel mixed integer optimization are the size of the reconstructed feature sets and their information values, which trade off the utility of additional information against the cost of feature combination depending on the decision variable. Unlike the conventional case matching method that uses all temporal features, the proposed method establishes effective classification rules based on the optimal temporal feature set. Moreover, this method also adapted multi-scale entropy to extract the dynamic features from multivariate temporal data. The numerical experiment verifies the effectiveness of the modeling framework and the robustness of the classification rules.},
journal = {Inf. Sci.},
month = nov,
pages = {766–782},
numpages = {17},
keywords = {Temporal case matching, Physiological states, Multi-scale feature, Information value, Bilevel mixed integer optimization}
}

@inproceedings{10.1145/2466715.2466733,
author = {Romero, Andr\'{e}s and Gouiff\`{e}s, Mich\`{e}le and Lacassagne, Lionel},
title = {Enhanced local binary covariance matrices (ELBCM) for texture analysis and object tracking},
year = {2013},
isbn = {9781450320238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466715.2466733},
doi = {10.1145/2466715.2466733},
abstract = {This paper presents a novel way to embed local binary texture information in the form of local binary patterns (LBP) into the covariance descriptor. Contrary to previous publications, our method is not based on the LBP decimal values where arithmetic operations have no texture meaning. Our method uses the angles described by the uniform LBP patterns and includes them into the set of features used to build the covariance descriptor. Our representation is not only more compact but more robust because it is less affected by noise and small neighborhood rotations. Experimental evaluations corroborate the performance of our descriptor for texture analysis and tracking applications. Our descriptor rivals with state-of-the-art methods and beats other covariance-based descriptors.},
booktitle = {Proceedings of the 6th International Conference on Computer Vision / Computer Graphics Collaboration Techniques and Applications},
articleno = {10},
numpages = {8},
keywords = {local log-euclidean, covariance descriptors, Riemannian manifolds, LBP},
location = {Berlin, Germany},
series = {MIRAGE '13}
}

@article{10.1007/s11219-016-9320-z,
author = {Carvalho, Rainara Maia and Castro Andrade, Rossana Maria and Oliveira, K\'{a}thia Mar\c{c}al and Sousa Santos, Ismayle and Bezerra, Carla Ilane},
title = {Quality characteristics and measures for human---computer interaction evaluation in ubiquitous systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9320-z},
doi = {10.1007/s11219-016-9320-z},
abstract = {The advent of ubiquitous systems places even more focus on users, since these systems must support their daily activities in such a transparent way that does not disturb them. Thus, much more attention should be provided to human---computer interaction (HCI) and, as a consequence, to its quality. Dealing with quality issues implies first the identification of the quality characteristics that should be achieved and, then, which software measures should be used to evaluate them in a target system. Therefore, this work aims to identify what quality characteristics and measures have been used for the HCI evaluation of ubiquitous systems. In order to achieve our goal, we performed a large literature review, using a systematic mapping study, and we present our results in this paper. We identified 41 pertinent papers that were deeply analyzed to extract quality characteristics and software measures. We found 186 quality characteristics, but since there were divergences on their definitions and duplicated characteristics, an analysis of synonyms by peer review based on the equivalence of definitions was also done. This analysis allowed us to define a final suitable set composed of 27 quality characteristics, where 21 are generic to any system but are particularized for ubiquitous applications and 6 are specific for this domain. We also found 218 citations of measures associated with the characteristics, although the majority of them are simple definitions with no detail about their measurement functions. Our results provide not only an overview of this area to guide researchers in directing their efforts but also it can help practitioners in evaluating ubiquitous systems using these measures.},
journal = {Software Quality Journal},
month = sep,
pages = {743–795},
numpages = {53},
keywords = {Ubiquitous systems, Systematic mapping study, Software measures, Quality model, Quality characteristics, Human---computer interaction}
}

@article{10.1007/s11257-019-09221-y,
author = {Deldjoo, Yashar and Ferrari Dacrema, Maurizio and Constantin, Mihai Gabriel and Eghbal-Zadeh, Hamid and Cereda, Stefano and Schedl, Markus and Ionescu, Bogdan and Cremonesi, Paolo},
title = {Movie genome: alleviating new item cold start in movie recommendation},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-019-09221-y},
doi = {10.1007/s11257-019-09221-y},
abstract = {As of today, most movie recommendation services base their recommendations on collaborative filtering (CF) and/or content-based filtering (CBF) models that use metadata (e.g., genre or cast). In most video-on-demand and streaming services, however, new movies and TV series are continuously added. CF models are unable to make predictions in such a scenario, since the newly added videos lack interactions--a problem technically known as new item cold start (CS). Currently, the most common approach to this problem is to switch to a purely CBF method, usually by exploiting textual metadata. This approach is known to have lower accuracy than CF because it ignores useful collaborative information and relies on human-generated textual metadata, which are expensive to collect and often prone to errors. User-generated content, such as tags, can also be rare or absent in CS situations. In this paper, we introduce a new movie recommender system that addresses the new item problem in the movie domain by (i) integrating state-of-the-art audio and visual descriptors, which can be automatically extracted from video content and constitute what we call the movie genome; (ii) exploiting an effective data fusion method named canonical correlation analysis, which was successfully tested in our previous works Deldjoo et al. (in: International Conference on Electronic Commerce and Web Technologies. Springer, Berlin, pp 34---45, 2016b; Proceedings of the Twelfth ACM Conference on Recommender Systems. ACM, 2018b), to better exploit complementary information between different modalities; (iii) proposing a two-step hybrid approach which trains a CF model on warm items (items with interactions) and leverages the learned model on the movie genome to recommend cold items (items without interactions). Experimental validation is carried out using a system-centric study on a large-scale, real-world movie recommendation dataset both in an absolute cold start and in a cold to warm transition; and a user-centric online experiment measuring different subjective aspects, such as satisfaction and diversity. Results show the benefits of this approach compared to existing approaches.},
journal = {User Modeling and User-Adapted Interaction},
month = apr,
pages = {291–343},
numpages = {53},
keywords = {Warm start, Visual descriptors, Semi-cold start, New item, Multimodal fusion, Multimedia features, Movie recommender systems, Hybrid recommender system, Feature weighting, Content-based, Collaborative-enriched content-based filtering, Cold start, Canonical correlations analysis, Audio descriptors}
}

@article{10.1016/j.jss.2016.06.101,
author = {Shatnawi, Anas and Seriai, Abdelhak-Djamel and Sahraoui, Houari and Alshara, Zakarea},
title = {Reverse engineering reusable software components from object-oriented APIs},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.101},
doi = {10.1016/j.jss.2016.06.101},
abstract = {Automatic component recovery from object-oriented APIs.We mined components as classes frequently used together and structural dependent.Experimented on 100 applications used four APIs. Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs.},
journal = {J. Syst. Softw.},
month = sep,
pages = {442–460},
numpages = {19},
keywords = {Software reuse, Software component, Reverse engineering, Object-oriented, Frequent usage pattern, API}
}

@inproceedings{10.1145/2830318.2830322,
author = {Hanford, Nathan and Tierney, Brian and Ghosal, Dipak},
title = {Optimizing data transfer nodes using packet pacing},
year = {2015},
isbn = {9781450340021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830318.2830322},
doi = {10.1145/2830318.2830322},
abstract = {An important performance problem that we foresee with Data Transfer Nodes (DTNs) in the near future is a fast sending host over-running a slow receiving host, and packets getting dropped, leading to poor performance. Due to the design of the Transmission Control Protocol (TCP), if this occurs over a high-latency path, the performance impact can be quite dramatic. For example, for a 40 Gbps DTN sending to a 10 Gbps DTN over a 100 ms path, throughput can drop to less than 1 Gbps. Slow firewalls, under-buffered switches, or other devices in the network path that can not handle high speed flows also have this negative impact on throughput. In this paper we describe a simple tuning daemon at the sending host that detects flows when this is happening, and then tells the Linux kernel to throttle those flows to a rate that the network and receive host can handle. We also describe the results of several experiments to test the feasibility of such a solution, and in doing so are able to present some of the bounds of flow pacing at these line rates.},
booktitle = {Proceedings of the Second Workshop on Innovating the Network for Data-Intensive Science},
articleno = {4},
numpages = {8},
location = {Austin, Texas},
series = {INDIS '15}
}

@article{10.1145/2801127,
author = {Wu, Ming-Ju and Jang, Jyh-Shing R.},
title = {Combining Acoustic and Multilevel Visual Features for Music Genre Classification},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/2801127},
doi = {10.1145/2801127},
abstract = {Most music genre classification approaches extract acoustic features from frames to capture timbre information, leading to the common framework of bag-of-frames analysis. However, time-frequency analysis is also vital for modeling music genres. This article proposes multilevel visual features for extracting spectrogram textures and their temporal variations. A confidence-based late fusion is proposed for combining the acoustic and visual features. The experimental results indicated that the proposed method achieved an accuracy improvement of approximately 14% and 2% in the world's largest benchmark dataset (MASD) and Unique dataset, respectively. In particular, the proposed approach won the Music Information Retrieval Evaluation eXchange (MIREX) music genre classification contests from 2011 to 2013, demonstrating the feasibility and necessity of combining acoustic and visual features for classifying music genres.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = aug,
articleno = {10},
numpages = {17},
keywords = {Music genre classification}
}

@article{10.1016/j.specom.2016.07.006,
author = {Morrison, Geoffrey Stewart and Enzinger, Ewald},
title = {Multi-laboratory evaluation of forensic voice comparison systems under conditions reflecting those of a real forensic case (forensic_eval_01) Introduction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2016.07.006},
doi = {10.1016/j.specom.2016.07.006},
abstract = {This paper introduces an evaluation of forensic voice comparison systems.It includes the rules for the evaluation.The training and test data reflect the conditions of a real case.Operational and research laboratories are invited to participate.Results will be published in a Virtual Special Issue of Speech Communication. There is increasing pressure on forensic laboratories to validate the performance of forensic analysis systems before they are used to assess strength of evidence for presentation in court. Different forensic voice comparison systems may use different approaches, and even among systems using the same general approach there can be substantial differences in operational details. From case to case, the relevant population, speaking styles, and recording conditions can be highly variable, but it is common to have relatively poor recording conditions and mismatches in speaking style and recording conditions between the known- and questioned-speaker recordings. In order to validate a system intended for use in casework, a forensic laboratory needs to evaluate the degree of validity and reliability of the system under forensically realistic conditions. The present paper is an introduction to a Virtual Special Issue consisting of papers reporting on the results of testing forensic voice comparison systems under conditions reflecting those of an actual forensic voice comparison case. A set of training and test data representative of the relevant population and reflecting the conditions of this particular case has been released, and operational and research laboratories are invited to use these data to train and test their systems. The present paper includes the rules for the evaluation and a description of the evaluation metrics and graphics to be used. The name of the evaluation is: forensic_eval_01. Display Omitted},
journal = {Speech Commun.},
month = dec,
pages = {119–126},
numpages = {8},
keywords = {Validity, Reliability, Forensic voice comparison, Evaluation, Casework conditions}
}

@article{10.1007/s10844-016-0436-1,
author = {Patra, Braja Gopal and Das, Dipankar and Bandyopadhyay, Sivaji},
title = {Labeling data and developing supervised framework for hindi music mood analysis},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-016-0436-1},
doi = {10.1007/s10844-016-0436-1},
abstract = {Digitization has created a wide platform for music, in the form of televisions, desktops and other hand held devices. This has increased the reach of musical content as well as its impact on people. Music is often associated with distinct emotional content, generally referred to as music mood. Literature focusing on analyzing the content of a music piece, often discusses music mood as an important metadata. The present article addresses the issue of Hindi music mood classification by considering important issues like taxonomy development, annotation and automated mood classification. We annotated a total of 1540 music clips of 60 seconds duration each, with either of a proposed set of five mood classes derived from Russell's circumplex model. We developed several supervised systems with the help of different classification algorithms and neural networks such as Support Vector Machines, Decision Trees, and Feed Forward Neural Networks. Our experiments reveal that features like timbre, rhythm, and intensity are associated with enhanced classification accuracy. Overall, the results were found satisfactory and Feed Forward Neural Networks based system achieved the maximum F-measure of 0.725 based on 10-fold cross validation.},
journal = {J. Intell. Inf. Syst.},
month = jun,
pages = {633–651},
numpages = {19},
keywords = {Music mood classification, Mood taxonomy, Intensity, Hindi songs, Feed forward neural networks, Arousal-valence}
}

@inbook{10.1145/3107990.3108002,
author = {Zhou, Jianlong and Yu, Kun and Chen, Fang and Wang, Yang and Arshad, Syed Z.},
title = {Multimodal behavioral and physiological signals as indicators of cognitive load},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3107990.3108002},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
pages = {287–329},
numpages = {43}
}

@article{10.1007/s10772-012-9131-y,
author = {Aggarwal, R. K. and Dave, M.},
title = {Integration of multiple acoustic and language models for improved Hindi speech recognition system},
year = {2012},
issue_date = {June      2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-012-9131-y},
doi = {10.1007/s10772-012-9131-y},
abstract = {Despite the significant progress of automatic speech recognition (ASR) in the past three decades, it could not gain the level of human performance, particularly in the adverse conditions. To improve the performance of ASR, various approaches have been studied, which differ in feature extraction method, classification method, and training algorithms. Different approaches often utilize complementary information; therefore, to use their combination can be a better option. In this paper, we have proposed a novel approach to use the best characteristics of conventional, hybrid and segmental HMM by integrating them with the help of ROVER system combination technique. In the proposed framework, three different recognizers are created and combined, each having its own feature set and classification technique. For design and development of the complete system, three separate acoustic models are used with three different feature sets and two language models. Experimental result shows that word error rate (WER) can be reduced about 4% using the proposed technique as compared to conventional methods. Various modules are implemented and tested for Hindi Language ASR, in typical field conditions as well as in noisy environment.},
journal = {Int. J. Speech Technol.},
month = jun,
pages = {165–180},
numpages = {16},
keywords = {System combination, ROVER, Language models, Hindi, Acoustic models, ASR}
}

@article{10.1016/j.csl.2017.10.001,
author = {Pal, Monisankha and Paul, Dipjyoti and Saha, Goutam},
title = {Synthetic speech detection using fundamental frequency variation and spectral features},
year = {2018},
issue_date = {March 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {48},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2017.10.001},
doi = {10.1016/j.csl.2017.10.001},
abstract = {Proposed synthetic speech detection using score fusion of CQCC, APGDF and fundamental frequency variation (FFV) features.Best spoofing detection performance on the ASVspoof 2015 evaluation dataset with an overall EER of 0.05%.Produced the state-of-the-art performance for ASV integrated with countermeasure framework.Superior performance in generalization ability assessment. Recent works on the vulnerability of automatic speaker verification (ASV) systems confirm that malicious spoofing attacks using synthetic speech can provoke significant increase in false acceptance rate. A reliable detection of synthetic speech is key to develop countermeasure for synthetic speech based spoofing attacks. In this paper, we targeted that by focusing on three major types of artifacts related to magnitude, phase and pitch variation, which are introduced during the generation of synthetic speech. We proposed a new approach to detect synthetic speech using score-level fusion of front-end features namely, constant Q cepstral coefficients (CQCCs), all-pole group delay function (APGDF) and fundamental frequency variation (FFV). CQCC and APGDF were individually used earlier for spoofing detection task and yielded the best performance among magnitude and phase spectrum related features, respectively. The novel FFV feature introduced in this paper to extract pitch variation at frame-level, provides complementary information to CQCC and APGDF. Experimental results show that the proposed approach produces the best stand-alone spoofing detection performance using Gaussian mixture model (GMM) based classifier on ASVspoof 2015 evaluation dataset. An overall equal error rate of 0.05% with a relative performance improvement of 76.19% over the next best-reported results is obtained using the proposed method. In addition to outperforming all existing baseline features for both known and unknown attacks, the proposed feature combination yields superior performance for ASV system (GMM with universal background model/i-vector) integrated with countermeasure framework. Further, the proposed method is found to have relatively better generalization ability when either one or both of copy-synthesized data and limited spoofing data are available a priori in the training pool.},
journal = {Comput. Speech Lang.},
month = mar,
pages = {31–50},
numpages = {20},
keywords = {Spoofing attack, Score-level fusion, Fundamental frequency variation (FFV), Constant Q cepstral coefficient (CQCC), Anti-spoofing, All-pole group delay function (APGDF)}
}

@article{10.1016/j.csl.2012.06.005,
author = {Keronen, Sami and Kallasjoki, Heikki and Remes, Ulpu and Brown, Guy J. and Gemmeke, Jort F. and Palom\"{a}Ki, Kalle J.},
title = {Mask estimation and imputation methods for missing data speech recognition in a multisource reverberant environment},
year = {2013},
issue_date = {May, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {3},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.06.005},
doi = {10.1016/j.csl.2012.06.005},
abstract = {We present an automatic speech recognition system that uses a missing data approach to compensate for challenging environmental noise containing both additive and convolutive components. The unreliable and noise-corrupted (''missing'') components are identified using a Gaussian mixture model (GMM) classifier based on a diverse range of acoustic features. To perform speech recognition using the partially observed data, the missing components are substituted with clean speech estimates computed using both sparse imputation and cluster-based GMM imputation. Compared to two reference mask estimation techniques based on interaural level and time difference-pairs, the proposed missing data approach significantly improved the keyword accuracy rates in all signal-to-noise ratio conditions when evaluated on the CHiME reverberant multisource environment corpus. Of the imputation methods, cluster-based imputation was found to outperform sparse imputation. The highest keyword accuracy was achieved when the system was trained on imputed data, which made it more robust to possible imputation errors.},
journal = {Comput. Speech Lang.},
month = may,
pages = {798–819},
numpages = {22},
keywords = {Speech recognition, Noise robust, Multicondition, Missing data, Imputation, Binaural}
}

@article{10.25300/MISQ/2014/38.1.03,
author = {Sykes, Tracy Ann and Venkatesh, Viswanath and Johnson, Jonathan L.},
title = {Enterprise system implementation and employee: understanding the role of advice networks},
year = {2014},
issue_date = {March 2014},
publisher = {Society for Information Management and The Management Information Systems Research Center},
address = {USA},
volume = {38},
number = {1},
issn = {0276-7783},
url = {https://doi.org/10.25300/MISQ/2014/38.1.03},
doi = {10.25300/MISQ/2014/38.1.03},
abstract = {The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory-- specifically, advice networks--to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get- and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.},
journal = {MIS Q.},
month = mar,
pages = {51–72},
numpages = {22},
keywords = {social networks, job performance, give-advice, get-advice, enterprise system implementation}
}

@inproceedings{10.1145/1882992.1883099,
author = {Ludwig, Simone A.},
title = {Prediction of breast cancer biopsy outcomes using a distributed genetic programming approach},
year = {2010},
isbn = {9781450300308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882992.1883099},
doi = {10.1145/1882992.1883099},
abstract = {Worldwide, breast cancer is the second most common type of cancer after lung cancer and the fifth most common cause of cancer death accounting for 519,000 deaths worldwide in 2004. The most effective method for breast cancer screening today is mammography. However, presently predictions of breast biopsies resulting from mammogram interpretation lead to approximately 70% biopsies with benign outcomes, which are preventable. Therefore, an automatic method is necessary to aid physicians in the prognosis of mammography interpretations. The data set used for this investigation is based on BI-RADS findings. Previous work has achieved good results using a decision tree, an artificial neural networks and a case-based reasoning approach to develop predictive classifiers. This paper uses a distributed genetic programming approach to predict the outcomes of the mammography achieving even better prediction results.},
booktitle = {Proceedings of the 1st ACM International Health Informatics Symposium},
pages = {694–699},
numpages = {6},
keywords = {malignant, classification, cancer recurrence, benign},
location = {Arlington, Virginia, USA},
series = {IHI '10}
}

@article{10.1007/s10257-014-0251-6,
author = {Lee, Chien-Hsiang and Hwang, San-Yih and Yen, I-Ling and Yu, Tao-Kang},
title = {A service pattern model for service composition with flexible functionality},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2},
issn = {1617-9846},
url = {https://doi.org/10.1007/s10257-014-0251-6},
doi = {10.1007/s10257-014-0251-6},
abstract = {A key feature with service-oriented-architecture is to allow flexible composition of services into a business process. Although previous works related to service composition have paved the way for automatic composition, the techniques have limited applicability when it comes to composing complex workflows based on functional requirements, partly due to the large search space of the available services. In this paper, we propose a novel concept, the prospect service. Unlike existing abstract services which possess fixed service interfaces, a prospect service has a flexible interface to allow functional flexibility. Furthermore, we define a meta-model to specify service patterns with prospect services and adaptable workflow constructs to model flexible and adaptable process templates. An automated instantiation method is introduced to instantiate concrete processes with different functionalities from a service pattern. Since the search space for automatically instantiating a process from a service pattern is greatly reduced compared to that for automatically composing a process from scratch, the proposed approach significantly improve the feasibility of automated composition. Empirical study of the service pattern shows that the use of the proposed model significantly outperforms manual composition in terms of composition time and accuracy, and simulation results demonstrate that the proposed automated instantiation method is efficient.},
journal = {Inf. Syst. E-Bus. Manag.},
month = may,
pages = {235–265},
numpages = {31},
keywords = {Web service composition, Variability modeling, Service pattern, Meta-model}
}

@inproceedings{10.1145/2348283.2348346,
author = {Shen, Jialie and Pang, HweeHwa and Wang, Meng and Yan, Shuicheng},
title = {Modeling concept dynamics for large scale music search},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348283.2348346},
doi = {10.1145/2348283.2348346},
abstract = {Continuing advances in data storage and communication technologies have led to an explosive growth in digital music collections. To cope with their increasing scale, we need effective Music Information Retrieval (MIR) capabilities like tagging, concept search and clustering. Integral to MIR is a framework for modelling music documents and generating discriminative signatures for them. In this paper, we introduce a multimodal, layered learning framework called DMCM. Distinguished from the existing approaches that encode music as an ensemble of order-less feature vectors, our framework extracts from each music document a variety of acoustic features, and translates them into low-level encodings over the temporal dimension. From them, DMCM elucidates the concept dynamics in the music document, representing them with a novel music signature scheme called Stochastic Music Concept Histogram (SMCH) that captures the probability distribution over all the concepts. Experiment results with two large music collections confirm the advantages of the proposed framework over existing methods on various MIR tasks.},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {455–464},
numpages = {10},
keywords = {similarity measure, music information retrieval, music concepts},
location = {Portland, Oregon, USA},
series = {SIGIR '12}
}

@inproceedings{10.5555/2009164.2009202,
author = {Wang, Han and Fazekas, John and Booth, Matthew and Liu, Qi and Che, Dongsheng},
title = {An integrative approach for genomic island prediction in Prokaryotic genomes},
year = {2011},
isbn = {9783642212598},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A genomic island (GI) is a segment of genomic sequence that is horizontally transferred from other genomes. The detection of genomic islands is extremely important to the medical research. Most of current computational approaches that use sequence composition to predict genomic islands have the problem of low prediction accuracy. In this paper, we report, for the first time, that gene information and inter-genic distance are different between genomic islands and non-genomic islands. Using these two sources and sequence information, we have trained the genomic island datasets from 113 genomes, and developed a decisiontree based bagging model for genomic island prediction. In order to test the performance our approach, we have applied it on three genomes: Salmonella typhimurium LT2, Streptococcus pyogenes MGAS315, and Escherichia coli O157:H7 str. Sakai. The performance metrics have shown that our approach is better than other sequence composition based approaches. We conclude that the incorporation of gene information and intergenic distance could improve genomic island prediction accuracy. Our prediction software, Genomic Island Hunter (GIHunter), is available at http://www.esu.edu/cpsc/che_lab/software/GIHunter.},
booktitle = {Proceedings of the 7th International Conference on Bioinformatics Research and Applications},
pages = {404–415},
numpages = {12},
keywords = {sequence composition, intergenic distance, genomic islands, gene information},
location = {Changsha, China},
series = {ISBRA'11}
}

@inproceedings{10.1145/1101149.1101288,
author = {Natsev, Apostol (Paul) and Naphade, Milind R. and Te\v{s}iundefined, Jelena},
title = {Learning the semantics of multimedia queries and concepts from a small number of examples},
year = {2005},
isbn = {1595930442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101149.1101288},
doi = {10.1145/1101149.1101288},
abstract = {In this paper we unify two supposedly distinct tasks in multimedia retrieval. One task involves answering queries with a few examples. The other involves learning models for semantic concepts, also with a few examples. In our view these two tasks are identical with the only differentiation being the number of examples that are available for training. Once we adopt this unified view, we then apply identical techniques for solving both problems and evaluate the performance using the NIST TRECVID benchmark evaluation data [15]. We propose a combination hypothesis of two complementary classes of techniques, a nearest neighbor model using only positive examples and a discriminative support vector machine model using both positive and negative examples. In case of queries, where negative examples are rarely provided to seed the search, we create pseudo-negative samples. We then combine the ranked lists generated by evaluating the test database using both methods, to create a final ranked list of retrieved multimedia items. We evaluate this approach for rare concept and query topic modeling using the NIST TRECVID video corpus.In both tasks we find that applying the combination hypothesis across both modeling techniques and a variety of features results in enhanced performance over any of the baseline models, as well as in improved robustness with respect to training examples and visual features. In particular, we observe an improvement of 6% for rare concept detection and 17% for the search task.},
booktitle = {Proceedings of the 13th Annual ACM International Conference on Multimedia},
pages = {598–607},
numpages = {10},
keywords = {support vector machines, semantics, TRECVID, MECBR},
location = {Hilton, Singapore},
series = {MULTIMEDIA '05}
}

@article{10.1016/j.robot.2008.08.009,
author = {Posner, Ingmar and Schroeter, Derik and Newman, Paul},
title = {Online generation of scene descriptions in urban environments},
year = {2008},
issue_date = {November, 2008},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {56},
number = {11},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2008.08.009},
doi = {10.1016/j.robot.2008.08.009},
abstract = {The ability to extract a rich set of semantic workspace labels from sensor data gathered in complex environments is a fundamental prerequisite to any form of semantic reasoning in mobile robotics. In this paper, we present an online system for the augmentation of maps of outdoor urban environments with such higher-order, semantic labels. The system employs a shallow supervised classification hierarchy to classify scene attributes, consisting of a mixture of 2D/3D geometric and visual scene information, into a range of different workspace classes. The union of classifier responses yields a rich, composite description of the local workspace. We present extensive experimental results, using two large urban data sets collected by our research platform.},
journal = {Robot. Auton. Syst.},
month = nov,
pages = {901–914},
numpages = {14},
keywords = {Support vector machine, Semantic robot maps, Outdoor mapping}
}

@inproceedings{10.5555/1699571.1699604,
author = {Zhang, Min and Li, Haizhou},
title = {Tree kernel-based SVM with structured syntactic knowledge for BTG-based phrase reordering},
year = {2009},
isbn = {9781932432626},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Structured syntactic knowledge is important for phrase reordering. This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.},
booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2},
pages = {698–707},
numpages = {10},
location = {Singapore},
series = {EMNLP '09}
}

@article{10.1016/j.csl.2013.09.005,
author = {Jeon, Je Hun and Xia, Rui and Liu, Yang},
title = {Level of interest sensing in spoken dialog using decision-level fusion of acoustic and lexical evidence},
year = {2014},
issue_date = {March, 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {28},
number = {2},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2013.09.005},
doi = {10.1016/j.csl.2013.09.005},
abstract = {Automatic detection of a user's interest in spoken dialog plays an important role in many applications, such as tutoring systems and customer service systems. In this study, we propose a decision-level fusion approach using acoustic and lexical information to accurately sense a user's interest at the utterance level. Our system consists of three parts: acoustic/prosodic model, lexical model, and a model that combines their decisions for the final output. We use two different regression algorithms to complement each other for the acoustic model. For lexical information, in addition to the bag-of-words model, we propose new features including a level-of-interest value for each word, length information using the number of words, estimated speaking rate, silence in the utterance, and similarity with other utterances. We also investigate the effectiveness of using more automatic speech recognition (ASR) hypotheses (n-best lists) to extract lexical features. The outputs from the acoustic and lexical models are combined at the decision level. Our experiments show that combining acoustic evidence with lexical information improves level-of-interest detection performance, even when lexical features are extracted from ASR output with high word error rate.},
journal = {Comput. Speech Lang.},
month = mar,
pages = {420–433},
numpages = {14},
keywords = {Level of interest, Human-machine interaction, Decision-level fusion}
}

@article{10.1145/2647954,
author = {Schneider, Reinhard and Goswami, Dip and Chakraborty, Samarjit and Bordoloi, Unmesh and Eles, Petru and Peng, Zebo},
title = {Quantifying Notions of Extensibility in FlexRay Schedule Synthesis},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/2647954},
doi = {10.1145/2647954},
abstract = {FlexRay has now become a well-established in-vehicle communication bus at most original equipment manufacturers (OEMs) such as BMW, Audi, and GM. Given the increasing cost of verification and the high degree of crosslinking between components in automotive architectures, an incremental design process is commonly followed. In order to incorporate FlexRay-based designs in such a process, the resulting schedules must be extensible, that is: (i) when messages are added in later iterations, they must preserve deadline guarantees of already scheduled messages, and (ii) they must accommodate as many new messages as possible without changes to existing schedules. Apart from extensible scheduling having not received much attention so far, traditional metrics used for quantifying them cannot be trivially adapted to FlexRay schedules. This is because they do not exploit specific properties of the FlexRay protocol. In this article we, for the first time, introduce new notions of extensibility for FlexRay that capture all the protocol-specific properties. In particular, we focus on the dynamic segment of FlexRay and we present a number of metrics to quantify extensible schedules. Based on the introduced metrics, we propose strategies to synthesize extensible schedules and compare the results of different scheduling algorithms. We demonstrate the applicability of the results with industrial-size case studies and also show that the proposed metrics may also be visually represented, thereby allowing for easy interpretation.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {32},
numpages = {37},
keywords = {schedule synthesis, extensibility, automotive, FlexRay}
}

@article{10.1016/j.dss.2013.06.002,
author = {Zheng, Xiaolin and Zhu, Shuai and Lin, Zhangxi},
title = {Capturing the essence of word-of-mouth for social commerce},
year = {2013},
issue_date = {December 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2013.06.002},
doi = {10.1016/j.dss.2013.06.002},
abstract = {In e-commerce, online product reviews significantly influence the purchase decisions of buyers and the marketing strategies employed by vendors. However, the abundance of reviews and their uneven quality make distinguishing between useful and useless reviews difficult for potential customers, thereby diminishing the benefits of online review systems. To address this problem, we develop a semi-supervised system called Online Review Quality Mining (ORQM). Embedded with independent component analysis and semi-supervised ensemble learning, ORQM exploits two opportunities: the improvement of classification performance through the use of a few labeled instances and numerous unlabeled instances, and the effectiveness of the social characteristics of e-commerce communities as identifiers of influential reviewers who write high-quality reviews. Three complementary experiments on datasets from Amazon.com show that ORQM exhibits remarkably higher performance in classifying reviews of different quality levels than do other well-accepted state-of-the-art text mining methods. The high performance of ORQM is also consistent and stable even under limited availability of labeled instances, thereby outperforming other baseline methods. The experiments also reveal that (1) the social features of reviewers are important in deriving better classification results; (2) classification results are affected by product type given the different purchase habits of consumers; and (3) reviews are contingent on the inherent nature of products, such as whether they are search goods or experience goods, and digital products or physical products, through which purchase decisions are influenced. Display Omitted We develop a semi-supervised system (ORQM) for estimating quality of online reviews.ORQM includes independent component analysis and semi-supervised ensemble learning.The system leverages unlabeled instances to improve its classification performance.Social features contribute most to our system.Classification results are affected by product type.},
journal = {Decis. Support Syst.},
month = dec,
pages = {211–222},
numpages = {12},
keywords = {Social network, Semi-supervised learning, Review quality, Review mining, Online review}
}

@article{10.1145/3126540,
author = {Rai, Siddharth and Chaudhuri, Mainak},
title = {Using Criticality of GPU Accesses in Memory Management for CPU-GPU Heterogeneous Multi-Core Processors},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3126540},
doi = {10.1145/3126540},
abstract = {Heterogeneous chip-multiprocessors with CPU and GPU integrated on the same die allow sharing of critical memory system resources among the CPU and GPU applications. Such architectures give rise to challenging resource scheduling problems. In this paper, we explore memory access scheduling algorithms driven by criticality of GPU accesses in such systems. Different GPU access streams originate from different parts of the GPU rendering pipeline, which behaves very differently from the typical CPU pipeline requiring new techniques for GPU access criticality estimation. We propose a novel queuing network model to estimate the performance-criticality of the GPU access streams. If a GPU application performs below the quality of service requirement (e.g., frame rate in 3D scene rendering), the memory access scheduler uses the estimated criticality information to accelerate the critical GPU accesses. Detailed simulations done on a heterogeneous chip-multiprocessor model with one GPU and four CPU cores running heterogeneous mixes of DirectX, OpenGL, and CPU applications show that our proposal improves the GPU performance by 15% on average without degrading the CPU performance much. Extensions proposed for the mixes containing GPGPU applications, which do not have any quality of service requirement, improve the performance by 7% on average for these mixes.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {133},
numpages = {23},
keywords = {GPU access criticality, GPGPU, DRAM access scheduling, CPU-GPU heterogeneous multi-core, 3D rendering}
}

@inproceedings{10.1145/2661806.2661818,
author = {Mitra, Vikramjit and Shriberg, Elizabeth and McLaren, Mitchell and Kathol, Andreas and Richey, Colleen and Vergyri, Dimitra and Graciarena, Martin},
title = {The SRI AVEC-2014 Evaluation System},
year = {2014},
isbn = {9781450331197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661806.2661818},
doi = {10.1145/2661806.2661818},
abstract = {Though depression is a common mental health problem with significant impact on human society, it often goes undetected. We explore a diverse set of features based only on spoken audio to understand which features correlate with self-reported depression scores according to the Beck depression rating scale. These features, many of which are novel for this task, include (1) estimated articulatory trajectories during speech production, (2) acoustic characteristics, (3) acoustic-phonetic characteristics and (4) prosodic features. Features are modeled using a variety of approaches, including support vector regression, a Gaussian backend and decision trees. We report results on the AVEC-2014 depression dataset and find that individual systems range from 9.18 to 11.87 in root mean squared error (RMSE), and from 7.68 to 9.99 in mean absolute error (MAE). Initial fusion brings further improvement; fusion and feature selection work is still in progress.},
booktitle = {Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge},
pages = {93–101},
numpages = {9},
keywords = {time series prediction, support vector regression, robust signal analysis, prosody, depression, decision trees, articulatory features, acoustic features},
location = {Orlando, Florida, USA},
series = {AVEC '14}
}

@article{10.1007/s10009-020-00560-5,
author = {Gabor, Thomas and Sedlmeier, Andreas and Phan, Thomy and Ritz, Fabian and Kiermeier, Marie and Belzner, Lenz and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner and Wieghardt, Jan and Zeller, Marc and Linnhoff-Popien, Claudia},
title = {The scenario coevolution paradigm: adaptive quality assurance for adaptive systems},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-020-00560-5},
doi = {10.1007/s10009-020-00560-5},
abstract = {Systems are becoming increasingly more adaptive, using techniques like machine learning to enhance their behavior on their own rather than only through human developers programming them. We analyze the impact the advent of these new techniques has on the discipline of rigorous software engineering, especially on the issue of quality assurance. To this end, we provide a general description of the processes related to machine learning and embed them into a formal framework for the analysis of adaptivity, recognizing that to test an adaptive system a new approach to adaptive testing is necessary. We introduce scenario coevolution as a design pattern describing how system and test can work as antagonists in the process of software evolution. While the general pattern applies to large-scale processes (including human developers further augmenting the system), we show all techniques on a smaller-scale example of an agent navigating a simple smart factory. We point out new aspects in software engineering for adaptive systems that may be tackled naturally using scenario coevolution. This work is a substantially extended take on Gabor et al. (International symposium on leveraging applications of formal methods, Springer, pp 137–154, 2018).},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = aug,
pages = {457–476},
numpages = {20},
keywords = {Coevolution, Software evolution, Artificial intelligence, Machine learning, Quality assurance, Software engineering, Self-adaptive systems, Adaptation}
}

@article{10.1016/j.asoc.2016.05.015,
author = {Dou, Dongyang and Zhou, Shishuai},
title = {Comparison of four direct classification methods for intelligent fault diagnosis of rotating machinery},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.015},
doi = {10.1016/j.asoc.2016.05.015},
abstract = {Display Omitted A rule-based method was proposed based on MLEM2 and enhanced by a new rule reasoning mechanism.Eight time-domain and five dimensionless frequency-domain parameters were adopted.The proposed method had the ability of feature reduction.The proposed method was an all-rounder compared with KNN, PNN and PSO-SVM as it was very friendly. Condition monitoring of rotating machinery is important to promptly detect early faults, identify potential problems, and prevent complete failure. Four direct classification methods were introduced to diagnose the regular condition, inner race defect, outer race defect, and rolling element defect of rolling bearings. These include the K-Nearest Neighbor algorithm (KNN), Probabilistic Neural Network (PNN), Particle Swarm Optimization optimized Support Vector Machine (PSO-SVM) and a Rule-Based Method (RBM) based on the MLEM2 algorithm and a new Rule Reasoning Mechanism (RRM). All of them can be run on the Fault Decision Table (FDT) containing numerical variables and output fault categories directly. The diagnosis results were discussed in terms of accuracy, time consumption, intelligibility, and maintainability. Especially, the interactions of the systems and human experts were compared in detail. It was concluded that all the four methods can work satisfactorily on accuracy, in an order of the PSO-SVM ranking the first, followed by the RBM that functioned the friendliest. Moreover, the RBM had the ability of feature reduction by itself, and would be most suitable for real-time applications.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {459–468},
numpages = {10},
keywords = {SVM, Rule, Rotating machinery, PNN, Fault diagnosis}
}

@article{10.1109/TASLP.2014.2362006,
author = {Su, Li and Lin, Hsin-Ming and Yang, Yi-Hsuan},
title = {Sparse modeling of magnitude and phase-derived spectra for playing technique classification},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2362006},
doi = {10.1109/TASLP.2014.2362006},
abstract = {Computational modeling of musical timbre is important for a variety of music information retrieval applications. While considerable progress has been made to recognize musical genres and instruments, relatively little attention has been paid to modeling playing techniques, which affect timbre in more subtle ways. In this paper, we contribute to this area of research by systematically evaluating various audio features and processing methods for multi-class playing technique classification, considering up to nine distinct playing techniques of bowed string instruments. Specifically, a collection of 6,759 chamber-recorded single notes of four bowed string instruments and a collection of 33 real-world solo violin recordings are used in the evaluation. Our evaluation shows that using sparse features extracted from the magnitude spectra and phase derivatives including group delay function (GDF) and instantaneous frequency deviation (IFD) leads to significantly better performance than using a combination of state-of-the-art temporal, spectral, cepstral and harmonic feature descriptors. For playing technique classification of violin singe notes, the former approach attains 0.915 macro-average F-score under a tenfold cross validation setting, while the latter only attains 0.835. Moreover, sparse modeling of magnitude and phase-derived spectra also performs well for single-note joint instrument-technique classification (F-score 0.770) and for playing technique classification of real-world violin solos (F-score 0.547). We find that phase information is particularly important in discriminating playing techniques with subtle differences, such as playing with different bowing positions (i.e., normal, sul tasto, and sul ponticello). A systematic investigation of the effect of parameters such as window sizes, hop factors, window types for phase-derived features is also reported to provide more insights.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2122–2132},
numpages = {11},
keywords = {sparse coding, playing technique classification, phase, instantaneous frequency deviation, group delay function}
}

@article{10.1007/s11042-013-1529-2,
author = {Kiktova-Vozarikova, Eva and Juhar, Jozef and Cizmar, Anton},
title = {Feature selection for acoustic events detection},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-013-1529-2},
doi = {10.1007/s11042-013-1529-2},
abstract = {The paper deals with the detection of abnormal situations via captured sound processing. Different settings of feature extraction algorithms were realized and evaluated. Chosen feature sets were used for building the effective parametric representation for gun shots and breaking glass. This way two types of high dimensional feature supervectors were created in regard to the best individual settings of each feature extraction algorithm. For improving the recognition rate Minimum Redundancy Maximum Relevance (MRMR) and Joint Mutual Information (JMI) feature selection algorithms were also applied. They were used for the selection of superior features and for the creation of n-dimensional feature supervectors. The investigation of the appropriate dimension of feature supervectors was performed too. The framework for recognition of potentially dangerous acoustic events such as breaking glass and gun shots, based on the MRMR and JMI selected feature supervector through Hidden Markov Models based classification is proposed in the paper.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {4213–4233},
numpages = {21},
keywords = {Supervector, MRMR, JMI, Acoustic event}
}

@article{10.1016/j.eswa.2012.10.009,
author = {Imamverdiyev, Yadigar and Teoh, Andrew Beng Jin and Kim, Jaihie},
title = {Biometric cryptosystem based on discretized fingerprint texture descriptors},
year = {2013},
issue_date = {April, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.10.009},
doi = {10.1016/j.eswa.2012.10.009},
abstract = {This paper focuses on a biometric cryptosystem implementation and evaluation based on a number of fingerprint texture descriptors. The texture descriptors, namely, the Gabor filter-based FingerCode, a local binary pattern (LBP), and a local direction pattern (LDP), and their various combinations are considered. These fingerprint texture descriptors are binarized using a biometric discretization method and used in a fuzzy commitment scheme (FCS). We constructed the biometric cryptosystems, which achieve a good performance, by fusing discretized fingerprint texture descriptors and using effective error-correcting codes. We tested the proposed system on a FVC2000 DB2a fingerprint database, and the results demonstrate that the new system significantly improves the performance of the FCS for texture-based fingerprints.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {1888–1901},
numpages = {14},
keywords = {SPA, ROI, PEG, MPA, Local binary pattern, LDerivP, LDPC, LDP, LCM, LBP, GAR, Fuzzy commitment scheme, Fingerprint texture descriptors, FRR, FCS, FAR, Error-correcting code, EER, ECC, DROBA, Biometric discretization, BPA, BCH}
}

@article{10.1016/j.micpro.2008.05.002,
author = {Wang, Xiaorui and Lu, Chenyang and Gill, Christopher},
title = {FCS/nORB: A feedback control real-time scheduling service for embedded ORB middleware},
year = {2008},
issue_date = {November, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {32},
number = {8},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2008.05.002},
doi = {10.1016/j.micpro.2008.05.002},
abstract = {Object Request Broker (ORB) middleware has shown promise in meeting the functional and real-time performance requirements of distributed real-time and embedded (DRE) systems. However, existing real-time ORB middleware standards such as RT-CORBA do not adequately address the challenges of (1) managing unpredictable workload, and (2) providing robust performance guarantees portably across different platforms. To overcome this limitation, we have developed software called FCS/nORB that integrates a Feedback Control real-time Scheduling (FCS) service with the nORB small-footprint real-time ORB designed for networked embedded systems. FCS/nORB features feedback control loops that provide real-time performance guarantees by automatically adjusting the rate of remote method invocations transparently to an application. FCS/nORB thus enables real-time applications to be truly portable in terms of real-time performance as well as functionality, without the need for hand tuning. This paper presents the design, implementation, and empirical evaluation of FCS/nORB. Our extensive experiments on a Linux testbed demonstrate that FCS/nORB can provide deadline miss ratio and utilization guarantees in the face of changes in platform and task execution times, while introducing only a small amount of overhead.},
journal = {Microprocess. Microsyst.},
month = nov,
pages = {413–424},
numpages = {12},
keywords = {Real-time scheduling, Performance portability, Object request broker, Middleware, Feedback control}
}

@article{10.1016/j.csl.2016.09.001,
author = {Kalantari, Shahram and Dean, David and Sridharan, Sridha},
title = {Cross database audio visual speech adaptation for phonetic spoken term detection},
year = {2017},
issue_date = {July 2017},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {44},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2016.09.001},
doi = {10.1016/j.csl.2016.09.001},
abstract = {We show that the use of visual information helps both phone recognition and spoken term detection accuracy.Fused HMM adaptation could be utilized to benefit from multiple databases when training audio visual phone modelsAn additional audio adaptation improves cross-database training accuracy for phone recognition and spoken term detection.A post training step can be used to update all HMM parameters and further improve phone recognition accuracy Spoken term detection (STD), the process of finding all occurrences of a specified search term in a large amount of speech segments, has many applications in multimedia search and retrieval of information. It is known that use of video information in the form of lip movements can improve the performance of STD in the presence of audio noise. However, research in this direction has been hampered by the unavailability of large annotated audio visual databases for development. We propose a novel approach to develop audio visual spoken term detection when only a small (low resource) audio visual database is available for development. First, cross database training is proposed as a novel framework using the fused hidden Markov modeling (HMM) technique, which is used to train an audio model using extensive large and publicly available audio databases; then it is adapted to the visual data of the given audio visual database. This approach is shown to perform better than standard HMM joint-training method and also improves the performance of spoken term detection when used in the indexing stage. In another attempt, the external audio models are first adapted to the audio data of the given audio visual database and then they are adapted to the visual data. This approach also improves both phone recognition and spoken term detection accuracy. Finally, the cross database training technique is used as HMM initialization, and an extra parameter re-estimation step is applied on the initialized models using Baum Welch technique. The proposed approaches for audio visual model training have allowed for benefiting from both large extensive out of domain audio databases that are available and the small audio visual database that is given for development to create more accurate audio-visual models.},
journal = {Comput. Speech Lang.},
month = jul,
pages = {1–21},
numpages = {21},
keywords = {Synchronous hidden Markov model, Spoken term detection, Phone recognition, Cross-database training}
}

@inproceedings{10.1145/2487575.2488201,
author = {Bhardwaj, Anurag and Das Sarma, Atish and Di, Wei and Hamid, Raffay and Piramuthu, Robinson and Sundaresan, Neel},
title = {Palette power: enabling visual search through colors},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488201},
doi = {10.1145/2487575.2488201},
abstract = {With the explosion of mobile devices with cameras, online search has moved beyond text to other modalities like images, voice, and writing. For many applications like Fashion, image-based search offers a compelling interface as compared to text forms by better capturing the visual attributes. In this paper we present a simple and fast search algorithm that uses color as the main feature for building visual search. We show that low level cues such as color can be used to quantify image similarity and also to discriminate among products with different visual appearances. We demonstrate the effectiveness of our approach through a mobile shopping applicationfootnote{eBay Fashion App available at https://itunes.apple.com/us/app/ebay-fashion/id378358380?mt=8 and eBay image swatch is the feature indexing millions of real world fashion images}. Our approach outperforms several other state-of-the-art image retrieval algorithms for large scale image data.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1321–1329},
numpages = {9},
keywords = {visual search, search engine, image search, e-commerce},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@article{10.1016/j.neucom.2006.11.020,
author = {Steil, J. J. and G\"{o}tting, M. and Wersing, H. and K\"{o}rner, E. and Ritter, H.},
title = {Adaptive scene dependent filters for segmentation and online learning of visual objects},
year = {2007},
issue_date = {March, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {70},
number = {7–9},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2006.11.020},
doi = {10.1016/j.neucom.2006.11.020},
abstract = {We propose the adaptive scene dependent filter (ASDF) hierarchy for unsupervised learning of image segmentation, which integrates several processing pathways into a flexible, highly dynamic, and real-time capable vision architecture. It is based on forming a combined feature space from basic feature maps like, color, disparity, and pixel position. To guarantee real-time performance, we apply an enhanced vector quantization method to partition this feature space. The learned codebook defines corresponding best-match segments for each prototype and yields an over-segmentation of the object and the surround. The segments are recombined into a final object segmentation mask based on a relevance map, which encodes a coarse bottom-up hypothesis where the object is located in the image. We apply the ASDF hierarchy for preprocessing input images in a feature-based biologically motivated object recognition learning architecture and show experiments with this real-time vision system running at 6Hz including the online learning of the segmentation. Because interaction with user is not perfect, the real-world system acquires useful views effectively only at about 1.5Hz, but we show that for training a new object one hundred views taking only one minute of interaction time is sufficient.},
journal = {Neurocomput.},
month = mar,
pages = {1235–1246},
numpages = {12},
keywords = {Visual online learning, Vector quantization, Unsupervised image segmentation, Object recognition, Human-machine interaction, Cognitive vision}
}

@article{10.1007/s11263-014-0747-z,
author = {Baltieri, Davide and Vezzani, Roberto and Cucchiara, Rita},
title = {Mapping Appearance Descriptors on 3D Body Models for People Re-identification},
year = {2015},
issue_date = {February  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {111},
number = {3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-014-0747-z},
doi = {10.1007/s11263-014-0747-z},
abstract = {People Re-identification aims at associating multiple instances of a person's appearance acquired from different points of view, different cameras, or after a spatial or a limited temporal gap to the same identifier. The basic hypothesis is that the person's appearance is mostly constant. Many appearance descriptors have been adopted in the past, but they are often subject to severe perspective and view-point issues. In this paper, we propose a complete re-identification framework which exploits non-articulated 3D body models to spatially map appearance descriptors (color and gradient histograms) into the vertices of a regularly sampled 3D body surface. The matching and the shot integration steps are directly handled in the 3D body model, reducing the effects of occlusions, partial views or pose changes, which normally afflict 2D descriptors. A fast and effective model to image alignment is also proposed. It allows operation on common surveillance cameras or image collections. A comprehensive experimental evaluation is presented using the benchmark suite 3DPeS.},
journal = {Int. J. Comput. Vision},
month = feb,
pages = {345–364},
numpages = {20},
keywords = {SARC3D, People re-identification, 3D human model, 3D appearance model}
}

@article{10.1287/msom.1080.0215,
author = {Masini, Andrea and Van Wassenhove, Luk N.},
title = {ERP Competence-Building Mechanisms: An Exploratory Investigation of Configurations of ERP Adopters in the European and U.S. Manufacturing Sectors},
year = {2009},
issue_date = {Spring 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {11},
number = {2},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.1080.0215},
doi = {10.1287/msom.1080.0215},
abstract = {This paper contributes to the literature on enterprise resource planning (ERP) by pursuing two objectives. First, it identifies configurations of ERP adopters that have similar needs and develop similar competencies. Second, it tests the hypothesis that, to maximize benefits from their ERP projects, organizations should align their ERP competence-building mechanisms with the ERP needs that arise from their operational environment. The analysis of a sample of manufacturing companies that implemented ERP between 1995 and 2001 uncovers four distinct configurations representing different degrees of fit between needs and competence-building mechanisms: the frugal ERP, the extensive business process reengineering (BPR), the adaptive ERP, and the straitjacket. The results support our hypothesis and suggest that the consequences of a misfit between needs and competence-building mechanisms are more severe for companies that operate in complex and dynamic environments and have informal organizational structures than for firms with rigid structures that operate in simple and stable environments.},
journal = {Manufacturing &amp; Service Operations Management},
month = apr,
pages = {274–298},
numpages = {25},
keywords = {operations strategy, information and communication technology, enterprise resource planning (ERP), empirical research, cluster analysis}
}

@inproceedings{10.22360/SpringSim.2016.HPC.032,
author = {Polok, Lukas and Smrz, Pavel},
title = {Increasing double precision throughput on NVIDIA Maxwell GPUs},
year = {2016},
isbn = {9781510823181},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.22360/SpringSim.2016.HPC.032},
doi = {10.22360/SpringSim.2016.HPC.032},
abstract = {This paper deals with the impact the architectural changes of modern GPUs have on their use in scientific computing. It particularly focuses on significant drops in the number of double precision functional units in NVIDIA Maxwell architecture. Proposed remedies of the potential negative impact on GPGPU applications that are based on multiple precision arithmetics are discussed. Two new algorithms for fast and precise multiplication and fused multiply add for double precision arithmetics emulation are also presented here.Using these methods, we were able to boost the double precision performance of NVIDIA GTX 980 Ti from 95 GFLOPS up to 286 GFLOPS. The proposed methods are applicable also to other GPUs.},
booktitle = {Proceedings of the 24th High Performance Computing Symposium},
articleno = {20},
numpages = {8},
keywords = {multiple precision arithmetics, double precision calculation, GPGPU},
location = {Pasadena, California},
series = {HPC '16}
}

@article{10.1145/3191747,
author = {Kamminga, Jacob W. and Le, Duc V. and Meijers, Jan Pieter and Bisby, Helena and Meratnia, Nirvana and Havinga, Paul J.M.},
title = {Robust Sensor-Orientation-Independent Feature Selection for Animal Activity Recognition on Collar Tags},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191747},
doi = {10.1145/3191747},
abstract = {Fundamental challenges faced by real-time animal activity recognition include variation in motion data due to changing sensor orientations, numerous features, and energy and processing constraints of animal tags. This paper aims at finding small optimal feature sets that are lightweight and robust to the sensor's orientation. Our approach comprises four main steps. First, 3D feature vectors are selected since they are theoretically independent of orientation. Second, the least interesting features are suppressed to speed up computation and increase robustness against overfitting. Third, the features are further selected through an embedded method, which selects features through simultaneous feature selection and classification. Finally, feature sets are optimized through 10-fold cross-validation. We collected real-world data through multiple sensors around the neck of five goats. The results show that activities can be accurately recognized using only accelerometer data and a few lightweight features. Additionally, we show that the performance is robust to sensor orientation and position. A simple Naive Bayes classifier using only a single feature achieved an accuracy of 94 % with our empirical dataset. Moreover, our optimal feature set yielded an average of 94 % accuracy when applied with six other classifiers. This work supports embedded, real-time, energy-efficient, and robust activity recognition for animals.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {15},
numpages = {27},
keywords = {Animal Activity Recognition, Decision Tree, Embedded Systems, Machine Learning, Naive Bayes, Sensor Orientation}
}

@article{10.1162/COLI_a_00199,
author = {Mairesse, Fran\c{c}ois and Young, Steve},
title = {Stochastic language generation in dialogue using factored language models},
year = {2014},
issue_date = {December 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00199},
doi = {10.1162/COLI_a_00199},
abstract = {Most previous work on trainable language generation has focused on two paradigms: (a) using a generation decisions of an existing generator. Both approaches rely on the existence of a handcrafted generation component, which is likely to limit their scalability to new domains. The first contribution of this article is to present Bagel, a fully data-driven generation method that treats the language generation task as a search for the most likely sequence of semantic concepts and realization phrases, according to Factored Language Models (FLMs). As domain utterances are not readily available for most natural language generation tasks, a large creative effort is required to produce the data necessary to represent human linguistic variation for nontrivial domains. This article is based on the assumption that learning to produce paraphrases can be facilitated by collecting data from a large sample of untrained annotators using crowdsourcing—rather than a few domain experts—by relying on a coarse meaning representation. A second contribution of this article is to use crowdsourced data to show how dialogue naturalness can be improved by learning to vary the output utterances generated for a given semantic input. Two data-driven methods for generating paraphrases in dialogue are presented: (a) by sampling from the n-best list of realizations produced by Bagel's FLM reranker; and (b) by learning a structured perceptron predicting whether candidate realizations are valid paraphrases. We train Bagel on a set of 1,956 utterances produced by 137 annotators, which covers 10 types of dialogue acts and 128 semantic concepts in a tourist information system for Cambridge. An automated evaluation shows that Bagel outperforms utterance class LM baselines on this domain. A human evaluation of 600 resynthesized dialogue extracts shows that Bagel's FLM output produces utterances comparable to a handcrafted baseline, whereas the perceptron classifier performs worse. Interestingly, human judges find the system sampling from the n-best list to be more natural than a system always returning the first-best utterance. The judges are also more willing to interact with the n-best system in the future. These results suggest that capturing the large variation found in human language using data-driven methods is beneficial for dialogue interaction.},
journal = {Comput. Linguist.},
month = dec,
pages = {763–799},
numpages = {37}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {self-adaptive system, search-based software engineering, performance engineering, multi-objective optimization, multi-objective evolutionary algorithm, Feature model}
}

@article{10.1016/j.jpdc.2012.03.003,
author = {Shestak, Vladimir and Chong, Edwin K. P. and Maciejewski, Anthony A. and Siegel, Howard Jay},
title = {Probabilistic resource allocation in heterogeneous distributed systems with random failures},
year = {2012},
issue_date = {October, 2012},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {72},
number = {10},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2012.03.003},
doi = {10.1016/j.jpdc.2012.03.003},
abstract = {The problem of finding efficient workload distribution techniques is becoming increasingly important today for heterogeneous distributed systems where the availability of compute nodes may change spontaneously over time. Resource-allocation policies designed for such systems should maximize the performance and, at the same time, be robust against failure and recovery of compute nodes. Such a policy, based on the concepts of the Derman-Lieberman-Ross theorem, is proposed in this work, and is applied to a simulated model of a dedicated system composed of a set of heterogeneous image processing servers. Assuming that each image results in a ''reward'' if its processing is completed before a certain deadline, the goal for the resource allocation policy is to maximize the expected cumulative reward. An extensive analysis was done to study the performance of the proposed policy and compare it with the performance of some existing policies adapted to this environment. Our experiments conducted for various types of task-machine heterogeneity illustrate the potential of our method for solving resource allocation problems in a broad spectrum of distributed systems that experience high failure rates.},
journal = {J. Parallel Distrib. Comput.},
month = oct,
pages = {1186–1194},
numpages = {9},
keywords = {Resource allocation, Load balancing, Fault tolerant systems, Distributed systems}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@article{10.1145/2996183,
author = {Alsaedi, Nasser and Burnap, Pete and Rana, Omer},
title = {Can We Predict a Riot? Disruptive Event Detection Using Twitter},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2996183},
doi = {10.1145/2996183},
abstract = {In recent years, there has been increased interest in real-world event detection using publicly accessible data made available through Internet technology such as Twitter, Facebook, and YouTube. In these highly interactive systems, the general public are able to post real-time reactions to “real world” events, thereby acting as social sensors of terrestrial activity. Automatically detecting and categorizing events, particularly small-scale incidents, using streamed data is a non-trivial task but would be of high value to public safety organisations such as local police, who need to respond accordingly. To address this challenge, we present an end-to-end integrated event detection framework that comprises five main components: data collection, pre-processing, classification, online clustering, and summarization. The integration between classification and clustering enables events to be detected, as well as related smaller-scale “disruptive events,” smaller incidents that threaten social safety and security or could disrupt social order. We present an evaluation of the effectiveness of detecting events using a variety of features derived from Twitter posts, namely temporal, spatial, and textual content. We evaluate our framework on a large-scale, real-world dataset from Twitter. Furthermore, we apply our event detection system to a large corpus of tweets posted during the August 2011 riots in England. We use ground-truth data based on intelligence gathered by the London Metropolitan Police Service, which provides a record of actual terrestrial events and incidents during the riots, and show that our system can perform as well as terrestrial sources, and even better in some cases.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {18},
numpages = {26},
keywords = {feature selection, event detection, evaluation, clustering, classification, Social media}
}

@article{10.1016/j.eswa.2011.05.004,
author = {Ekbal, Asif and Saha, Sriparna},
title = {A multiobjective simulated annealing approach for classifier ensemble: Named entity recognition in Indian languages as case studies},
year = {2011},
issue_date = {November, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.05.004},
doi = {10.1016/j.eswa.2011.05.004},
abstract = {In this paper, we propose a simulated annealing (SA) based multiobjective optimization (MOO) approach for classifier ensemble. Several different versions of the objective functions are exploited. We hypothesize that the reliability of prediction of each classifier differs among the various output classes. Thus, in an ensemble system, it is necessary to find out the appropriate weight of vote for each output class in each classifier. Diverse classification methods such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) are used to build different models depending upon the various representations of the available features. One most important characteristics of our system is that the features are selected and developed mostly without using any deep domain knowledge and/or language dependent resources. The proposed technique is evaluated for Named Entity Recognition (NER) in three resource-poor Indian languages, namely Bengali, Hindi and Telugu. Evaluation results yield the recall, precision and F-measure values of 93.95%, 95.15% and 94.55%, respectively for Bengali, 93.35%, 92.25% and 92.80%, respectively for Hindi and 84.02%, 96.56% and 89.85%, respectively for Telugu. Experiments also suggest that the classifier ensemble identified by the proposed MOO based approach optimizing the F-measure values of named entity (NE) boundary detection outperforms all the individual models, two conventional baseline models and three other MOO based ensembles.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {14760–14772},
numpages = {13},
keywords = {Weighted voting, Support Vector Machine (SVM), Simulated annealing (SA), Natural language processing, Named entity recognition, Multiobjective optimization (MOO), Maximum Entropy (ME), Conditional Random Field (CRF), Classifier ensemble}
}

@article{10.1147/JRD.2015.2390017,
author = {Abedini, M. and Codella, N. C. F. and Connell, J. H. and Garnavi, R. and Merler, M. and Pankanti, S. and Smith, J. R. and Syeda-Mahmood, T.},
title = {A generalized framework for medical image classification and recognition},
year = {2015},
issue_date = {March/May 2015},
publisher = {IBM Corp.},
address = {USA},
volume = {59},
number = {2–3},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2015.2390017},
doi = {10.1147/JRD.2015.2390017},
abstract = {In this work, we study the performance of a two-stage ensemble visual machine learning framework for classification of medical images. In the first stage, models are built for subsets of features and data, and in the second stage, models are combined. We demonstrate the performance of this framework in four contexts: 1) The public ImageCLEF (Cross Language Evaluation Forum) 2013 medical modality recognition benchmark, 2) echocardiography view and mode recognition, 3) dermatology disease recognition across two datasets, and 4) a broad medical image dataset, merged from multiple data sources into a collection of 158 categories covering both general and specific medical conceptsVincluding modalities, body regions, views, and disease states. In the first context, the presented system achieves state-of-art performance of 82.2% multiclass accuracy. In the second context, the system attains 90.48% multiclass accuracy. In the third, state-of-art performance of 90% specificity and 90% sensitivity is obtained on a small standardized dataset of 200 images using a leave-one-out strategy. For a larger dataset of 2,761 images, 95% specificity and 98% sensitivity is obtained on a 20% held-out test set. Finally, in the fourth context, the system achieves sensitivity and specificity of 94.7% and 98.4%, respectively, demonstrating the ability to generalize over domains.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {1:1–1:18},
numpages = {18}
}

@article{10.1016/j.jnca.2015.03.006,
title = {Implementing multiple biometric features for a recall-based graphical keystroke dynamics authentication system on a smart phone},
year = {2015},
issue_date = {July 2015},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {53},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2015.03.006},
doi = {10.1016/j.jnca.2015.03.006},
abstract = {Keystroke Dynamics-based Authentication (KDA) is a type of behavioral biometric method. It verifies user identity via the keystroke features gathered from the keystroke events provided by users on a QWERTY keyboard. With the growing use of smart phones, the traditional keypad on mobile phones has been replaced by touch screen devices. The keypad-based KDA is no longer suitable for smart phones. This paper proposes a KDA system implemented using multiple biometric features applied to the pattern lock layout on a smart phone. Except for the time, pressure and size of the keystroke features presented in previous research, we additionally adopted a novel angle keystroke feature and determined the best combination of these features in a series of experiments. As the results show, with 10 training samples involved, the combination of time, pressure and angle offers the best utility (Equal Error Rate of 3.03%).},
journal = {J. Netw. Comput. Appl.},
month = jul,
pages = {128–139},
numpages = {12}
}

@inproceedings{10.5555/1780533.1780535,
author = {Viitaniemi, Ville and Laaksonen, Jorma},
title = {Improving the accuracy of global feature fusion based image categorisation},
year = {2007},
isbn = {354077033X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we consider the task of categorising images of the Corel collection into semantic classes. In our earlier work, we demonstrated that state-of-the-art accuracy of supervised categorising of these images could be improved significantly by fusion of a large number of global image features. In this work, we preserve the general framework, but improve the components of the system: we modify the set of image features to include interest point histogram features, perform elementary feature classification with support vector machines (SVM) instead of self-organising map (SOM) based classifiers, and fuse the classification results with either an additive, multiplicative or SVM-based technique. As the main result of this paper, we are able to achieve a significant improvement of image categorisation accuracy by applying these generic state-of-the-art image content analysis techniques.},
booktitle = {Proceedings of the Semantic and Digital Media Technologies 2nd International Conference on Semantic Multimedia},
pages = {1–14},
numpages = {14},
location = {Genoa, Italy},
series = {SAMT'07}
}

@article{10.1016/j.vlsi.2009.09.001,
author = {Barros, Manuel and Guilherme, Jorge and Horta, Nuno},
title = {Analog circuits optimization based on evolutionary computation techniques},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {1},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2009.09.001},
doi = {10.1016/j.vlsi.2009.09.001},
abstract = {This paper presents a new design automation tool, based on a modified genetic algorithm kernel, in order to improve efficiency on the analog IC design cycle. The proposed approach combines a robust optimization with corner analysis, machine learning techniques and distributed processing capability able to deal with multi-objective and constrained optimization problems. The resulting optimization tool and the improvement in design productivity is demonstrated for the design of CMOS operational amplifiers.},
journal = {Integr. VLSI J.},
month = jan,
pages = {136–155},
numpages = {20},
keywords = {Learning strategies, Evolutionary optimization, Design automation, Analog integrated circuit synthesis}
}

@inproceedings{10.1145/1409240.1409315,
author = {van den Bosch, Antal and Bogers, Toine},
title = {Efficient context-sensitive word completion for mobile devices},
year = {2008},
isbn = {9781595939524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409240.1409315},
doi = {10.1145/1409240.1409315},
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
booktitle = {Proceedings of the 10th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {465–470},
numpages = {6},
keywords = {word completion, scaling, predictive text processing, mobile devices, ergonomics, context sensitivity},
location = {Amsterdam, The Netherlands},
series = {MobileHCI '08}
}

@inproceedings{10.1145/2737166.2737176,
author = {Wahler, Michael and Oriol, Manuel and Monot, Aur\'{e}lien},
title = {Real-time Multi-core Components for Cyber-physical Systems},
year = {2015},
isbn = {9781450334716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737166.2737176},
doi = {10.1145/2737166.2737176},
abstract = {Developing correct, efficient, and maintainable real-time control software for cyber-physical systems is a notoriously difficult interdisciplinary challenge. Ever more complex control algorithms and the advent of multi-core hardware in embedded systems have made this challenge even harder. Component-based software development promises to help reduce the complexity and to increase the timing predictability for time-critical software. This paper presents FASA, a component-based approach for scalable real-time systems. This approach offers a platform-independent development method with a high degree of predictability, supports multi-core systems by design, and simplifies maintenance. Two case studies validate FASA: an application handling a magnetic levitation device and an example of scalability.},
booktitle = {Proceedings of the 18th International ACM SIGSOFT Symposium on Component-Based Software Engineering},
pages = {37–42},
numpages = {6},
keywords = {real-time, predictability, multi-core, maintenance, component},
location = {Montr\'{e}al, QC, Canada},
series = {CBSE '15}
}

@inproceedings{10.5555/1784829.1784844,
author = {Domont, Xavier and Heckmann, Martin and Wersing, Heiko and Joublin, Frank and Menzel, Stefan and Sendhoff, Bernhard and Goerick, Christian},
title = {Word recognition with a hierarchical neural network},
year = {2007},
isbn = {3540773460},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we propose a feedforward neural network for syllable recognition. The core of the recognition system is based on a hierarchical architecture initially developed for visual object recognition. We show that, given the similarities between the primary auditory and visual cortexes, such a system can successfully be used for speech recognition. Syllables are used as basic units for the recognition. Their spectrograms, computed using a Gammatone filterbank, are interpreted as images and subsequently feed into the neural network after a preprocessing step that enhances the formant frequencies and normalizes the length of the syllables. The performance of our system has been analyzed on the recognition of 25 different monosyllabic words. The parameters of the architecture have been optimized using an evolutionary strategy. Compared to the Sphinx-4 speech recognition system, our system achieves better robustness and generalization capabilities in noisy conditions.},
booktitle = {Proceedings of the 2007 International Conference on Advances in Nonlinear Speech Processing},
pages = {142–151},
numpages = {10},
keywords = {speechrecognition, robustfeatures, feed-forward architecture},
location = {Paris, France},
series = {NOLISP'07}
}

@article{10.5555/2594952.2594988,
author = {Qin, Linchan and Zhong, Ning and Lu, Shengfu and Li, Mi},
title = {Decision Prediction Using Visual Patterns},
year = {2013},
issue_date = {January 2013},
publisher = {IOS Press},
address = {NLD},
volume = {127},
number = {1–4},
issn = {0169-2968},
abstract = {Lack of understanding of users' underlying decision making process results in the bottleneck of EB-HCI eye movement-based human-computer interaction systems. Meanwhile, considerable findings on visual features of decision making have been derived from cognitive researches over past few years. A promising method of decision prediction in EB-HCI systems is presented in this article, which is inspired by the looking behavior when a user makes a decision. As two features of visual decision making, gaze bias and pupil dilation are considered into judging intensions. This method combines the history of eye movements to a given interface and the visual traits of users. Hence, it improves the prediction performance in a more natural and objective way. We apply the method to an either-or choice making task on the commercial Web pages to test its effectiveness. Although the result shows a good performance only of gaze bias but not of pupil dilation to predict a decision, it proves that hiring the visual traits of users is an effective approach to improve the performance of automatic triggering in EB-HCI systems.},
journal = {Fundam. Inf.},
month = jan,
pages = {545–560},
numpages = {16}
}

@article{10.1155/2019/9293617,
author = {Rahman, Chnoor M. and Rashid, Tarik A. and Lawrynczuk, Maciej},
title = {Dragonfly Algorithm and Its Applications in Applied Science Survey},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/9293617},
doi = {10.1155/2019/9293617},
abstract = {One of the most recently developed heuristic optimization algorithms is dragonfly by Mirjalili. Dragonfly algorithm has shown its ability to optimizing different real-world problems. It has three variants. In this work, an overview of the algorithm and its variants is presented. Moreover, the hybridization versions of the algorithm are discussed. Furthermore, the results of the applications that utilized the dragonfly algorithm in applied science are offered in the following area: machine learning, image processing, wireless, and networking. It is then compared with some other metaheuristic algorithms. In addition, the algorithm is tested on the CEC-C06 2019 benchmark functions. The results prove that the algorithm has great exploration ability and its convergence rate is better than the other algorithms in the literature, such as PSO and GA. In general, in this survey, the strong and weak points of the algorithm are discussed. Furthermore, some future works that will help in improving the algorithm’s weak points are recommended. This study is conducted with the hope of offering beneficial information about dragonfly algorithm to the researchers who want to study the algorithm.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {21}
}

@article{10.5555/2882728.2882739,
author = {Anderson, Erin},
title = {The Salesperson as Outside Agent or Employee: A Transaction Cost Analysis},
year = {2008},
issue_date = {January 2008},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {27},
number = {1},
issn = {1526-548X},
abstract = {This descriptive study explores the reasons for integration of the personal selling function, i.e., the use of employee “direct” salespeople rather than manufacturers' representatives “reps”. A hypothesized model is developed based on both transaction cost analysis and the sales force management literature. Data from 13 electronic component manufacturers covering 159 U.S. sales districts are used to estimate a logistic model of the probability of “going direct” in a given district. Results are shown to be stable across specification and estimation methods and to fit the data well. The transaction cost model is generally supported. The principal finding is that the greater the difficulty of evaluating a salesperson's performance, the more likely the firm to substitute surveillance for commission as a control mechanism, i.e., to use a direct sales force. Among other findings, direct sales forces are also associated with complex, hard-to-learn product lines and with districts that demand considerable nonselling activities. Several factors prove unrelated, including company size, the amount of travel a district requires, and the importance of key accounts.This article was originally published in Marketing Science, Volume 4, Issue 3, Pages 234--254, in 1985.},
journal = {Marketing Science},
month = jan,
pages = {70–84},
numpages = {15},
keywords = {vertical integration, sales force management, organizational design, organization control}
}

@inproceedings{10.5555/800288.811223,
author = {Lavenberg, Stephen S. and Welch, Peter D.},
title = {Variance reduction techniques},
year = {1978},
publisher = {IEEE Press},
abstract = {This talk is concerned with statistical techniques, known as variance reduction techniques, for increasing simulation efficiency. The emphasis will be on critically assessing the practical applicability of variance reduction techniques in discrete event simulations of complex stochastic models of systems. Examples of the successful application of such techniques will be presented.},
booktitle = {Proceedings of the 10th Conference on Winter Simulation - Volume 1},
pages = {167–170},
numpages = {4},
location = {Miami Beach, FL},
series = {WSC '78}
}

@inproceedings{10.1145/1743384.1743477,
author = {Hare, Jonathon S. and Lewis, Paul H.},
title = {Automatically annotating the MIR Flickr dataset: experimental protocols, openly available data and semantic spaces},
year = {2010},
isbn = {9781605588155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743384.1743477},
doi = {10.1145/1743384.1743477},
abstract = {The availability of a large, freely redistributable set of high-quality annotated images is critical to allowing researchers in the area of automatic annotation, generic object recognition and concept detection to compare results. The recent introduction of the MIR Flickr dataset allows researchers such access. A dataset by itself is not enough, and a set of repeatable guidelines for performing evaluations that are comparable is required. In many cases it also is useful to compare the machine-learning components of different automatic annotation techniques using a common set of image features.This paper seeks to provide a solid, repeatable methodology and protocol for performing evaluations of automatic annotation software using the MIR Flickr dataset together with freely available tools for measuring performance in a controlled manner. This protocol is demonstrated through a set of experiments using a "semantic space" auto-annotator previously developed by the authors, in combination with a set of visual term features for the images that has been made publicly available for download. The paper also discusses how much training data is required to train the semantic space annotator with the MIR Flickr dataset. It is the hope of the authors that researchers will adopt this methodology and produce results from their own annotators that can be directly compared to those presented in this work.},
booktitle = {Proceedings of the International Conference on Multimedia Information Retrieval},
pages = {547–556},
numpages = {10},
keywords = {visual-terms, semantic spaces, semantic image retrieval, image content analysis, evaluation, automatic annotation},
location = {Philadelphia, Pennsylvania, USA},
series = {MIR '10}
}

@article{10.25300/MISQ/2014/38.2.08,
author = {Ho, Shuk Ying and Bodoff, David},
title = {The effects of web personalization on user attitude and behavior: an integration of the elaboration likelihood model and consumer search theory},
year = {2014},
issue_date = {June 2014},
publisher = {Society for Information Management and The Management Information Systems Research Center},
address = {USA},
volume = {38},
number = {2},
issn = {0276-7783},
url = {https://doi.org/10.25300/MISQ/2014/38.2.08},
doi = {10.25300/MISQ/2014/38.2.08},
abstract = {Web personalization can achieve two business goals: increased advertising revenue and increased sales revenue. The realization of the two goals is related to two kinds of user behavior: item sampling and item selection. Prior research does not provide a model of attitude formation toward a personalization agent nor of how attitudes relate to these two behaviors. This limits our understanding of how web personalization can be managed to increase advertising revenues and/or sales revenues. To fill this gap, the current research develops and tests a theoretical model of user attitudes and behaviors toward a personalization agent. The model is based on an integration of two theories: the elaboration likelihood model (ELM) and consumer search theory (CST). In the integrated model, a user's attitude toward a personalization agent is influenced by both the number of items he/she has sampled so far (from CST) and the degree to which he/she cognitively processes each one (from ELM). In turn, attitude is modeled to influence both behaviors--that is, item selection and any further item sampling. We conducted a lab study and a field study to test six hypotheses. This research extends the theory on web personalization by providing a more complete picture of how sampling and processing of personalized recommendations influence a user's attitude and behavior toward the personalization agent. For online merchants, this research highlights the trade-off between item sampling and item selection and provides practical guidance on how to steer users toward the attitudes and behaviors that will realize their business goals.},
journal = {MIS Q.},
month = jun,
pages = {497–520},
numpages = {24},
keywords = {web personalization, elaboration likelihood model, consumer search theory, attitude persistence, attitude confidence}
}

@inproceedings{10.1145/2627373.2627385,
author = {Li, Shuo and Geva, Robert},
title = {Extract and Extend Parallelism using C/C++ Extension for Array Notation on Multicore and Many-core Platforms: An Empirical Investigation with Quantitative Finance Examples},
year = {2014},
isbn = {9781450329378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627373.2627385},
doi = {10.1145/2627373.2627385},
abstract = {In this paper, we explore the newly introduced array notion syntax extension in recent release of Intel Compiler with a few representative quantitative finance workloads. We will explore the array syntax both as an abstraction tool to allow the user to succinctly express the intended operations and as a performance tool that facilitates the most efficient implementation which can take advantage of the parallel hardware resource such as vector processing units and ever increasing number of processor cores. We specifically look at how these new array style programming capability can help the financial modeler and software developers to extract the parallelism from the numerical algorithm and extend it from multicore host processor to a hybrid of multicore many-core accelerated computing environment.We start with a functional introduction to the C++ array notation syntax that will be used in the subsequent examples. We, then, present background information on a few derivative pricing algorithms in quantitative finance. For each algorithm, we present a scalar program first and take a performance measurement as baseline. As we choose to use the array programming mechanism, we will look at the programming language related issues and postulate what syntax motivate the developer to use then what the alternative syntax are and why some might be more popular than others. Then we look at the performance related issues and look at the code generation on Intel Architecture based multicore and many-core platforms, and investigate mechanism for performance optimization. We conclude the paper by creating a hybrid program that runs both on multicore and many-core environment, concurrently.},
booktitle = {Proceedings of ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {68–75},
numpages = {8},
location = {Edinburgh, United Kingdom},
series = {ARRAY'14}
}

@article{10.1155/2010/465612,
author = {Zou, Tongyuan and Yang, Wen and Dai, Dengxin and Sun, Hong},
title = {Polarimetric SAR image classification using multifeatures combination and extremely randomized clustering forests},
year = {2010},
issue_date = {January 2010},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2010},
issn = {1110-8657},
url = {https://doi.org/10.1155/2010/465612},
doi = {10.1155/2010/465612},
abstract = {Terrain classification using polarimetric SAR imagery has been a very active research field over recent years. Although lots of features have been proposed and many classifiers have been employed, there are few works on comparing these features and their combination with different classifiers. In this paper, we firstly evaluate and compare different features for classifying polarimetric SAR imagery. Then, we propose two strategies for feature combination: manual selection according to heuristic rules and automatic combination based on a simple but efficient criterion. Finally, we introduce extremely randomized clustering forests (ERCFs) to polarimetric SAR image classification and compare it with other competitive classifiers. Experiments on ALOS PALSAR image validate the effectiveness of the feature combination strategies and also show that ERCFs achieves competitive performance with other widely used classifiers while costing much less training and testing time.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
articleno = {4},
numpages = {12}
}

@article{10.1145/2996465,
author = {Guo, Guangming and Zhu, Feida and Chen, Enhong and Liu, Qi and Wu, Le and Guan, Chu},
title = {From Footprint to Evidence: An Exploratory Study of Mining Social Data for Credit Scoring},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/2996465},
doi = {10.1145/2996465},
abstract = {With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users’ credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-data-based credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users’ social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online social media data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring.},
journal = {ACM Trans. Web},
month = dec,
articleno = {22},
numpages = {38},
keywords = {user profiling, social data, features, consumer finance, Personal credit scoring, P2P lending}
}

@article{10.1017/S0269888903000699,
author = {Anonymous},
title = {From the journals …},
year = {2003},
issue_date = {June 2003},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {2},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888903000699},
doi = {10.1017/S0269888903000699},
journal = {Knowl. Eng. Rev.},
month = jun,
pages = {183–191},
numpages = {9}
}

@article{10.5555/1349897.1350159,
author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
title = {Software architecture reliability analysis using failure scenarios},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
journal = {J. Syst. Softw.},
month = apr,
pages = {558–575},
numpages = {18},
keywords = {Scenario-based architectural evaluation, Reliability analysis, Fault trees, FMEA}
}

@article{10.1017/S0269888909990099,
title = {From the journals…},
year = {2009},
issue_date = {December 2009},
publisher = {Cambridge University Press},
address = {USA},
volume = {24},
number = {4},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888909990099},
doi = {10.1017/S0269888909990099},
journal = {Knowl. Eng. Rev.},
month = dec,
pages = {417–437},
numpages = {21}
}

@inproceedings{10.5555/976440.976462,
author = {Xie, Huayang and Andreae, Peter and Zhang, Mengjie and Warren, Paul},
title = {Detecting stress in spoken English using Decision Trees and Support Vector Machines},
year = {2004},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {This paper describes an approach to the detection of stress in spoken New Zealand English. After identifying the vowel segments of the speech signal, the approach extracts two different sets of features - prosodic features and vowel quality features - from the vowel segments. These features are then normalised and scaled to obtain speaker independent feature values that can be used to classify each vowel segment as stressed or unstressed. We used Decision Trees (C4.5) and Support Vector Machines (LIBSVM) to learn stress-detecting classifiers with various combinations of the features. The approach was evaluated on 60 adult female utterances with 703 vowels and a maximum accuracy of 84.72% was achieved. The results showed that a combination of features derived from duration and amplitude achieved the best performance but the vowel quality features also achieved quite reasonable results.},
booktitle = {Proceedings of the Second Workshop on Australasian Information Security, Data Mining and Web Intelligence, and Software Internationalisation - Volume 32},
pages = {145–150},
numpages = {6},
keywords = {decision tree, feature extraction, machine learning, speech recognition, stress detection, support vector machine},
location = {Dunedin, New Zealand},
series = {ACSW Frontiers '04}
}

@inproceedings{10.1145/2420950.2420969,
author = {Bilge, Leyla and Balzarotti, Davide and Robertson, William and Kirda, Engin and Kruegel, Christopher},
title = {Disclosure: detecting botnet command and control servers through large-scale NetFlow analysis},
year = {2012},
isbn = {9781450313124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420950.2420969},
doi = {10.1145/2420950.2420969},
abstract = {Botnets continue to be a significant problem on the Internet. Accordingly, a great deal of research has focused on methods for detecting and mitigating the effects of botnets. Two of the primary factors preventing the development of effective large-scale, wide-area botnet detection systems are seemingly contradictory. On the one hand, technical and administrative restrictions result in a general unavailability of raw network data that would facilitate botnet detection on a large scale. On the other hand, were this data available, real-time processing at that scale would be a formidable challenge. In contrast to raw network data, NetFlow data is widely available. However, NetFlow data imposes several challenges for performing accurate botnet detection.In this paper, we present Disclosure, a large-scale, wide-area botnet detection system that incorporates a combination of novel techniques to overcome the challenges imposed by the use of NetFlow data. In particular, we identify several groups of features that allow Disclosure to reliably distinguish C&amp;C channels from benign traffic using NetFlow records (i.e., flow sizes, client access patterns, and temporal behavior). To reduce Disclosure's false positive rate, we incorporate a number of external reputation scores into our system's detection procedure. Finally, we provide an extensive evaluation of Disclosure over two large, real-world networks. Our evaluation demonstrates that Disclosure is able to perform real-time detection of botnet C&amp;C channels over datasets on the order of billions of flows per day.},
booktitle = {Proceedings of the 28th Annual Computer Security Applications Conference},
pages = {129–138},
numpages = {10},
location = {Orlando, Florida, USA},
series = {ACSAC '12}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@article{10.4018/jdst.2010040105,
author = {Brightwell, Ron and Camp, William J. and Dosanjh, Sudip and Kelly, Suzanne M. and Levesque, John and Lin, Paul T. and Tipparaju, Vinod and Vaughan, Courtenay T. and Tomkins, James L.},
title = {The Red Storm Architecture and Early Experiences with Multi-Core Processors},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {2},
issn = {1947-3532},
url = {https://doi.org/10.4018/jdst.2010040105},
doi = {10.4018/jdst.2010040105},
abstract = {The Red Storm architecture, which was conceived by Sandia National Laboratories and implemented by Cray, Inc., has become the basis for most successful line of commercial supercomputers in history. The success of the Red Storm architecture is due largely to the ability to effectively and efficiently solve a wide range of science and engineering problems. The Cray XT series of machines that embody the Red Storm architecture have allowed for unprecedented scaling and performance of parallel applications spanning many areas of scientific computing. This paper describes the fundamental characteristics of the architecture and its implementation that have enabled this success, even through successive generations of hardware and software.},
journal = {Int. J. Distrib. Syst. Technol.},
month = apr,
pages = {74–93},
numpages = {20},
keywords = {Supercomputers, Parallel Processing Systems, Multiprocessing Systems, Massively Parallel Processor, Large-Scale Computers, High-Performance Computing}
}

@inproceedings{10.1145/2030376.2030402,
author = {Porenta, Jernej and Ciglari\v{c}, Mojca},
title = {Empirical comparison of IP reputation databases},
year = {2011},
isbn = {9781450307888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030376.2030402},
doi = {10.1145/2030376.2030402},
abstract = {IP reputation is a common technique to address email spam problem and while there are commercial implementations available, the algorithms behind them are confidential. A few open source implementations (gossip, RepuScore, IP-GroupREP, etc.) are available, but few studies compare their commercial counterparts.For this reason, we have made an empirical comparison of six popular commercial IP reputation databases and three different open-source IP reputation algorithms. We built our own IP reputation database from our email corpus, containing 931,576 email messages from real-time email traffic at an academic ISP. After we processed and classified the corpus, we compared the open-source IP reputation algorithms' results with commercial IP reputation databases by using the Spearman rank correlation coefficient to identify the optimal parameters for open-source algorithms.The results show lower correlation coefficients when the frequency of emails from a single IP is rising. Open-source algorithms performed sufficiently for IP numbers with more than five and less than 50 emails from a single IP, while (surprisingly) the correlation dropped with a higher number of emails from a single IP. For this reason, we believe there should be some additional fine-tuning of open-source algorithms to make them comparable to their commercial counterparts that have IP reputation scores built from many sensors around the world.We also compared commercial IP reputation databases and found mixed correlations between them, which raised many questions regarding the algorithms used for building IP reputation scores. The research also identified the problem of finding a good methodology for comparing IP reputation databases.},
booktitle = {Proceedings of the 8th Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference},
pages = {220–226},
numpages = {7},
keywords = {email, antispam filtering, IP reputation},
location = {Perth, Australia},
series = {CEAS '11}
}

@inproceedings{10.1007/978-3-642-27609-5_10,
author = {Epstein, Susan L. and Passonneau, Rebecca and Ligorio, Tiziana and Gordon, Joshua},
title = {Data mining to support human-machine dialogue for autonomous agents},
year = {2011},
isbn = {9783642276088},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27609-5_10},
doi = {10.1007/978-3-642-27609-5_10},
abstract = {Next-generation autonomous agents will be expected to converse with people to achieve their mutual goals. Human-machine dialogue, however, is challenged by noisy acoustic data, and by people's preference for more natural interaction. This paper describes an ambitious project that embeds human subjects in a spoken dialogue system. It collects a rich and novel data set, including spoken dialogue, human behavior, and system features. During data collection, subjects were restricted to the same databases, action choices, and noisy automated speech recognition output as a spoken dialogue system. This paper mines that data to learn how people manage the problems that arise during dialogue under such restrictions. Two different approaches to successful, goal-directed dialogue are identified this way, from which supervised learning can predict appropriate dialogue choices. The resultant models can then be incorporated into an autonomous agent that seeks to assist its user.},
booktitle = {Proceedings of the 7th International Conference on Agents and Data Mining Interaction},
pages = {132–155},
numpages = {24},
keywords = {wizard of oz, spoken dialogue systems, human-machine interaction},
location = {Taipei, Taiwan},
series = {ADMI'11}
}

@inproceedings{10.1145/268437.268617,
author = {Shady, Ron and Spake, Gary and Armstrong, Brad},
title = {Simulation of a new product workcell},
year = {1997},
isbn = {078034278X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1145/268437.268617},
doi = {10.1145/268437.268617},
booktitle = {Proceedings of the 29th Conference on Winter Simulation},
pages = {739–743},
numpages = {5},
location = {Atlanta, Georgia, USA},
series = {WSC '97}
}

@inproceedings{10.1145/1052898.1052902,
author = {Tesanovic, Aleksandra and Amirijoo, Mehdi and Bj\"{o}rk, Mikael and Hansson, J\"{o}rgen},
title = {Empowering configurable QoS management in real-time systems},
year = {2005},
isbn = {1595930426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1052898.1052902},
doi = {10.1145/1052898.1052902},
abstract = {Current Quality of Service (QoS) management methods in real-time systems using feedback control loop lack support for configurability and reusability as they cannot be configured for a target application or reused across different applications. In this paper we present a method for developing reconfigurable feedback-based QoS management for real-time systems, denoted Re-QoS. By combining component-based design with aspect-oriented software development Re-QoS enables successful handling of crosscutting nature of QoS policies, as well as evolutionary design of real-time systems and QoS management architectures. Re-QoS defines a QoS aspect package, which is an implementation of a set of aspects and components that provide a number of different QoS policies. By adding a QoS aspect package to an existing system without QoS guarantees, we are able to use the same system in unpredictable environments where performance guarantees are essential. Furthermore, by exchanging aspects within the QoS aspect package one can efficiently tailor the QoS management of a real-time system based on the application requirements. We demonstrate the usefulness of the concept on a case study of an embedded real-time database system, called COMET. Using the COMET example we show how a real-time database system can be adapted to be used in different applications with distinct QoS needs.},
booktitle = {Proceedings of the 4th International Conference on Aspect-Oriented Software Development},
pages = {39–50},
numpages = {12},
location = {Chicago, Illinois},
series = {AOSD '05}
}

@article{10.1016/j.cviu.2010.11.006,
author = {Chen, Shi and Wang, Jinqiao and Ouyang, Yi and Wang, Bo and Xu, Changsheng and Lu, Hanqing},
title = {Boosting part-sense multi-feature learners toward effective object detection},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {115},
number = {3},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2010.11.006},
doi = {10.1016/j.cviu.2010.11.006},
abstract = {AdaBoost has been applied to object detection to construct the detectors with high performance of discrimination and generalization by single-feature learner. However, the poor discriminative power of extremely weak single-feature learners limits its application for general object detection. In this paper, we propose a novel comprehensive learner design mechanism toward effective object detection in terms of both discrimination and generalization abilities. Firstly, the part-sense multi-feature learners are designed to linearly combine the multiple local features to improve the descriptive and discriminative capacity of the learner. Secondly, we formulate the feature selection in part-sense multi-feature learner as a weighted LASSO regression. Using Least Angle Regression (LARS) method, our approach can choose features adaptively, efficiently and as few as possible to guarantee generalization performance. Finally, a robust L1-regularized gradient boosting is proposed to integrate our part-sense sparse features learner into an object classifier. Extensive experiments and comparisons on the face dataset and the human dataset show the proposed approach outperforms the traditional single-feature learner and other multi-feature learners in discriminative and generalization abilities.},
journal = {Comput. Vis. Image Underst.},
month = mar,
pages = {364–374},
numpages = {11},
keywords = {Object detection, Multi-feature learners, L1-regularized gradient boosting, AdaBoost}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {Systems engineering, Systematic mapping study, SysML}
}

@article{10.1007/s10270-013-0370-4,
author = {Syriani, Eugene and Vangheluwe, Hans and Lashomb, Brian},
title = {T-Core: a framework for custom-built model transformation engines},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0370-4},
doi = {10.1007/s10270-013-0370-4},
abstract = {A large number of model transformation languages and tools have emerged since the early 2000s. A transformation engineer is thus left with too many choices for the language he use to perform a specific transformation task. Furthermore, it is currently not possible to combine or reuse transformations implemented in different languages. We therefore propose T-Core, a framework where primitive transformation constructs can be combined to define and encapsulate reusable model transformation idioms. In this context, the transformation engineer is free to use existing transformation building blocks from an extensible library or define his own transformation units. The proposed primitive transformation operators are the result of deconstructing different existing transformation languages. Reconstructing these languages offers a common basis to compare their expressiveness, provides a framework for inter-operating them, and allows the transformation engineer to design transformations with the most appropriate constructs for the task at hand.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {1215–1243},
numpages = {29},
keywords = {Transformation library, Reengineering, Model transformation, Domain-specific model transformation}
}

@article{10.1016/j.compbiomed.2015.08.012,
author = {Tucker, Conrad S. and Behoora, Ishan and Nembhard, Harriet Black and Lewis, Mechelle and Sterling, Nicholas W. and Huang, Xuemei},
title = {Machine learning classification of medication adherence in patients with movement disorders using non-wearable sensors},
year = {2015},
issue_date = {November 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2015.08.012},
doi = {10.1016/j.compbiomed.2015.08.012},
abstract = {Medication non-adherence is a major concern in the healthcare industry and has led to increases in health risks and medical costs. For many neurological diseases, adherence to medication regimens can be assessed by observing movement patterns. However, physician observations are typically assessed based on visual inspection of movement and are limited to clinical testing procedures. Consequently, medication adherence is difficult to measure when patients are away from the clinical setting. The authors propose a data mining driven methodology that uses low cost, non-wearable multimodal sensors to model and predict patients' adherence to medication protocols, based on variations in their gait. The authors conduct a study involving Parkinson's disease patients that are "on" and "off" their medication in order to determine the statistical validity of the methodology. The data acquired can then be used to quantify patients' adherence while away from the clinic. Accordingly, this data-driven system may allow for early warnings regarding patient safety. Using whole-body movement data readings from the patients, the authors were able to discriminate between PD patients on and off medication, with accuracies greater than 97% for some patients using an individually customized model and accuracies of 78% for a generalized model containing multiple patient gait data. The proposed methodology and study demonstrate the potential and effectiveness of using low cost, non-wearable hardware and data mining models to monitor medication adherence outside of the traditional healthcare facility. These innovations may allow for cost effective, remote monitoring of treatment of neurological diseases. Display Omitted Data driven methodology to detect adherence using non-invasive sensors in PD patient.Effectiveness of approach illustrated with case study.Solution for non-hospital remote monitoring of patients.},
journal = {Comput. Biol. Med.},
month = nov,
pages = {120–134},
numpages = {15},
keywords = {Smart healthcare system, Parkinson's disease (PD) diagnosis, Multimodal sensor, Medication adherence, Machine learning, Healthcare delivery, Healthcare data, Decision support system}
}

@inproceedings{10.5555/2816272.2816276,
author = {Smeaton, Alan F. and Kraaij, Wessel and Over, Paul},
title = {The TREC video retrieval evaluation (TRECVID): a case study and status report},
year = {2004},
isbn = {905450096},
publisher = {LE CENTRE DE HAUTES ETUDES INTERNATIONALES D'INFORMATIQUE DOCUMENTAIRE},
address = {Paris, FRA},
abstract = {The TREC Video Retrieval Evaluation (TRECVID) is an annual international effort, funded by the US Advanced Research and Development Agency (ARDA) and the National Institute of Standards and Technology (NIST) to promote progress in content-based retrieval from digital video via open, metrics-based evaluation. Now beginning its fourth year, TRECVID aims over time to develop both a better understanding of how systems can effectively accomplish video retrieval and how one can reliably benchmark their performance. This paper is a case study in the development of video retrieval systems and their evaluation as well as a report on the TRECVID status to-date. After an introduction to the evolution of TRECVID over the past 3 years, we report on the most recent evaluation TRECVID 2003 in terms of the 4 tasks (shot boundary determination, high-level feature extraction, story segmentation and classification, search), the data (133 hours of US television news), the measures, the results obtained, and the approaches taken by some of the 24 participating groups.},
booktitle = {Coupling Approaches, Coupling Media and Coupling Languages for Information Retrieval},
pages = {25–37},
numpages = {13},
location = {Vaucluse, France},
series = {RIAO '04}
}

@article{10.1007/s11219-018-9437-3,
author = {Ma, Tao and Ali, Shaukat and Yue, Tao and Elaasar, Maged},
title = {Testing self-healing cyber-physical systems under uncertainty: a fragility-oriented approach},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9437-3},
doi = {10.1007/s11219-018-9437-3},
abstract = {As an essential feature of smart cyber-physical systems (CPSs), self-healing behaviors play a major role in maintaining the normality of CPSs in the presence of faults and uncertainties. It is important to test whether self-healing behaviors can correctly heal faults under uncertainties to ensure their reliability. However, the autonomy of self-healing behaviors and impact of uncertainties make it challenging to conduct such testing. To this end, we devise a fragility-oriented testing approach, which is comprised of two novel algorithms: fragility-oriented testing (FOT) and uncertainty policy optimization (UPO). The two algorithms utilize the fragility, obtained from test executions, to learn the optimal policies for invoking operations and introducing uncertainties, respectively, to effectively detect faults. We evaluated their performance by comparing them against a coverage-oriented testing (COT) algorithm and a random uncertainty generation method (R). The evaluation results showed that the fault detection ability of FOT+UPO was significantly higher than the ones of FOT+R, COT+UPO, and COT+R, in 73 out of 81 cases. In the 73 cases, FOT+UPO detected more than 70% of faults, while the others detected 17% of faults, at the most.},
journal = {Software Quality Journal},
month = jun,
pages = {615–649},
numpages = {35},
keywords = {Uncertainty, Self-healing, Reinforcement learning, Model execution, Cyber-physical systems}
}

@article{10.1287/msom.2017.0644,
author = {Lu, Mengshi and Chen, Zhihao and Shen, Siqian},
title = {Optimizing the Profitability and Quality of Service in Carshare Systems Under Demand Uncertainty},
year = {2018},
issue_date = {May 2018},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {2},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2017.0644},
doi = {10.1287/msom.2017.0644},
abstract = {Carsharing has been considered as an effective means to increase mobility and reduce personal vehicle usage and related carbon emissions. In this paper, we consider problems of allocating a carshare fleet to service zones under uncertain one-way and round-trip rental demand. We employ a two-stage stochastic integer programming model, in the first stage of which we allocate shared vehicle fleet and purchase parking lots or permits in reservation-based or free-floating systems. In the second stage, we generate a finite set of samples to represent demand uncertainty and construct a spatial-temporal network for each sample to model vehicle movement and the corresponding rental revenue, operating cost, and penalties from unserved demand. We minimize the expected total costs minus profit and develop branch-and-cut algorithms with mixed-integer, rounding-enhanced Benders cuts, which can significantly improve computation efficiency when implemented in parallel computing. We apply our model to a data set of Zipcar in the Boston-Cambridge, Massachusetts, area to demonstrate the efficacy of our approaches and draw insights on carshare management. Our results show that exogenously given one-way demand can increase carshare profitability under given one-way and round-trip price differences and vehicle relocation cost whereas endogenously generated one-way demand as a result of pricing and strategic customer behavior may decrease carshare profitability. Our model can also be applied in a rolling-horizon framework to deliver optimized vehicle relocation decisions and achieve significant improvement over an intuitive fleet-rebalancing policy.The online appendix is available at &lt;ext-link ext-link-type="uri" href="https://doi.org/10.1287/msom.2017.0644"&gt;https://doi.org/10.1287/msom.2017.0644&lt;/ext-link&gt;.},
journal = {Manufacturing &amp; Service Operations Management},
month = may,
pages = {162–180},
numpages = {19},
keywords = {two-stage stochastic integer programming, mixed-integer rounding, demand uncertainty, carshare fleet management, Benders decomposition}
}

@article{10.1007/s10664-021-10000-w,
author = {Abid, Shamsa and Shamail, Shafay and Basit, Hamid Abdul and Nadi, Sarah},
title = {FACER: An API usage-based code-example recommender for opportunistic reuse},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10000-w},
doi = {10.1007/s10664-021-10000-w},
abstract = {To save time, developers often search for code examples that implement their desired software features. Existing code search techniques typically focus on finding code snippets for a single given query, which means that developers need to perform a separate search for each desired functionality. In this paper, we propose FACER (F eature-driven A PI usage-based C ode E xamples R ecommender), a technique that avoids repeated searches through opportunistic reuse. Specifically, given the selected code snippet that matches the initial search query, FACER finds and suggests related code snippets that represent features that the developer may want to implement next. FACER first constructs a code fact repository by parsing the source code of open-source Java projects to obtain methods’ textual information, call graphs, and Application Programming Interface (API) usages. It then detects unique features by clustering methods based on similar API usages, where each cluster represents a feature or functionality. Finally, it detects frequently co-occurring features across projects using frequent pattern mining and recommends related methods from the mined patterns. To evaluate FACER, we run it on 120 Java Android apps from GitHub. We first manually validate that the detected method clusters represent methods with similar functionality. We then perform an automated evaluation to determine the best parameters (e.g., similarity threshold) for FACER. We recruit 10 professional developers along with 39 experienced students to judge FACER’s recommendation of related methods. Our results show that, on average, FACER’s recommendations are 80% precise. We also survey a total of 20 professional Android and Java developers to understand their code search and reuse experiences, and also to obtain their feedback on the usability and usefulness of FACER. The survey results show that 95% of our surveyed professional developers find the idea of related method recommendations useful during code reuse.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {58},
keywords = {Code clones, API usage, Software features, Code search engine, Code recommendation}
}

@article{10.1016/S0167-8655(98)00014-2,
author = {Lovell, D. R. and Dance, C. R. and Niranjan, M. and Prager, R. W. and Dalton, K. J. and Derom, R.},
title = {Feature selection using expected attainable discrimination},
year = {1998},
issue_date = {April 1, 1998},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {19},
number = {5–6},
issn = {0167-8655},
url = {https://doi.org/10.1016/S0167-8655(98)00014-2},
doi = {10.1016/S0167-8655(98)00014-2},
abstract = {We propose expected attainable discrimination (EAD) as a measure to select discrete valued features for reliable discrimination between two classes of data. EAD is an average of the area under the ROC curves obtained when a simple histogram probability density model is trained and tested on many random partitions of a data set. EAD can be incorporated into various stepwise search methods to determine promising subsets of features, particularly when misclassification costs are difficult or impossible to specify. Experimental application to the problem of risk prediction in pregnancy is described.},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {393–402},
numpages = {10},
keywords = {Risk prediction in pregnancy, Receiver operating characteristic (ROC), Feature selection, Failure to progress, Area under the ROC curve}
}

@article{10.1145/2651422,
author = {Rodeh, Ohad and Helman, Haim and Chambliss, David},
title = {Visualizing Block IO Workloads},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/2651422},
doi = {10.1145/2651422},
abstract = {Massive block IO systems are the workhorses powering many of today’s largest applications. Databases, health care systems, and virtual machine images are examples for block storage applications. The massive scale of these workloads, and the complexity of the underlying storage systems, makes it difficult to pinpoint problems when they occur. This work attempts to shed light on workload patterns through visualization, aiding our intuition.We describe our experience in the last 3 years of analyzing and visualizing customer traces from XIV, an IBM enterprise block storage system. We also present results from applying the same visualization technology to Linux filesystems.We show how visualization aids our understanding of workloads and how it assists in resolving customer performance problems.},
journal = {ACM Trans. Storage},
month = mar,
articleno = {6},
numpages = {18},
keywords = {Visualization}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.5555/2590467.2590471,
author = {Aksu, \"{O}zg\"{u}r and Kal\i{}nl\i{}, Adem},
title = {DEVELOPMENT AND IMPROVEMENT OF ANALOG CIRCUIT DESIGN: A MESSAGE PASSING INTERFACE PARALEL COMPUTING APPROACH WITH GENETIC ALGORITHMS},
year = {2010},
issue_date = {July 2010},
publisher = {IOS Press},
address = {NLD},
volume = {14},
number = {3},
issn = {1092-0617},
abstract = {The world, we live and will live in the future, is Analog. Everything we can see, hear and perceive in life is Analog, from voice, music and seismic activity to visual perception, voice recognition and energy delivery. 20th Century scientists are still trying to understand and formulate world events. For this aim they have produced a system, which is different human machine language, name is Digital Systems. Digital Systems is only human known and used systems. If anything in the world is wanted to be done in digital language, firstly it has to be converted to digital language. After digital conversion if you find any results, you have to re convert results into Analog again, which is necessary for the real world, because world is absolutely Analog. Sometimes this method brings us solutions, but the 20th century designer scientists have reached some limits about creating answers in hard, long ways. Therefore it is impossible to fathom engineering real-life solutions without the help and support of high-performance Analog Electronics. If we have a universal solution, Analog, in our hands why do we prefer to use Digital Systems, Digital Application Specific Integrated Circuit ASIC systems? The only answer must be "we cannot know or understand all details in Analog Channel". At this point, this work is all about understanding and rebuilding fundamentals of Analog Channels for Electronic Systems. This is one of the most important engineering problems of our century. As a result, the aim of selecting this work is recreating models and autonomous real world testing system for new Analog Models, instead of Digital System Models. Selected solution method is development of the MPI parallel computing architecture, based on Artificial Intelligence Algorithms. In this proposed model, we introduce an approach to develop a Message Passing Interface MPI Parallel Computing Architecture using Genetic Algorithms GA. We validate our approach by applying the technique Analog Circuit Design technique.},
journal = {J. Integr. Des. Process Sci.},
month = jul,
pages = {37–52},
numpages = {16},
keywords = {Spice, Mpi, Genetic Algorithm, Automated Circuit Designer, Asic, Analog Circuit}
}

@article{10.1145/1967293.1967296,
author = {Ekbal, Asif and Saha, Sriparna},
title = {Weighted Vote-Based Classifier Ensemble for Named Entity Recognition: A Genetic Algorithm-Based Approach},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1530-0226},
url = {https://doi.org/10.1145/1967293.1967296},
doi = {10.1145/1967293.1967296},
abstract = {In this article, we report the search capability of Genetic Algorithm (GA) to construct a weighted vote-based classifier ensemble for Named Entity Recognition (NER). Our underlying assumption is that the reliability of predictions of each classifier differs among the various named entity (NE) classes. Thus, it is necessary to quantify the amount of voting of a particular classifier for a particular output class. Here, an attempt is made to determine the appropriate weights of voting for each class in each classifier using GA. The proposed technique is evaluated for four leading Indian languages, namely Bengali, Hindi, Telugu, and Oriya, which are all resource-poor in nature. Evaluation results yield the recall, precision and F-measure values of 92.08%, 92.22%, and 92.15%, respectively for Bengali; 96.07%, 88.63%, and 92.20%, respectively for Hindi; 78.82%, 91.26%, and 84.59%, respectively for Telugu; and 88.56%, 89.98%, and 89.26%, respectively for Oriya. Finally, we evaluate our proposed approach with the benchmark dataset of CoNLL-2003 shared task that yields the overall recall, precision, and F-measure values of 88.72%, 88.64%, and 88.68%, respectively. Results also show that the vote based classifier ensemble identified by the GA-based approach outperforms all the individual classifiers, three conventional baseline ensembles, and some other existing ensemble techniques. In a part of the article, we formulate the problem of feature selection in any classifier under the single objective optimization framework and show that our proposed classifier ensemble attains superior performance to it.},
journal = {ACM Transactions on Asian Language Information Processing},
month = jun,
articleno = {9},
numpages = {37},
keywords = {support vector machine, maximum entropy, genetic algorithm, feature selection, conditional random field, classifier ensemble, Language independent named entity recognition}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Software reuse, Software library, Search-based software engineering, Multi-objective optimization}
}

@article{10.1162/COLI_a_00178,
author = {Shaalan, Khaled},
title = {A survey of arabic named entity recognition and classification},
year = {2014},
issue_date = {June 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00178},
doi = {10.1162/COLI_a_00178},
abstract = {As more and more Arabic textual information becomes available through the Web in homes and businesses, via Internet and Intranet services, there is an urgent need for technologies and tools to process the relevant information. Named Entity Recognition NER is an Information Extraction task that has become an integral part of many other Natural Language Processing NLP tasks, such as Machine Translation and Information Retrieval. Arabic NER has begun to receive attention in recent years. The characteristics and peculiarities of Arabic, a member of the Semitic languages family, make dealing with NER a challenge. The performance of an Arabic NER component affects the overall performance of the NLP system in a positive manner. This article attempts to describe and detail the recent increase in interest and progress made in Arabic NER research. The importance of the NER task is demonstrated, the main characteristics of the Arabic language are highlighted, and the aspects of standardization in annotating named entities are illustrated. Moreover, the different Arabic linguistic resources are presented and the approaches used in Arabic NER field are explained. The features of common tools used in Arabic NER are described, and standard evaluation metrics are illustrated. In addition, a review of the state of the art of Arabic NER research is discussed. Finally, we present our conclusions. Throughout the presentation, illustrative examples are used for clarification.},
journal = {Comput. Linguist.},
month = jun,
pages = {469–510},
numpages = {42}
}

@inbook{10.5555/2172290.2172301,
author = {Te\v{s}anovi\'{c}, Aleksandra and Amirijoo, Mehdi and Hansson, J\"{o}rgen},
title = {Providing configurable qos management in real-time systems with qos aspect packages},
year = {2006},
isbn = {3540488901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Current quality of service (QoS) management approaches in real-time systems lack support for configurability and reusability as they cannot be configured for a target application or reused across many applications. In this paper we present the concept of a QoS aspect package that enables developing configurable QoS management for real-time systems. A QoS aspect package represents both the specification and the implementation of a set of aspects and components that provide a number of QoS policies. A QoS aspect package enables upgrades of already existing systems to support QoS performance assurance by adding aspects and components from the package. Furthermore, a family of real-time systems can easily be developed by adding aspects from the QoS aspect package into an existing system configuration. We illustrate the way a family of real-time database systems is developed using the QoS aspect package with a case study of an embedded real-time database system, called COMET. Our experiments with the COMET database have shown that it is indeed possible to design a real-time system without QoS management and then with a reasonable effort add the QoS dimension to the system using a QoS aspect package.},
booktitle = {Transactions on Aspect-Oriented Software Development II},
pages = {256–288},
numpages = {33}
}

@article{10.1016/j.sysarc.2011.04.003,
author = {Sallai, Janos and Hedgecock, Will and Volgyesi, Peter and Nadas, Andras and Balogh, Gyorgy and Ledeczi, Akos},
title = {Weapon classification and shooter localization using distributed multichannel acoustic sensors},
year = {2011},
issue_date = {November, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {10},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2011.04.003},
doi = {10.1016/j.sysarc.2011.04.003},
abstract = {A wireless sensor network-based wearable countersniper system prototype is presented. The sensor board is connected to a small helmet-mounted microphone array that uses time of arrival (ToA) estimates of the ballistic shockwave and the muzzle blast to compute the angle of arrival (AoA) of both acoustic events. A low-power radio is used to form an ad-hoc multihop network that shares the detections among the nodes. Utilizing all available ToA and AoA data, a novel sensor fusion algorithm then estimates the shooter position, bullet trajectory, miss distance, caliber, and weapon type. A single sensor relying only on its own detections is able determine the shooter position when both the shockwave and the muzzle blast are detected by at least three microphones each. Even with just one shockwave and one muzzle blast detection, the miss distance and range can be accurately estimated by a single sensor. The system has been tested multiple times at the US Army Aberdeen Test Center and the Nashville Police Academy. The demonstrated performance is 1-degree trajectory precision, over 95% caliber estimation accuracy, and close to 100% weapon estimation accuracy for 4 out of the 6 guns tested.},
journal = {J. Syst. Archit.},
month = nov,
pages = {869–885},
numpages = {17},
keywords = {Weapon classification, Sensor networks, Data fusion, Caliber estimation, Acoustic source localization}
}

@article{10.1017/S1351324901002741,
author = {Mani, Inderjeet and Klein, Gary and House, David and Hirschman, Lynette and Firmin, Therese and Sundheim, Beth},
title = {SUMMAC: a text summarization evaluation},
year = {2002},
issue_date = {March 2002},
publisher = {Cambridge University Press},
address = {USA},
volume = {8},
number = {1},
issn = {1351-3249},
url = {https://doi.org/10.1017/S1351324901002741},
doi = {10.1017/S1351324901002741},
abstract = {The TIPSTER Text Summarization Evaluation (SUMMAC) has developed several new extrinsic and intrinsic methods for evaluating summaries. It has established definitively that automatic text summarization is very effective in relevance assessment tasks on news articles. Summaries as short as 17% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in accuracy. Analysis of feedback forms filled in after each decision indicated that the intelligibility of present-day machine-generated summaries is high. Systems that performed most accurately in the production of indicative and informative topic-related summaries used term frequency and co-occurrence statistics, and vocabulary overlap comparisons between text passages. However, in the absence of a topic, these statistical methods do not appear to provide any additional leverage: in the case of generic summaries, the systems were indistinguishable in accuracy. The paper discusses some of the tradeoffs and challenges faced by the evaluation, and also lists some of the lessons learned, impacts, and possible future directions. The evaluation methods used in the SUMMAC evaluation are of interest to both summarization evaluation as well as evaluation of other ‘output-related’ NLP technologies, where there may be many potentially acceptable outputs, with no automatic way to compare them.},
journal = {Nat. Lang. Eng.},
month = mar,
pages = {43–68},
numpages = {26}
}

@article{10.5555/3288647.3288714,
author = {Alshanqiti, Abdullah and Heckel, Reiko and Kehrer, Timo},
title = {Inferring visual contracts from Java programs},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {0928-8910},
abstract = {Visual contracts model the operation of components or services by pre- and post-conditions formalised as graph transformation rules. They provide a precise intuitive notation to support testing, understanding and analysis of software. Their detailed specification of internal data states and transformations, referred to as deep behavioural modelling, is an error-prone activity. In this paper we propose a dynamic approach to reverse engineering visual contracts from Java based on tracing the execution of Java operations. The resulting contracts give an accurate description of the observed object transformations, their effects and preconditions in terms of object structures, parameter and attribute values, and their generalised specification by universally quantified (multi) objects, patterns, and invariants. While this paper focusses on the fundamental technique rather than a particular application, we explore potential uses in our evaluation, including in program understanding, review of test reports and debugging.},
journal = {Automated Software Engg.},
month = dec,
pages = {745–784},
numpages = {40},
keywords = {Visual contracts, Specification mining, Reverse engineering, Model extraction, Graph transformation, Dynamic analysis}
}

@article{10.5555/2804890.2804892,
author = {Pauwels, Koen and Hanssens, Dominique M.},
title = {Performance Regimes and Marketing Policy Shifts},
year = {2007},
issue_date = {May 2007},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {26},
number = {3},
issn = {1526-548X},
abstract = {Even in mature markets, managers are expected to improve their brands' performance year after year. When successful, they can expect to continue executing on an established marketing strategy. However, when the results are disappointing, a change or turnaround strategy may be called for to help performance get back on track. In such cases, performance diagnostics are needed to identify turnarounds and to quantify the role of marketing policy shifts in this process. This paper proposes a framework for such a diagnosis and applies several methods to provide converging evidence for two main findings. First, contrary to prevailing beliefs, the performance of brands in mature markets is not always stable. Instead, brands systematically improve or deteriorate their performance outlook in clearly identifiable time windows that are relatively short compared to windows of stability. Second, these shifts in performance regimes are associated with the brand's marketing actions and policy shifts, as opposed to competitive marketing. Promotion-oriented marketing policy shifts are particularly potent in improving a brand's performance outlook.},
journal = {Marketing Science},
month = may,
pages = {293–311},
numpages = {19},
keywords = {turnaround strategy, promotion, performance improvement, marketing mix, advertising}
}

@inproceedings{10.5555/979922.979961,
author = {Xie, Huayang and Andreae, Peter and Zhang, Mengjie and Warren, Paul},
title = {Learning models for English speech recognition},
year = {2004},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {This paper reports on an experiment to determine the optimal parameters for a speech recogniser that is part of a computer aided instruction system for assisting learners of English as a Second Language. The recogniser uses Hidden Markov Model (HMM) technology. To find the best choice of parameters for the recogniser, an exhaustive experiment with 2370 combinations of parameters was performed on a data set of 1119 different English utterances produced by 6 female adults. A server-client computer network was used to carry out the experiment. The experimental results give a clear preference for certain sets of parameters. An analysis of the results also identified some of the causes of errors and the paper proposes two approaches to reduce these errors.},
booktitle = {Proceedings of the 27th Australasian Conference on Computer Science - Volume 26},
pages = {323–329},
numpages = {7},
keywords = {HMM design, frame period, signal processing, speech encoding, window size},
location = {Dunedin, New Zealand},
series = {ACSC '04}
}

@inproceedings{10.1145/62437.62438,
author = {Jones, K. Sparck},
title = {A look back and a look forward},
year = {1988},
isbn = {2706103094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62437.62438},
doi = {10.1145/62437.62438},
abstract = {This paper is in two parts, following the suggestion that I first comment on my own past experience in information retrieval, and then present my views on the present and future.},
booktitle = {Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {13–29},
numpages = {17},
location = {Grenoble, France},
series = {SIGIR '88}
}

@article{10.1016/j.compind.2015.09.005,
author = {Tanguy, Ludovic and Tulechki, Nikola and Urieli, Assaf and Hermann, Eric and Raynal, C\'{e}line},
title = {Natural language processing for aviation safety reports},
year = {2016},
issue_date = {May 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {78},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2015.09.005},
doi = {10.1016/j.compind.2015.09.005},
abstract = {HighlightsWe identify the needs for NLP for the analysis of aviation safety reports.Automatic document classification can be performed.Probabilistic topic modelling is of limited use for identifying new aspects.We present an interactive search tool that relies on document similarity.We present another interactive tool that helps experts identify non-technical aspects. In this paper we describe the different NLP techniques designed and used in collaboration between the CLLE-ERSS research laboratory and the CFH/Safety Data company to manage and analyse aviation incident reports. These reports are written every time anything abnormal occurs during a civil air flight. Although most of them relate routine problems, they are a valuable source of information about possible sources of greater danger. These texts are written in plain language, show a wide range of linguistic variation (telegraphic style overcrowded by acronyms or standard prose) and exist in different languages, even for a single company/country (although our main focus is on English and French). In addition to their variety, their sheer quantity (e.g. 600/month for a large airline company) clearly requires the use of advanced NLP and text mining techniques in order to extract useful information from them. Although this context and objectives seem to indicate that standard NLP techniques can be applied in a straightforward manner, innovative techniques are required to handle the specifics of aviation report text and the complex classification systems. We present several tools that aim at a better access to this data (classification and information retrieval), and help aviation safety experts in their analyses (data/text mining and interactive analysis).Some of these tools are currently in test or in use both at the national and international levels, by airline companies as well as by regulation authorities (DGAC,11Direction G\'{e}n\'{e}rale de l'Aviation Civile. EASA,22European Aviation Safety Agency. ICAO33International Civil Aviation Organization.).},
journal = {Comput. Ind.},
month = may,
pages = {80–95},
numpages = {16},
keywords = {Text mining, Safety reports, NLP, Document classification, Aviation}
}

@article{10.4018/jdm.2011010102,
author = {Lin, Zhangxi and Xu, Bo and Xu, Yan},
title = {A Study of Open Source Software Development from Control Perspective},
year = {2011},
issue_date = {January 2011},
publisher = {IGI Global},
address = {USA},
volume = {22},
number = {1},
issn = {1063-8016},
url = {https://doi.org/10.4018/jdm.2011010102},
doi = {10.4018/jdm.2011010102},
abstract = {Open source software OSS has achieved great success and exerted significant impact on the software industry. OSS development takes online community as its organizational form, and developers voluntarily work for the project. In the project execution process, control aligns individual behaviors toward the organizational goals via the Internet and becomes critical to the success of OSS projects. This paper investigates the control modes in OSS project communities, and their effects on project performance. Based on a web survey and archival data from OSS projects, it is revealed that three types of control modes, that is, outcome, clanship, and self-control, are effective in an OSS project community. The study contributes to a better understanding of OSS project organizations and processes, and provides advice for OSS development.},
journal = {J. Database Manage.},
month = jan,
pages = {26–42},
numpages = {17},
keywords = {Software Development, Project Management, Open Source Software OSS, OSS Development, Control}
}

@article{10.1016/j.patcog.2005.07.006,
author = {Cheng, H. D. and Shi, X. J. and Min, R. and Hu, L. M. and Cai, X. P. and Du, H. N.},
title = {Approaches for automated detection and classification of masses in mammograms},
year = {2006},
issue_date = {April, 2006},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {39},
number = {4},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2005.07.006},
doi = {10.1016/j.patcog.2005.07.006},
abstract = {Breast cancer continues to be a significant public health problem in the world. Early detection is the key for improving breast cancer prognosis. Mammography has been one of the most reliable methods for early detection of breast carcinomas. However, it is difficult for radiologists to provide both accurate and uniform evaluation for the enormous mammograms generated in widespread screening. The estimated sensitivity of radiologists in breast cancer screening is only about 75%, but the performance would be improved if they were prompted with the possible locations of abnormalities. Breast cancer CAD systems can provide such help and they are important and necessary for breast cancer control. Microcalcifications and masses are the two most important indicators of malignancy, and their automated detection is very valuable for early breast cancer diagnosis. Since masses are often indistinguishable from the surrounding parenchymal, automated mass detection and classification is even more challenging. This paper discusses the methods for mass detection and classification, and compares their advantages and drawbacks.},
journal = {Pattern Recogn.},
month = apr,
pages = {646–668},
numpages = {23},
keywords = {Wavelet, Mass, Mammogram, Fuzzy logic, Feature selection, Contrast enhancement, CAD}
}

@article{10.1016/j.infsof.2010.05.003,
author = {Ali, Muhammad Sarmad and Ali Babar, Muhammad and Chen, Lianping and Stol, Klaas-Jan},
title = {A systematic review of comparative evidence of aspect-oriented programming},
year = {2010},
issue_date = {September, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.003},
doi = {10.1016/j.infsof.2010.05.003},
abstract = {Context: Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective: This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method: We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results: Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion: AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {871–887},
numpages = {17},
keywords = {Systematic literature review, Evidence-based software engineering, Aspect-oriented programming}
}

@article{10.1023/A:1019984910490,
author = {Farserotu, J. and Hutter, A. and Platbrood, F. and Ayadi, J. and Gerrits, J. and Pollini, A.},
title = {UWB Transmission and MIMO Antenna Systems for Nomadic Users and Mobile PANs},
year = {2002},
issue_date = {August 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1023/A:1019984910490},
doi = {10.1023/A:1019984910490},
abstract = {Personal Area Networks (PANs) are expected to play an 
important role in future mobile communications and information systems. A 
proliferation of low data rate sensor and control devices is envisioned. 
These devices must be able to communicate across various networks in order 
to provide seamless end-to-end service. At the physical and link level, 
several factors are critical in order to realize a nomadic PAN: co-existence 
with other systems, efficient use of increasingly scarce spectrum resources 
and capacity, robustness in the presence of interference, as well as, the 
availability of low cost individual user devices. In this paper, we examine 
a concept for nomadic PANs employing low cost, low data rate ultra wideband 
(UWB) communication links between personal devices and a handset, or mobile 
bridge, coupled with a multiple input multiple output (MIMO) antennas for 
communication from a mobile bridge to other networks. Key issues are 
identified and potential capacity and quality-of-service (QoS) enhancements 
are evaluated.},
journal = {Wirel. Pers. Commun.},
month = aug,
pages = {297–317},
numpages = {21},
keywords = {wireless, WLAN, UWB, QoS, PAN, MIMO}
}

@article{10.1145/1515693.1516680,
author = {Madnick, Stuart E. and Wang, Richard Y. and Lee, Yang W. and Zhu, Hongwei},
title = {Overview and Framework for Data and Information Quality Research},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1516680},
doi = {10.1145/1515693.1516680},
abstract = {Awareness of data and information quality issues has grown rapidly in light of the critical role played by the quality of information in our data-intensive, knowledge-based economy. Research in the past two decades has produced a large body of data quality knowledge and has expanded our ability to solve many data and information quality problems. In this article, we present an overview of the evolution and current landscape of data and information quality research. We introduce a framework to characterize the research along two dimensions: topics and methods. Representative papers are cited for purposes of illustrating the issues addressed and the methods used. We also identify and discuss challenges to be addressed in future research.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {2},
numpages = {22},
keywords = {research topics, research methods, information quality, Data quality}
}

@article{10.1007/s00500-019-04503-4,
author = {Abboud, Ralph and Tekli, Joe},
title = {Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04503-4},
doi = {10.1007/s00500-019-04503-4},
abstract = {Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.},
journal = {Soft Comput.},
month = jul,
pages = {9875–9925},
numpages = {51},
keywords = {Fuzzy classification, Supervised learning, Algorithmic composition, Evolutionary algorithms, MIDI, Music sentiment analysis}
}

@article{10.1155/2008/258184,
author = {Hosseinzadeh, Danoush and Krishnan, Sridhar},
title = {On the use of complementary spectral features for speaker recognition},
year = {2008},
issue_date = {January 2008},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2008},
issn = {1110-8657},
url = {https://doi.org/10.1155/2008/258184},
doi = {10.1155/2008/258184},
abstract = {The most popular features for speaker recognition are Mel frequency cepstral coefficients (MFCCs) and linear prediction cepstral coefficients (LPCCs). These features are used extensively because they characterize the vocal tract configuration which is known to be highly speaker-dependent. In this work, several features are introduced that can characterize the vocal system in order to complement the traditional features and produce better speaker recognition models. The spectral centroid (SC), spectral bandwidth (SBW), spectral band energy (SBE), spectral crest factor (SCF), spectral flatness measure (SFM), Shannon entropy (SE), and Renyi entropy (RE) were utilized for this purpose. This work demonstrates that these features are robust in noisy conditions by simulating some common distortions that are found in the speakers' environment and a typical telephone channel. Babble noise, additive white Gaussian noise (AWGN), and a bandpass channel with 1 dB of ripple were used to simulate these noisy conditions. The results show significant improvements in classification performance for all noise conditions when these features were used to complement the MFCC and ΔMFCC features. In particular, the SC and SCF improved performance in almost all noise conditions within the examined SNR range (10-40 dB). For example, in cases where there was only one source of distortion, classification improvements of up to 8% and 10% were achieved under babble noise and AWGN, respectively, using the SCF feature.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
articleno = {46},
numpages = {10}
}

@article{10.1016/j.istr.2009.03.003,
author = {Shabtai, Asaf and Moskovitch, Robert and Elovici, Yuval and Glezer, Chanan},
title = {Detection of malicious code by applying machine learning classifiers on static features: A state-of-the-art survey},
year = {2009},
issue_date = {Feb 2009},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {14},
number = {1},
issn = {1363-4127},
url = {https://doi.org/10.1016/j.istr.2009.03.003},
doi = {10.1016/j.istr.2009.03.003},
journal = {Inf. Secur. Tech. Rep.},
month = feb,
pages = {16–29},
numpages = {14}
}

@inproceedings{10.5555/2486788.2486883,
author = {Petre, Marian},
title = {UML in practice},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {UML has been described by some as "the lingua franca" of software engineering. Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {722–731},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1007/11559573_147,
author = {Gong, Minglun and Langille, Aaron and Gong, Mingwei},
title = {Real-Time image processing using graphics hardware: a performance study},
year = {2005},
isbn = {3540290699},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11559573_147},
doi = {10.1007/11559573_147},
abstract = {Programmable graphics hardware have proven to be a powerful resource for general computing. Previous research has shown that using a GPU for local image processing operations can be much faster than using a CPU. The actual speedup obtained is influenced by many factors. In this paper, we quantify the performance gain that can be achieved by using the GPU for different image processing operations under different conditions. We also compare the strengths and weaknesses of two of the current leaders in mainstream GPUs – ATI's Radeon and nVidia's GeForce FX. Many interesting observations are obtained through the evaluation.},
booktitle = {Proceedings of the Second International Conference on Image Analysis and Recognition},
pages = {1217–1225},
numpages = {9},
location = {Toronto, Canada},
series = {ICIAR'05}
}

@article{10.1016/j.compind.2007.02.001,
author = {Chien, Shih-Wen and Tsaur, Shu-Ming},
title = {Investigating the success of ERP systems: Case studies in three Taiwanese high-tech industries},
year = {2007},
issue_date = {December, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {8–9},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2007.02.001},
doi = {10.1016/j.compind.2007.02.001},
abstract = {The measurement of enterprise resource planning (ERP) systems success or effectiveness is critical to our understanding of the value and efficacy of ERP investment and managerial actions. Whether traditional information systems success models can be extended to investigating ERP systems success is yet to be investigated. This paper proposes a partial extension and respecification of the DeLone and MacLean model of IS success to ERP systems. The purpose of the present research is to re-examine the updated DeLone and McLean model [W. DeLone, E. McLean, The DeLone McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 3-9] of ERP systems success. The updated DeLone and McLean model was applied to collect data from the questionnaires answered by 204 users of ERP systems at three high-tech firms in Taiwan. Finally, this study suggests that system quality, service quality, and information quality are most important successful factors.},
journal = {Comput. Ind.},
month = dec,
pages = {783–793},
numpages = {11},
keywords = {High-tech firms, ERP success model, DeLone and McLean model}
}

@article{10.1016/j.jengtecman.2011.09.012,
author = {Zhu, Qinghua and Sarkis, Joseph and Lai, Kee-Hung},
title = {Green supply chain management innovation diffusion and its relationship to organizational improvement: An ecological modernization perspective},
year = {2012},
issue_date = {January, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {1},
issn = {0923-4748},
url = {https://doi.org/10.1016/j.jengtecman.2011.09.012},
doi = {10.1016/j.jengtecman.2011.09.012},
abstract = {Drawing on diffusion of innovation and ecological modernization theories, we identify three types of industrial manufacturers, namely early adopters, followers, and laggards, based on the adoption of green supply chain management (GSCM) practices among Chinese manufacturers. Test results indicate that differences exist between the three types of GSCM adopters in terms of their environmental, operational, and economic performance. Understanding how Chinese manufacturers adopt GSCM practices and if this adoption affects their performance contributes theoretical advancement to the diffusion of innovation theory. Practically, the results provide managerial insights for manufacturers to benchmark for environmental management practices and performance improvement.},
journal = {J. Eng. Technol. Manag.},
month = jan,
pages = {168–185},
numpages = {18},
keywords = {Supply chain management, Q56, Environmental issues, Empirical taxonomy, Diffusion of innovation theory, Cluster analysis, C83, C12}
}

@inproceedings{10.1145/1645953.1646031,
author = {Armstrong, Timothy G. and Moffat, Alistair and Webber, William and Zobel, Justin},
title = {Improvements that don't add up: ad-hoc retrieval results since 1998},
year = {2009},
isbn = {9781605585123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1645953.1646031},
doi = {10.1145/1645953.1646031},
abstract = {The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998--2008) and CIKM (2004--2008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical significance. However, there is little evidence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this finding, we question the value of achieving even a statistically significant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.},
booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
pages = {601–610},
numpages = {10},
keywords = {system measurement, survey, retrieval experiment, evaluation},
location = {Hong Kong, China},
series = {CIKM '09}
}

@article{10.1016/j.jengtecman.2010.06.002,
author = {Lindstr\"{o}m, Veronica and Winroth, Mats},
title = {Aligning manufacturing strategy and levels of automation: A case study},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {3–4},
issn = {0923-4748},
url = {https://doi.org/10.1016/j.jengtecman.2010.06.002},
doi = {10.1016/j.jengtecman.2010.06.002},
abstract = {Research has shown that alignment between manufacturing strategy and decisions regarding automation are often of an ad hoc nature, i.e. the support for automation decisions is poor. Support tools to find an appropriate level of automation are thus needed in order to achieve more efficient and robust production systems. The methodology presented in this paper contains five sub-processes where the chosen level of automation is aligned with the manufacturing strategy. Together they form an automation strategy, which secures a desired direction of the firm and also supports robustness and reliability of the manufacturing system due to the holistic approach chosen.},
journal = {J. Eng. Technol. Manag.},
month = sep,
pages = {148–159},
numpages = {12},
keywords = {Swedish industry, Strategy formulation, Methodology, Manufacturing strategy and systems, Levels of automation, L23}
}

@article{10.1017/S026988890700121X,
title = {From the journals …},
year = {2007},
issue_date = {September 2007},
publisher = {Cambridge University Press},
address = {USA},
volume = {22},
number = {3},
issn = {0269-8889},
url = {https://doi.org/10.1017/S026988890700121X},
doi = {10.1017/S026988890700121X},
journal = {Knowl. Eng. Rev.},
month = sep,
pages = {297–314},
numpages = {18}
}

@article{10.1007/s10664-009-9121-0,
author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9121-0},
doi = {10.1007/s10664-009-9121-0},
abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {250–276},
numpages = {27},
keywords = {Software architecture, Empirical software engineering}
}

@article{10.1023/A:1008013204871,
author = {Gustafson, John L. and Todi, Rajat},
title = {Conventional Benchmarks as a Sample of the Performance Spectrum},
year = {1999},
issue_date = {May 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1023/A:1008013204871},
doi = {10.1023/A:1008013204871},
abstract = {Most benchmarks are smaller than actual application programs. One reason is to improve benchmark universality by demanding resources every computer is likely to have. However, users dynamically increase the size of application programs to match the power available, whereas most benchmarks are static and of a size appropriate for computers available when the benchmark was created; this is particularly true for parallel computers. Thus, the benchmark overstates computer performance, since smaller problems spend more time in cache. Scalable benchmarks, such as HINT, examine the full spectrum of performance through various memory regimes, and express a superset of the information given by any particular fixed-size benchmark. Using 5,000 experimental measurements, we have found that performance on the NAS Parallel Benchmarks, SPEC, LINPACK, and other benchmarks is predicted accurately by subsets of HINT performance curve. Correlations are typically better than 0.995. Predicted ranking is often perfect.},
journal = {J. Supercomput.},
month = may,
pages = {321–342},
numpages = {22},
keywords = {performance analysis, hierarchical memory, benchmarks, HINT}
}

@article{10.1145/945846.945849,
author = {Cherkasova, Ludmila and Fu, Yun and Tang, Wenting and Vahdat, Amin},
title = {Measuring and characterizing end-to-end Internet service performance},
year = {2003},
issue_date = {November 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/945846.945849},
doi = {10.1145/945846.945849},
abstract = {Fundamental to the design of reliable, high-performance network services is an understanding of the performance characteristics of the service as perceived by the client population as a whole. Understanding and measuring such end-to-end service performance is a challenging task. Current techniques include periodic sampling of service characteristics from strategic locations in the network and instrumenting Web pages with code that reports client-perceived latency back to a performance server. Limitations to these approaches include potentially nonrepresentative access patterns in the first case and determining the location of a performance bottleneck in the second.This paper presents EtE monitor, a novel approach to measuring Web site performance. Our system passively collects packet traces from a server site to determine service performance characteristics. We introduce a two-pass heuristic and a statistical filtering mechanism to accurately reconstruct different client page accesses and to measure performance characteristics integrated across all client accesses. Relative to existing approaches, EtE monitor offers the following benefits: i) a latency breakdown between the network and server overhead of retrieving a Web page, ii) longitudinal information for all client accesses, not just the subset probed by a third party, iii) characteristics of accesses that are aborted by clients, iv) an understanding of the performance breakdown of accesses to dynamic, multitiered services, and v) quantification of the benefits of network and browser caches on server performance. Our initial implementation and performance analysis across three different commercial Web sites confirm the utility of our approach.},
journal = {ACM Trans. Internet Technol.},
month = nov,
pages = {347–391},
numpages = {45},
keywords = {web site performance, reconstruction of web page composition, passive monitoring, network packet traces, QoS, End-to-end service performance}
}

@article{10.1016/j.engappai.2007.04.009,
author = {Fu, Tak-chung and Chung, Fu-lai and Luk, Robert and Ng, Chak-man},
title = {Representing financial time series based on data point importance},
year = {2008},
issue_date = {March, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {21},
number = {2},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2007.04.009},
doi = {10.1016/j.engappai.2007.04.009},
abstract = {Recently, the increasing use of time series data has initiated various research and development attempts in the field of data and knowledge management. Time series data is characterized as large in data size, high dimensionality and update continuously. Moreover, the time series data is always considered as a whole instead of individual numerical fields. Indeed, a large set of time series data is from stock market. Stock time series has its own characteristics over other time series. Moreover, dimensionality reduction is an essential step before many time series analysis and mining tasks. For these reasons, research is prompted to augment existing technologies and build new representation to manage financial time series data. In this paper, financial time series is represented according to the importance of the data points. With the concept of data point importance, a tree data structure, which supports incremental updating, is proposed to represent the time series and an access method for retrieving the time series data point from the tree, which is according to their order of importance, is introduced. This technique is capable to present the time series in different levels of detail and facilitate multi-resolution dimensionality reduction of the time series data. In this paper, different data point importance evaluation methods, a new updating method and two dimensionality reduction approaches are proposed and evaluated by a series of experiments. Finally, the application of the proposed representation on mobile environment is demonstrated.},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
pages = {277–300},
numpages = {24},
keywords = {Tree data structure, Multi-resolution visualization, Mobile application, Incremental updating, Financial time series representation, Dimensionality reduction}
}

@book{10.5555/1972541,
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
title = {Data Mining: Concepts and Techniques},
year = {2011},
isbn = {0123814790},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data}
}

@inproceedings{10.1145/2038642.2038683,
author = {Broy, Manfred and Chakraborty, Samarjit and Goswami, Dip and Ramesh, S. and Satpathy, M. and Resmerita, Stefan and Pree, Wolfgang},
title = {Cross-layer analysis, testing and verification of automotive control software},
year = {2011},
isbn = {9781450307147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038642.2038683},
doi = {10.1145/2038642.2038683},
abstract = {Automotive architectures today consist of up to 100 electronic control units (ECUs) that communicate via one or more FlexRay and CAN buses. Multiple control applications - like cruise control, brake control, etc. are specified as Simulink/Stateflow models, from which code is generated and mapped onto the different ECUs. In addition, scheduling policies and parameters, both for the ECUs and the buses, need to be specified. Code generation/optimization from the Simulink/Stateflow models, task partitioning and mapping decisions, as well as the parameters chosen for the schedulers all of these impact the execution times and timing behaviour of the control tasks and control messages. These in turn affect control performance, such as stability and steady-/transient-state behaviour. This paper discusses different aspects of this multi-layered design flow and the associated research challenges. The emphasis is on model-based code generation, analysis, testing and verification of control software for automotive architectures, as well as on architecture or platform configuration to ensure that the required control performance requirements are satisfied.},
booktitle = {Proceedings of the Ninth ACM International Conference on Embedded Software},
pages = {263–272},
numpages = {10},
keywords = {model-based testing and verification, model-based code-generation, automotive control systems},
location = {Taipei, Taiwan},
series = {EMSOFT '11}
}

@article{10.1162/COLI_a_00276,
author = {Habernal, Ivan and Gurevych, Iryna},
title = {Argumentation mining in user-generated web discourse},
year = {2017},
issue_date = {April 2017},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {43},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00276},
doi = {10.1162/COLI_a_00276},
abstract = {The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people's argumentation. In this article, we go beyond the state of the art in several ways. i We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. ii We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. iii We create a new gold standard corpus 90k tokens in 340 documents and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.},
journal = {Comput. Linguist.},
month = apr,
pages = {125–179},
numpages = {55}
}

@article{10.1287/isre.1090.0250,
author = {Tanriverdi, H\"{u}seyin and Uysal, Vahap B\"{u}lent},
title = {Cross-Business Information Technology Integration and Acquirer Value Creation in Corporate Mergers and Acquisitions},
year = {2011},
issue_date = {12 2011},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {22},
number = {4},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.1090.0250},
doi = {10.1287/isre.1090.0250},
abstract = {This study develops and tests the idea that the cross-business information technology integration (CBITI) capability of an acquirer creates significant value for shareholders of the acquirer in mergers and acquisitions (M&amp;A). In M&amp;A, integrating the IT systems and IT management processes of acquirer and target could generate benefits such as (a) the consolidation of IT resources and the reduction of overall IT costs of the combined firm, (b) the development of an IT-based coordination mechanism and the realization of cross-firm business synergies, (c) the minimization of potential disruptions to business operations, and (d) greater ability to comply with relevant laws and regulations and the reduction of regulatory compliance costs. We test these ideas in a sample of 141 acquisitions conducted by 86 Fortune 1000 firms. In the short run, acquirers that have high levels of CBITI capabilities receive positive and significant cumulative abnormal returns to their M&amp;A announcements. Announcement period returns indicate that the capital markets value CBITI similarly in same-industry and different-industry acquisitions. In the long run, acquirers with high levels of CBITI capabilities obtain significantly higher abnormal operating performance. They create significantly greater value in complementary acquisitions from different industries than in related acquisitions from the same industry. The findings have important implications for M&amp;A research and practice.},
journal = {Info. Sys. Research},
month = dec,
pages = {703–720},
numpages = {18},
keywords = {short-run abnormal stock returns, long-run abnormal operating performance, cross-business IT integration, corporate mergers and acquisitions}
}

@article{10.5555/2010483.2010486,
author = {Barry, Michael and Gutknecht, Juerg and Kulka, Irena and Lukowicz, Paul and Stricker, Thomas},
title = {From motion to emotion: a wearable system for the multimedia enrichment of a Butoh dace performance},
year = {2005},
issue_date = {June 2005},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {1},
number = {2},
issn = {1550-4646},
abstract = {We present a mobile, multimedia system based on a network of body worn motion sensors, a wearable computer and a visualization engine that is used to produce a visual enhancement of Butoh dance performance. The core of the system is a novel motion classification scheme that allows us to capture the emotion expressed by the dancer during the performance and map it onto scripted visual effects. We describe the artistic concept behind the multimedia enhancement, the motion classification scheme and the system architecture. In an experimental evaluation we investigate the usefulness and the robustness of the wearable computer as well as the classification accuracy of the motion-sensing system. We also summarize the experiences with using the system for live performances on stage in several shows.},
journal = {J. Mob. Multimed.},
month = jun,
pages = {112–132},
numpages = {21}
}

@inproceedings{10.1145/1386352.1386393,
author = {Byrne, Daragh and Wilkins, Peter and Jones, Gareth J.F. and Smeaton, Alan F. and O'Connor, Noel E.},
title = {Measuring the impact of temporal context on video retrieval},
year = {2008},
isbn = {9781605580708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1386352.1386393},
doi = {10.1145/1386352.1386393},
abstract = {In this paper we describe the findings from the K-Space interactive video search experiments in TRECVid 2007, which examined the effects of including temporal context in video retrieval. The traditional approach to presenting video search results is to maximise recall by offering a user as many potentially relevant shots as possible within a limited amount of time. 'Context'-oriented systems opt to allocate a portion of the results presentation space to providing additional contextual cues about the returned results. In video retrieval these cues often include temporal information such as a shot's location within the overall video broadcast and/or its neighbouring shots. We developed two interfaces with identical retrieval functionality in order to measure the effects of such context on user performance. The first system had a 'recall-oriented' interface, where results from a query were presented as a ranked list of shots. The second was 'context-oriented', with results presented as a ranked list of broadcasts. 10 users participated in the experiments, of which 8 were novices and 2 experts. Participants completed a number of retrieval topics using both the recall-oriented and context-oriented systems.},
booktitle = {Proceedings of the 2008 International Conference on Content-Based Image and Video Retrieval},
pages = {299–308},
numpages = {10},
keywords = {video retrieval, user-evaluation, content-based video retrieval},
location = {Niagara Falls, Canada},
series = {CIVR '08}
}

@article{10.1287/inte.2014.0759,
author = {Avrahami, Assaf and Herer, Yale T. and Levi, Retsef},
title = {Matching Supply and Demand: Delayed Two-Phase Distribution at Yedioth Group-Models, Algorithms, and Information Technology},
year = {2014},
issue_date = {October 2014},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {44},
number = {5},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2014.0759},
doi = {10.1287/inte.2014.0759},
abstract = {This paper details collaboration between the distribution organization within the Yedioth Group, the largest media group in Israel, Technion-Israel Institute of Technology in Haifa, Israel, and the Massachusetts Institute of Technology in Cambridge, Massachusetts. This collaboration has led to fundamental changes in how Yedioth distributes print magazines and newspapers. In our work, we developed and implemented decision support tools that are based on new models and algorithms, which were enabled through an electronic data interchange system, and in the future will be enabled with a specialized radio-frequency identification technological solution. The underlying concept is to use real-time information about the sales at the retail outlets to enable pooling of inventory in the network. In particular, we leverage this information to implement an additional redistribution period during the week, delaying the distribution of some magazines until after we receive partial sales data. We model the system as a two-stage stochastic optimization problem. Moreover, we show that the resulting cost is jointly convex in the decision variables. This approach gives rise to a multidimensional dynamic program. By formulating the second-stage subproblem as a linear program, we develop an innovative stochastic gradient-based optimization algorithm that finds the optimal solution in a matter of seconds. The changes resulting from this collaboration generated substantial cost savings at Yedioth-from both a reduction in magazine production levels and a reduction in the return levels. These savings were achieved while maintaining the same sales levels.},
journal = {Interfaces},
month = oct,
pages = {445–460},
numpages = {16},
keywords = {supply chain management, newsvendor problem, RFID}
}

@techreport{10.5555/1698215,
author = {Van De Vanter, Michael L. and Wood, Alan and Vick, Christopher and Faulk, Stuart and Squires, Susan and Votta, Lawrence G.},
title = {Productive petascale computing: requirements, hardware, and software},
year = {2009},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {Supercomputer designers traditionally focus on low-level hardware performance criteria such as CPU cycle speed, disk bandwidth, and memory latency. The High-Performance Computing (HPC) community has more recently begun to realize that escalating hardware performance is, by itself, contributing less and less to real productivity-the ability to develop and deploy high-performance supercomputer applications at acceptable time and cost.The Defense Advanced Research Projects Agency (DARPA) High Productivity Computing Systems (HPCS) initiative challenged industry vendors to design a new generation of supercomputers that would deliver a 10x improvement in this newly acknowledged but poorly understood domain of real productivity. Sun Microsystems, choosing to abandon customary evolutionary approaches, responded with two revolutionary decisions. The first was to investigate the nature of supercomputer productivity in the full context of use, which includes people, organizations, goals, practices, and skills as well as processors, disks, memory, and software. The second decision was to rethink completely the design of supercomputing systems, informed by productivity-based requirements and driven by recent technological breakthroughs. Crucial to the implementation of these decisions was the establishment of multidisciplinary, closely collaborating teams that conducted research into productivity and developed the many closely intertwined design decisions needed to meet DARPA's challenge.Among the most significant results from Sun's productivity research was a detailed diagnosis of software development as the dominant barrier to productivity improvements in the HPC community. The level of expertise required, combined with the amount of effort needed to develop conventional HPC codes, has already created a crisis of productivity. Even worse, there is no path forward within the existing paradigm that will significantly increase productivity as hardware systems scale up. The same issues also prevent HPC from "scaling out" to a broader class of applications. This diagnosis led to design requirements that address specific issues behind the expertise and effort bottlenecks.Sun's design teams explored complex, system-wide tradeoffs needed to meet these requirements in all aspects of the design, including reliability, performance, programmability, and ease of administration. These tradeoffs drew on technological advances in massive chip multithreading, extremely high-performance interconnects, resource virtualization, and programming language design. The outcome was the design for a machine to operate at petascale, with extremely high reliability and a greatly simplified programming model. Although this design supports existing codes and software technologies-crucial requirements-it also anticipates that the greatest productivity breakthroughs will follow from dramatic changes in how HPC codes are developed, changes that require a system of the type designed by Sun's HPCS team.}
}

@inproceedings{10.5555/782010.782022,
author = {Kunz, Thomas and Seuren, Michiel F. H.},
title = {Fast detection of communication patterns in distributed executions},
year = {1997},
publisher = {IBM Press},
abstract = {Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.},
booktitle = {Proceedings of the 1997 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {12},
location = {Toronto, Ontario, Canada},
series = {CASCON '97}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1504/IJBIS.2009.025206,
author = {Azadeh, A. and Keramati, A. and Panahi, H.},
title = {A hybrid GA-ant colony approach for exploring the relationship between IT and firm performance},
year = {2009},
issue_date = {May 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {5},
issn = {1746-0972},
url = {https://doi.org/10.1504/IJBIS.2009.025206},
doi = {10.1504/IJBIS.2009.025206},
abstract = {Several studies were conducted during recent years on exploring the impact of Information Technology (IT) on the performance of the organisation. It is quite important to find a robust technique to identify the relationship between IT and organisational performance. A hybrid Genetic Algorithm (GA) Ant Colony Optimisation (ACO) approach is proposed for data clustering. This is because of the need for the application of metaheurisitic algorithms parallel to deterministic approaches. This study discusses and analyses data from 90 companies in a unique supply chain. The data includes 26 indices about IT and 11 indices about performance. The companies are classified with respect to the IT and performance indices (indicators). Then, IT clusters and performance clusters are mapped to one another and, consequently, the relationship between them is explored. In general, the result shows that there is a linear relationship between the IT status and performance of the companies, with few exceptions. This is the first study which integrates ant colony approach and GA for exploring the relationship between IT and firm performance.},
journal = {Int. J. Bus. Inf. Syst.},
month = may,
pages = {542–563},
numpages = {22},
keywords = {metaheurisitics, information technology, hybrid, genetic algorithms, firm performance, data clustering, cluster analysis, ant colony optimisation, GAs, ACO}
}

@article{10.1016/j.csl.2009.03.002,
author = {Jung, Sangkeun and Lee, Cheongjae and Kim, Kyungduk and Jeong, Minwoo and Lee, Gary Geunbae},
title = {Data-driven user simulation for automated evaluation of spoken dialog systems},
year = {2009},
issue_date = {October, 2009},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {23},
number = {4},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2009.03.002},
doi = {10.1016/j.csl.2009.03.002},
abstract = {This paper proposes a novel integrated dialog simulation technique for evaluating spoken dialog systems. A data-driven user simulation technique for simulating user intention and utterance is introduced. A novel user intention modeling and generating method is proposed that uses a linear-chain conditional random field, and a two-phase data-driven domain-specific user utterance simulation method and a linguistic knowledge-based ASR channel simulation method are also presented. Evaluation metrics are introduced to measure the quality of user simulation at intention and utterance. Experiments using these techniques were carried out to evaluate the performance and behavior of dialog systems designed for car navigation dialogs and a building guide robot, and it turned out that our approach was easy to set up and showed similar tendencies to real human users.},
journal = {Comput. Speech Lang.},
month = oct,
pages = {479–509},
numpages = {31},
keywords = {User simulation, Spoken dialog system, Evaluation, Dialog system, Dialog simulation, Data-driven}
}

@article{10.1023/A:1008155020711,
author = {Tessier, Russell and Burleson, Wayne},
title = {Reconfigurable Computing for Digital Signal Processing: A Survey},
year = {2001},
issue_date = {May-June 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1–2},
issn = {0922-5773},
url = {https://doi.org/10.1023/A:1008155020711},
doi = {10.1023/A:1008155020711},
abstract = {Steady advances in VLSI technology and design tools have extensively expanded the application domain of digital signal processing over the past decade. While application-specific integrated circuits (ASICs) and programmable digital signal processors (PDSPs) remain the implementation mechanisms of choice for many DSP applications, increasingly new system implementations based on reconfigurable computing are being considered. These flexible platforms, which offer the functional efficiency of hardware and the programmability of software, are quickly maturing as the logic capacity of programmable devices follows Moore's Law and advanced automated design techniques become available. As initial reconfigurable technologies have emerged, new academic and commercial efforts have been initiated to support power optimization, cost reduction, and enhanced run-time performance.This paper presents a survey of academic research and commercial development in reconfigurable computing for DSP systems over the past fifteen years. This work is placed in the context of other available DSP implementation media including ASICs and PDSPs to fully document the range of design choices available to system engineers. It is shown that while contemporary reconfigurable computing can be applied to a variety of DSP applications including video, audio, speech, and control, much work remains to realize its full potential. While individual implementations of PDSP, ASIC, and reconfigurable resources each offer distinct advantages, it is likely that integrated combinations of these technologies will provide more complete solutions.},
journal = {J. VLSI Signal Process. Syst.},
month = may,
pages = {7–27},
numpages = {21},
keywords = {survey, signal processing, reconfigurable computing, FPGA}
}

@article{10.1162/089120102762671936,
author = {Teufel, Simone and Moens, Marc},
title = {Summarizing scientific articles: experiments with relevance and rhetorical status},
year = {2002},
issue_date = {December 2002},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {28},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/089120102762671936},
doi = {10.1162/089120102762671936},
abstract = {In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work.We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations.We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.},
journal = {Comput. Linguist.},
month = dec,
pages = {409–445},
numpages = {37}
}

@article{10.1016/j.robot.2016.10.013,
author = {St-Onge, David and Brches, Pierre-Yves and Sharf, Inna and Reeves, Nicolas and Rekleitis, Ioannis and Abouzakhm, Patrick and Girdhar, Yogesh and Harmat, Adam and Dudek, Gregory and Gigure, Philippe},
title = {Control, localization and human interaction with an autonomous lighter-than-air performer},
year = {2017},
issue_date = {February 2017},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {88},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2016.10.013},
doi = {10.1016/j.robot.2016.10.013},
abstract = {Due to the recent technological progress, HumanRobotInteraction (HRI) has become a major field of research in both engineering and artistic realms, particularly so in the last decade. The mainstream interests are, however, extremely diverse: challenges are continuously shifting, the evolution of robot skills, as well as the advances in methods for understanding their environment radically impact the design and implementation of research prototypes. When directly deployed in a public installation or artistic performances, robots help foster the next level of understanding in HRI. To this effect, this paper presents a successful interdisciplinary art-science-technology project, the Aerostabiles, leading to a new way of conducting HRI research. The project consists of developing a mechatronic, intelligent platform embodied in multiple geometric blimps cubes that hover and move in the air. The artistic context of this project required a number of advances in engineering on the aspects of localization and control systems, flight dynamics, as well as interaction strategies, and their evolution through periods of collective activities called researchcreation residencies. These events involve artists, engineers, and performers working in close collaboration, sometimes, over several weeks at a time. They generate fruitful exchanges between all researchers, but most of all, they present a unique and creative way to direct and focus the robotics development. This paper represents an overview of the technical contributions from a range of expertise through the artistic drive of the Aerostabiles project. Presents an art-science-technology project for HRI research in the artistic realm.Details the dynamics, with effects of its added mass and vision-based localization.Experiments with controlled trajectories of the blimp for evoking emotions.Denes a sonar-based sphere of intimacy and a vision-based perplexing features scanner.Uses of electromyography sensors to interpret performers moods.},
journal = {Robot. Auton. Syst.},
month = feb,
pages = {165–186},
numpages = {22},
keywords = {Theater, Robotic blimp, Robotic art, Mobile robotics, Humanrobot interaction, Dynamic modeling, Airship}
}

@techreport{10.5555/1698198,
author = {Vengerov, David and Mastroleon, Lykomidis and Murphy, Declan and Bambos, Nick},
title = {Adaptive data-aware utility-based scheduling in resource-constrained systems},
year = {2007},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {This paper addresses the problem of dynamic scheduling of data-intensive multiprocessor jobs. Each job requires some number of CPUs and some amount of data that needs to be downloaded into a local storage space before starting the job. The completion of each job brings some benefit (utility) to the system, and the goal is to find the optimal scheduling policy that maximizes the average utility per unit of time obtained from all completed jobs. A co-evolutionary solution methodology is proposed, where the utility-based policies for managing local storage and for scheduling jobs onto the available CPUs mutually affect each other's environments, with both policies being adaptively tuned using the Reinforcement Learning methodology. Our simulation results demonstrate the feasibility of this approach and show that it performs better than the best heuristic scheduling policy we could find for this domain.}
}

@article{10.1007/s11263-007-0095-3,
author = {Leibe, Bastian and Leonardis, Ale\v{s} and Schiele, Bernt},
title = {Robust Object Detection with Interleaved Categorization and Segmentation},
year = {2008},
issue_date = {May       2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1–3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-007-0095-3},
doi = {10.1007/s11263-007-0095-3},
abstract = {This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.

The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion.

An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {259–289},
numpages = {31},
keywords = {Segmentation, Object detection, Object categorization, MDL, Hypothesis selection, Hough transform, Clustering}
}

@article{10.1155/S1110865703305050,
author = {Cohen, Israel and Gannot, Sharon and Berdugo, Baruch},
title = {An integrated real-time beamforming and postfiltering system for nonstationary noise environments},
year = {2003},
issue_date = {January 2003},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2003},
issn = {1110-8657},
url = {https://doi.org/10.1155/S1110865703305050},
doi = {10.1155/S1110865703305050},
abstract = {We present a novel approach for real-time multichannel speech enhancement in environments of nonstationary noise and time-varying acoustical transfer functions (ATFs). The proposed system integrates adaptive beamforming, ATF identification, soft signal detection, and multichannel postfiltering. The noise canceller branch of the beamformer and the ATF identification are adaptively updated online, based on hypothesis test results. The noise canceller is updated only during stationary noise frames, and the ATF identification is carried out only when desired source components have been detected. The hypothesis testing is based on the nonstationarity of the signals and the transient power ratio between the beamformer primary output and its reference noise signals. Following the beamforming and the hypothesis testing, estimates for the signal presence probability and for the noise power spectral density are derived. Subsequently, an optimal spectral gain function that minimizes the mean square error of the log-spectral amplitude (LSA) is applied. Experimental results demonstrate the usefulness of the proposed system in nonstationary noise environments.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {1064–1073},
numpages = {10},
keywords = {speech enhancement, spectral analysis, signal detection, array signal processing, adaptive signal processing, acoustic noise measurement}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@inproceedings{10.1145/256563.256594,
author = {Strickland, Stephen G.},
title = {Gradient/sensitivity estimation in discrete-event simulation},
year = {1993},
isbn = {078031381X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/256563.256594},
doi = {10.1145/256563.256594},
booktitle = {Proceedings of the 25th Conference on Winter Simulation},
pages = {97–105},
numpages = {9},
location = {Los Angeles, California, USA},
series = {WSC '93}
}

@inproceedings{10.5555/41824.41839,
author = {Lulu, Menberu and Black, J. T.},
title = {Just-in-time (JIT) production and process cycle time variability},
year = {1987},
isbn = {0818607661},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {The effects of process cycle time variability on the performance of a zero inventory simulation model of a Just-In-Time Manufacturing and Production System (JITMPS) is investigated. Process cycle time variability does not adversely affect the performance of a Kanban- linked process structure. However, it does lengthen product cycle time and product throughput time, and lower system utilization on directly linked process structure. Modifying a directly linked process structure to include one accumulator with one unit of float between each individual process eliminates these adverse effects.},
booktitle = {Proceedings of the 20th Annual Symposium on Simulation},
pages = {215–228},
numpages = {14},
location = {Tampa, Florida, USA},
series = {ANSS '87}
}

@article{10.1007/s10462-009-9117-6,
author = {Hristea, Florentina and Popescu, Marius and Dumitrescu, Monica},
title = {Performing word sense disambiguation at the border between unsupervised and knowledge-based techniques},
year = {2008},
issue_date = {December  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {1–4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-009-9117-6},
doi = {10.1007/s10462-009-9117-6},
abstract = {This paper aims to fully present a new word sense disambiguation method that has been introduced in Hristea and Popescu (Fundam Inform 91(3---4):547---562, 2009) and so far tested in the case of adjectives (Hristea and Popescu in Fundam Inform 91(3---4):547---562, 2009) and verbs (Hristea in Int Rev Comput Softw 4(1):58---67, 2009). We hereby extend the method to the case of nouns and draw conclusions regarding its performance with respect to all these parts of speech. The method lies at the border between unsupervised and knowledge-based techniques. It performs unsupervised word sense disambiguation based on an underlying Na\"{\i}ve Bayes model, while using WordNet as knowledge source for feature selection. The performance of the method is compared to that of previous approaches that rely on completely different feature sets. Test results for all involved parts of speech show that feature selection using a knowledge source of type WordNet is more effective in disambiguation than local type features (like part-of-speech tags) are.},
journal = {Artif. Intell. Rev.},
month = dec,
pages = {67–86},
numpages = {20},
keywords = {WordNet, Word sense disambiguation, Unsupervised disambiguation, The EM algorithm, Knowledge-based disambiguation, Bayesian classification}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/1450095.1450132,
author = {Hom, Jerry and Kremer, Ulrich},
title = {Execution context optimization for disk energy},
year = {2008},
isbn = {9781605584690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1450095.1450132},
doi = {10.1145/1450095.1450132},
abstract = {Power, energy, and thermal concerns have constrained embedded systems designs. Computing capability and storage density have increased dramatically, enabling the emergence of handheld devices from special to general purpose computing. In many mobile systems, the disk is among the top energy consumers. Many previous optimizations for disk energy have assumed uniprogramming environments. However, many optimizations degrade in multiprogramming because programs are unaware of other programs (execution context). We introduce a framework to make programs aware of and adapt to their runtime execution context.We evaluated real workloads by collecting user activity traces and characterizing the execution contexts. The study confirms that many users run a limited number of programs concurrently. We applied execution context optimizations to eight programs and tested ten combinations. The programs ran concurrently while the disk's power was measured. Our measurement infrastructure allows interactive sessions to be scripted, recorded, and replayed to compare the optimizations' effects against the baseline. Our experiments covered two write cache policies. For write-through, energy savings was in the range 3-63% with an average of 21%. For write-back, energy savings was in the range -33-61% with an average of 8%. In all cases, our optimizations incurred less than 1% performance penalty.},
booktitle = {Proceedings of the 2008 International Conference on Compilers, Architectures and Synthesis for Embedded Systems},
pages = {255–264},
numpages = {10},
keywords = {user study, synchronization, runtime adaptation, multiprogramming},
location = {Atlanta, GA, USA},
series = {CASES '08}
}

@book{10.1145/3107990,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {21},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces: user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces that often include biosignals.This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This second volume of the handbook begins with multimodal signal processing, architectures, and machine learning. It includes recent deep-learning approaches for processing multisensorial and multimodal user data and interaction, as well as context-sensitivity. A further highlight is processing of information about users' states and traits, an exciting emerging capability in next-generation user interfaces. These chapters discuss real-time multimodal analysis of emotion and social signals from various modalities and perception of affective expression by users. Further chapters discuss multimodal processing of cognitive state using behavioral and physiological signals to detect cognitive load, domain expertise, deception, and depression. This collection of chapters provides walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology, and tutorial support for mastering this rapidly expanding field. In the final section of this volume, experts exchange views on the timely and controversial challenge topic of multimodal deep learning. The discussion focuses on how multimodal-multisensor interfaces are most likely to advance human performance during the next decade.}
}

@article{10.1016/j.comnet.2007.05.003,
author = {Norden, Samphel and Guo, Katherine},
title = {Support for resilient Peer-to-Peer gaming},
year = {2007},
issue_date = {October, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {51},
number = {14},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2007.05.003},
doi = {10.1016/j.comnet.2007.05.003},
abstract = {In areas such as Massively-Multiplayer Online Games (MMOGs), the conventional centralized server model does not scale with the sheer number of simultaneous clients that need to be supported. P2P architectures are increasingly being considered as replacements for traditional client-server architectures in MMOGs. A distributed P2P architecture that uses ''Coordinator'' nodes for handling smaller groups of players has been shown to be especially effective in supporting MMOGs. However, the drawback of moving from centralized to distributed architectures is the loss of control, and more specifically the increase in the vulnerability of the system as a whole to compromises. There has been no prior work on handling the specific case when the Coordinator itself is compromised and cheats, a scenario akin to cheating conducted by the network. We address this problem by proposing an architecture that is resilient to Coordinator compromises and demonstrate the effectiveness of this architecture. We believe that this is an essential step towards enabling a widespread deployment of P2P-based MMOGs.},
journal = {Comput. Netw.},
month = oct,
pages = {4212–4233},
numpages = {22},
keywords = {P2P networks, P2P games, Overlay networks, Multiplayer, Cheating}
}

@article{10.1145/1102934.1102938,
author = {Kleijnen, Jack P. C.},
title = {Monte Carlo simulation and its statistical design and analysis},
year = {1974},
issue_date = {October 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {0163-6103},
url = {https://doi.org/10.1145/1102934.1102938},
doi = {10.1145/1102934.1102938},
abstract = {In this paper we shall discuss statistical aspects of the numerical technical called Monte Carlo simulation. In the present section we examine Monte Carlo simulation in general. Any solution method using random numbers (or pseudorandom numbers) we call a Monte Carlo method. As we know random numbers are stochastic variables that are uniformly distributed on the interval [0.1] and are statistically independent. On digital computers random numbers are approximated by pseudorandom numbers, i.e. numbers generated by deterministic algebraic formulas and for practical purposes considered to be purely random. Presently, the most used formula is the multiplicative congruential one, i.e. xi = a xi-1 + b (mod m) for i = 1, 2, ..., with prespecified a,b, m and x0. It is recommended to permute the resulting (pseudo) random numbers, ri = xi/m, in order to improve their statistical independence. Numerous publications on random number generation are available. For briefness' sake we mention only the bibliography (with 491 references) by Nance and Overstreet (1972): Marsaglia (1972) giving a table of recommended values for a when m = 232, 235, 236; and Lewis (1972). Transformations of random numbers can yield any other stochastic variable. For recent results, see Ahrens and Dieter (1972), Andrews et al. (1972, p. 56), Mihram (1972, p. 94--146), Nance and Overstreet (1972), Newman and Odell (1971, p. 18--52).},
journal = {SIGSIM Simul. Dig.},
month = oct,
pages = {23–29},
numpages = {7}
}

@book{10.5555/2838856,
author = {Jeffers, Jim and Reinders, James},
title = {High Performance Parallelism Pearls Volume Two: Multicore and Many-core Programming Approaches},
year = {2015},
isbn = {0128038195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {High Performance Parallelism Pearls Volume 2 offers another set of examples that demonstrate how to leverage parallelism. Similar to Volume 1, the techniques included here explain how to use processors and coprocessors with the same programming illustrating the most effective ways to combine Xeon Phi coprocessors with Xeon and other multicore processors. The book includes examples of successful programming efforts, drawn from across industries and domains such as biomed, genetics, finance, manufacturing, imaging, and more. Each chapter in this edited work includes detailed explanations of the programming techniques used, while showing high performance results on both Intel Xeon Phi coprocessors and multicore processors. Learn from dozens of new examples and case studies illustrating "success stories" demonstrating not just the features of Xeon-powered systems, but also how to leverage parallelism across these heterogeneous systems. Promotes write-once, run-anywhere coding, showing how to code for high performance on multicore processors and Xeon Phi Examples from multiple vertical domains illustrating real-world use of Xeon Phi coprocessors Source code available for download to facilitate further exploration}
}

@techreport{10.5555/974958,
author = {Sun Microsystems Laboratories Staff},
title = {Fiscal 1996 Project Portfolio Report},
year = {1996},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {A summary of the significant accomplishments of Sun Microsystems Laboratories for the Fiscal Year ending June 30, 1996.}
}

@article{10.1162/evco.1994.2.1.67,
author = {Greene, David Perry and Smith, Stephen F.},
title = {Using coverage as a model building constraint in learning classifier systems},
year = {1994},
issue_date = {Spring 1994},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {2},
number = {1},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco.1994.2.1.67},
doi = {10.1162/evco.1994.2.1.67},
abstract = {Promoting and maintaining diversity is a critical requirement of search in learning classifier systems (LCSs). What is required of the genetic algorithm (GA) in an LCS context is not convergence to a single global maximum, as in the standard optimization framework, but instead the generation of individuals (i.e., rules) that collectively cover the overall problem space. COGIN (COverage-based Genetic INduction) is a system designed to exploit genetic recombination for the purpose of constructing rule-based classification models from examples. The distinguishing characteristic of COGIN is its use of coverage of training set examples as an explicit constraint on the search, which acts to promote appropriate diversity in the population of rules over time. By treating training examples as limited resources, COGIN creates an ecological model that simultaneously accommodates a dynamic range of niches while encouraging superior individuals within a niche, leading to concise and accurate decision models. Previous experimental studies with COGIN have demonstrated its performance advantages over several well-known symbolic induction approaches. In this paper, we examine the effects of two modifications to the original system configuration, each designed to inject additional diversity into the search: increasing the carrying capacity of training set examples (i.e., increasing coverage redundancy) and increasing the level of disruption in the recombination operator used to generate new rules. Experimental results are given that show both types of modifications to yield substantial improvements to previously published results.},
journal = {Evol. Comput.},
month = mar,
pages = {67–91},
numpages = {25}
}

@article{10.5555/336744.336747,
author = {Muller, Nathan J.},
title = {Managing service level agreements},
year = {1999},
issue_date = {May–June 1999},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {9},
number = {3},
abstract = {Service level agreements are increasingly being used in enterprise networks and are contracts that specify the performance parameters within which a network service is provided. In this article their application, preparation, and effects on IT departments are considered. Copyright © 1999 John Wiley &amp; Sons, Ltd.},
journal = {Int. J. Netw. Manag.},
month = may,
pages = {155–166},
numpages = {12}
}

@book{10.5555/2597852,
author = {Tam, Kenneth and Hoz Salvador, Mart\'{\i}n H. and McAlpine, Ken and Basile, Rick and Matsugu, Bruce and More, Josh},
title = {UTM Security with Fortinet: Mastering FortiOS},
year = {2012},
isbn = {9781597499774},
publisher = {Syngress Publishing},
abstract = {Traditionally, network security (firewalls to block unauthorized users, Intrusion Prevention Systems (IPS) to keep attackers out, Web filters to avoid misuse of Internet browsing, and antivirus software to block malicious programs) required separate boxes with increased cost and complexity. Unified Threat Management (UTM) makes network security less complex, cheaper, and more effective by consolidating all these components. This book explains the advantages of using UTM and how it works, presents best practices on deployment, and is a hands-on, step-by-step guide to deploying Fortinets FortiGate in the enterprise. Provides tips, tricks, and proven suggestions and guidelines to set up FortiGate implementations Presents topics that are not covered (or are not covered in detail) by Fortinets documentation Discusses hands-on troubleshooting techniques at both the project deployment level and technical implementation area Table of Contents Foreword Introduction Part I: General Introduction Chapter 1: Introduction to Unified Threat Management (UTM) Chapter 2: FortiGate Hardware Platform Overview Chapter 3: FortiOS Introduction Part II: UTM Technologies Explained Chapter 4: Connectivity and Networking Technologies Chapter 5: Base Network Security Chapter 6: Application Security Chapter 7: Extended UTM Functionality Chapter 8: Analyzing Your Security Information with FortiAnalyzer Chapter 9: Managing Your Security Configurations with FortiManager Part III: Implementing a Security (UTM) Project Chapter 10: Designing a Security Solution Chapter 11: Security on Distributed EnterprisesRetail (UTM Goes Shopping) Chapter 12: Security on Financial Institutions (UTM Goes to the Bank) Appendix A: Troubleshooting the Project Appendix B: Troubleshooting Technically}
}

@article{10.1145/506309.506313,
author = {Zhu, Lei and Rao, Al Bing and Zhang, Aldong},
title = {Theory of keyblock-based image retrieval},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/506309.506313},
doi = {10.1145/506309.506313},
abstract = {The success of text-based retrieval motivates us to investigate analogous techniques which can support the querying and browsing of image data. However, images differ significantly from text both syntactically and semantically in their mode of representing and expressing information. Thus, the generalization of information retrieval from the text domain to the image domain is non-trivial. This paper presents a framework for information retrieval in the image domain which supports content-based querying and browsing of images. A critical first step to establishing such a framework is to construct a codebook of "keywords" for images which is analogous to the dictionary for text documents. We refer to such "keywords" in the image domain as "keyblocks." In this paper, we first present various approaches to generating a codebook containing keyblocks at different resolutions. Then we present a keyblock-based approach to content-based image retrieval. In this approach, each image is encoded as a set of one-dimensional index codes linked to the keyblocks in the codebook, analogous to considering a text document as a linear list of keywords. Generalizing upon text-based information retrieval methods, we then offer various techniques for image-based information retrieval. By comparing the performance of this approach with conventional techniques using color and texture features, we demonstrate the effectiveness of the keyblock-based approach to content-based image retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {224–257},
numpages = {34},
keywords = {keyblock, content-based image retrieval, codebook, clustering}
}

@article{10.1007/s10590-011-9100-2,
author = {Haque, Rejwanul and Naskar, Sudip Kumar and Bosch, Antal and Way, Andy},
title = {Integrating source-language context into phrase-based statistical machine translation},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0922-6567},
url = {https://doi.org/10.1007/s10590-011-9100-2},
doi = {10.1007/s10590-011-9100-2},
abstract = {The translation features typically used in Phrase-Based Statistical Machine Translation (PB-SMT) model dependencies between the source and target phrases, but not among the phrases in the source language themselves. A swathe of research has demonstrated that integrating source context modelling directly into log-linear PB-SMT can positively influence the weighting and selection of target phrases, and thus improve translation quality. In this contribution we present a revised, extended account of our previous work on using a range of contextual features, including lexical features of neighbouring words, supertags, and dependency information. We add a number of novel aspects, including the use of semantic roles as new contextual features in PB-SMT, adding new language pairs, and examining the scalability of our research to larger amounts of training data. While our results are mixed across feature selections, classifier hyperparameters, language pairs, and learning curves, we observe that including contextual features of the source sentence in general produces improvements. The most significant improvements involve the integration of long-distance contextual features, such as dependency relations in combination with part-of-speech tags in Dutch-to-English subtitle translation, the combination of dependency parse and semantic role information in English-to-Dutch parliamentary debate translation, or supertag features in English-to-Chinese translation.},
journal = {Machine Translation},
month = sep,
pages = {239–285},
numpages = {47},
keywords = {Word alignment, Translation modelling, Syntax in machine translation, Statistical machine translation, Phrase-based statistical machine translation, Memory-based classification}
}

@article{10.1145/358234.358252,
author = {Trevathan, Charles E. and Taylor, Thomas D. and Hartenstein, Raymond G. and Merwarth, Ann C. and Stewart, William N.},
title = {Development and application of NASA's first standard spacecraft computer},
year = {1984},
issue_date = {Sept. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/358234.358252},
doi = {10.1145/358234.358252},
abstract = {To provide the autonomy needed by low, earth-orbiting satellites, NASA's first standard on-board processor requires changing only interfacing hardware from mission to mission.},
journal = {Commun. ACM},
month = sep,
pages = {902–913},
numpages = {12},
keywords = {avionics system, PASS}
}

@article{10.5555/374109.374113,
author = {Tegarden, David P.},
title = {Business information visualization},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Information Systems},
address = {USA},
volume = {1},
number = {1es},
journal = {Commun. AIS},
month = jan,
pages = {4–es},
numpages = {38}
}

@article{10.1016/j.comnet.2009.08.005,
author = {Heegaard, Poul E. and Wittner, Otto J.},
title = {Overhead reduction in a distributed path management system},
year = {2010},
issue_date = {April, 2010},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {54},
number = {6},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2009.08.005},
doi = {10.1016/j.comnet.2009.08.005},
abstract = {CEAS (cross entropy ant system) is a distributed, robust and adaptive swarm intelligence system for path management in communication networks. This paper focuses on strategies for handling the overhead in terms of processing cycles, memory storage, and number of management packets (ants) generated by CEAS when the state of the network changes. Pheromone sharing is introduced such that virtual connections with common sub-paths are sharing information and cooperate in the path finding when the paths have the same destination and the same objective function. The sharing of information reduces the required memory in each node significantly on the expense of an increase in the size of the management packets. However, the packets are still rather small. The cooperation also leads to an improvement in convergence rates which again results in reduced transmission overhead. A rate adjustment scheme is also proposed. The scheme is self-tuned and detects state changes implicitly and sets packet rates accordingly by monitoring parameter values in the management system. Rate adaptation can be done both in the network nodes and at the end-points of a virtual path. Compared to a fixed rate strategy the self-tuned strategies show a significant reduction in the number of packets generated, while maintaining the same data packet delay and service availability level. The self-tuned rate adjustment in the network nodes provides fast restoration with short path detection times, which ensures high service availability. The self-tuned ant rate in the end-points avoids flooding the network with management packets when these are not required. The performance and overhead of CEAS are compared to those of the link state routing currently in use in today's networks. The results show that CEAS outperforms link state routing both with respect to performance and overhead when the network experiences transient link failures, while the opposite is the case with long lived failures.},
journal = {Comput. Netw.},
month = apr,
pages = {1019–1041},
numpages = {23},
keywords = {Swarm intelligence, Path management, Cross entropy}
}

@inproceedings{10.5555/800289.811266,
author = {Lord, R. J.},
title = {Probabilistic budgeting: One practical experience},
year = {1977},
publisher = {Winter Simulation Conference},
abstract = {Probabilistic budgeting has been recommended in the accounting literature for nearly fifteen years as a basis for enabling organizations to better plan for and cope with uncertainty. Unfortunately, while it has been argued that the information contained in a probabilistic budget is essential for improved planning and management neither the feasibility or relevance of probabilistic budgets has been demonstrated.An attempt to develop a probabilistic budget utilizing Monte-Carlo Simulation is reported in this paper. It describes how the model of a small transport firm was developed and how the data necessary to support the model was gathered. It then discusses the apparent implications of this probabilistic budget for the firm, for the practical application of simulation techniques to the budget development process, and for the broader field of planning and management control.},
booktitle = {Proceedings of the 9th Conference on Winter Simulation - Volume 2},
pages = {608–616},
numpages = {9},
location = {Gaitersburg, MD},
series = {WSC '77}
}

@proceedings{10.1145/2998392,
title = {SCALA 2016: Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
year = {2016},
isbn = {9781450346481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/1499949.1500109,
author = {Butcher, Daniel D. and Jenks, Steven G. and McNeeley, Curtis P. and Mahan, Robert E.},
title = {A protocol for evaluating computer systems for application in a physician's office},
year = {1975},
isbn = {9781450379199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1499949.1500109},
doi = {10.1145/1499949.1500109},
abstract = {In this paper we present some of our experiences in the development of protocols for the evaluation of manual and automated medical information systems. The objective of this work has been to determine the feasibility of implementing computer technology in the office practices of private practicing physicians in Clackamas County, Oregon.},
booktitle = {Proceedings of the May 19-22, 1975, National Computer Conference and Exposition},
pages = {739–747},
numpages = {9},
location = {Anaheim, California},
series = {AFIPS '75}
}

@book{10.5555/2621967,
author = {Harmon, Paul},
title = {Business Process Change, Third Edition: A Business Process Management Guide for Managers and Process Professionals},
year = {2014},
isbn = {0128003871},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {Business Process Change, 3rd Edition provides a balanced view of the field of business process change. Bestselling author Paul Harmon offers concepts, methods, cases for all aspects and phases of successful business process improvement. Updated and added for this edition is new material on the development of business models and business process architecture development, on integrating decision management models and business rules, on service processes and on dynamic case management, and on integrating various approaches in a broad business process management approach. New to this edition: How to develop business models and business process architecture How to integrate decision management models and business rules New material on service processes and on dynamic case management Learn to integrate various approaches in a broad business process management approach Extensive revision and update addresses Business Process Management Systems, and the integration of process redesign and Six Sigma Learn how all the different process elements fit together in this best first book on business process, now completely updated Tailor the presented methodology, which is based on best practices, to your organization's specific needs Understand the human aspects of process redesign Benefit from all new detailed case studies showing how these methods are implemented}
}

@inproceedings{10.1145/1463891.1463921,
author = {Rowe, Alan J.},
title = {Computer simulation: a solution technique for management problems},
year = {1965},
isbn = {9781450378857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1463891.1463921},
doi = {10.1145/1463891.1463921},
abstract = {Although computers are currently being used primarily for rapid processing of data, there is little doubt that computer-processed information will be a requirement in providing management with timely and accurate data for evaluation, analysis, and as an aid in decision making. At the top management level, decisions are concerned with directing the organization and providing means of assuring its survival. To achieve maximum effectiveness at the operating level, plans and policies must be applied to the available resources, subject to specified constraints and risks. However, there is generally insufficient information for these decisions, and they often cannot be structured as a set of procedures. But, most important, policy decisions are based on a blend of intuition, experience and emotion.},
booktitle = {Proceedings of the November 30--December 1, 1965, Fall Joint Computer Conference, Part I},
pages = {259–267},
numpages = {9},
location = {Las Vegas, Nevada},
series = {AFIPS '65 (Fall, part I)}
}

@inproceedings{10.1145/1480083.1480125,
author = {Cureton, Henry O.},
title = {A philosophy to system measurement},
year = {1972},
isbn = {9781450379137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1480083.1480125},
doi = {10.1145/1480083.1480125},
abstract = {Although a great deal of emphasis has been placed on system measurement during the past few years, each of us has his own opinion and ideas as to what it means. From a practical and immediate point of view, System Measurement is the process of obtaining useful information on the performance of software and software-controlled computer hardware. From the same viewpoint, System Measurement is the key to system efficiency. The measurement tools currently available commercially are normally artificially classed as either hardware or software, based on whether they are constructed with electronics or software elements. This particular distinction is rather meaningless to the user since what is being measured and how useful the measurements are, is of far more importance.},
booktitle = {Proceedings of the December 5-7, 1972, Fall Joint Computer Conference, Part II},
pages = {965–969},
numpages = {5},
location = {Anaheim, California},
series = {AFIPS '72 (Fall, part II)}
}

@book{10.5555/2901646,
author = {Frishberg, Leo and Lambdin, Charles},
title = {Presumptive Design: Design Provocations for Innovation},
year = {2015},
isbn = {0128030860},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Everything you know about the future is wrong. Presumptive Design: Design Provocations for Innovation is for people inventing the future: future products, services, companies, strategies and policies. It introduces a design-research method that shortens time to insights from months to days. Presumptive Design is a fundamentally agile approach to identifying your audiences key needs. Offering rapidly crafted artifacts, your teams collaborate with your customers to identify preferred and profitable elements of your desired outcome. Presumptive Design focuses on your users problem space, informing your business strategy, your projects early stage definition, and your innovation pipeline. Comprising discussions of design theory with case studies and how-tos, the book offers business leadership, management and innovators the benefits of design thinking and user experience in the context of early stage problem definition. Presumptive Design is an advanced technique and quick to use: within days of reading this book, your research and design teams can apply the approach to capture a risk-reduced view of your future.Provides actionable approaches to inform strategy and problem definition through design thinkingOffers a design-based research method to complement existing market, ethnographic and customer research methodsDemonstrates a powerful technique for identifying disruptive innovation early in the innovation pipeline by putting customers firstPresents each concept with case studies and exploration of risk factors involved including warnings for situations in which the technique can be misapplied}
}

@techreport{10.5555/974969,
author = {Yung, Robert},
title = {Evaluation of a Commercial Microprocessor},
year = {1998},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate Division of the University of California, Berkeley.}
}

@book{10.5555/1207001,
author = {Hornick, Mark F. and Marcad\'{e}, Erik and Venkayala, Sunil},
title = {Java Data Mining: Strategy, Standard, and Practice: A Practical Guide for architecture, design, and implementation},
year = {2006},
isbn = {0123704529},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Whether you are a software developer, systems architect, data analyst, or business analyst, if you want to take advantage of data mining in the development of advanced analytic applications, Java Data Mining, JDM, the new standard now implemented in core DBMS and data mining/analysis software, is a key solution component. This book is the essential guide to the usage of the JDM standard interface, written by contributors to the JDM standard. The book discusses and illustrates how to solve real problems using the JDM API. The authors provide you with: * Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems; * JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; * JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. * Free, downloadable KJDM source code referenced in the book available here * Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems;* JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; * JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. * Free, downloadable KJDM source code referenced in the book available here}
}

@inproceedings{10.5555/352925.352955,
author = {Bensaou, M.},
title = {Electronically-mediated partnerships: the use of CAD technologies in supplier relations},
year = {1999},
publisher = {Association for Information Systems},
address = {USA},
booktitle = {Proceedings of the 20th International Conference on Information Systems},
pages = {307–323},
numpages = {17},
location = {Charlotte, North Carolina, USA},
series = {ICIS '99}
}

@book{10.5555/2155698,
author = {Martin, Grant and Bailey, Brian and Piziali, Andrew},
title = {ESL Design and Verification: A Prescription for Electronic System Level Methodology},
year = {2007},
isbn = {9780080488837},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Visit the authors companion site! http: - Includes interactive forum with the authors! Electronic System Level (ESL) design has mainstreamed --- it is now an established approach at most of the worlds leading system-on-chip (SoC) design companies and is being used increasingly in system design. From its genesis as an algorithm modeling methodology with no links to implementation, ESL is evolving into a set of complementary methodologies that enable embedded system design, verification and debug through to the hardware and software implementation of custom SoC, system-on-FPGA, system-on-board, and entire multi-board systems. This book arises from experience the authors have gained from years of work as industry practitioners in the Electronic System Level design area; they have seen SLD or ESL go through many stages and false starts, and have observed that the shift in design methodologies to ESL is finally occurring. This is partly because of ESL technologies themselves are stabilizing on a useful set of languages being standardized (SystemC is the most notable), and use models are being identified that are beginning to get real adoption. ESL DESIGN &amp; VERIFICATION offers a true prescriptive guide to ESL that reviews its past and outlines the best practices of today. Table of Contents CHAPTER 1: WHAT IS ESL CHAPTER 2: TAXONOMY AND DEFINITIONS FOR THE ELECTRONIC SYSTEM LEVEL CHAPTER 3: EVOLUTION OF ESL DEVELOPMENT CHAPTER 4: WHAT ARE THE ENABLERS OF ESL CHAPTER 5: ESL FLOW CHAPTER 6: SPECIFICATIONS AND MODELING CHAPTER 7: PRE-PARTITIONING ANALYSIS CHAPTER 8: PARTITIONING CHAPTER 9: POST-PARTITIONING ANALYSIS AND DEBUG CHAPTER 10: POST-PARTITIONING VERIFICATION CHAPTER 11: HARDWARE IMPLEMENTATION CHAPTER 12: SOFTWARE IMPLEMENTATION CHAPTER 13: USE OF ESL FOR IMPLEMENTATION VERIFICATION CHAPTER 14: RESEARCH, EMERGING AND FUTURE PROSPECTS APPENDIX: LIST OF ACRONYMS * Provides broad, comprehensive coverage not available in any other such book * Massive global appeal with an internationally recognised author team * Crammed full of state of the art content from notable industry experts}
}

@article{10.1017/S0269888901000108,
author = {Kobsa, Alfred and Koenemann, J\"{u}rgen and Pohl, Wolfgang},
title = {Personalised hypermedia presentation techniques for improving online customer relationships},
year = {2001},
issue_date = {March 2001},
publisher = {Cambridge University Press},
address = {USA},
volume = {16},
number = {2},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888901000108},
doi = {10.1017/S0269888901000108},
abstract = {This article gives a comprehensive overview of techniques for personalised hypermedia presentation. It describes the data about the computer user, the computer usage and the physical environment that can be taken into account when adapting hypermedia pages to the needs of the current user. Methods for acquiring these data, for representing them as models in formal systems and for making generalisations and predictions about the user based thereon are discussed. Different types of hypermedia adaptation to the individual user's needs are distinguished and recommendations for further research and applications given. While the focus of the article is on hypermedia adaptation for improving customer relationship management utilising the World Wide Web, many of the techniques and distinctions also apply to other types of personalised hypermedia applications within and outside the World Wide Web, like adaptive educational systems.},
journal = {Knowl. Eng. Rev.},
month = mar,
pages = {111–155},
numpages = {45}
}

@proceedings{10.1145/2993148,
title = {ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@book{10.5555/2809012,
author = {Leito, Paulo and Karnouskos, Stamatis},
title = {Industrial Agents: Emerging Applications of Software Agents in Industry},
year = {2015},
isbn = {0128003413},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Industrial Agents explains how multi-agent systems improve collaborative networks to offer dynamic service changes, customization, improved quality and reliability, and flexible infrastructure. Learn how these platforms can offer distributed intelligent management and control functions with communication, cooperation and synchronization capabilities, and also provide for the behavior specifications of the smart components of the system. The book offers not only an introduction to industrial agents, but also clarifies and positions the vision, on-going efforts, example applications, assessment and roadmap applicable to multiple industries. This edited work is guided and co-authored by leaders of the IEEE Technical Committee on Industrial Agents who represent both academic and industry perspectives and share the latest research along with their hands-on experiences prototyping and deploying industrial agents in industrial scenarios.Learn how new scientific approaches and technologies aggregate resources such next generation intelligent systems, manual workplaces and information and material flow systemGain insight from experts presenting the latest academic and industry research on multi-agent systemsExplore multiple case studies and example applications showing industrial agents in a variety of scenariosUnderstand implementations across the enterprise, from low-level control systems to autonomous and collaborative management units}
}

@book{10.5555/1999263,
author = {Hennessy, John L. and Patterson, David A.},
title = {Computer Architecture, Fifth Edition: A Quantitative Approach},
year = {2011},
isbn = {012383872X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {5th},
abstract = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the "cloud" are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change. Updated to cover the mobile computing revolutionEmphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms.Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next")Includes three review appendices in the printed text. Additional reference appendices are available online.Includes updated Case Studies and completely new exercises.}
}

@book{10.5555/2633591,
author = {Luisi, James},
title = {Pragmatic Enterprise Architecture: Strategies to Transform Information Systems in the Era of Big Data},
year = {2014},
isbn = {0128002050},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Pragmatic Enterprise Architecture is a practical hands-on instruction manual for enterprise architects. This book prepares you to better engage IT, management, and business users by equipping you with the tools and knowledge you need to address the most common enterprise architecture challenges. You will come away with a pragmatic understanding of and approach to enterprise architecture and actionable ideas to transform your enterprise. Experienced enterprise architect James V. Luisi generously shares life cycle architectures, transaction path analysis frameworks, and more so you can save time, energy, and resources on your next big project. As an enterprise architect, you must have relatable frameworks and excellent communication skills to do your job. You must actively engage and support a large enterprise involving a hundred architectural disciplines with a modest number of subject matter experts across business, information systems, control systems, and operations architecture. They must achieve their mission using the influence of ideas and business benefits expressed in simple terms so that any audience can understand what to do and why. Pragmatic Enterprise Architecture gives you the tools to accomplish your goals in less time with fewer resources. Expand your Enterprise Architecture skills so you can do more in less time with less money with the priceless tips presented Understand the cost of creating new Enterprise Architecture disciplines and contrast those costs to letting them go unmanaged Includes 10 life cycle architectures so that you can properly assess the ROI of performing activities such as outsourcing, insourcing, restructuring, mergers and acquisitions, and more Complete appendix of eight transaction path analysis frameworks provide DBA guidelines for proper physical database design}
}

@book{10.5555/1196696,
author = {Arnowitz, Jonathan and Arent, Michael and Berger, Nevin},
title = {Effective Prototyping for Software Makers},
year = {2006},
isbn = {0120885689},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Much as we hate to admit it, most prototyping practice lacks a sophisticated understanding of the broad concepts of prototyping--and its strategic position within the development process. Often we overwhelm with a high fidelity prototype that designs us into a corner. Or, we can underwhelm with a prototype with too much ambiguity and flexibility to be of much use in the software development process. This book will help software makers--developers, designers, and architects--build effective prototypes every time: prototypes that convey enough information about the product at the appropriate time and thus set expectations appropriately. This practical, informative book will help anyone--whether or not one has artistic talent, access to special tools, or programming ability--to use good prototyping style, methods, and tools to build prototypes and manage for effective prototyping. Features * A prototyping process with guidelines, templates, and worksheets; * Overviews and step-by-step guides for 9 common prototyping techniques; * An introduction with step-by-step guidelines to a variety of prototyping tools that do not require advanced artistic skills; * Templates and other resources used in the book available on the Web for reuse; * Clearly-explained concepts and guidelines; * Full-color illustrations, and examples from a wide variety of prototyping processes, methods, and tools. Jonathan Arnowitz is a principal user experience designer at SAP Labs and is the co-editor-in-chief of Interactions Magazine. Most recently Jonathan was a senior user experience designer at Peoplesoft. He is a member of the SIGCHI executive committee, and was a founder of DUX, the first ever joint conference of ACM SIGCHI, ACM SIGGRAPH, AIGA Experience Design Group, and STC. Michael Arent is the manager of user experience design at SAP Labs, and has previously held positions at Peoplesoft, Inc, Adobe Systems, Inc, Sun Microsystems, and Apple Computer, Inc. He holds several U.S. patents. Nevin Berger is design director at Ziff Davis Media. Previously he was a senior interaction designer at Oracle Corporation and Peoplesoft, Inc., and has held creative director positions at ZDNet, World Savings, and OFOTO, Inc. * A prototyping process with guidelines, templates, and worksheets; * Overviews and step-by-step guides for 9 common prototyping techniques; * An introduction with step-by-step guidelines to a variety of prototyping tools that do not require advanced artistic skills; * Templates and other resources used in the book available on the Web for reuse; * Clearly-explained concepts and guidelines; * Full-color illustrations, and examples from a wide variety of prototyping processes, methods, and tools. * www.mkp.com/prototyping}
}

@inproceedings{10.5555/800072.808800,
author = {Rundren, William P. and Standridge, Charles R.},
title = {A database supported discrete parts manufacturing simulation},
year = {1981},
publisher = {IEEE Press},
abstract = {This paper discusses a prototype decision support system for discrete parts manufacturing. Basic decision support system concepts are presented. The context of this analysis effort within the entire technical operations of the firm is shown. The use of typical, real production data stored in a database for both traditional reporting purposes and as data input to a simulation model is discussed. Thus, the model can process both currently known future orders and generate currently unknown future orders in analyzing future production requirements. Furthermore, model outputs which measure the ability of the production system to meet future requirements are stored in the database. Thus, they may be analyzed and reported independently of the running of the model.},
booktitle = {Proceedings of the 13th Conference on Winter Simulation - Volume 1},
pages = {153–162},
numpages = {10},
location = {Atlanta, Georgia},
series = {WSC '81}
}

@article{10.1145/1147444.1147450,
author = {Kolence, Kenneth},
title = {Software techniques},
year = {1971},
issue_date = {April 1971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {8},
issn = {0163-5972},
url = {https://doi.org/10.1145/1147444.1147450},
doi = {10.1145/1147444.1147450},
abstract = {Kolence: Thank you very much, Fran. You know, it's really a pleasure to be here; I doubt if Mr. Ramsay realizes that I'm the one that founded SIGCOSIM--back in 1960 when I was in New Jersey at RCA. I felt very deeply the need for installation management techniques for improvement of ways to run an installation. That was while I was at RCA, and shortly thereafter, I moved to North American. I tried to get people in L. A. interested in forming a chapter and moving ahead with SIGCOSIM. It turned out that most of the people there were interested in SHARE; I got involved with the SHARE Installation Management Division. I did quite a bit in SHARE, but finally turned SIGCOSIM over to somebody else because I realized I wasn't able to make it successful. I suppose now somebody turned it over to you, and you're doing a great job. I want to applaud you very, very strongly.},
journal = {Install Manag. Rev.},
month = apr,
pages = {18–25},
numpages = {8}
}

@book{10.5555/1543376,
author = {Jacob, Bruce and Ng, Spencer and Wang, David},
title = {Memory Systems: Cache, DRAM, Disk},
year = {2007},
isbn = {0123797519},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Is your memory hierarchy stopping your microprocessor from performing at the high level it should be? Memory Systems: Cache, DRAM, Disk shows you how to resolve this problem. The book tells you everything you need to know about the logical design and operation, physical design and operation, performance characteristics and resulting design trade-offs, and the energy consumption of modern memory hierarchies. You learn how to to tackle the challenging optimization problems that result from the side-effects that can appear at any point in the entire hierarchy.As a result you will be able to design and emulate the entire memory hierarchy. . Understand all levels of the system hierarchy -Xcache, DRAM, and disk. . Evaluate the system-level effects of all design choices. . Model performance and energy consumption for each component in the memory hierarchy.}
}

@techreport{10.5555/887708,
author = {Michael T., Palmer and William H., Rogers and Hayes N., Press and Kara A., Latorella and Terence S., Abbott},
title = {A Crew-Centered Flight Deck Design Philosophy for High-Speed Civil Transport (HSCT) Aircraft},
year = {1995},
publisher = {NASA Langley Technical Report Server},
abstract = {Past flight deck design practices used within the U.S. commercial transport aircraft industry have been highly successful in producing safe and efficient aircraft. However, recent advances in automation have changed the way pilots operate aircraft, and these changes make it necessary to reconsider overall flight deck design. The High Speed Civil Transport (HSCT) mission will likely add new information requirements, such as those for sonic boom management and supersonic/subsonic speed management. Consequently, whether one is concerned with the design of the HSCT, or a next generation subsonic aircraft that will include technological leaps in automated systems, basic issues in human usability of complex systems will be magnified. These concerns must be addressed, in part, with an explicit, written design philosophy focusing on human performance and systems operability in the context of the overall flight crew/flight deck system (i.e., a crew-centered philosophy). This document provides such a philosophy, expressed as a set of guiding design principles, and accompanied by information that will help focus attention on flight crew issues earlier and iteratively within the design process. This document is Part 1 of a two-part set. Part 2 will provide more detailed descriptions of design guidelines, test and evaluation issues, recommendations for how to apply the philosophy, and methods for identifying and resolving conflicts among design principles and among design guidelines.}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@proceedings{10.1145/3132787,
title = {SA '17: SIGGRAPH Asia 2017 Mobile Graphics &amp; Interactive Applications},
year = {2017},
isbn = {9781450354103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Mobile Graphics and Interactive Applications will offer attendees the opportunity to explore the opportunities and challenges of mobile applications relevant to the global graphics community.The program will cover the development, technology, and marketing of mobile graphics and interactive applications. It will especially highlight novel uses of graphics and interactivity on mobile devices. Attendees can expect to be exposed to the latest in mobile graphics and interactive applications through expert keynote talks, paper presentations, panel discussions, industry case studies, and hands-on demonstrations.},
location = {Bangkok, Thailand}
}

@rfc{10.17487/RFC1470,
author = {Enger, R. and Reynolds, J.},
title = {RFC1470: FYI on a Network Management Tool Catalog: Tools for Monitoring and Debugging TCP/IP Internets and Interconnected Devices},
year = {1993},
publisher = {RFC Editor},
address = {USA},
abstract = {The goal of this FYI memo is to provide an update to FYI 2, RFC 1147   [1], which provided practical information to site administrators and   network managers.  New and/or updated tools are listed in this RFC.   Additonal descriptions are welcome, and should be sent to: noctools-   entries@merit.edu.}
}

@book{10.5555/2785635,
author = {Rosing, Mark von and Scheel, Henrik von and Scheer, August-Wilhelm},
title = {The Complete Business Process Handbook: Body of Knowledge from Process Modeling to BPM, Volume I},
year = {2014},
isbn = {0127999590},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {This is the most comprehensive body of knowledge on business processes. Written as a practical guide for Executives, Practitioners, Managers and Students by the authorities that have shaped the way we think and work with process today. This book is one of tree volumes, representing the most comprehensive body of knowledge publised on business process. Second volume uniquely bridging theory with how BPM is applied today with the most extensive information on extended BPM. The third volume explores award winning real-life examples of leading business process practices and how it can be replaced to your advantage. Covering what Practitioners, Managers, Executives and Students need to know about Key Features Learn what Business Process is and how to get started Comprehensive historical process evolution In-depth look at the Process Anatomy, Semantics and Ontology Find out how to link Strategy to Operation with value driven BPM Uncover how to establish a way of Thinking, Working, Modelling and Implementation Explore comprehensive Frameworks, Methods and Approaches How to build BPM competencies and establish a Center of Excellence Discover how to apply Social BPM, Sustainable and Evidence based BPM Learn how Value &amp; Performance Measurement and Management Learn how to roll-out and deploy process Explore how to enable Process Owners, Roles and Knowledge Workers Discover how to Process and Application Modelling Uncover Process Lifecycle, Maturity, Alignment and Continuous Improvement Practical continuous improvement with the way of Governance Future BPM trends that will affect business Explore the BPM Body of Knowledge}
}

@techreport{10.5555/974955,
author = {Chang, Ching-Chih and Hall, Amy and Treichel, Jeanie},
title = {Sun Labs-The First Five Years: The First Fifty Technical Reports. A Commemorative Issue},
year = {1998},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {This commemorative issue in the technical report series encompasses the first five years of Sun Labs' existence from 1991 through 1996. In addition to the Abstracts of the first fifty reports, the contents include a list of patents issued during that time, staff presentations at conferences and meetings around the world, and a comprehensive article on the formation of Sun Labs as featured on Sun's external Website in February of 1996.}
}

@book{10.5555/2028568,
author = {Lund, Arnie},
title = {User Experience Management: Essential Skills for Leading Effective UX Teams},
year = {2011},
isbn = {9780123854964},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The role of UX manager is of vital importance -- it means leading a productive team, influencing businesses to adopt user-centered design, and delivering valuable products customers. Few UX professionals who find themselves in management positions have formal training in management. More often than not they are promoted to a management position after having proven themselves as an effective and successful practitioner.Yet as important as the position of manager is to the advancement of the field there are no books that specifically address the needs of user experience managers. Though information is available on the Web, nothing ties that advice together in the way a manager would need to integrate it in their work. User Experience Management speaks directly to the UX manager and to the unique challenges one may face. It outlines the robust framework for how to be an effective UX manager, from creating a team, to orchestrating product development, to ensuring UX is not compromised, to achieving company buy-in on results. This acts as a checklist readers can use to make sure they have covered the bases as they think about how to build their own user experience programs. Written by an experienced UX manager, and containing testamonials from many leading managers in the field, managers both current and aspiring will find this an invaluable reference loaded with ideas and techniques for managing user experience. *Gives a UX leadership boot-camp from putting together a winning team, to giving them a driving focus, to acting as their spokesman, to handling difficult situations *Full of practical advice and experiences for managers and leaders in virtually any area of the user experience field *Contains best practices, real-world stories, and insights from UX leaders at IBM, Microsoft, SAP, and many more! Table of Contents Chapter 1- Introduction Chapter2- Building the Team Chapter 3- Creating Your Team Chapter 4- Equipping Your Team Chapter 5- Focusing the Team Chapter 6- Creating A High-Performing Team Chapter 7- Communication and Collaboration Chapter 8- Transforming the Organization Chapter 9- Evangelizing UX Chapter 10- Conclusion}
}

@book{10.5555/2331269,
author = {Barry, Peter and Crowley, Patrick},
title = {Modern Embedded Computing: Designing Connected, Pervasive, Media-Rich Systems},
year = {2012},
isbn = {0123914906},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Modern embedded systems are used for connected, media-rich, and highly integrated handheld devices such as mobile phones, digital cameras, and MP3 players. All of these embedded systems require networking, graphic user interfaces, and integration with PCs, as opposed to traditional embedded processors that can perform only limited functions for industrial applications. While most books focus on these controllers, Modern Embedded Computing provides a thorough understanding of the platform architecture of modern embedded computing systems that drive mobile devices. The book offers a comprehensive view of developing a framework for embedded systems-on-chips. Examples feature the Intel Atom processor, which is used in high-end mobile devices such as e-readers, Internet-enabled TVs, tablets, and net books. Beginning with a discussion of embedded platform architecture and Intel Atom-specific architecture, modular chapters cover system boot-up, operating systems, power optimization, graphics and multi-media, connectivity, and platform tuning. Companion lab materials compliment the chapters, offering hands-on embedded design experience.Learn embedded systems design with the Intel Atom Processor, based on the dominant PC chip architecture. Examples use Atom and offer comparisons to other platformsDesign embedded processors for systems that support gaming, in-vehicle infotainment, medical records retrieval, point-of-sale purchasing, networking, digital storage, and many more retail, consumer and industrial applicationsExplore companion lab materials online that offer hands-on embedded design experience}
}

@book{10.5555/2597865,
author = {Loshin, David},
title = {Business Intelligence: The Savvy Manager's Guide},
year = {2012},
isbn = {9780123858900},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Following the footsteps of the first edition, the second edition of Business Intelligence is a full overview of what comprises business intelligence. It is intended to provide an introduction to the concepts to uncomplicate the learning process when implementing a business intelligence program. Over a relatively long lifetime (7 years), the current edition of book has received numerous accolades from across the industry for its straightforward introduction to both business and technical aspects of business intelligence. As an author, David Loshin has a distinct ability to translate challenging topics into a framework that is easily digestible by managers, business analysts, and technologists alike. In addition, his material has developed a following (such as the recent Master Data Management book) among practitioners and key figures in the industry (both analysts and vendors) and that magnifies our ability to convey the value of this book. Guides managers through developing, administering, or simply understanding business intelligence technology Keeps pace with the changes in best practices, tools, methods and processes used to transform an organizations data into actionable knowledge Contains a handy, quick-reference to technologies and terminology. Table of Contents 1. Business Intelligence - An Introduction 2. Value Drivers 3. Planning for Success 4. Developing a Business Intelligence Roadmap 5. The Business Intelligence Environment 6. Business Models and Information Flow 7. Data Requirements Analysis 8. Data Warehouses and the Technical BI Architecture 9. Business Metadata 10. Data Profiling 11. Business Rules 12. Data Quality 13. Data Integration 14. High Performance BI 15. Alternate Information Contexts 16. Location Intelligence and Spatial analysis 17. Knowledge Discovery, Data Mining, and Analytics 18. Using Publicly Available Data 19. Knowledge Delivery 20. New and Emerging Techniques 21. Quick Reference}
}

@article{10.1145/359327.359337,
author = {Case, Richard P. and Padegs, Andris},
title = {Architecture of the IBM system/370},
year = {1978},
issue_date = {Jan. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/359327.359337},
doi = {10.1145/359327.359337},
abstract = {This paper discusses the design considerations for the architectural extensions that distinguish System/370 from System/360. It comments on some experiences with the original objectives for System/360 and on the efforts to achieve them, and it describes the reasons and objectives for extending the architecture. It covers virtual storage, program control, data-manipulation instructions, timing facilities, multiprocessing, debugging and monitoring, error handling, and input/output operations. A final section tabulates some of the important parameters of the various IBM machines which implement the architecture.},
journal = {Commun. ACM},
month = jan,
pages = {73–96},
numpages = {24},
keywords = {virtual storage, instruction sets, error handling, computer systems, architecture}
}

@article{10.1561/1100000004,
author = {Iachello, Giovanni and Hong, Jason},
title = {End-user privacy in human-computer interaction},
year = {2007},
issue_date = {January 2007},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {1},
number = {1},
issn = {1551-3955},
url = {https://doi.org/10.1561/1100000004},
doi = {10.1561/1100000004},
abstract = {The purpose of this article is twofold. First, we summarize research on the topic of privacy in Human-Computer Interaction (HCI), outlining current approaches, results, and trends. Practitioners and researchers can draw upon this review when working on topics related to privacy in the context of HCI and CSCW. The second purpose is that of charting future research trends and of pointing out areas of research that are timely but lagging. This work is based on a comprehensive analysis of published academic and industrial literature spanning three decades, and on the experience of both ourselves and of many of our colleagues.},
journal = {Found. Trends Hum.-Comput. Interact.},
month = jan,
pages = {1–137},
numpages = {137}
}

@book{10.5555/2671171,
author = {Lee, Gary},
title = {Cloud Networking: Understanding Cloud-based Data Center Networks},
year = {2014},
isbn = {0128007281},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Networking: Understanding Cloud-Based Data Center Networks explains the evolution of established networking technologies into distributed, cloud-based networks. Starting with an overview of cloud technologies, the book explains how cloud data center networksleverage distributed systems for network virtualization, storage networking, and software-defined networking. The author offers insider perspective to key components that make a cloud network possible such as switch fabric technology and data center networking standards. The final chapters look ahead to developments in architectures, fabric technology, interconnections, and more. By the end of the book, readers will understand core networking technologies and how they're used in a cloud data center. Understand existing and emerging networking technologies that combine to form cloud data center networks Explains the evolution of data centers from enterprise to private and public cloud networks Reviews network virtualization standards for multi-tenant data center environments Includes cutting-edge detail on the latest switch fabric technologies from the networking team in Intel}
}

@book{10.5555/2821563,
author = {McGilvray, Danette},
title = {Executing Data Quality Projects: Ten Steps to Quality Data and Trusted InformationTM},
year = {2008},
isbn = {9780080558394},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Information is currency. Recent studies show that data quality problems are costing businesses billions of dollars each year, with poor data linked to waste and inefficiency, damaged credibility among customers and suppliers, and an organizational inability to make sound decisions. In this important and timely new book, Danette McGilvray presents her Ten Steps approach to information quality, a proven method for both understanding and creating information quality in the enterprise. Her trademarked approach-in which she has trained Fortune 500 clients and hundreds of workshop attendees-applies to all types of data and to all types of organizations. * Includes numerous templates, detailed examples, and practical advice for executing every step of the Ten Steps approach. * Allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, and best practices. * A companion Web site includes links to numerous data quality resources, including many of the planning and information-gathering templates featured in the text, quick summaries of key ideas from the Ten Step methodology, and other tools and information available online. Table of Contents Introduction The Reason for This Book Intended Audiences Structure of This Book How to Use This Book Acknowledgements Chapter 1 Overview Impact of Information and Data Quality About the Methodology Approaches to Data Quality in Projects Engaging Management Chapter 2 Key Concepts Introduction Framework for Information Quality (FIQ) Information Life Cycle Data Quality Dimensions Business Impact Techniques Data Categories Data Specifications Data Governance and Stewardship The Information and Data Quality Improvement Cycle The Ten Steps Process Best Practices and Guidelines Chapter 3 The Ten Steps 1. Define Business Need and Approach 2. Analyze Information Environment 3. Assess Data Quality 4. Assess Business Impact 5. Identify Root Causes 6. Develop Improvement Plans 7. Prevent Future Data Errors 8. Correct Current Data Errors 9. Implement Controls 10. Communicate Actions and Results Chapter 4 Structuring Your Project Projects and The Ten Steps Data Quality Project Roles Project Timing Chapter 5 Other Techniques and Tools Introduction Information Life Cycle Approaches Capture Data Analyze and Document Results Metrics Data Quality Tools The Ten Steps and Six Sigma Chapter 6 A Few Final Words Appendix Quick References Framework for Information Quality POSMAD Interaction Matrix Detail POSMAD Phases and Activities Data Quality Dimensions Business Impact Techniques The Ten Steps Overview Definitions of Data Categories}
}

@book{10.5555/1534359,
author = {Porter, Thomas and Gough, Michael},
title = {How to Cheat at VoIP Security},
year = {2007},
isbn = {9780080553535},
publisher = {Syngress Publishing},
abstract = {The Perfect Reference for the Multitasked SysAdmin This is the perfect guide if VoIP engineering is not your specialty. It is the perfect introduction to VoIP security, covering exploit tools and how they can be used against VoIP (Voice over IP) systems. It gives the basics of attack methodologies used against the SIP and H.323 protocols as well as VoIP network infrastructure. * VoIP Isn't Just Another Data Protocol IP telephony uses the Internet architecture, similar to any other data application. However, from a security administrator's point of view, VoIP is different. Understand why. * What Functionality Is Gained, Degraded, or Enhanced on a VoIP Network Find out the issues associated with quality of service, emergency 911 service, and the major benefits of VoIP. * The Security Considerations of Voice Messaging Learn about the types of security attacks you need to protect against within your voice messaging system. * Understand the VoIP Communication Architectures Understand what PSTN is and what it does as well as the H.323 protocol specification, and SIP Functions and features. * The Support Protocols of VoIP Environments Learn the services, features, and security implications of DNS, TFTP, HTTP, SNMP, DHCP, RSVP, SDP, and SKINNY. * Securing the Whole VoIP Infrastructure Learn about Denial-of-Service attacks, VoIP service disruption, call hijacking and interception, H.323-specific attacks, and SIP-specific attacks. * Authorized Access Begins with Authentication Learn the methods of verifying both the user identity and the device identity in order to secure a VoIP network. * Understand Skype Security Skype does not log a history like other VoIP solutions; understand the implications of conducting business over a Skype connection. * Get the Basics of a VoIP Security Policy Use a sample VoIP Security Policy to understand the components of a complete policy. *Provides system administrators with hundreds of tips, tricks, and scripts to complete administration tasks more quickly and efficiently *Short on theory, history, and technical data that ultimately is not helpful in performing their jobs *Avoid the time drains associated with securing VoIP}
}

@book{10.5555/2385466,
author = {McCool, Michael and Reinders, James and Robison, Arch},
title = {Structured Parallel Programming: Patterns for Efficient Computation},
year = {2012},
isbn = {9780123914439},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Programming is now parallel programming. Much as structured programming revolutionized traditional serial programming decades ago, a new kind of structured programming, based on patterns, is relevant to parallel programming today. Parallel computing experts and industry insiders Michael McCool, Arch Robison, and James Reinders describe how to design and implement maintainable and efficient parallel algorithms using a pattern-based approach. They present both theory and practice, and give detailed concrete examples using multiple programming models. Examples are primarily given using two of the most popular and cutting edge programming models for parallel programming: Threading Building Blocks, and Cilk Plus. These architecture-independent models enable easy integration into existing applications, preserve investments in existing code, and speed the development of parallel applications. Examples from realistic contexts illustrate patterns and themes in parallel algorithm design that are widely applicable regardless of implementation technology. The patterns-based approach offers structure and insight that developers can apply to a variety of parallel programming models Develops a composable, structured, scalable, and machine-independent approach to parallel computing Includes detailed examples in both Cilk Plus and the latest Threading Building Blocks, which support a wide variety of computers Table of Contents 1. Introduction 2. Map 3. Collectives 4. Data reorganization 5. Fork-join 6. Examples 7. Further Reading}
}

@book{10.5555/2331379,
author = {Das, Sajal K. and Kant, Krishna and Zhang, Nan},
title = {Handbook on Securing Cyber-Physical Critical Infrastructure},
year = {2012},
isbn = {9780124158153},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The worldwide reach of the Internet allows malicious cyber criminals to coordinate and launch attacks on both cyber and cyber-physical infrastructure from anywhere in the world. This purpose of this handbook is to introduce the theoretical foundations and practical solution techniques for securing critical cyber and physical infrastructures as well as their underlying computing and communication architectures and systems. Examples of such infrastructures include utility networks (e.g., electrical power grids), ground transportation systems (automotives, roads, bridges and tunnels), airports and air traffic control systems, wired and wireless communication and sensor networks, systems for storing and distributing water and food supplies, medical and healthcare delivery systems, as well as financial, banking and commercial transaction assets. The handbook focus mostly on the scientific foundations and engineering techniques - while also addressing the proper integration of policies and access control mechanisms, for example, how human-developed policies can be properly enforced by an automated system. *Addresses the technical challenges facing design of secure infrastructures by providing examples of problems and solutions from a wide variety of internal and external attack scenarios *Includes contributions from leading researchers and practitioners in relevant application areas such as smart power grid, intelligent transportation systems, healthcare industry and so on. *Loaded with examples of real world problems and pathways to solutions utilizing specific tools and techniques described in detail throughout Table of Contents Introduction: Securing Cyber-Physical Infrastructures--An Overview Part 1: Theoretical Foundations of Security Chapter 1: Security and Vulnerability of Cyber-Physical Infrastructure Networks: A Control-Theoretic Approach Chapter 2: Game Theory for Infrastructure Security - The Power of Intent-Based Adversary Models Chapter 3: An Analytical Framework for Cyber-Physical Networks Chapter 4: Evolution of Widely Spreading Worms and Countermeasures : Epidemic Theory and Application Part 2: Security for Wireless Mobile Networks Chapter 5: Mobile Wireless Network Security Chapter 6: Robust Wireless Infrastructure against Jamming Attacks Chapter 7: Security for Mobile Ad Hoc Networks Chapter 8: Defending against Identity-Based Attacks in Wireless Networks Part 3: Security for Sensor Networks Chapter 9: Efficient and Distributed Access Control for Sensor Networks Chapter 10: Defending against Physical Attacks in Wireless Sensor Networks Chapter 11: Node Compromise Detection in Wireless Sensor Networks Part 4: Platform Security Chapter 12: Hardware and Security: Vulnerabilities and Solutions Chapter 13: Languages and Security: Safer Software Through Language and Compiler Techniques Part 5: Cloud Computing and Data Security Chapter 14: Protecting Data in Outsourcing Scenarios Chapter 15: Data Security in Cloud Computing Chapter 16: Secure Mobile Cloud Computing Chapter 17: Relation Privacy Preservation in Online Social Networks Part 6: Event Monitoring and Situation Awareness Chapter 18: Distributed Network and System Monitoring for Securing Cyber-Physical Infrastructure Chapter 19: Discovering and Tracking Patterns of Interest in Security Sensor Streams Chapter 20: Pervasive Sensing and Monitoring for Situational Awareness Chapter 21: Sense and Response Systems for Crisis Management Part 7. Policy Issues in Security Management Chapter 22: Managing and Securing Critical Infrastructure -- A Semantic Policy and Trust-Driven Approach Chapter 23: Policies, Access Control, and Formal Methods Chapter 24: Formal Analysis of Policy based Security Con gurations in Enterprise Networks Part 8: Security Issues in Real-World Systems Chapter 25: Security and Privacy in the Smart Grid Chapter 26: Cyber-physical Security of Automotive Information Technology Chapter¿27: Security and Privacy for Mobile Healthcare (m-Health) Systems Chapter¿28: Security and Robustness in the Internet Infrastructure Chapter¿29: Emergency Vehicular Networks Chapter¿30: Security Issues in VoIP Telecommunication Networks}
}

@book{10.5555/2500962,
author = {Sheikh, Nauman},
title = {Implementing Analytics: A Blueprint for Design, Development, and Adoption},
year = {2013},
isbn = {9780124016811},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Implementing Analytics demystifies the concept, technology and application of analytics and breaks its implementation down to repeatable and manageable steps, making it possible for widespread adoption across all functions of an organization. Implementing Analytics simplifies and helps democratize a very specialized discipline to foster business efficiency and innovation without investing in multi-million dollar technology and manpower. A technology agnostic methodology that breaks down complex tasks like model design and tuning and emphasizes business decisions rather than the technology behind analytics. Simplifies the understanding of analytics from a technical and functional perspective and shows a wide array of problems that can be tackled using existing technology Provides a detailed step by step approach to identify opportunities, extract requirements, design variables and build and test models. It further explains the business decision strategies to use analytics models and provides an overview for governance and tuning Helps formalize analytics projects from staffing, technology and implementation perspectives Emphasizes machine learning and data mining over statistics and shows how the role of a Data Scientist can be broken down and still deliver the value by building a robust development process Table of Contents 1. Introduction 2. What is Analytics 3. Analytics Project Lifecycle 4. Analytics Project Business Case 5. Analytics Project Architecture 6. Analytics Project Team 7. Analytics Project Development Methodology 8. Existing Technology 9. Specialized Databases 10. Statistical Tools 11. Scoring and Rating Engine 12. Strategy Design Tool}
}

@book{10.5555/1564780,
author = {Hauck, Scott and DeHon, Andre},
title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation},
year = {2007},
isbn = {9780080556017},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The main characteristic of Reconfigurable Computing is the presence of hardware that can be reconfigured to implement specific functionality more suitable for specially tailored hardware than on a simple uniprocessor. Reconfigurable computing systems join microprocessors and programmable hardware in order to take advantage of the combined strengths of hardware and software and have been used in applications ranging from embedded systems to high performance computing. Many of the fundamental theories have been identified and used by the Hardware/Software Co-Design research field. Although the same background ideas are shared in both areas, they have different goals and use different approaches.This book is intended as an introduction to the entire range of issues important to reconfigurable computing, using FPGAs as the context, or "computing vehicles" to implement this powerful technology. It will take a reader with a background in the basics of digital design and software programming and provide them with the knowledge needed to be an effective designer or researcher in this rapidly evolving field. · Treatment of FPGAs as computing vehicles rather than glue-logic or ASIC substitutes · Views of FPGA programming beyond Verilog/VHDL · Broad set of case studies demonstrating how to use FPGAs in novel and efficient ways}
}

@book{10.5555/2480824,
author = {Gaster, Benedict and Howes, Lee and Kaeli, David R. and Mistry, Perhaad and Schaa, Dana},
title = {Heterogeneous Computing with OpenCL: Revised OpenCL 1.2 Edition},
year = {2012},
isbn = {9780124055209},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Heterogeneous Computing with OpenCL teaches OpenCL and parallel programming for complex systems that may include a variety of device architectures: multi-core CPUs, GPUs, and fully-integrated Accelerated Processing Units (APUs) such as AMD Fusion technology. Designed to work on multiple platforms and with wide industry support, OpenCL will help you more effectively program for a heterogeneous future. Written by leaders in the parallel computing and OpenCL communities, this book will give you hands-on OpenCL experience to address a range of fundamental parallel algorithms. The authors explore memory spaces, optimization techniques, graphics interoperability, extensions, and debugging and profiling. Intended to support a parallel programming course, Heterogeneous Computing with OpenCL includes detailed examples throughout, plus additional online exercises and other supporting materials. Explains principles and strategies to learn parallel programming with OpenCL, from understanding the four abstraction models to thoroughly testing and debugging complete applications. Covers image processing, web plugins, particle simulations, video editing, performance optimization, and more. Shows how OpenCL maps to an example target architecture and explains some of the tradeoffs associated with mapping to various architectures Addresses a range of fundamental programming techniques, with multiple examples and case studies that demonstrate OpenCL extensions for a variety of hardware platforms Table of Contents Introduction to Parallel Programming Introduction to OpenCL OpenCL Device Architectures Basic OpenCL Examples Understanding OpenCLs Concurrency and Execution Model Dissecting a CPUGPU OpenCL Implementation Data Management OpenCL Case Study: Convolution OpenCL Case Study: Histogram OpenCL Case Study: Mixed Particle Simulation OpenCL Extensions Foreign Lands: Plugging OpenCL In OpenCL Profiling and Debugging Performance Optimization of an Image Analysis Application}
}

@book{10.5555/1534409,
author = {Righi, Carol and James, Janice},
title = {User-Centered Design Stories: Real-World UCD Case Studies},
year = {2007},
isbn = {9780080481555},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Intended for both the student and the practitioner, this is the first user-centered design casebook. It follows the Harvard Case study method, where the reader is placed in the role of the decision-maker in a real-life professional situation. In this book, the reader is asked to perform analysis of dozens of UCD work situations and propose solutions for the problem set. The problems posed in the cases cover a wide variety of key tasks and issues facing practitioners today, including those that are related to organizational/managerial topics, UCD methods and processes, and technical/ project issues. The benefit of the casebook and its organization is that it offers the new practitioner (as well as experienced practitioners working in new settings) the valuable practice in decision-making that one cannot get by reading a book or attending a seminar. *The first User-Centered Design Casebook, with cases covering the key tasks and issues facing UCD practitioners today. *Each chapter based on real world cases with complex problems, giving readers as close to a real-world experience as possible. * Offers "the things you don't learn in school," such as innovative and hybrid solutions that were actually used on the problems discussed.}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/2837614,
title = {POPL '16: Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg, FL, USA}
}

@book{10.5555/2755633,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML, Third Edition: The Systems Modeling Language},
year = {2014},
isbn = {0128002026},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {A Practical Guide to SysML, Third Edition, fully updated for SysML version 1.4, provides a comprehensive and practical guide for modeling systems with SysML. With their unique perspective as leading contributors to the language, Friedenthal, Moore, and Steiner provide a full description of the language along with a quick reference guide and practical examples to help you use SysML. The book begins with guidance on the most commonly used features to help you get started quickly. Part 1 explains the benefits of a model-based approach, providing an overview of the language and how to apply SysML to model systems. Part 2 includes a comprehensive description of SysML that provides a detailed understanding that can serve as a foundation for modeling with SysML, and as a reference for practitioners. Part 3 includes methods for applying model-based systems engineering using SysML to specify and design systems, and how these methods can help manage complexity. Part 4 deals with topics related to transitioning MBSE practice into your organization, including integration of the system model with other engineering models, and strategies for adoption of MBSE. Learn how and why to deploy MBSE in your organization with an introduction to systems and model-based systems engineering Use SysML to describe systems with this general overview and a detailed description of the Systems Modeling Language Review practical examples of MBSE methodologies to understand their application to specifying and designing a system Includes comprehensive modeling notation tables as an appendix that can be used as a standalone reference}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@book{10.5555/2086747,
author = {Laszewski, Tom and Nauduri, Prakash},
title = {Migrating to the Cloud: Oracle Client/Server Modernization},
year = {2011},
isbn = {9781597496476},
publisher = {Syngress Publishing},
edition = {1st},
abstract = {Whether your company is planning on database migration, desktop application migration, or has IT infrastructure consolidation projects, this book gives you all the resources youll need. It gives you recommendations on tools, strategy and best practices and serves as a guide as you plan, determine effort and budget, design, execute and roll your modern Oracle system out to production. Focusing on Oracle grid relational database technology and Oracle Fusion Middleware as the target cloud-based architecture, your company can gain organizational efficiency, agility, increase innovation and reduce IT Total Cost of Ownership (TCO) by moving to service-oriented, Web-based cloud architectures. Focuses on Oracle architecture, Middleware and COTS business applications Explains the tools and technologies necessary for your legacy migration Gives useful information about various strategies, migration methodologies and efficient plans for executing migration projects Table of Contents Introduction Chapter 1: Migrating to the Cloud: ClientServer Migrations to the Oracle Cloud Chapter 2: Identifying the Level of Effort and Cost Chapter 3: Methodology and Design Chapter 4: Relational Migration Tools Chapter 5: Database Schema and Data Migration Chapter 6: Database Stored Object Migration Chapter 7: Application MigrationPorting Due to Database Migration Chapter 8: Migrating Applications to the Cloud Chapter 9: Service Enablement of ClientServer Applications Chapter 10: Oracle Database Cloud Infrastructure Planning and Implementation Chapter 11: Sybase Migrations from a Systems Integrator Perspective, and Case Study Chapter 12: Application Migration: Oracle Forms to Oracle Application Development Framework 11g Chapter 13: Application Migration: PowerBuilder to Oracle APEX Chapter 14: Challenges and Emerging Trends}
}

@techreport{10.1145/2594501,
author = {Ashenhurst, R. L.},
title = {ACM Curricula Recommendations for Information Systems},
year = {1983},
isbn = {089791 1180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This report contains curriculum recommendations prepared by the ACM Curriculum Committee on Computer Education for Management.}
}

@techreport{10.1145/2594496,
author = {Atchison, William F. and Conte, Samuel D. and Hamblen, John W. and Hull, Thomas E. and Keenan, Thomas A. and Kehl, William B. and McCluskey, Edward J. and Navarro, Silvio O. and Rheinboldt, Werner C. and Schweppe, Earl J. and Viavant, William and Young, David M.},
title = {ACM Recommended Curricula for Computer Science and Information Processing Programs in Colleges and Universities, 1968-1981},
year = {1981},
isbn = {0897910583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {When we enter the twenty-first century, computers will have influenced our lives more than any other technology known to civilization. The need for knowledge about computers, computer technology, and the theoretical aspects of computers has grown exponentially. Since its founding in 1947, the Association for Computing Machinery has attempted to address this need for computer knowledge.The study of computers is an endeavor demanding careful, thorough, and organized development. Recognizing the need for a comprehensive model curriculum for the growing number of institutions offering computer science courses, ACM, in the mid-sixties, established the Curriculum Committee on Computer Science to make recommendations and provide guidelines to institutions. Chaired by Dr. William Atchison of the University of Maryland, this dedicated group of forward-thinking people published preliminary recommendations in September 1965. With financial assistance from the National Science Foundation, they published their final report, "Recommendations for Academic Programs in Computer Science," in Communications of the ACM in March 1968. This curriculum, known as "Curriculum '68," formed the basis of formal computer science study in colleges and universities for the next ten years.}
}

@book{10.5555/2843494,
author = {Fisher, Joseph A. and Faraboschi, Paolo and Young, Cliff},
title = {Embedded Computing: A VLIW Approach to Architecture, Compilers and Tools},
year = {2005},
isbn = {9780080477541},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The fact that there are more embedded computers than general-purpose computers and that we are impacted by hundreds of them every day is no longer news. What is news is that their increasing performance requirements, complexity and capabilities demand a new approach to their design. Fisher, Faraboschi, and Young describe a new age of embedded computing design, in which the processor is central, making the approach radically distinct from contemporary practices of embedded systems design. They demonstrate why it is essential to take a computing-centric and system-design approach to the traditional elements of nonprogrammable components, peripherals, interconnects and buses. These elements must be unified in a system design with high-performance processor architectures, microarchitectures and compilers, and with the compilation tools, debuggers and simulators needed for application development. In this landmark text, the authors apply their expertise in highly interdisciplinary hardwaresoftware development and VLIW processors to illustrate this change in embedded computing. VLIW architectures have long been a popular choice in embedded systems design, and while VLIW is a running theme throughout the book, embedded computing is the core topic. Embedded Computing examines both in a book filled with fact and opinion based on the authors many years of R&amp;D experience. Complemented by a unique, professional-quality embedded tool-chain on the authors website, http: Combines technical depth with real-world experience Comprehensively explains the differences between general purpose computing systems and embedded systems at the hardware, software, tools and operating system levels. Uses concrete examples to explain and motivate the trade-offs. Table of Contents Preface Chapter 1: An Introduction to Embedded Processing Chapter 2: An Overview of VLIW and ILP Chapter 3: An Overview of ISA Design Chapter 4: Architectural Structures in ISA design Chapter 5: Microarchitecture Design Chapter 6: System Design and Simulation Chapter 7: Embedded Compiling and Toolchains Chapter 8: Compiling for VLIWs and ILP Chapter 9: The Run-time System Chapter 10: Application Design and Customization Chapter 11: Application Areas Appendix A: The VEX System Appendix B: Glossary Appendix C: Bibliography}
}

@article{10.1145/355602.361320,
author = {Ashenhurst, R. L.},
title = {Curriculum recommendations for graduate professional programs in information systems},
year = {1972},
issue_date = {May 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/355602.361320},
doi = {10.1145/355602.361320},
journal = {Commun. ACM},
month = may,
pages = {363–398},
numpages = {36},
keywords = {systems analysis, system design, management systems, management information systems, information systems development, information analysis, education}
}

@book{10.5555/983652,
author = {Sloss, Andrew and Symes, Dominic and Wright, Chris},
title = {ARM System Developer's Guide: Designing and Optimizing System Software},
year = {2004},
isbn = {1558608745},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA}
}

@book{10.5555/1477660,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML: Systems Modeling Language},
year = {2008},
isbn = {9780080558363},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This book provides a comprehensive and practical guide for modeling systems with SysML. It includes a full description of the language along with a quick reference guide, and shows how the language can be applied to specify, analyze, and design systems. It contains examples to help readers understand how SysML can be used in practice. The book also includes guidance on how an organization or project can transition to model based systems engineering using SysML, with considerations for processes, methods, tools, and training. *The authoritative guide for understanding and applying SysML *Authored by the foremost experts on the language *Language description, examples, and quick reference guide included}
}

@book{10.5555/1537180,
author = {Harley, David and Bechtel, Ken and Blanchard, Michael and Diemer, Henk K. and Lee, Andrew and Muttik, Igor and Zdrnja, Bojan},
title = {AVIEN Malware Defense Guide for the Enterprise},
year = {2007},
isbn = {9780080558660},
publisher = {Syngress Publishing},
abstract = {Members of AVIEN (the Anti-Virus Information Exchange Network) have been setting agendas in malware management for several years: they led the way on generic filtering at the gateway, and in the sharing of information about new threats at a speed that even anti-virus companies were hard-pressed to match. AVIEN members represent the best-protected large organizations in the world, and millions of users. When they talk, security vendors listen: so should you. AVIENs sister organization AVIEWS is an invaluable meeting ground between the security vendors and researchers who know most about malicious code and anti-malware technology, and the top security administrators of AVIEN who use those technologies in real life. This new book uniquely combines the knowledge of these two groups of experts. Anyone who is responsible for the security of business information systems should be aware of this major addition to security literature. * Customer Power takes up the theme of the sometimes stormy relationship between the antivirus industry and its customers, and tries to dispel some common myths. It then considers the roles of the independent researcher, the vendor-employed specialist, and the corporate security specialist. * Stalkers on Your Desktop considers the thorny issue of malware nomenclature and then takes a brief historical look at how we got here, before expanding on some of the malware-related problems we face today. * A Tangled Web discusses threats and countermeasures in the context of the World Wide Web. * Big Bad Bots tackles bots and botnets, arguably Public Cyber-Enemy Number One. * Cr me de la CyberCrime takes readers into the underworld of old-school virus writing, criminal business models, and predicting future malware hotspots. * Defense in Depth takes a broad look at DiD in the enterprise, and looks at some specific tools and technologies. * Perilous Outsorcery offers sound advice on how to avoid the perils and pitfalls of outsourcing, incorporating a few horrible examples of how not to do it. * Education in Education offers some insights into user education from an educationalists perspective, and looks at various aspects of security in schools and other educational establishments. * DIY Malware Analysis is a hands-on, hands-dirty approach to security management, considering malware analysis and forensics techniques and tools. * Antivirus Evaluation &amp; Testing continues the D-I-Y theme, discussing at length some of the thorny issues around the evaluation and testing of antimalware software. * AVIEN &amp; AVIEWS: the Future looks at future developments in AVIEN and AVIEWS. .}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@book{10.5555/1526227,
author = {Gregg, Michael and Seagren, Eric and Orebaugh, Angela and Jonkman, Matt and Marty, Raffael},
title = {How to Cheat at Configuring Open Source Security Tools},
year = {2007},
isbn = {9780080553566},
publisher = {Syngress Publishing},
abstract = {The Perfect Reference for the Multitasked SysAdmin This is the perfect guide if network security tools is not your specialty. It is the perfect introduction to managing an infrastructure with freely available, and powerful, Open Source tools. Learn how to test and audit your systems using products like Snort and Wireshark and some of the add-ons available for both. In addition, learn handy techniques for network troubleshooting and protecting the perimeter. * Take Inventory See how taking an inventory of the devices on your network must be repeated regularly to ensure that the inventory remains accurate. * Use Nmap Learn how Nmap has more features and options than any other free scanner. * Implement Firewalls Use netfilter to perform firewall logic and see how SmoothWall can turn a PC into a dedicated firewall appliance that is completely configurable. * Perform Basic Hardening Put an IT security policy in place so that you have a concrete set of standards against which to measure. * Install and Configure Snort and Wireshark Explore the feature set of these powerful tools, as well as their pitfalls and other security considerations. * Explore Snort Add-Ons Use tools like Oinkmaster to automatically keep Snort signature files current. * Troubleshoot Network Problems See how to reporting on bandwidth usage and other metrics and to use data collection methods like sniffing, NetFlow, and SNMP. * Learn Defensive Monitoring Considerations See how to define your wireless network boundaries, and monitor to know if they're being exceeded and watch for unauthorized traffic on your network. *Covers the top 10 most popular open source security tools including Snort, Nessus, Wireshark, Nmap, and Kismet *Companion Web site contains dozens of working scripts and tools for readers *Follows Syngress' proven "How to Cheat" pedagogy providing readers with everything they need and nothing they don't}
}

@book{10.5555/2755638,
author = {Sherman, Rick},
title = {Business Intelligence Guidebook: From Data Integration to Analytics},
year = {2014},
isbn = {012411461X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Between the high-level concepts of business intelligence and the nitty-gritty instructions for using vendors' tools lies the essential, yet poorly-understood layer of architecture, design and process. Without this knowledge, Big Data is belittled - projects flounder, are late and go over budget. Business Intelligence Guidebook: From Data Integration to Analytics shines a bright light on an often neglected topic, arming you with the knowledge you need to design rock-solid business intelligence and data integration processes. Practicing consultant and adjunct BI professor Rick Sherman takes the guesswork out of creating systems that are cost-effective, reusable and essential for transforming raw data into valuable information for business decision-makers. After reading this book, you will be able to design the overall architecture for functioning business intelligence systems with the supporting data warehousing and data-integration applications. You will have the information you need to get a project launched, developed, managed and delivered on time and on budget - turning the deluge of data into actionable information that fuels business knowledge. Finally, you'll give your career a boost by demonstrating an essential knowledge that puts corporate BI projects on a fast-track to success. Provides practical guidelines for building successful BI, DW and data integration solutions. Explains underlying BI, DW and data integration design, architecture and processes in clear, accessible language. Includes the complete project development lifecycle that can be applied at large enterprises as well as at small to medium-sized businesses Describes best practices and pragmatic approaches so readers can put them into action. Companion website includes templates and examples, further discussion of key topics, instructor materials, and references to trusted industry sources.}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@book{10.5555/1479757,
author = {Azad, Tariq},
title = {Securing Citrix Presentation Server in the Enterprise},
year = {2008},
isbn = {9780080569987},
publisher = {Syngress Publishing},
abstract = {Citrix Presentation Server allows remote users to work off a network server as if they weren't remote. That means: Incredibly fast access to data and applications for users, no third party VPN connection, and no latency issues. All of these features make Citrix Presentation Server a great tool for increasing access and productivity for remote users. Unfortunately, these same features make Citrix just as dangerous to the network it's running on. By definition, Citrix is granting remote users direct access to corporate servers ..achieving this type of access is also the holy grail for malicious hackers. To compromise a server running Citrix Presentation Server, a hacker need not penetrate a heavily defended corporate or government server. They can simply compromise the far more vulnerable laptop, remote office, or home office of any computer connected to that server by Citrix Presentation Server. All of this makes Citrix Presentation Server a high-value target for malicious hackers. And although it is a high-value target, Citrix Presentation Servers and remote workstations are often relatively easily hacked, because they are often times deployed by overworked system administrators who haven't even configured the most basic security features offered by Citrix. "The problem, in other words, isn't a lack of options for securing Citrix instances; the problem is that administrators aren't using them." (eWeek, October 2007). In support of this assertion Security researcher Petko D. Petkov, aka "pdp", said in an Oct. 4 posting that his recent testing of Citrix gateways led him to "tons" of "wide-open" Citrix instances, including 10 on government domains and four on military domains. * The most comprehensive book published for system administrators providing step-by-step instructions for a secure Citrix Presentation Server. * Special chapter by Security researcher Petko D. Petkov'aka "pdp detailing tactics used by malicious hackers to compromise Citrix Presentation Servers. * Companion Web site contains custom Citrix scripts for administrators to install, configure, and troubleshoot Citrix Presentation Server.}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@book{10.5555/1386263,
author = {Schubert, Max and Bennett, Derrick and Gines, Jonathan and Hay, Andrew and Strand, John},
title = {Nagios 3 Enterprise Network Monitoring: Including Plug-Ins and Hardware Devices},
year = {2008},
isbn = {9780080560182},
publisher = {Syngress Publishing},
abstract = {Nagios is an Open Source network, hardware, and application monitoring program. It is designed to inform system administrators of problems on their networks before their clients, end-users or managers do. Nagios is a SysAdmin's best friend. Nagios is installed on over 300,000 machines worldwide, and truly is a global product: approximately 25.6% of users are in the U.S., and 30% in EMEA. Nagios can monitor everything from network bandwidth to the temperature and humidity in a server room. SysAdmins are able to use Nagios for such a variety of purposes through custom software "plug ins" and third party hardware. SysAdmins customize these plug ins instructing Nagios to monitor the servers, applications, or devices that are most critical to their network infrastructure. These plug ins also allow SysAdmins to integrate Nagios with other monitoring devices and applications like Snort and Wireshark. Nagios can also be fully integrated with third party environmental monitoring devices and remote power supplies. When Nagios detects a problem, it can notify the SysAdmin in a variety of different ways (email, instant message, SMS, etc.). Current status information, historical logs, and reports can all be accessed via a web browser. Nagios could send a text message to a SysAdmin sitting on his couch at home that the temperature in the server room is too hot and could potentially damage the equipment. The SysAdmin can then check the status of the server from home using his Nagios Web interface, and then coordinate with the appropriate facility management personnel to check the air conditioning in the server room. This is merely one example of Nagios capabilities. The same scenario could be applied to an overloaded Exchange server, a router being pounded by a Denial of Service Attack, or a user accessing or downloading unauthorized materials. * Contains complete case study on deploying Nagios in an enterprise environment. * Companion Web site offers 100 working Scripts for customizing Nagios plug-ins. * Helps organizations adhere to federally mandated compliance regulations such as Sarbanes Oxley, or HIPAA. * Details how to integrate Nagios with third-party hardware.}
}

@book{10.5555/1481619,
author = {Wright, Craig S.},
title = {The IT Regulatory and Standards Compliance Handbook: How to Survive Information Systems Audit and Assessments},
year = {2008},
isbn = {9780080560175},
publisher = {Syngress Publishing},
abstract = {This book provides comprehensive methodology, enabling the staff charged with an IT security audit to create a sound framework, allowing them to meet the challenges of compliance in a way that aligns with both business and technical needs. This "roadmap" provides a way of interpreting complex, often confusing, compliance requirements within the larger scope of an organization's overall needs. Key Features: * The ulitmate guide to making an effective security policy and controls that enable monitoring and testing against them * The most comprehensive IT compliance template available, giving detailed information on testing all your IT security, policy and governance requirements * A guide to meeting the minimum standard, whether you are planning to meet ISO 27001, PCI-DSS, HIPPA, FISCAM, COBIT or any other IT compliance requirement * Both technical staff responsible for securing and auditing information systems and auditors who desire to demonstrate their technical expertise will gain the knowledge, skills and abilities to apply basic risk analysis techniques and to conduct a technical audit of essential information systems from this book * This technically based, practical guide to information systems audit and assessment will show how the process can be used to meet myriad compliance issues}
}

@book{10.5555/2821575,
author = {Nielsen, Jakob},
title = {Usability Engineering},
year = {1994},
isbn = {9780080520292},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Written by the author of the best-selling HyperText &amp; HyperMedia, this book is an excellent guide to the methods of usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality. Step-by-step information on which method to use at various stages during the development lifecycle are included, along with detailed information on how to run a usability test and the unique issues relating to international usability. * Emphasizes cost-effective methods that developers can implement immediately * Instructs readers about which methods to use when, throughout the development lifecycle, which ultimately helps in cost-benefit analysis. * Shows readers how to avoid the four most frequently listed reasons for delay in software projects. * Includes detailed information on how to run a usability test. * Covers unique issues of international usability. * Features an extensive bibliography allowing readers to find additional information. * Written by an internationally renowned expert in the field and the author of the best-selling HyperText &amp; HyperMedia. Table of Contents Executive Summary. What is Usability Generations of User Interfaces. The Usability Engineering Lifecycle. Usability Heuristics. Usability Testing. Usability Assessment Methods Beyond Testing. Interface Standards. International User Interfaces. Future Developments. Appendix A: Exercises. Appendix B: Bibliography. Author Index. Subject Index.}
}

@book{10.5555/1795551,
author = {Craig, Alan and Sherman, William R. and Will, Jeffrey D.},
title = {Developing Virtual Reality Applications: Foundations of Effective Design},
year = {2009},
isbn = {9780080959085},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Virtual Reality systems enable organizations to cut costs and time, maintain financial and organizational control over the development process, digitally evaluate products before having them created, and allow for greater creative exploration. In this book, VR developers Alan Craig, William Sherman, and Jeffrey Will examine a comprehensive collection of current,unique, and foundational VR applications in a multitude of fields, such as business, science, medicine, art, entertainment, and public safety among others. An insider's view of what works, what doesn't work, and why, Developing Virtual Reality Applications explores core technical information and background theory as well as the evolution of key applications from their genesis to their most current form. Developmental techniques are cross-referenced between different applications linking information to describe overall VR trends and fundamental best practices. This synergy, coupled with the most up to date research being conducted, provides a hands-on guide for building applications, and an enhanced, panoramic view of VR development. Developing Virtual Reality Applications is an indispensable one-stop reference for anyone working in this burgeoning field. Dozens of detailed application descriptions provide practical ideas for VR development in ALL areas of interest! Development techniques are cross referenced between different application areas, providing fundamental best practices! Includes a media-rich companion website with hours of footage from application demonstrations}
}

@book{10.5555/1349794,
author = {Wiles, Jack and Reyes, Anthony},
title = {The Best Damn Cybercrime and Digital Forensics Book Period},
year = {2007},
isbn = {9780080556086},
publisher = {Syngress Publishing},
abstract = {Electronic discovery refers to a process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a legal case. Computer forensics is the application of computer investigation and analysis techniques to perform an investigation to find out exactly what happened on a computer and who was responsible. IDC estimates that the U.S. market for computer forensics will be grow from $252 million in 2004 to $630 million by 2009. Business is strong outside the United States, as well. By 2011, the estimated international market will be $1.8 billion dollars. The Techno Forensics Conference has increased in size by almost 50% in its second year; another example of the rapid growth in the market. This book is the first to combine cybercrime and digital forensic topics to provides law enforcement and IT security professionals with the information needed to manage a digital investigation. Everything needed for analyzing forensic data and recovering digital evidence can be found in one place, including instructions for building a digital forensics lab. * Digital investigation and forensics is a growing industry * Corporate I.T. departments needing to investigate incidents related to corporate espionage or other criminal activities are learning as they go and need a comprehensive step-by-step guide to e-discovery * Appeals to law enforcement agencies with limited budgets}
}

@book{10.5555/2222504,
author = {Holtsnider, Bill and Jaffe, Brian D.},
title = {IT Manager's Handbook, Third Edition: Getting your new job done},
year = {2012},
isbn = {0124159494},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {Making the move from an IT technician or team member to management is one of the most difficult career steps you'll face. Help from management and targeted training can be hard to come by - and your success depends on your ability to adapt to your new role almost overnight. You might have years of experience in the trenches, but you'll quickly find that managing a team, setting budgets, and creating a winning strategy for the first time can be daunting tasks. Now in its third edition, IT Manager's Handbook provides a practical reference that you will return to again and again in an ever-changing corporate environment where the demands on IT continue to increase. Make your first 100 days really count with the fundamental principles and core concepts critical to your success as a new IT Manager. The book also includes discusses how to develop an overall IT strategy as well as demonstrate the value of IT to the company.In this book, you'll learn how to: Manage your enterprise's new level of connectivity with a NEW chapter covering social media, handheld devices, and moreImplement and optimize cloud services to provide a better experience for your mobile and virtual workforce at a lower cost to your bottom lineIntegrate mobile applications into your company's strategyManage the money, including topics such as department budgets and leasing versus buyingWork with your "customers", whomever those might be for your IT shopHire, train, and manage your team and their projects so that you come in on time and budgetSecure your systems to face some of today's most challenging security challenges}
}

@book{10.5555/2843512,
author = {Rossi, Francesca and van Beek, Peter and Walsh, Toby},
title = {Handbook of Constraint Programming},
year = {2006},
isbn = {9780080463803},
publisher = {Elsevier Science Inc.},
address = {USA},
abstract = {Constraint programming is a powerful paradigm for solving combinatorial search problems that draws on a wide range of techniques from artificial intelligence, computer science, databases, programming languages, and operations research. Constraint programming is currently applied with success to many domains, such as scheduling, planning, vehicle routing, configuration, networks, and bioinformatics. The aim of this handbook is to capture the full breadth and depth of the constraint programming field and to be encyclopedic in its scope and coverage. While there are several excellent books on constraint programming, such books necessarily focus on the main notions and techniques and cannot cover also extensions, applications, and languages. The handbook gives a reasonably complete coverage of all these lines of work, based on constraint programming, so that a reader can have a rather precise idea of the whole field and its potential. Of course each line of work is dealt with in a survey-like style, where some details may be neglected in favor of coverage. However, the extensive bibliography of each chapter will help the interested readers to find suitable sources for the missing details. Each chapter of the handbook is intended to be a self-contained survey of a topic, and is written by one or more authors who are leading researchers in the area. The intended audience of the handbook is researchers, graduate students, higher-year undergraduates and practitioners who wish to learn about the state-of-the-art in constraint programming. No prior knowledge about the field is necessary to be able to read the chapters and gather useful knowledge. Researchers from other fields should find in this handbook an effective way to learn about constraint programming and to possibly use some of the constraint programming concepts and techniques in their work, thus providing a means for a fruitful cross-fertilization among different research areas. The handbook is organized in two parts. The first part covers the basic foundations of constraint programming, including the history, the notion of constraint propagation, basic search methods, global constraints, tractability and computational complexity, and important issues in modeling a problem as a constraint problem. The second part covers constraint languages and solver, several useful extensions to the basic framework (such as interval constraints, structured domains, and distributed CSPs), and successful application areas for constraint programming. - Covers the whole field of constraint programming - Survey-style chapters - Five chapters on applications Table of Contents Foreword (Ugo Montanari) Part I : Foundations Chapter 1. Introduction (Francesca Rossi, Peter van Beek, Toby Walsh) Chapter 2. Constraint Satisfaction: An Emerging Paradigm (Eugene C. Freuder, Alan K. Mackworth) Chapter 3. Constraint Propagation (Christian Bessiere) Chapter 4. Backtracking Search Algorithms (Peter van Beek) Chapter 5. Local Search Methods (Holger H. Hoos, Edward Tsang) Chapter 6. Global Constraints (Willem-Jan van Hoeve, Irit Katriel) Chapter 7. Tractable Structures for CSPs (Rina Dechter) Chapter 8. The Complexity of Constraint Languages (David Cohen, Peter Jeavons) Chapter 9. Soft Constraints (Pedro Meseguer, Francesca Rossi, Thomas Schiex) Chapter 10. Symmetry in Constraint Programming (Ian P. Gent, Karen E. Petrie, Jean-Francois Puget) Chapter 11. Modelling (Barbara M. Smith) Part II : Extensions, Languages, and Applications Chapter 12. Constraint Logic Programming (Kim Marriott, Peter J. Stuckey, Mark Wallace) Chapter 13. Constraints in Procedural and Concurrent Languages (Thom Fruehwirth, Laurent Michel, Christian Schulte) Chapter 14. Finite Domain Constraint Programming Systems (Christian Schulte, Mats Carlsson) Chapter 15. Operations Research Methods in Constraint Programming (John Hooker) Chapter 16. Continuous and Interval Constraints(Frederic Benhamou, Laurent Granvilliers) Chapter 17. Constraints over Structured Domains (Carmen Gervet) Chapter 18. Randomness and Structure (Carla Gomes, Toby Walsh) Chapter 19. Temporal CSPs (Manolis Koubarakis) Chapter 20. Distributed Constraint Programming (Boi Faltings) Chapter 21. Uncertainty and Change (Kenneth N. Brown, Ian Miguel) Chapter 22. Constraint-Based Scheduling and Planning (Philippe Baptiste, Philippe Laborie, Claude Le Pape, Wim Nuijten) Chapter 23. Vehicle Routing (Philip Kilby, Paul Shaw) Chapter 24. Configuration (Ulrich Junker) Chapter 25. Constraint Applications in Networks (Helmut Simonis) Chapter 26. Bioinformatics and Constraints (Rolf Backofen, David Gilbert)}
}

