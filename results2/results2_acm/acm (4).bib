@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {software product lines, performance prediction, machine learning, configurable systems},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@article{10.1007/s10515-020-00273-8,
author = {Velez, Miguel and Jamshidi, Pooyan and Sattler, Florian and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {ConfigCrusher: towards white-box performance analysis for configurable systems},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00273-8},
doi = {10.1007/s10515-020-00273-8},
abstract = {Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems.},
journal = {Automated Software Engg.},
month = dec,
pages = {265–300},
numpages = {36},
keywords = {Dynamic analysis, Static analysis, Performance analysis, Configurable systems}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106204,
author = {Luthmann, Lars and Stephan, Andreas and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106204},
doi = {10.1145/3106195.3106204},
abstract = {Real-time requirements are crucial for embedded software in many modern application domains of software product lines. Hence, techniques for modeling and analyzing time-critical software have to be lifted to software product line engineering, too. Existing approaches extend timed automata (TA) by feature constraints to so-called featured timed automata (FTA) facilitating efficient verification of real-time properties for entire product lines in a single run. In this paper, we propose a novel modeling formalism, called configurable parametric timed automata (CoPTA), extending expressiveness of FTA by supporting freely configurable and therefore a-priori unbounded timing intervals for real-time constraints, which are defined as feature attributes in extended feature models with potentially infinite configuration spaces. We further describe an efficient test-suite generation methodology for CoPTA models, achieving location coverage on every possible model configuration. Finally, we present evaluation results gained from applying our tool implementation to a collection of case studies, demonstrating efficiency improvements compared to a variant-by-variant analysis.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {104–113},
numpages = {10},
keywords = {Timed Automata, Software Product Lines, Real-Time Systems, Model-based Testing},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Quality assurance, Machine learning, Software testing, Software variability, Configurable system, Software product line}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.5555/3155562.3155625,
author = {Jamshidi, Pooyan and Siegmund, Norbert and Velez, Miguel and K\"{a}stner, Christian and Patel, Akshay and Agarwal, Yuvraj},
title = {Transfer learning for performance modeling of configurable systems: an exploratory analysis},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {497–508},
numpages = {12},
keywords = {transfer learning, Performance analysis},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {T-wise sampling, Family model, Model learning, Software product lines}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@inproceedings{10.1007/978-3-030-41418-4_17,
author = {Chen, Yuntianyi and Gu, Yongfeng and He, Lulu and Xuan, Jifeng},
title = {Regression Models for Performance Ranking of Configurable Systems: A Comparative Study},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_17},
doi = {10.1007/978-3-030-41418-4_17},
abstract = {Finding the best configurations for a highly configurable system is challenging. Existing studies learned regression models to predict the performance of potential configurations. Such learning suffers from the low accuracy and the high effort of examining the actual performance for data labeling. A recent approach uses an iterative strategy to sample a small number of configurations from the training pool to reduce the number of sampled ones. In this paper, we conducted a comparative study on the rank-based approach of configurable systems with four regression methods. These methods are compared on 21 evaluation scenarios of 16 real-world configurable systems. We designed three research questions to check the impacts of different methods on the rank-based approach. We find out that the decision tree method of Classification And Regression Tree (CART) and the ensemble learning method of Gradient Boosted Regression Trees (GBRT) can achieve better ranks among four regression methods under evaluation; the sampling strategy in the rank-based approach is useful to save the cost of sampling configurations; the measurement, i.e., rank difference correlates with the relative error in several evaluation scenarios.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {243–258},
numpages = {16},
keywords = {Software configurations, Sampling, Performance prediction, Regression methods},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {software product lines, searching configuration spaces, finding optimal configurations},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {performance, fourier transform, feature interactions, boolean functions},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {regularization, configurable systems, adversarial learning, Software performance prediction},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1016/j.jss.2015.05.006,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
title = {Feature extraction approaches from natural language requirements for reuse in software product lines},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.05.006},
doi = {10.1016/j.jss.2015.05.006},
abstract = {Hybrid NLP approaches were more common for extracting textual requirements.There is a mixture of automated and semi-automated approaches from IR and data mining.Support tools were not made available to the public.Not all studies use software metrics in conjunction with experiments and case studies.Reconfirm practitioners guidelines' absence from selected studies (Alves et al., 2010). Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
journal = {J. Syst. Softw.},
month = aug,
pages = {132–149},
numpages = {18},
keywords = {Systematic literature review, Software product lines, Requirements reuse, Natural language requirements, Feature extractions}
}

@inproceedings{10.1145/3030207.3030216,
author = {Valov, Pavel and Petkovich, Jean-Christophe and Guo, Jianmei and Fischmeister, Sebastian and Czarnecki, Krzysztof},
title = {Transferring Performance Prediction Models Across Different Hardware Platforms},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030216},
doi = {10.1145/3030207.3030216},
abstract = {Many software systems provide configuration options relevant to users, which are often called features. Features influence functional properties of software systems as well as non-functional ones, such as performance and memory consumption. Researchers have successfully demonstrated the correlation between feature selection and performance. However, the generality of these performance models across different hardware platforms has not yet been evaluated.We propose a technique for enhancing generality of performance models across different hardware environments using linear transformation. Empirical studies on three real-world software systems show that our approach is computationally efficient and can achieve high accuracy (less than 10% mean relative error) when predicting system performance across 23 different hardware platforms. Moreover, we investigate why the approach works by comparing performance distributions of systems and structure of performance models across different platforms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {39–50},
numpages = {12},
keywords = {regression trees, performance modelling, model transfer, linear transformation},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3205651.3208267,
author = {Xuan, Jifeng and Gu, Yongfeng and Ren, Zhilei and Jia, Xiangyang and Fan, Qingna},
title = {Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208267},
doi = {10.1145/3205651.3208267},
abstract = {A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.In this paper, we propose genetic configuration sampling, a new method of learning a sampling strategy for configuration-related faults. Genetic configuration sampling encodes a sequence of sampling actions as a chromosome in the genetic algorithm. Given a set of known configuration-related faults, genetic configuration sampling evolves the sequence of sampling actions and applies the learnt sequence to new configuration data. A pilot study on three highly-configurable systems shows that genetic configuration sampling performs well among nine sampling strategies in comparison.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1624–1631},
numpages = {8},
keywords = {software configurations, highly-configurable systems, genetic improvement, fault detection, configuration sampling},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1109/ASE.2013.6693089,
author = {Guo, Jianmei and Czarnecki, Krzysztof and Apely, Sven and Siegmundy, Norbert and Wasowski, Andrzej},
title = {Variability-aware performance prediction: a statistical learning approach},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693089},
doi = {10.1109/ASE.2013.6693089},
abstract = {Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {301–311},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1109/ASE.2015.15,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof},
title = {Performance prediction of configurable software systems by fourier learning},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.15},
doi = {10.1109/ASE.2015.15},
abstract = {Understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant. Given a software system with n optional features, measuring all its 2n possible configurations to determine their performances is usually infeasible. Thus, various techniques have been proposed to predict software performances based on a small sample of measured configurations. We propose a novel algorithm based on Fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user, while using minimum number of samples up to a constant factor. Empirical results on the case studies constructed from real-world configurable systems demonstrate the effectiveness of our algorithm.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {365–373},
numpages = {9},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1007/s11554-014-0483-1,
author = {Concha, David and Cabido, Ra\'{u}l and Pantrigo, Juan Jos\'{e} and Montemayor, Antonio S.},
title = {Performance evaluation of a 3D multi-view-based particle filter for visual object tracking using GPUs and multicore CPUs},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-014-0483-1},
doi = {10.1007/s11554-014-0483-1},
abstract = {This paper presents a deep and extensive performance analysis of the particle filter (PF) algorithm for a very compute intensive 3D multi-view visual tracking problem. We compare different implementations and parameter settings of the PF algorithm in a CPU platform taking advantage of the multithreading capabilities of the modern processors and a graphics processing unit (GPU) platform using NVIDIA CUDA computing environment as developing framework. We extend our experimental study to each individual stage of the PF algorithm, and evaluate the quality versus performance trade-off among different ways to design these stages. We have observed that the GPU platform performs better than the multithreaded CPU platform when handling a large number of particles, but we also demonstrate that hybrid CPU/GPU implementations can run almost as fast as only GPU solutions.},
journal = {J. Real-Time Image Process.},
month = aug,
pages = {309–327},
numpages = {19},
keywords = {Performance evaluation, Particle filtering, Multi-view, GPU computing, 3D visual tracking}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Challenges, Multiple-case study, Software product lines, Variability management}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Verification, Software product lines, Reliability analysis, Product-line analysis, Model checking}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@inproceedings{10.5555/1129601.1129704,
author = {Li, Xin and Le, Jiayong and Pileggi, L. T. and Strojwas, A.},
title = {Projection-based performance modeling for inter/intra-die variations},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Large-scale process fluctuations in nano-scale IC technologies suggest applying high-order (e.g., quadratic) response surface models to capture the circuit performance variations. Fitting such models requires significantly more simulation samples and solving much larger linear equations. In this paper, we propose a novel projection-based extraction approach, PROBE, to efficiently create quadratic response surface models and capture both inter-die and intra-die variations with affordable computation cost. PROBE applies a novel projection scheme to reduce the response surface modeling cost (i.e., both the required number of samples and the linear equation size) and make the modeling problem tractable even for large problem sizes. In addition, a new implicit power iteration algorithm is developed to find the optimal projection space and solve for the unknown model coefficients. Several circuit examples from both digital and analog circuit modeling applications demonstrate that PROBE can generate accurate response surface models while achieving up to 12/spl times/ speedup compared with the traditional methods.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {721–727},
numpages = {7},
location = {San Jose, CA},
series = {ICCAD '05}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@article{10.1016/j.specom.2009.06.002,
author = {Chen, Jianfeng and Phua, Koksoon and Shue, Louis and Sun, Hanwu},
title = {Performance evaluation of adaptive dual microphone systems},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {12},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2009.06.002},
doi = {10.1016/j.specom.2009.06.002},
abstract = {In this paper, the performance of the adaptive noise cancellation method is evaluated on several possible dual microphone system (DMS) configurations. Two groups of DMS are taken into consideration with one consisting of two omnidirectional microphones and another involving directional microphones. The properties of these methods are theoretically analyzed under incoherent, coherent and diffuse noise respectively. To further investigate their achievable noise reduction performance in real situations, a series of experiments in simulated and real office environments are carried out. Some recommendations are given at the end for designing and choosing the suitable methods in real applications.},
journal = {Speech Commun.},
month = dec,
pages = {1180–1193},
numpages = {14},
keywords = {Noise reduction, Microphone array, Adaptive beamforming}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10664-021-09994-0,
author = {Vitui, Arthur and Chen, Tse-Hsun (Peter)},
title = {MLASP: Machine learning assisted capacity planning: An industrial experience report},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09994-0},
doi = {10.1007/s10664-021-09994-0},
abstract = {In industrial environments it is critical to find out the capacity of a system and plan for a deployment layout that meets the production traffic demands. The system capacity is influenced by both the performance of the system’s constituting components and the physical environment setup. In a large system, the configuration parameters of individual components give the flexibility to developers and load test engineers to tune system performance without changing the source code. However, due to the large search space, estimating the capacity of the system given different configuration values is a challenging and costly process. In this paper, we propose an approach, called MLASP, that uses machine learning models to predict the system key performance indicators (i.e., KPIs), such as throughput, given a set of features made off configuration parameter values, including server cluster setup, to help engineers in capacity planning for production environments. Under the same load, we evaluate MLASP on two large-scale mission-critical enterprise systems developed by Ericsson and on one open-source system. We find that: 1) MLASP can predict the system throughput with a very high accuracy. The difference between the predicted and the actual throughput is less than 1%; and 2) By using only a small subset of the training data (e.g., 3% of the entire data for the open-source system), MLASP can still predict the throughput accurately. We also document our experience of successfully integrating the approach into an industrial setting. In summary, this paper highlights the benefits and potential of using machine learning models to assist load test engineers in capacity planning.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {27},
keywords = {Deep learning, Machine learning, Performance testing, Capacity testing, Load testing}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {type checking, product-line analysis, fuji, feature-oriented programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1016/j.jss.2013.01.038,
author = {Thurimella, Anil Kumar and Br\"{u}Gge, Bernd},
title = {A mixed-method approach for the empirical evaluation of the issue-based variability modeling},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.01.038},
doi = {10.1016/j.jss.2013.01.038},
abstract = {Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1831–1849},
numpages = {19},
keywords = {Variability, Software product lines, Requirements engineering, Rationale management, Mixed-methods, Empirical software engineering}
}

@inproceedings{10.1145/3358960.3379127,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Transferring Pareto Frontiers across Heterogeneous Hardware Environments},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379127},
doi = {10.1145/3358960.3379127},
abstract = {Software systems provide user-relevant configuration options called features. Features affect functional and non-functional system properties, whereas selections of features represent system configurations. A subset of configuration space forms a Pareto frontier of optimal configurations in terms of multiple properties, from which a user can choose the best configuration for a particular scenario. However, when a well-studied system is redeployed on a different hardware, information about property value and the Pareto frontier might not apply. We investigate whether it is possible to transfer this information across heterogeneous hardware environments. We propose a methodology for approximating and transferring Pareto frontiers of configurable systems across different hardware environments. We approximate a Pareto frontier by training an individual predictor model for each system property, and by aggregating predictions of each property into an approximated frontier. We transfer the approximated frontier across hardware by training a transfer model for each property, by applying it to a respective predictor, and by combining transferred properties into a frontier. We evaluate our approach by modeling Pareto frontiers as binary classifiers that separate all system configurations into optimal and non-optimal ones. Thus we can assess quality of approximated and transferred frontiers using common statistical measures like sensitivity and specificity. We test our approach using five real-world software systems from the compression domain, while paying special attention to their performance. Evaluation results demonstrate that accuracy of approximated frontiers depends linearly on predictors' training sample sizes, whereas transferring introduces only minor additional error to a frontier even for small training sizes.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {12–23},
numpages = {12},
keywords = {regression trees, performance prediction, linear regression, configurable software, Pareto frontier transferring, Pareto frontier},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/2254756.2254791,
author = {Yoo, Wucherl and Larson, Kevin and Baugh, Lee and Kim, Sangkyum and Campbell, Roy H.},
title = {ADP: automated diagnosis of performance pathologies using hardware events},
year = {2012},
isbn = {9781450310970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254756.2254791},
doi = {10.1145/2254756.2254791},
abstract = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge.},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {283–294},
numpages = {12},
keywords = {resource bottleneck, performance analysis, micro-benchmark, machine learning, hardware event, fingerprint},
location = {London, England, UK},
series = {SIGMETRICS '12}
}

@inproceedings{10.1145/3368089.3409744,
author = {Baranov, Eduard and Legay, Axel and Meel, Kuldeep S.},
title = {Baital: an adaptive weighted sampling approach for improved t-wise coverage},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409744},
doi = {10.1145/3368089.3409744},
abstract = {The rise of highly configurable complex software and its widespread usage requires design of efficient testing methodology. t-wise coverage is a leading metric to measure the quality of the testing suite and the underlying test generation engine. While uniform sampling-based test generation is widely believed to be the state of the art approach to achieve t-wise coverage in presence of constraints on the set of configurations, such a scheme often fails to achieve high t-wise coverage in presence of complex constraints. In this work, we propose a novel approach Baital, based on adaptive weighted sampling using literal weighted functions, to generate test sets with high t-wise coverage. We demonstrate that our approach reaches significantly higher t-wise coverage than uniform sampling. The novel usage of literal weighted sampling leaves open several interesting directions, empirical as well as theoretical, for future research.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1114–1126},
numpages = {13},
keywords = {t-wise coverage, Weighted sampling, Configurable software},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1007/s00165-021-00563-2,
author = {Cordy, Maxime and Lazreg, Sami and Papadakis, Mike and Legay, Axel},
title = {Statistical model checking for variability-intensive systems: applications to bug detection and minimization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00563-2},
doi = {10.1007/s00165-021-00563-2},
abstract = {We propose a new Statistical Model Checking (SMC) method to
identify bugs in variability-intensive systems (VIS). The
state-space of such systems is exponential in the number of
variants, which makes the verification problem harder than for
classical systems. To reduce verification time, we propose to
combine SMC with featured transition systems (FTS)—a
model that represents jointly the state spaces of all variants. Our
new methods allow the sampling of executions from one or more
(potentially all) variants. We investigate their utility in two
complementary use cases. The first case considers the problem of
finding all variants that violate a given property expressed in
Linear-Time Logic (LTL) within a given simulation budget. To achieve
this, we perform random walks in the featured transition system
seeking accepting lassos. We show that our method allows us to find
bugs much faster (up to 16 times according to our experiments) than
exhaustive methods. As any simulation-based approach, however, the
risk of Type-1 error exists. We provide a lower bound and an upper
bound for the number of simulations to perform to achieve the
desired level of confidence. Our empirical study involving 59
properties over three case studies reveals that our method manages
to discover all variants violating 41 of the properties.
This indicates that SMC can act as a coarse-grained
analysis method to quickly identify the set of buggy variants.
The second case complements the first one. In case the
coarse-grained analysis reveals that no variant can guarantee to
satisfy an intended property in all their executions, one should
identify the variant that minimizes the probability of violating
this property. Thus, we propose a fine-grained SMC method that
quickly identifies promising variants and accurately estimates their
violation probability. We evaluate different selection strategies
and reveal that a genetic algorithm combined with elitist selection
yields the best results.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1147–1172},
numpages = {26},
keywords = {Sampling, Simulation, Verification, Variability, Statistical model checking}
}

@article{10.5555/1361151.1387978,
author = {Shankaran, Nishanth and Roy, Nilabja and Schmidt, Douglas C. and Koutsoukos, Xenofon D. and Chen, Yingming and Lu, Chenyang},
title = {Design and performance evaluation of an adaptive resource management framework for distributed real-time and embedded systems},
year = {2008},
issue_date = {January 2008},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2008},
issn = {1687-3955},
abstract = {Achieving end-to-end quality of service (QoS) in distributed real-time embedded (DRE) systems require QoS support and enforcement from their underlying operating platforms that integrates many real-time capabilities, such as QoS-enabled network protocols, real-time operating system scheduling mechanisms and policies, and real-time middleware services. As standards-based quality of service (QoS) enabled component middleware automates integration and configuration activities, it is increasingly being used as a platform for developing open DRE systems that execute in environments where operational conditions, input workload, and resource availability cannot be characterized accurately a priori. Although QoS-enabled component middleware offers many desirable features, however, it historically lacked the ability to allocate resources efficiently and enable the system to adapt to fluctuations in input workload, resource availability, and operating conditions. This paper presents three contributions to research on adaptive resource management for component-based open DRE systems. First, we describe the structure and functionality of the resource allocation and control engine (RACE), which is an open-source adaptive resource management framework built atop standards-based QoS-enabled component middleware. Second, we demonstrate and evaluate the effectiveness of RACE in the context of a representative open DRE system: NASA's magnetospheric multiscale mission system. Third, we present an empirical evaluation of RACE's scalability as the number of nodes and applications in a DRE system grows. Our results show that RACE is a scalable adaptive resource management framework and yields a predictable and high-performance system, even in the face of changing operational conditions and input workload.},
journal = {EURASIP J. Embedded Syst.},
month = jan,
articleno = {9},
numpages = {20}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@inproceedings{10.3115/100964.1138530,
author = {Zue, Victor},
title = {Prosody, performance evaluation, databases, and ISAT},
year = {1989},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/100964.1138530},
doi = {10.3115/100964.1138530},
abstract = {This session began with a one-hour invited tutorial on prosody given by Dr. Janet Pierrehumbert of AT&amp;T Bell Laboratories, which was followed by several short reports on database collection and performance evaluation activities dealing with various aspects of the spoken speech system development effort. Issues and discussions on methods of formal evaluation were particularly relevant for natural language processing, where formal evaluation is in many respects an elusive goal.},
booktitle = {Proceedings of the Workshop on Speech and Natural Language},
pages = {5–36},
numpages = {32},
location = {Philadelphia, Pennsylvania},
series = {HLT '89}
}

@inproceedings{10.1145/3417990.3419488,
author = {Schiedermeier, Maximilian},
title = {A concern-oriented software engineering methodology for micro-service architectures},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3419488},
doi = {10.1145/3417990.3419488},
abstract = {Component-Based Systems (CBS) allow for the construction of modular, highly scalable software. Decomposing a system into individually maintainable and deployable components enables a targeted replication of performance bottlenecks, and promotes code modularity. Over the last years, the Micro-Service Architecture (MSA) style has become a popular approach to maximize the benefits of CBS. However, MSA introduces new challenges, by imposing a conceptual and technological stack on adherent projects, which require new critical design choices. Throughout my PhD I want to investigate to which extent a systematic reuse of MSA solutions of various granularity can streamline MSA application development by guiding design decisions.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {28},
numpages = {5},
keywords = {representational state transfer, model-driven engineering, micro-service architectures, concern-oriented reuse},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1007/s10515-017-0225-2,
author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
title = {Faster discovery of faster system configurations with spectral learning},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0225-2},
doi = {10.1007/s10515-017-0225-2},
abstract = {Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors--less than 10% prediction error, with a standard deviation of less than 2%. When compared to the state of the art, WHAT (a) requires 2---10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance.},
journal = {Automated Software Engg.},
month = jun,
pages = {247–277},
numpages = {31},
keywords = {Spectral learning, Search-based software engineering, Sampling, Performance prediction, Decision trees}
}

@inproceedings{10.1145/3106237.3106238,
author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
title = {Using bad learners to find good configurations},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106238},
doi = {10.1145/3106237.3106238},
abstract = {Finding the optimally performing configuration of a software system for a given setting is often challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, building an accurate performance model can be very expensive (and is often infeasible in practice). The central insight of this paper is that exact performance values (e.g., the response time of a software system) are not required to rank configurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the difference between actual and predicted performance) can still be used rank configurations and hence find the optimal configuration. This novel rank-based approach allows us to significantly reduce the cost (in terms of number of measurements of sample configuration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 software systems and demonstrate that our approach is beneficial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Sampling, SBSE, Rank-based method, Performance Prediction},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1007/978-3-030-69244-5_35,
author = {HoseinyFarahabady, Mohammad Reza and Taheri, Javid and Zomaya, Albert Y. and Tari, Zahir},
title = {Graceful Performance Degradation in Apache Storm},
year = {2021},
isbn = {978-3-030-69243-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69244-5_35},
doi = {10.1007/978-3-030-69244-5_35},
abstract = {The concept of stream data processing is becoming challenging in most business sectors where try to improve their operational efficiency by deriving valuable information from unstructured, yet, contentiously generated high volume raw data in an expected time spans. A modern streamlined data processing platform is required to execute analytical pipelines over a continues flow of data-items that might arrive in a high rate. In most cases, the platform is also expected to dynamically adapt to dynamic characteristics of the incoming traffic rates and the ever-changing condition of underlying computational resources while fulfill the tight latency constraints imposed by the end-users. Apache Storm has emerged as an important open source technology for performing stream processing with very tight latency constraints over a cluster of computing nodes. To increase the overall resource utilization, however, the service provider might be tempted to use a consolidation strategy to pack as many applications as possible in a (cloud-centric) cluster with limited number of working nodes. However, collocated applications can negatively compete with each other, for obtaining the resource capacity in a shared platform that, in turn, the result may lead to a severe performance degradation among all running applications.The main objective of this work is to develop an elastic solution in a modern stream processing ecosystem, for addressing the shared resource contention problem among collocated applications. We propose a mechanism, based on design principles of Model Predictive Control theory, for coping with the extreme conditions in which the collocated analytical applications have different quality of service (QoS) levels while the shared-resource interference is considered as a key performance limiting parameter. Experimental results confirm that the proposed controller can successfully enhance the p-99 latency of high priority applications by 67%, compared to the default round robin resource allocation strategy in Storm, during the high traffic load, while maintaining the requested quality of service levels.},
booktitle = {Parallel and Distributed Computing, Applications and Technologies: 21st International Conference, PDCAT 2020, Shenzhen, China, December 28–30, 2020, Proceedings},
pages = {389–400},
numpages = {12},
keywords = {Quality of Services (QoS), Performance modeling of computer system, Elastic resource controller, Apache storm streaming processing platform},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3477244.3477985,
author = {van der Sanden, Bram and Li, Yonghui and van den Aker, Joris and Akesson, Benny and Bijlsma, Tjerk and Hendriks, Martijn and Triantafyllidis, Kostas and Verriet, Jacques and Voeten, Jeroen and Basten, Twan},
title = {Model-driven system-performance engineering for cyber-physical systems},
year = {2021},
isbn = {9781450387125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477244.3477985},
doi = {10.1145/3477244.3477985},
abstract = {System-Performance Engineering (SysPE) encompasses modeling formalisms, methods, techniques, and industrial practices to design systems for performance, where performance is taken integrally into account during the whole system life cycle. Industrial SysPE state of practice is generally model-based. Due to the rapidly increasing complexity of systems, there is a need to develop and establish model-driven methods and techniques. To structure the field of SysPE, we identify (1) industrial challenges motivating the importance of SysPE, (2) scientific challenges that need to be addressed to establish model-driven SysPE, (3) important focus areas for SysPE and (4) best practices. We conducted a survey to collect feedback on our views. The responses were used to update and validate the identified challenges, focus areas, and best practices. The final result is presented in this paper. Interesting observations are that industry sees a need for better design-space exploration support, more than for additional performance modeling and analysis techniques. Also tools and integral methods for SysPE need attention. From the identified focus areas, scheduling and supervisory control is seen as lacking established best practices.},
booktitle = {Proceedings of the 2021 International Conference on Embedded Software},
pages = {11–22},
numpages = {12},
keywords = {system-performance engineering, model-driven design, CPS},
location = {Virtual Event},
series = {EMSOFT '21}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2884781.2884794,
author = {Chen, Bihuan and Liu, Yang and Le, Wei},
title = {Generating performance distributions via probabilistic symbolic execution},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884794},
doi = {10.1145/2884781.2884794},
abstract = {Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are very important software engineering tasks. There have been model-based and program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are typically coarse-grained and could be imprecise. Program analysis-based approaches collect program profiles to identify performance bottlenecks, which often fail to capture the overall program performance. In this paper, we propose a performance analysis framework PerfPlotter. It takes the program source code and usage profile as inputs and generates a performance distribution that captures the input probability distribution over execution times for the program. It heuristically explores high-probability and low-probability paths through probabilistic symbolic execution. Once a path is explored, it generates and runs a set of test inputs to model the performance of the path. Finally, it constructs the performance distribution for the program. We have implemented PerfPlotter based on the Symbolic PathFinder infrastructure, and experimentally demonstrated that PerfPlotter could accurately capture the best-case, worst-case and distribution of program execution times. We also show that performance distributions can be applied to various important tasks such as performance understanding, bug validation, and algorithm selection.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {49–60},
numpages = {12},
keywords = {symbolic execution, performance analysis},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2661136.2661143,
author = {Walkingshaw, Eric and K\"{a}stner, Christian and Erwig, Martin and Apel, Sven and Bodden, Eric},
title = {Variational Data Structures: Exploring Tradeoffs in Computing with Variability},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661143},
doi = {10.1145/2661136.2661143},
abstract = {Variation is everywhere, and in the construction and analysis of customizable software it is paramount. In this context, there arises a need for variational data structures for efficiently representing and computing with related variants of an underlying data type. So far, variational data structures have been explored and developed ad hoc. This paper is a first attempt and a call to action for systematic and foundational research in this area. Research on variational data structures will benefit not only customizable software, but many other application domains that must cope with variability. In this paper, we show how support for variation can be understood as a general and orthogonal property of data types, data structures, and algorithms. We begin a systematic exploration of basic variational data structures, exploring the tradeoffs among different implementations. Finally, we retrospectively analyze the design decisions in our own previous work where we have independently encountered problems requiring variational data structures.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {213–226},
numpages = {14},
keywords = {variation, variability-aware analyses, software product lines, data structures, configurable software},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@article{10.1007/s10515-015-0188-0,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kotr\u{a}\'{z}, Jaroslav and Marek, Luk\'{a}\v{s} and Troj\'{a}nek, Tom\'{a}\v{s} and T\'{z}Ma, Petr},
title = {Unit testing performance with Stochastic Performance Logic},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0188-0},
doi = {10.1007/s10515-015-0188-0},
abstract = {Unit testing is an attractive quality management tool in the software development process, however, practical obstacles make it difficult to use unit tests for performance testing. We present Stochastic Performance Logic, a formalism for expressing performance requirements, together with interpretations that facilitate performance evaluation in the unit test context. The formalism and the interpretations are implemented in a performance testing framework and evaluated in multiple experiments, demonstrating the ability to identify performance differences in realistic unit test scenarios.},
journal = {Automated Software Engg.},
month = mar,
pages = {139–187},
numpages = {49},
keywords = {Unit testing, Performance evaluation, Java}
}

@inproceedings{10.1145/3106237.3119880,
author = {Tang, Chong},
title = {System performance optimization via design and configuration space exploration},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3119880},
doi = {10.1145/3106237.3119880},
abstract = {The runtime performance of a software system often depends on a large number of static parameters, which usually interact in complex ways to carry out system functionality and influence system performance. It's hard to understand such configuration spaces and find good combinations of parameter values to gain available levels of performance. Engineers in practice often just accept the default settings, leading such systems to significantly underperform relative to their potential. This problem, in turn, has impacts on cost, revenue, customer satisfaction, business reputation, and mission effectiveness. To improve the overall performance of the end-to-end systems, we propose to systematically explore (i) how to design new systems towards good performance through design space synthesis and evaluation, and (ii) how to auto-configure an existing system to obtain better performance through heuristic configuration space search. In addition, this research further studies execution traces of a system to predict runtime performance under new configurations.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1046–1049},
numpages = {4},
keywords = {Performance Prediction, Performance Optimization, Design Space, Configuration Space},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1007/978-3-030-45234-6_15,
author = {Cordy, Maxime and Papadakis, Mike and Legay, Axel},
title = {Statistical Model Checking for Variability-Intensive Systems},
year = {2020},
isbn = {978-3-030-45233-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45234-6_15},
doi = {10.1007/978-3-030-45234-6_15},
abstract = {We propose a new Statistical Model Checking (SMC) method to discover bugs in variability-intensive systems (VIS). The state-space of such systems is exponential in the number of variants, which makes the verification problem harder than for classical systems. To reduce verification time, we sample executions from a featured transition system – a model that represents jointly the state spaces of all variants. The combination of this compact representation and the inherent efficiency of SMC allows us to find bugs much faster (up to 16 times according to our experiments) than other methods. As any simulation-based approach, however, the risk of Type-1 error exists. We provide a lower bound and an upper bound for the number of simulations to perform to achieve the desired level of confidence. Our empirical study involving 59 properties over three case studies reveals that our method manages to discover all variants violating 41 of the properties. This indicates that SMC can act as a low-cost-high-reward method for verifying VIS.},
booktitle = {Fundamental Approaches to Software Engineering: 23rd International Conference, FASE 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {294–314},
numpages = {21},
location = {Dublin, Ireland}
}

@article{10.1007/s00530-019-00606-y,
author = {Morillo, Pedro and Ordu\~{n}a, Juan M. and Casas, Sergio and Fern\'{a}ndez, Marcos},
title = {A comparison study of AR applications versus pseudo-holographic systems as virtual exhibitors for luxury watch retail stores},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-019-00606-y},
doi = {10.1007/s00530-019-00606-y},
abstract = {The market of luxury watches has been continuously growing across the world, regardless of economic crisis. However, consumers can purchase online, on the web pages of the luxury watchmakers, the same product they can acquire in physical retail stores, producing a significant reduction in the overall sales of the latter ones. To reduce this trend, retail stores should increase their added-value services, one of which could be the use of virtual exhibitors in the shop. In this paper, we have developed two multimedia solutions (an Augmented Reality application and a pseudo-holographic system) for the creation of virtual exhibitors, and we have carried out a comparative study (based on real users) to measure which system would produce the best impact on users when used in traditional luxury watch retail stores. Our primary hypothesis was that there would be significant differences between the use of the mobile AR application and the use of the pseudo-holographic system. Our secondary hypothesis was that user preference for the mobile AR application would be higher than for the pseudo-holographic system.},
journal = {Multimedia Syst.},
month = aug,
pages = {307–321},
numpages = {15},
keywords = {Pseudo-holographic systems, Performance evaluation, AR applications}
}

@inproceedings{10.1145/3053600.3053636,
author = {Ferme, Vincenzo and Pautasso, Cesare},
title = {Towards Holistic Continuous Software Performance Assessment},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053636},
doi = {10.1145/3053600.3053636},
abstract = {In agile, fast and continuous development lifecycles, software performance analysis is fundamental to confidently release continuously improved software versions. Researchers and industry practitioners have identified the importance of integrating performance testing in agile development processes in a timely and efficient way. However, existing techniques are fragmented and not integrated taking into account the heterogeneous skills of the users developing polyglot distributed software, and their need to automate performance practices as they are integrated in the whole lifecycle without breaking its intrinsic velocity. In this paper we present our vision for holistic continuous software performance assessment, which is being implemented in the BenchFlow tool. BenchFlow enables performance testing and analysis practices to be pervasively integrated in continuous development lifecycle activities. Users can specify performance activities (e.g., standard performance tests) by relying on an expressive Domain Specific Language for objective-driven performance analysis. Collected performance knowledge can be thus reused to speed up performance activities throughout the entire process.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {159–164},
numpages = {6},
keywords = {performance test, performance analysis, continuous software performance assessment, continuous integration},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/2517208.2517209,
author = {Siegmund, Norbert and von Rhein, Alexander and Apel, Sven},
title = {Family-based performance measurement},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517209},
doi = {10.1145/2517208.2517209},
abstract = {Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {95–104},
numpages = {10},
keywords = {performance prediction, featurehouse, family-based analysis},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/3194554.3194576,
author = {Alsafrjalani, Mohamad Hammam and Adegbija, Tosiron},
title = {TaSaT: Thermal-Aware Scheduling and Tuning Algorithm for Heterogeneous and Configurable Embedded Systems},
year = {2018},
isbn = {9781450357241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194554.3194576},
doi = {10.1145/3194554.3194576},
abstract = {Heterogeneous and configurable systems (HaCS) have been widely used to meet stringent runtime performance and energy constraints in embedded systems. However, no prior work has addressed the emerging runtime thermal constraints in these systems. To leverage HaCS' capabilities to meet thermal constraints, in addition to performance and energy constraints, we propose TaSaT, a Thermal-aware Scheduling and Tuning algorithm for HaCS. TaSaT reduces HaCS temperature while meeting performance and energy constraints during runtime, without a priori knowledge of applications.},
booktitle = {Proceedings of the 2018 Great Lakes Symposium on VLSI},
pages = {75–80},
numpages = {6},
keywords = {configurable caches, dynamic optimization, heterogeneous and configurable hardware, thermal-aware scheduling, tuning},
location = {Chicago, IL, USA},
series = {GLSVLSI '18}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.cie.2014.09.024,
author = {Ba\c{s}ak, \"{O}zkan and Albayrak, Y. Esra},
title = {Petri net based decision system modeling in real-time scheduling and control of flexible automotive manufacturing systems},
year = {2015},
issue_date = {August 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.09.024},
doi = {10.1016/j.cie.2014.09.024},
abstract = {High level Petri nets and object-oriented design approaches have been presented.PN based method is adapted to the flexible automotive manufacturing system (FAMS).PN based method is illustrated by modeling a real-time scheduling and control for FAMS.The method formulates the dynamic behavior of the system.Artifex PN software tool is employed to model the system. This paper presents the design and the implementation of a Petri net (PN) model for the control of a flexible manufacturing system (FMS). A flexible automotive manufacturing system used in this environment enables quick cell configuration, and the efficient operation of cells. In this paper, we attempt to propose a flexible automotive manufacturing approach for modeling and analysis of shop floor scheduling problem of FMSs using high-level PNs. Since PNs have emerged as the principal performance modeling tools for FMS, this paper provides an object-oriented Petri nets (OOPNs) approach to performance modeling and to implement efficient production control. In this study, we modeled the system as a timed marked graph (TMG), a well-known subclass of PNs, and we showed that the problem of performance evaluation can be reduced to a simple linear programming (LP) problem with m-n+1 variables and n constraints, where m and n represent the number of places and transitions in the marked graph, respectively. The presented PN based method is illustrated by modeling a real-time scheduling and control for flexible automotive manufacturing system (FAMS) in Valeo Turkey.},
journal = {Comput. Ind. Eng.},
month = aug,
pages = {116–126},
numpages = {11},
keywords = {Timed marked graph (TMG), Time scheduling, Petri net, Linear programming (LP), Flexible automotive manufacturing systems, Decision system modeling}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1145/2020976.2069288,
author = {Lilja, David J. and Mirandola, Raffaela and Sachs, Kai},
title = {Paper Abstracts of the 2nd International Conferernce on Performance Engineering (ICPE 2011)},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2069288},
doi = {10.1145/2020976.2069288},
abstract = {Foreword This issue of SEN contains the abstracts of the papers, which were presented on the Second Joint WOSP/SIPEW International Conference (ICPE 2011), held in Karlsruhe, Germany, March 14-16, 2011, now established as a regular event known as ACM/SPEC International Conference on Performance Engineering (ICPE). The primary goal of this conference series is to bridge the gap between theory and practice in the field of computer systems performance engineering by providing a forum for sharing ideas and experiences between industry and academia. This years conference brought together researchers and industry practitioners to share and present their experiences, discuss challenges, and report on both state-of-the-art research and work-in-progress on performance engineering of software and systems, including performance measurement, modeling, benchmark design, and run-time performance management. The ICPE gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of computer systems performance engineering. The call for papers attracted 63 research and 24 industrial paper submissions from Europe, Asia, Africa, and North America. The program committees accepted 19 full research papers and 7 short papers together with 13 industrial papers. These papers cover a variety of topics, including performance modeling and techniques and measurement and benchmarking strategies for adaptive systems, power management, virtualized environments, and large-scale and distributed systems. We are confident that you will find the abstracts stimulating and that they will provide you with many new ideas and insights. The full paper are available at the ACM Digital Library. David J. Lilja: Program Co-Chair - Research Track Raffaela Mirandola: Program Co-Chair - Research Track Kai Sachs: Program Co-Chair - Industrial Track},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {36–53},
numpages = {18}
}

@article{10.1007/s00500-019-03795-w,
author = {Ramgouda, P. and Chandraprakash, V.},
title = {Constraints handling in combinatorial interaction testing using multi-objective crow search and fruitfly optimization},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {8},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03795-w},
doi = {10.1007/s00500-019-03795-w},
abstract = {Combinatorial testing strategies are the recent interest of the researchers because of their wide variety of applications. The combinatorial testing strategy posses a great deal of minimizing the count of the input parameters of a system such that a small set of parameters is obtained depending on their interaction. Practically, the input models of the software system are subjected to the constraints mainly in highly configurable systems. There exist a number of issues while integrating the constraint in the testing strategy that is overcome using the proposed method. The proposed method aims at developing the combinatorial interaction test suites in the presence of constraints. The proposed strategy is multi-objective crow search and fruitfly optimization that is developed by the integration of the crow search algorithm and the chaotic fruitfly optimization algorithm. The proposed algorithm offers an optimal selection of the test suites at the better convergence. The experimentation based on the constraints and the analysis are carried out in terms of average size and average time with their values as 10 and 30 s, respectively.},
journal = {Soft Comput.},
month = apr,
pages = {2713–2726},
numpages = {14},
keywords = {Multi-objectives, Crow search algorithm (CSA), Constraints, Combinatorial interaction testing, Chaotic FOA}
}

@inproceedings{10.1145/2528265.2528267,
author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Garvin, Brady},
title = {Exploring feature interactions in the wild: the new feature-interaction challenge},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528267},
doi = {10.1145/2528265.2528267},
abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature-oriented software development, feature-interaction problem, feature modularity, feature interactions},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@inproceedings{10.1145/3358960.3379144,
author = {Musaafir, Ahmed and Uta, Alexandru and Dreuning, Henk and Varbanescu, Ana-Lucia},
title = {A Sampling-Based Tool for Scaling Graph Datasets},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379144},
doi = {10.1145/3358960.3379144},
abstract = {Graph processing has become a topic of interest in many domains. However, we still observe a lack of representative datasets for in-depth performance and scalability analysis. Neither data collections, nor graph generators provide enough diversity and control for thorough analysis. To address this problem, we proposea heuristic method for scaling existing graphs. Our approach, based onsampling andinterconnection, can provide a scaled "version" of a given graph. Moreover, we provide analytical models to predict the topological properties of the scaled graphs (such as the diameter, degree distribution, density, or the clustering coefficient), and further enable the user to tweak these properties. Property control is achieved through a portfolio of graph interconnection methods (e.g., star, ring, chain, fully connected) applied for combining the graph samples. We further implement our method as an open-source tool which can be used to quickly provide families of datasets for in-depth benchmarking of graph processing algorithms. Our empirical evaluation demonstrates our tool provides scaled graphs of a wide range of sizes, whose properties match well with model predictions and/or user requirements. Finally, we also illustrate, through a case-study, how scaled graphs can be used for in-depth performance analysis of graph processing algorithms.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {heuristic methods, graph scaling tool, graph sampling, graph datasets scaling},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.phycom.2021.101482,
author = {Serrano, Salvatore and Scarpa, Marco and Maali, Asmaa and Soulmani, Abdallah and Boumaaz, Najib},
title = {Random sampling for effective spectrum sensing in cognitive radio time slotted environment},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1874-4907},
url = {https://doi.org/10.1016/j.phycom.2021.101482},
doi = {10.1016/j.phycom.2021.101482},
journal = {Phys. Commun.},
month = dec,
numpages = {13},
keywords = {GSM, Neural network, Classification, Random sampling, Spectrum sensing, Cognitive radio}
}

@inproceedings{10.1145/2961111.2962602,
author = {Han, Xue and Yu, Tingting},
title = {An Empirical Study on Performance Bugs for Highly Configurable Software Systems},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962602},
doi = {10.1145/2961111.2962602},
abstract = {Modern computer systems are highly-configurable, complicating the testing and debugging process. The sheer size of the configuration space makes the quality of software even harder to achieve. Performance is one of the key aspects of non-functional qualities, where performance bugs can cause significant performance degradation and lead to poor user experience. However, performance bugs are difficult to expose, primarily because detecting them requires specific inputs, as well as a specific execution environment (e.g., configurations). While researchers have developed techniques to analyze, quantify, detect, and fix performance bugs, we conjecture that many of these techniques may not be effective in highly-configurable systems. In this paper, we study the challenges that configurability creates for handling performance bugs. We study 113 real-world performance bugs, randomly sampled from three highly-configurable open-source projects: Apache, MySQL and Firefox. The findings of this study provide a set of lessons learned and guidance to aid practitioners and researchers to better handle performance bugs in highly-configurable software systems.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {23},
numpages = {10},
keywords = {Performance, Empirical Study, Configuration},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.5555/2820518.2820523,
author = {Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
title = {Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {13–23},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s11276-021-02742-8,
author = {Zhang, Runzhou and Ning, Lei and Li, Mengkun and Wang, Chengcai and Li, Wei and Wang, Yongjian},
title = {Feature extraction of trajectories for mobility modeling in 5G NB-IoT networks},
year = {2021},
issue_date = {Jul 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02742-8},
doi = {10.1007/s11276-021-02742-8},
abstract = {Industry applications that support massive mobile nodes are an important part of 5G Narrow Band Internet of Things (NB-IoT). The mobile node is abstracted as a terminal scene with many shapes and complex behaviors in the cellular internet of things, which makes the network need a better radio resource management strategy to meet more mobility, larger connection requests and higher access rate services. Therefore, the feature extraction of the node trajectories will greatly facilitate the development of optimal algorithms for radio resource management in the large mobile scene of 5G NB-IoT networks. This paper presents a feature mining of the real trajectories from the urban operating vehicles in the city of Shenzhen, China. Meanwhile, the generated trajectories by four common mobility models are also handled as a comparison. The self-similarity, hot-spots, long-tails and travel time are evaluated due to the widely recognized four features of human traces. Mining results show that the vehicles to serve the daily trip of human in the city always take a short travel and activate in several hot-spots with randomly roaming, while the vehicles to serve the goods are showing the opposite characteristics. Moreover, the trajectory generated by the models does not have the characteristics of short travel and roaming in multiple hot spots, nor does it have different movement trends in various time periods.},
journal = {Wirel. Netw.},
month = aug,
pages = {3781–3793},
numpages = {13},
keywords = {Trajectory mining, Vehicle mobility, Mobility model, 5G NB-IoT networks}
}

@inproceedings{10.1145/3238147.3238175,
author = {Bao, Liang and Liu, Xin and Xu, Ziheng and Fang, Baoyin},
title = {AutoConfig: automatic configuration tuning for distributed message systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238175},
doi = {10.1145/3238147.3238175},
abstract = {Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {29–40},
numpages = {12},
keywords = {automatic configuration tuning, comparison-based model, distributed message system, weighted Latin hypercube sampling},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1016/j.compbiomed.2019.103359,
author = {Koya, Aneesh M. and Deepthi, P.P.},
title = {Plug and play self-configurable IoT gateway node for telemonitoring of ECG},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {112},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103359},
doi = {10.1016/j.compbiomed.2019.103359},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {12},
keywords = {Wireless body area network (WBAN), Inter-pulse interval (IPI), Heterogeneous multi-processing (HMP), Energy efficiency, Electrocardiogram (ECG) monitoring, Deterministic binary block diagonal (DBBD) matrix, Compressed sensing (CS)}
}

@inproceedings{10.1145/3427921.3450255,
author = {Han, Xue and Yu, Tingting and Pradel, Michael},
title = {ConfProf: White-Box Performance Profiling of Configuration Options},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450255},
doi = {10.1145/3427921.3450255},
abstract = {Modern software systems are highly customizable through configuration options. The sheer size of the configuration space makes it challenging to understand the performance influence of individual configuration options and their interactions under a specific usage scenario. Software with poor performance may lead to low system throughput and long response time. This paper presents ConfProf, a white-box performance profiling technique with a focus on configuration options. ConfProf helps developers understand how configuration options and their interactions influence the performance of a software system. The approach combines dynamic program analysis, machine learning, and feedback-directed configuration sampling to profile the program execution and analyze the performance influence of configuration options. Compared to existing approaches, ConfProf uses a white-box approach combined with machine learning to rank performance-influencing configuration options from execution traces. We evaluate the approach with 13 scenarios of four real-world, highly-configurable software systems. The results show that ConfProf ranks performance-influencing configuration options with high accuracy and outperform a state of the art technique.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {1–8},
numpages = {8},
keywords = {performance profiling, software performance},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1109/SEAMS.2017.11,
author = {Jamshidi, Pooyan and Velez, Miguel and K\"{a}stner, Christian and Siegmund, Norbert and Kawthekar, Prasad},
title = {Transfer learning for improving model predictions in highly configurable software},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.11},
doi = {10.1109/SEAMS.2017.11},
abstract = {Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {31–41},
numpages = {11},
keywords = {highly configurable software, machine learning, model learning, model prediction, transfer learning},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2487336.2487350,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Run-time adaptation of mobile applications using genetic algorithms},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {Mobile applications run in environments where the context is continuously changing. Therefore, it is necessary to provide support for the run-time adaptation of these applications. This support is usually achieved by middleware platforms that offer a context-aware dynamic reconfiguration service. However, the main shortcoming of existing approaches is that both the list of possible configurations and the plans to adapt the application to a new configuration are usually specified at design-time. In this paper we present an approach that allows the automatic generation at run-time of application configurations and of reconfiguration plans. Moreover, the generated configurations are optimal regarding the provided functionality and, more importantly, without exceeding the available resources (e.g. battery). This is performed by: (1) having the information about the application variability available at runtime using feature models, and (2) using a genetic algorithm that allows generating an optimal configuration at runtime. We have specified a case study and evaluated our approach, and the results show that it is efficient enough as to be used on mobile devices without introducing an excessive overhead.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {73–82},
numpages = {10},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {machine learning, uncertainty},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@article{10.1186/s13640-020-00516-4,
author = {Bibi, Ruqia and Mehmood, Zahid and Yousaf, Rehan Mehmood and Tahir, Muhammad and Rehman, Amjad and Sardaraz, Muhammad and Rashid, Muhammad},
title = {BoVW model based on adaptive local and global visual words modeling and log-based relevance feedback for semantic retrieval of the images},
year = {2020},
issue_date = {Apr 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-5176},
url = {https://doi.org/10.1186/s13640-020-00516-4},
doi = {10.1186/s13640-020-00516-4},
abstract = {The core of a content-based image retrieval (CBIR) system is based on an effective understanding of the visual contents of images due to which a CBIR system can be termed as accurate. One of the most prominent issues which affect the performance of a CBIR system is the semantic gap. It is a variance that exists between low-level patterns of an image and high-level abstractions as perceived by humans. A robust image visual representation and relevance feedback (RF) can bridge this gap by extracting distinctive local and global features from the image and by incorporating valuable information stored as feedback. To handle this issue, this article presents a novel adaptive complementary visual word integration method for a robust representation of the salient objects of the image using local and global features based on the bag-of-visual-words (BoVW) model. To analyze the performance of the proposed method, three integration methods based on the BoVW model are proposed in this article: (a) integration of complementary features before clustering (called as non-adaptive complementary feature integration), (b) integration of non-adaptive complementary features after clustering (called as a non-adaptive complementary visual words integration), and (c) integration of adaptive complementary feature weighting after clustering based on self-paced learning (called as a proposed method based on adaptive complementary visual words integration). The performance of the proposed method is further enhanced by incorporating a log-based RF (LRF) method in the proposed model. The qualitative and quantitative analysis of the proposed method is carried on four image datasets, which show that the proposed adaptive complementary visual words integration method outperforms as compared with the non-adaptive complementary feature integration, non-adaptive complementary visual words integration, and state-of-the-art CBIR methods in terms of performance evaluation metrics.},
journal = {J. Image Video Process.},
month = jul,
numpages = {30},
keywords = {Relevance feedback, Robust learning, Adaptive weighting features, Visual feature integration, Query-by-image}
}

@inproceedings{10.5555/978-3-030-29983-5_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {i–xxii},
location = {Paris, France}
}

@article{10.5555/1517332.1517333,
author = {Xia, Wei and Ho, Danny and Capretz, Luiz Fernando and Ahmed, Faheem},
title = {Updating weight values for function point counting},
year = {2009},
issue_date = {January 2009},
publisher = {IOS Press},
address = {NLD},
volume = {6},
number = {1},
issn = {1448-5869},
abstract = {While software development productivity has grown rapidly, the weight values assigned to count standard Function Point (FP) created at IBM twenty-five years ago have never been updated. This obsolescence raises critical questions about the validity of the weight values; it also creates other problems such as ambiguous classification, crisp boundary, as well as subjective and locally defined weight values. All of these challenges reveal the need to calibrate FP in order to reflect both the specific software application context and the trend of today's software development techniques more accurately. We have created a FP calibration model that incorporates the learning ability of neural networks as well as the capability of capturing human knowledge using fuzzy logic. The empirical validation using ISBSG Data Repository (release 8) shows an average improvement of 22% in the accuracy of software effort estimations with the new calibration.},
journal = {Int. J. Hybrid Intell. Syst.},
month = jan,
pages = {1–14},
numpages = {14},
keywords = {software size measure, software estimation, effort prediction model, Function point analysis}
}

@article{10.1186/s13677-020-00195-6,
author = {Koo, Jahwan and Faseeh Qureshi, Nawab Muhammad and Siddiqui, Isma Farah and Abbas, Asad and Bashir, Ali Kashif},
title = {IoT-enabled directed acyclic graph in spark cluster},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {9},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-020-00195-6},
doi = {10.1186/s13677-020-00195-6},
abstract = {Real-time data streaming fetches live sensory segments of the dataset in the heterogeneous distributed computing environment. This process assembles data chunks at a rapid encapsulation rate through a streaming technique that bundles sensor segments into multiple micro-batches and extracts into a repository, respectively. Recently, the acquisition process is enhanced with an additional feature of exchanging IoT devices’ dataset comprised of two components: (i) sensory data and (ii) metadata. The body of sensory data includes record information, and the metadata part consists of logs, heterogeneous events, and routing path tables to transmit micro-batch streams into the repository. Real-time acquisition procedure uses the Directed Acyclic Graph (DAG) to extract live query outcomes from in-place micro-batches through MapReduce stages and returns a result set. However, few bottlenecks affect the performance during the execution process, such as (i) homogeneous micro-batches formation only, (ii) complexity of dataset diversification, (iii) heterogeneous data tuples processing, and (iv) linear DAG workflow only. As a result, it produces huge processing latency and the additional cost of extracting event-enabled IoT datasets. Thus, the Spark cluster that processes Resilient Distributed Dataset (RDD) in a fast-pace using Random access memory (RAM) defies expected robustness in processing IoT streams in the distributed computing environment. This paper presents an IoT-enabled Directed Acyclic Graph (I-DAG) technique that labels micro-batches at the stage of building a stream event and arranges stream elements with event labels. In the next step, heterogeneous stream events are processed through the I-DAG workflow, which has non-linear DAG operation for extracting queries’ results in a Spark cluster. The performance evaluation shows that I-DAG resolves homogeneous IoT-enabled stream event issues and provides an effective stream event heterogeneous solution for IoT-enabled datasets in spark clusters.},
journal = {J. Cloud Comput.},
month = sep,
numpages = {15},
keywords = {Micro-batch stream, MapReduce, Directed acyclic graph, Internet of Things (IoT), Apache spark}
}

@inproceedings{10.1007/978-3-030-27544-0_5,
author = {Polceanu, Mihai and Harrouet, Fabrice and Buche, C\'{e}dric},
title = {Fast Multi-scale fHOG Feature Extraction Using Histogram Downsampling},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_5},
doi = {10.1007/978-3-030-27544-0_5},
abstract = {Object detection is crucial for autonomous robotic systems to interact with the world around them but, in robots with low computational resources, deep learning is difficult to take advantage of. We develop incremental improvements to related work on feature approximation and describe an adaptive fHOG feature pyramid construction scheme based on histogram downsampling, together with a SVM classifier. Varying the pyramid level to which the scheme is applied gives control over the trade-off between precision or speed. We evaluate the proposed scheme on a modern computer and on a NAO humanoid robot in the context of the RoboCup competition, i.e., robot and soccer ball detection, in which we obtain significant increase (1.57x and 1.68x on PC and robot respectively) in pyramid construction speed relative to our baseline (the dlib library) without any loss in detection performance. The scheme can be adapted to increase speed while trading off precision until it reaches the conditions of a state-of-the-art power law feature scaling method.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {57–69},
numpages = {13},
keywords = {Histogram of Oriented Gradients, Feature approximation, Object detection, Vision for robotics},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/1167999.1168029,
author = {Moreno, Ram\'{o}n Piedrafita and Salcedo, Jos\'{e} Luis Villarroel},
title = {Implementation of time petri nets in real-time Java},
year = {2006},
isbn = {1595935444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1167999.1168029},
doi = {10.1145/1167999.1168029},
abstract = {In this paper, we propose the use of the Java Real-Time extension as a target language in the automatic code generation from Time Petri Net models of Real-Time Systems. To support the classic centralized implementation techniques of Time Petri Nets, the temporal and concurrency Java primitives were analyzed. Performance analyses determined the overhead of the Java virtual machine and of the centralized implementation control structures.},
booktitle = {Proceedings of the 4th International Workshop on Java Technologies for Real-Time and Embedded Systems},
pages = {178–187},
numpages = {10},
keywords = {modeling, petri nets, real-time java, real-time systems},
location = {Paris, France},
series = {JTRES '06}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Software architecture, Reliability, Performance, Patterns, Elastic computing, Distributed and concurrent architecture, Autonomic computing}
}

@book{10.5555/2671152,
author = {Goransson, Paul and Black, Chuck},
title = {Software Defined Networks: A Comprehensive Approach},
year = {2014},
isbn = {012416675X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Defined Networks discusses the historical networking environment that gave rise to SDN, as well as the latest advances in SDN technology. The book gives you the state of the art knowledge needed for successful deployment of an SDN, including: How to explain to the non-technical business decision makers in your organization the potential benefits, as well as the risks, in shifting parts of a network to the SDN model How to make intelligent decisions about when to integrate SDN technologies in a network How to decide if your organization should be developing its own SDN applications or looking to acquire these from an outside vendor How to accelerate the ability to develop your own SDN application, be it entirely novel or a more efficient approach to a long-standing problem Discusses the evolution of the switch platforms that enable SDN Addresses when to integrate SDN technologies in a network Provides an overview of sample SDN applications relevant to different industries Includes practical examples of how to write SDN applications}
}

@article{10.1145/3376921,
author = {Renner, Bernd-Christian and Heitmann, Jan and Steinmetz, Fabian},
title = {ahoi: Inexpensive, Low-power Communication and Localization for Underwater Sensor Networks and μAUVs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3376921},
doi = {10.1145/3376921},
abstract = {The recent development of small, cheap AUVs enables a plethora of underwater near- and inshore applications. Among these are monitoring of wind parks, detection of pollution sources, water-quality inspection, and the support of divers during disaster management. These tasks profit from online reporting, control, and AUV swarm interaction; yet they require underwater communication. Unfortunately, commercial devices are prohibitively expensive and typically closed-source, hampering their application in affordable products and research. Therefore, we developed the open-source ahoi acoustic modem. It is (i)&nbsp;small enough to be carried by micro AUVs, (ii)&nbsp;consumes little enough energy to not diminish operation times of its host, (iii)&nbsp;comes at an attractive unit cost below $600, (iv)&nbsp;can reliably communicate at distances of 150 m and more, and (v)&nbsp;supports ranging without additional hardware. Due to its modular build, the modem can be customized and is suitable as research platform to analyze, e.g., MAC and routing protocols. We conducted extensive real-world studies and present results of communication range, packet reception rate, ranging accuracy, and efficient and reliable self-localization. Finally, we draw conclusions regarding acoustic communication, ranging, and localization with inexpensive and low-power devices that go beyond a particular device. Our study, hence, encompasses general insights, observations, and recommendations.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {18},
numpages = {46},
keywords = {AUV, Acoustic, ahoi, communication, localization, modem, swarm, underwater}
}

@inproceedings{10.1145/1292414.1292424,
author = {Gerlich, Rainer and Gerlich, Ralf and Boll, Thomas},
title = {Random testing: from the classical approach to a global view and full test automation},
year = {2007},
isbn = {9781595938817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1292414.1292424},
doi = {10.1145/1292414.1292424},
abstract = {Research has brought a number of different methods for automated test and test data generation in the last decades. These methods range from simple applications such as random testing, to complex analysis systems, such as constraint-based methods.While research on these methods has been extensive, industrial application to large-scale systems is still seldom. One of the reasons may be that the simple methods fail - e.g. in terms of achieved coverage -- for complex systems-under-test, while the complex methods are difficult to implement and often have limitations in terms of the language scope they can be applied to.This paper aims to present practical experience with a number of different methods based on random testing in various fields of application, ranging from automated unit tests to automated system and integration tests. This experience led to strategic combination of different methods, which shall be described and discussed based on actual results.},
booktitle = {Proceedings of the 2nd International Workshop on Random Testing: Co-Located with the 22nd IEEE/ACM International Conference on Automated Software Engineering (ASE 2007)},
pages = {30–37},
numpages = {8},
keywords = {automated software test, path set coverage, random testing, test coverage},
location = {Atlanta, Georgia},
series = {RT '07}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.1145/3131473.3131486,
author = {Kazdaridis, Giannis and Keranidis, Stratos and Symeonidis, Polychronis and Dias, Paulo Sousa and Gon\c{c}alves, Pedro and Loureiro, Bruno and Gjanci, Petrika and Petrioli, Chiara},
title = {EVERUN: Enabling Power Consumption Monitoring in Underwater Networking Platforms},
year = {2017},
isbn = {9781450351478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131473.3131486},
doi = {10.1145/3131473.3131486},
abstract = {The energy restricted nature of underwater sensor networks directly affects the expected lifetime of autonomous deployments and limits the capabilities for long term underwater monitoring. Towards the goal of developing energy-efficient protocols and algorithms, researchers and equipment vendors require in-depth understanding of the power consumption characteristics of underwater hardware when deployed in-field. In this work, we introduce the EVERUN power monitoring framework, consisting of hardware and software components that were integrated with real equipment of the SUNRISE testbed facilities. Through the execution of a wide set of experiments under realistic conditions, we highlighted the limitations of model-based energy evaluation tools and characterized the energy efficiency performance of key protocols and mechanisms. The accuracy of the collected power data, along with the interesting derived findings, verified the applicability of our approach in evaluating the energy efficiency performance of proposed solutions.},
booktitle = {Proceedings of the 11th Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {83–90},
numpages = {8},
keywords = {energy efficiency, power consumption monitorin, testbed experimentation, underwater networking},
location = {Snowbird, Utah, USA},
series = {WiNTECH '17}
}

@inproceedings{10.1145/3472456.3472521,
author = {Lehr, Jan-Patrick and Bischof, Christian and Dewald, Florian and Mantel, Heiko and Norouzi, Mohammad and Wolf, Felix},
title = {Tool-Supported Mini-App Extraction to Facilitate Program Analysis and Parallelization},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472521},
doi = {10.1145/3472456.3472521},
abstract = {The size and complexity of high-performance computing applications present a serious challenge to manual reasoning about program behavior. The vastness and diversity of code bases often break automatic analysis tools, which could otherwise be used. As a consequence, developers resort to mini-apps, i.e., trimmed-down proxies of the original programs that retain key performance characteristics. Unfortunately, their construction is difficult and time consuming and prevents their mass production. In this paper, we propose a systematic and tool-supported approach to extract mini-apps from large-scale applications that reduces the manual effort needed to create them. Our approach covers the stages kernel identification, data capture, code extraction and representativeness validation. We demonstrate it using an astrophysics simulation with ≈ 8.5 million lines of code and extract a mini-app with only ≈ 1, 100 lines of code. For the mini-app, we evaluate the reduction of code complexity and execution similarity, and show how it enables the tool-supported discovery of unexploited parallelization opportunities, reducing the simulation’s runtime significantly.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {35},
numpages = {10},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@article{10.1023/A:1008013204871,
author = {Gustafson, John L. and Todi, Rajat},
title = {Conventional Benchmarks as a Sample of the Performance Spectrum},
year = {1999},
issue_date = {May 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1023/A:1008013204871},
doi = {10.1023/A:1008013204871},
abstract = {Most benchmarks are smaller than actual application programs. One reason is to improve benchmark universality by demanding resources every computer is likely to have. However, users dynamically increase the size of application programs to match the power available, whereas most benchmarks are static and of a size appropriate for computers available when the benchmark was created; this is particularly true for parallel computers. Thus, the benchmark overstates computer performance, since smaller problems spend more time in cache. Scalable benchmarks, such as HINT, examine the full spectrum of performance through various memory regimes, and express a superset of the information given by any particular fixed-size benchmark. Using 5,000 experimental measurements, we have found that performance on the NAS Parallel Benchmarks, SPEC, LINPACK, and other benchmarks is predicted accurately by subsets of HINT performance curve. Correlations are typically better than 0.995. Predicted ranking is often perfect.},
journal = {J. Supercomput.},
month = may,
pages = {321–342},
numpages = {22},
keywords = {performance analysis, hierarchical memory, benchmarks, HINT}
}

@article{10.5555/2882728.2882739,
author = {Anderson, Erin},
title = {The Salesperson as Outside Agent or Employee: A Transaction Cost Analysis},
year = {2008},
issue_date = {January 2008},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {27},
number = {1},
issn = {1526-548X},
abstract = {This descriptive study explores the reasons for integration of the personal selling function, i.e., the use of employee “direct” salespeople rather than manufacturers' representatives “reps”. A hypothesized model is developed based on both transaction cost analysis and the sales force management literature. Data from 13 electronic component manufacturers covering 159 U.S. sales districts are used to estimate a logistic model of the probability of “going direct” in a given district. Results are shown to be stable across specification and estimation methods and to fit the data well. The transaction cost model is generally supported. The principal finding is that the greater the difficulty of evaluating a salesperson's performance, the more likely the firm to substitute surveillance for commission as a control mechanism, i.e., to use a direct sales force. Among other findings, direct sales forces are also associated with complex, hard-to-learn product lines and with districts that demand considerable nonselling activities. Several factors prove unrelated, including company size, the amount of travel a district requires, and the importance of key accounts.This article was originally published in Marketing Science, Volume 4, Issue 3, Pages 234--254, in 1985.},
journal = {Marketing Science},
month = jan,
pages = {70–84},
numpages = {15},
keywords = {vertical integration, sales force management, organizational design, organization control}
}

@article{10.1145/378893.378896,
author = {Glew, Andy},
title = {An empirical investigation of OR indexing},
year = {1990},
issue_date = {Jan. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/378893.378896},
doi = {10.1145/378893.378896},
abstract = {This paper considers OR indexing as a substitute for, or an optimization of, addition in an addressing mode for a high speed processor. OR indexing is evaluated in the context of existing address streams, using time based sampling, and through compiler modifications.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jan,
pages = {41–49},
numpages = {9}
}

@article{10.1016/j.specom.2017.01.001,
author = {Lin, Wenliang and Deng, Zhongliang},
title = {Satellite speech quality measurement model based on a combination of auditory envelope feature and link loss},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {88},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.01.001},
doi = {10.1016/j.specom.2017.01.001},
abstract = {This paper focuses on Speech Quality Measurement for a satellite mobile communication system. In contrast to ground mobile communication systems, satellite speech quality measurement suffers from obvious jitter of long delays and severe satellite link losses. Auditory feature sensation and link loss measurement are most popular speech quality measurement models. However, long delay would cause the auditory feature sensation model with spreading of power spectrum, which furtherly diffuses the auditory spectrums to prevent human from hear correct responses. Nevertheless, measured error of link loss measurement would be enlarged gradually during the process of voice services. Therefore, a new speech quality measurement model based on the combination of auditory feature extraction from voice signal envelope and link loss from channel distortion is proposed. We analyze signal temporal envelope features and make power spectrum into auditory feature spectrum. The jitter of long delay and link loss are modeled as parameters to modify and compensate for the auditory feature spectrum, which also reduce the measurement distortion from transmission Doppler frequency offset to voice tone in satellite channel. A more understandable sensation estimation scores is proposed to present speech quality scores. The experimental results reveal that the new model reduces evaluation RMSE by 9.8% and is suited to the satellite mobile communications environment.},
journal = {Speech Commun.},
month = apr,
pages = {149–159},
numpages = {11},
keywords = {Speech quality measurement, Signal envelope analysis, Satellite mobile communication, Auditory feature}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Systematic mapping study, Software component, Component-based software engineering, Component-based software development}
}

@article{10.1016/j.compeleceng.2021.107215,
author = {Ardagna, Claudio A. and Bellandi, Valerio and Damiani, Ernesto and Bezzi, Michele and Hebert, Cedric},
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107215},
doi = {10.1016/j.compeleceng.2021.107215},
journal = {Comput. Electr. Eng.},
month = jul,
numpages = {10},
keywords = {Security and privacy, Machine learning, Big Data Analytics, Artificial intelligence}
}

@article{10.1016/j.cad.2010.09.008,
author = {Unal, Gozde and Nain, Delphine and Slabaugh, Greg and Fang, Tong},
title = {Generating shapes by analogies: An application to hearing aid design},
year = {2011},
issue_date = {January, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {43},
number = {1},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2010.09.008},
doi = {10.1016/j.cad.2010.09.008},
abstract = {3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to the estimation of customized hearing aid devices.},
journal = {Comput. Aided Des.},
month = jan,
pages = {47–56},
numpages = {10},
keywords = {Statistical shape modeling, Shape regression, Shape estimation, Hearing aid design, Customized shape modeling}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1016/j.cmpb.2015.04.003,
author = {Di Angelo, L. and Di Stefano, P.},
title = {A new method for the automatic identification of the dimensional features of vertebrae},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {121},
number = {1},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2015.04.003},
doi = {10.1016/j.cmpb.2015.04.003},
abstract = {This paper proposes a new approach to determine the measure of human vertebrae.Typical approaches perform measurements with lack of repeatability and reproducibility.The proposed method is based on morphological features recognition from 3D high point density model of the vertebrae.The paper proposes unambiguous rules to identify geometric references and the associated dimensions.Compared to typical approaches, the proposed method proved to be more repeatable and reproducible. In this paper a new automatic approach to determine the accurate measure of human vertebrae is proposed. The aim is to speed up the measurement process and to reduce the uncertainties that typically affect the measurement carried out by traditional approaches. The proposed method uses a 3D model of the vertebra obtained from CT-scans or 3D scanning, from which some characteristic dimensions are detected. For this purpose, specific rules to identify morphological features, from which to detect dimensional features unambiguously and accurately, are put forward and implemented in original software. The automatic method which is here proposed is verified by analysing real vertebrae and is then compared with the state-of-the-art methods for vertebra measurement.},
journal = {Comput. Methods Prog. Biomed.},
month = aug,
pages = {36–48},
numpages = {13},
keywords = {Vertebral dimensions, Shape segmentation, Measurement protocols in biomedicine, Measurement accuracy, Computer methods for vertebra analysis, 3D medical-image analysis}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@inproceedings{10.1145/1951365.1951416,
author = {Wang, Fan and Agrawal, Gagan},
title = {Effective and efficient sampling methods for deep web aggregation queries},
year = {2011},
isbn = {9781450305280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1951365.1951416},
doi = {10.1145/1951365.1951416},
abstract = {A large part of the data on the World Wide Web resides in the deep web. Executing structured, high-level queries on deep web data sources involves a number of challenges, several of which arise because query execution engines have a very limited access to data. In this paper, we consider the problem of executing aggregation queries involving data enumeration on these data sources, which requires sampling. The existing work in this area (HDSampler and its variants) is based on simple random sampling. We observe that this approach cannot obtain good estimates when the data is skewed. While there has been a lot of work on sampling skewed data, the existing methods are based on prior knowledge of data, and are therefore not applicable to hidden databases.In this paper, we present two prior-knowledge-free sampling algorithms, Adaptive Neighborhood Sampling (ANS) and Two Phase adaptive Sampling (TPS), which allow an aggregation query to be answered with a high accuracy (even when there is a skew), and a low sampling cost. For this purpose, we have developed robust estimators for aggregation functions including AVG, MAX, and MIN.Our experiments show that for data with a moderate or a large skew, ANS and TPS yield more accurate estimates, outperforming HDSampler by a factor of 4 on the average. Even for the cases where data has a small skew, our TPS method has an important advantage, which is that it has only one-third of the sampling costs of HDSampler.},
booktitle = {Proceedings of the 14th International Conference on Extending Database Technology},
pages = {425–436},
numpages = {12},
location = {Uppsala, Sweden},
series = {EDBT/ICDT '11}
}

@article{10.1145/1365490.1365498,
author = {Fatahalian, Kayvon and Houston, Mike},
title = {GPUs: A Closer Look: As the line between GPUs and CPUs begins to blur, it’s important to understand what makes GPUs tick.},
year = {2008},
issue_date = {March/April 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1542-7730},
url = {https://doi.org/10.1145/1365490.1365498},
doi = {10.1145/1365490.1365498},
abstract = {A gamer wanders through a virtual world rendered in near- cinematic detail. Seconds later, the screen fills with a 3D explosion, the result of unseen enemies hiding in physically accurate shadows. Disappointed, the user exits the game and returns to a computer desktop that exhibits the stylish 3D look-and-feel of a modern window manager. Both of these visual experiences require hundreds of gigaflops of computing performance, a demand met by the GPU (graphics processing unit) present in every consumer PC.},
journal = {Queue},
month = mar,
pages = {18–28},
numpages = {11}
}

@book{10.5555/1457550,
author = {Xiao, XiPeng},
title = {Technical, Commercial and Regulatory Challenges of QoS: An Internet Service Model Perspective},
year = {2008},
isbn = {9780080920313},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This book provides a comprehensive examination of Internet QoS theory, standards, vendor implementation and network deployment from the practitioner's point of view, including extensive discussion of related economic and regulatory issues. Written in a technology-light way so that a variety of professionals and researchers in the information and networking industries can easily grasp the material. Includes case studies based on real-world experiences from industry. The author starts by discussing the economic, regulatory and technical challenges of the existing QoS model. Key coverage includes defining a clear business model for selling and buying QoS in relation to current and future direction of government regulation and QoS interoperability (or lack thereof) between carriers and networking devices. The author then demonstrates how to improve the current QoS model to create a clear selling point, less regulation uncertainty, and higher chance of deployment success. This includes discussion of QoS re-packaging to end-users; economic and regulatory benefits of the re-packaging; and the overall benefits of an improved technical approach. Finally, the author discusses the future evolution of QoS from an Internet philosophy perspective and lets the reader draw the conclusions. This book is the first QoS book to provide in depth coverage on the commercial and regulatory aspects of QoS, in addition to the technical aspect. From that, readers can grasp the commercial and regulatory issues of QoS and their implications on the overall QoS business model. This book is also the first QoS book to provide case studies of real world QoS deployments, contributed by the people who did the actual deployments. From that, readers can grasp the practical issues of QoS in real world. This book is also the first QoS book to cover both wireline QoS and wireless QoS. Readers can grasp the QoS issues in the wireless world. The book was reviewed and endorsed by a long list of prominent industrial and academic figures. * The only book to discuss QoS technology in relation to economic and regulatory issues * Includes case studies based on real-world examples from industry practitioners. * Provides unique insight into how to improve the current QoS model to create a clear selling point, less regulatory uncertainty, and higher chance of deployment success.}
}

@article{10.1016/j.compag.2008.04.005,
author = {Morais, Raul and Matos, Samuel G. and Fernandes, Miguel A. and Valente, Ant\'{o}nio L. G. and Soares, Salviano F. S. P. and Ferreira, P. J. S. G. and Reis, M. J. C. S.},
title = {Sun, wind and water flow as energy supply for small stationary data acquisition platforms},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {64},
number = {2},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2008.04.005},
doi = {10.1016/j.compag.2008.04.005},
abstract = {The deployment of large mesh-type wireless networks is a challenge due to the multitude of arising issues. Perpetual operation of a network node is undoubtedly one of the major goals of any energy-aware protocol or power-efficient hardware platform. Energy harvesting has emerged as the natural way to keep small stationary hardware platforms running, even when operating continuously as network routing devices. This paper analyses solar radiation, wind and water flow as feasible energy sources that can be explored to meet the energy needs of a wireless sensor network router within the context of precision agriculture, and presents a multi-powered platform solution for wireless devices. Experimental results prove that our prototype, the MPWiNodeX, can manage simultaneously the three energy sources for charging a NiMH battery pack, resulting in an almost perpetual operation of the evaluated ZigBee network router. In addition to this, the energy scavenging techniques double up as sensors, yielding data on the amount of solar radiation, water flow and wind speed, a capability that avoids the use of specific sensors.},
journal = {Comput. Electron. Agric.},
month = dec,
pages = {120–132},
numpages = {13},
keywords = {Acquisition station, Energy harvesting, Energy sources, Power management, Precision agriculture}
}

@book{10.5555/2285538,
author = {Kramer, Kem-Laurin},
title = {User Experience in the Age of Sustainability: A Practitioner's Blueprint},
year = {2012},
isbn = {0123877954},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {User Experience in the Age of Sustainability focuses on the economic, sociological and environmental movement in business to make all products including digital ones more sustainable. Not only are businesses finding a significant ROI from these choices, customers are demanding this responsible behaviour. The author looks at user experience practice through the lens of sustainability whether it be a smart phone, service - based subscription solutions or sustainable packaging to expose the ways in which user researchers and designers can begin to connect to the sustainability not merely as a theoretical. This book has a practical take on the matter providing a framework along with case studies and personal stories from doing this work successfully. Both hardware and software design are covered. Learn about the fundamentals of sustainability and how it can change the future of user experience professionals Learn how to integrate sustainability into designs with a solid framework using user research methodology, techniques, and purposeful metrics Find out how to integrate sustainability frameworks into the software and product development cycles Find out how sustainability applies to mobile and digital products with discussions on user messaging, dematerialization, and efficient design See how companies have made it work with case studies}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Tertiary study, Systematic literature review, Software engineering, Threats to validity, Replication, Digital library}
}

@inproceedings{10.1007/978-3-642-53956-5_12,
author = {Mooij, Arjan J. and Hooman, Jozef and Albers, Rob},
title = {Early Fault Detection Using Design Models for Collision Prevention in Medical Equipment},
year = {2013},
isbn = {9783642539558},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-53956-5_12},
doi = {10.1007/978-3-642-53956-5_12},
abstract = {In the medical domain there is a tension between the requested speed of innovation and the time needed to deliver a certifiable system. To ensure the required safety, usually a long test and integration phase is needed. To shorten this phase and to avoid late bug fixing, the aim is to detect faults if any much earlier in the development process. This can be achieved by combining a number of model-based techniques such as 1 architecture validation by simulating executable models, 2 development of a Domain-Specific Language DSL to combine precision with higher levels of abstraction, and 3 transformations from DSLs to analysis models for performance evaluation and formal verification. We illustrate such techniques using an industrial study project on a new architecture for movement control including collision prevention.},
booktitle = {Revised Selected Papers of the Third International Symposium on Foundations of Health Information Engineering and Systems - Volume 8315},
pages = {170–187},
numpages = {18},
location = {Macau, China},
series = {FHIES 2013}
}

@article{10.1145/1823838.1823840,
author = {Thomasian, Alexander},
title = {Storage research in industry and universities},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/1823838.1823840},
doi = {10.1145/1823838.1823840},
abstract = {We review activities at universities and industrial research centers in the storage area, but also briefly mention topics such as processor design, operating systems, databases, and performance analysis. Our starting point is the Berkeley RAID proposal and the associated taxonomy two decades ago. Important research groups are listed and key researchers are identified. We pay special attention to faculty/student relationships, listing PhD theses and articles related to storage. We also describe innovative storage products and the companies behind them. This paper complements author's "Publications in Storage and Systems", ACM CAN, Sept. 2009.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {1–48},
numpages = {48}
}

@article{10.1023/A:1013349419545,
author = {Khoshgoftaar, Taghi M. and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
title = {Data Mining of Software Development Databases},
year = {2001},
issue_date = {November 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1023/A:1013349419545},
doi = {10.1023/A:1013349419545},
abstract = {Software quality models can predict which modules will have high risk, enabling developers to target enhancement activities to the most problematic modules. However, many find collection of the underlying software product and process metrics a daunting task.Many software development organizations routinely use very large databases for project management, configuration management, and problem reporting which record data on events during development. These large databases can be an unintrusive source of data for software quality modeling. However, multiplied by many releases of a legacy system or a broad product line, the amount of data can overwhelm manual analysis. The field of data mining is developing ways to find valuable bits of information in very large databases. This aptly describes our software quality modeling situation.This paper presents a case study that applied data mining techniques to software quality modeling of a very large legacy telecommunications software system's configuration management and problem reporting databases. The case study illustrates how useful models can be built and applied without interfering with development.},
journal = {Software Quality Journal},
month = nov,
pages = {161–176},
numpages = {16},
keywords = {classification trees, data mining, knowledge discovery, software metrics, software quality modeling}
}

@inproceedings{10.1145/2554850.2554938,
author = {Guo, Hai-Feng and Qiu, Zongyan and Siy, Harvey},
title = {Locating fault-inducing patterns from structural inputs},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2554938},
doi = {10.1145/2554850.2554938},
abstract = {In this paper, we propose a new fault localization technique for testing software which requires structured input data. We adopt a symbolic grammar to represent structured data input, and use an automatic grammar-based test generator to produce a set of well-distributed test cases, each of which is equipped with a set of structural features. We show that structural features can be effectively used as test coverage criteria for test suite reduction. By learning structural features associated with failed test cases, we present an automatic fault localization approach to find out software defects which result in the testing failures. Preliminary experiments justify that our fault localization approach is able to accurately locate fault-inducing patterns.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1100–1107},
numpages = {8},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/3460319.3464823,
author = {Mordahl, Austin and Wei, Shiyi},
title = {The impact of tool configuration spaces on the evaluation of configurable taint analysis for Android},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464823},
doi = {10.1145/3460319.3464823},
abstract = {The most popular static taint analysis tools for Android allow users to change the underlying analysis algorithms through configuration options. However, the large configuration spaces make it difficult for developers and users alike to understand the full capabilities of these tools, and studies to-date have only focused on individual configurations. In this work, we present the first study that evaluates the configurations in Android taint analysis tools, focusing on the two most popular tools, FlowDroid and DroidSafe. First, we perform a manual code investigation to better understand how configurations are implemented in both tools. We formalize the expected effects of configuration option settings in terms of precision and soundness partial orders which we use to systematically test the configuration space. Second, we create a new dataset of 756 manually classified flows across 18 open-source real-world apps and conduct large-scale experiments on this dataset and micro-benchmarks. We observe that configurations make significant tradeoffs on the performance, precision, and soundness of both tools. The studies to-date would reach different conclusions on the tools' capabilities were they to consider configurations or use real-world datasets. In addition, we study the individual options through a statistical analysis and make actionable recommendations for users to tune the tools to their own ends. Finally, we use the partial orders to test the tool configuration spaces and detect 21 instances where options behaved in unexpected and incorrect ways, demonstrating the need for rigorous testing of configuration spaces.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {466–477},
numpages = {12},
keywords = {Android taint analysis, configurable static analysis, empirical study},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1109/TASLP.2016.2599275,
author = {van de Laar, Thijs and de Vries, Bert and van de Laar, Thijs and de Vries, Bert},
title = {A Probabilistic Modeling Approach to Hearing Loss Compensation},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2599275},
doi = {10.1109/TASLP.2016.2599275},
abstract = {Hearing Aid HA algorithms need to be tuned “fitted” to match the impairment of each specific patient. The lack of a fundamental HA fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20% of HA patients. This paper proposes a probabilistic modeling approach to the design of HA algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding 1 signal processing algorithm, 2 the fitting solution as well as 3 a principled performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on HA or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2200–2213},
numpages = {14}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@article{10.1007/s10772-017-9400-x,
author = {Touazi, Azzedine and Debyeche, Mohamed},
title = {An experimental framework for Arabic digits speech recognition in noisy environments},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9400-x},
doi = {10.1007/s10772-017-9400-x},
abstract = {In this paper we present an experimental framework for Arabic isolated digits speech recognition named ARADIGITS-2. This framework provides a performance evaluation of Modern Standard Arabic devoted to a Distributed Speech Recognition system, under noisy environments at various Signal-to-Noise Ratio (SNR) levels. The data preparation and the evaluation scripts are designed by deploying a similar methodology to that followed in AURORA-2 database. The original speech data contains a total of 2704 clean utterances, spoken by 112 (56 male and 56 female) Algerian native speakers, down-sampled at 8 kHz. The feature vectors, which consist of a set of Mel Frequency Cepstral Coefficients and log energy, are extracted from speech samples using ETSI Advanced Front-End (ETSI-AFE) standard; whereas, the Hidden Markov Models (HMMs) Toolkit is used for building the speech recognition engine. The recognition task is conducted in speaker-independent mode by considering both word and syllable as acoustic units. Therefore, an optimal fitting of HMM parameters, as well as the temporal derivatives window, is carried out through a series of experiments performed on the two training modes: clean and multi-condition. Better results are obtained by exploiting the polysyllabic nature of Arabic digits. These results show the effectiveness of syllable-like unit in building Arabic digits recognition system, which exceeds word-like unit by an overall Word Accuracy Rate of 0.44 and 0.58% for clean and multi-condition training modes, respectively.},
journal = {Int. J. Speech Technol.},
month = jun,
pages = {205–224},
numpages = {20},
keywords = {AURORA-2 database, Distributed speech recognition, ETSI-AFE, Hidden Markov Models, Spoken Arabic digits recognition}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Curriculum learning, Self-paced learning, Learning with privileged information}
}

@inproceedings{10.1145/2503210.2503216,
author = {Liu, Mingliang and Jin, Ye and Zhai, Jidong and Zhai, Yan and Shi, Qianqian and Ma, Xiaosong and Chen, Wenguang},
title = {ACIC: automatic cloud I/O configurator for HPC applications},
year = {2013},
isbn = {9781450323789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503210.2503216},
doi = {10.1145/2503210.2503216},
abstract = {The cloud has become a promising alternative to traditional HPC centers or in-house clusters. This new environment highlights the I/O bottleneck problem, typically with top-of-the-line compute instances but sub-par communication and I/O facilities. It has been observed that changing cloud I/O system configurations leads to significant variation in the performance and cost efficiency of I/O intensive HPC applications. However, storage system configuration is tedious and error-prone to do manually, even for experts.This paper proposes ACIC, which takes a given application running on a given cloud platform, and automatically searches for optimized I/O system configurations. ACIC utilizes machine learning models to perform black-box performance/cost predictions. To tackle the high-dimensional parameter exploration space unique to cloud platforms, we enable affordable, reusable, and incremental training guided by Plackett and Burman Matrices. Results with four representative applications indicate that ACIC consistently identifies near-optimal configurations among a large group of candidate settings.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {38},
numpages = {12},
keywords = {cloud computing, modeling, performance, storage},
location = {Denver, Colorado},
series = {SC '13}
}

@article{10.1016/j.neucom.2015.05.134,
author = {Hu, Huijun and Liu, Ya and Liu, Maofu and Nie, Liqiang},
title = {Surface defect classification in large-scale strip steel image collection via hybrid chromosome genetic algorithm},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.05.134},
doi = {10.1016/j.neucom.2015.05.134},
abstract = {In this paper, hybrid chromosome genetic algorithm is applied to establishing the real-time classification model for surface defects in a large-scale strip steel image collection. After image preprocessing, four types of visual features, comprising geometric feature, shape feature, texture feature and grayscale feature, are extracted from the defect target image and its corresponding preprocessed image. In order to use genetic algorithm to optimize classification model based on hybrid chromosome, the structure of hybrid chromosome is designed to seamlessly integrate the kernel function, visual features and model parameters. And then the chromosome and the SVM classification model will be evolved and optimized according to the genetic operations and the fitness evaluation. In the end, the final SVM classifier is established using the decoding result of the optimal chromosome. The experimental results show that our method is effective and efficient in classifying the surface defects in a large-scale strip steel image collection.},
journal = {Neurocomput.},
month = mar,
pages = {86–95},
numpages = {10},
keywords = {Hybrid chromosome genetic algorithm, Kernel function, SVM model, Strip steel surface defect, Visual feature selection}
}

@article{10.1007/s11219-016-9320-z,
author = {Carvalho, Rainara Maia and Castro Andrade, Rossana Maria and Oliveira, K\'{a}thia Mar\c{c}al and Sousa Santos, Ismayle and Bezerra, Carla Ilane},
title = {Quality characteristics and measures for human---computer interaction evaluation in ubiquitous systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9320-z},
doi = {10.1007/s11219-016-9320-z},
abstract = {The advent of ubiquitous systems places even more focus on users, since these systems must support their daily activities in such a transparent way that does not disturb them. Thus, much more attention should be provided to human---computer interaction (HCI) and, as a consequence, to its quality. Dealing with quality issues implies first the identification of the quality characteristics that should be achieved and, then, which software measures should be used to evaluate them in a target system. Therefore, this work aims to identify what quality characteristics and measures have been used for the HCI evaluation of ubiquitous systems. In order to achieve our goal, we performed a large literature review, using a systematic mapping study, and we present our results in this paper. We identified 41 pertinent papers that were deeply analyzed to extract quality characteristics and software measures. We found 186 quality characteristics, but since there were divergences on their definitions and duplicated characteristics, an analysis of synonyms by peer review based on the equivalence of definitions was also done. This analysis allowed us to define a final suitable set composed of 27 quality characteristics, where 21 are generic to any system but are particularized for ubiquitous applications and 6 are specific for this domain. We also found 218 citations of measures associated with the characteristics, although the majority of them are simple definitions with no detail about their measurement functions. Our results provide not only an overview of this area to guide researchers in directing their efforts but also it can help practitioners in evaluating ubiquitous systems using these measures.},
journal = {Software Quality Journal},
month = sep,
pages = {743–795},
numpages = {53},
keywords = {Human---computer interaction, Quality characteristics, Quality model, Software measures, Systematic mapping study, Ubiquitous systems}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Image classification, Curriculum learning, Self-paced learning, Multi-modal}
}

@article{10.1016/j.eswa.2010.03.052,
author = {Berzal, Fernando and Cortijo, Francisco J. and Jim\'{e}nez, A\'{\i}da},
title = {TMiner aspects: Crosscutting concerns in the TMiner component-based data mining framework},
year = {2010},
issue_date = {September, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {9},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.03.052},
doi = {10.1016/j.eswa.2010.03.052},
abstract = {TMiner (Berzal, Cubero, &amp; Jimenez, 2009) is a component-based data mining framework that has been designed to support the whole KDD process and facilitate the implementation of complex data mining scenarios. This paper shows how aspect-oriented programming techniques support some tasks whose implementation using conventional object-oriented programming would be extremely time-consuming and error-prone. In particular, we have successfully employed aspects in TMiner to evaluate and monitor the I/O performance of alternative data mining techniques. Without having to modify the source code of the system under analysis, aspects provide an unintrusive mechanism to perform this kind of performance analysis. In fact, aspects let us probe a system implementation so that we can identify potential bottlenecks, detect redundant computations, and characterize system behavior+lessons learned during the development of TMiner.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {6675–6681},
numpages = {7},
keywords = {Aspect-oriented programming, Component-based systems, Crosscutting concerns, Data mining, Design patterns, Performance monitoring}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {SysML, Systematic mapping study, Systems engineering}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {cross-project defect prediction, transfer learning, balancing distribution, effort-aware indicator}
}

@article{10.1023/A:1008297832410,
author = {Klein, Hans W.},
title = {The EPAC Architecture: An Expert Cell Approach to Field Programmable Analog Devices},
year = {1998},
issue_date = {Sept. 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {1–2},
issn = {0925-1030},
url = {https://doi.org/10.1023/A:1008297832410},
doi = {10.1023/A:1008297832410},
abstract = {This paper describes the architectural configuration and various design trade-offs of the Electrically Programmable Analog Circuit (EPAC ^TM ), an expert-cell approach to meeting the market needs for an analog counterpart to the digital FPGA. The paper provides an overview of the technology, discusses architectural issues, and describes the internal operation of the first commercial EPAC devices. The paper concludes with various application examples and performance measurements.},
journal = {Analog Integr. Circuits Signal Process.},
month = sep,
pages = {91–103},
numpages = {13},
keywords = {field programmable analog IC, high-performance cells, in-system programming}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@proceedings{10.1145/2984043,
title = {SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/306792.306845,
author = {Taheri, H. Reza and Askins, Bradley J.},
title = {Simulating the performance of a multiprocessor operating system},
year = {1991},
isbn = {0818621699},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the 24th Annual Symposium on Simulation},
pages = {81–90},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {ANSS '91}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Genetic algorithms, Markov decision processes, Software configuration, Software customization}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Apriori Algorithm, DNA, K-NN, Naive Bayes, Rough Set}
}

@article{10.1016/j.jss.2019.06.100,
author = {Han, Xue and Carroll, Daniel and Yu, Tingting},
title = {Reproducing performance bug reports in server applications: The researchers’ experiences},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.100},
doi = {10.1016/j.jss.2019.06.100},
journal = {J. Syst. Softw.},
month = oct,
pages = {268–282},
numpages = {15},
keywords = {Performance bug reproduction, Bug characteristics study, Experience report}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@article{10.1016/j.jss.2009.06.032,
author = {Fung, Kam Hay and Low, Graham Cedric},
title = {Methodology evaluation framework for dynamic evolution in composition-based distributed applications},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.032},
doi = {10.1016/j.jss.2009.06.032},
abstract = {Dynamic evolution can be used to upgrade distributed applications without shutdown and restart as a way of improving service levels while minimising the loss of business revenue caused by the downtime. An evaluation framework assessing the level of support offered by existing methodologies in composition-based application (e.g. component-based and service-oriented) development is proposed. It was developed by an analysis of the literature and existing methodologies together with a refinement based on a survey of experienced practitioners and researchers. The use of the framework is demonstrated by applying it to twelve methodologies to assess their support for dynamic evolution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1950–1965},
numpages = {16},
keywords = {Composition-based applications, Dynamic evolution, Evaluation framework, Feature analysis, Method engineering, Service-oriented computing}
}

@article{10.1504/ijbis.2019.103793,
author = {Santos, Carlos Habekost Dos and Thom, Lucin\'{e}ia Heloisa and Cota, \'{E}rika and Fantinato, Marcelo},
title = {Supporting BPMN tool developers through meta-algorithms},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {32},
number = {4},
issn = {1746-0972},
url = {https://doi.org/10.1504/ijbis.2019.103793},
doi = {10.1504/ijbis.2019.103793},
abstract = {Business process model and notation (BPMN) provides an extensive set of notational elements, such as activities, events and gateways, which enable the representation of a wide variety of business processes. The purpose of this paper is to propose an approach to develop logical models (referred here as 'meta-algorithms'), whose goal is to express the content described in the textual rules, for each BPMN element, considering the BPMN2.0.2 specification. We identified that there is completeness and correctness in this approach, applying decision tables and control-flow graphs to checking the proposed meta-algorithms. In addition, we applied two surveys, considering a potential audience for the proposed meta-algorithms in order to check users' acceptance. For validation results, we identified that only textual rules are not enough to implement the notational elements for 84% of the survey participants.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {460–488},
numpages = {28},
keywords = {BPMN, tool development, notational elements, meta-algorithms, business information systems, developer, business rule}
}

@article{10.1016/j.eswa.2019.113087,
author = {Nam, Hyoju and Yun, Unil and Yoon, Eunchul and Lin, Jerry Chun-Wei},
title = {Efficient approach for incremental weighted erasable pattern mining with list structure},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113087},
doi = {10.1016/j.eswa.2019.113087},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {22},
keywords = {Data mining, Erasable patterns, Incremental mining, Weighted conditions, Pruning techniques}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {Energy consumption, Cyber–physical systems, Green plugin, HADAS eco-assistant}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {Deep learning, active learning, deep active learning}
}

@inproceedings{10.1109/FOSE.2007.32,
author = {Woodside, Murray and Franks, Greg and Petriu, Dorina C.},
title = {The Future of Software Performance Engineering},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.32},
doi = {10.1109/FOSE.2007.32},
abstract = {Performance is a pervasive quality of software systems; everything affects it, from the software itself to all underlying layers, such as operating system, middleware, hardware, communication networks, etc. Software Performance Engineering encompasses efforts to describe and improve performance, with two distinct approaches: an early-cycle predictive modelbased approach, and a late-cycle measurement-based approach. Current progress and future trends within these two approaches are described, with a tendency (and a need) for them to converge, in order to cover the entire development cycle.},
booktitle = {2007 Future of Software Engineering},
pages = {171–187},
numpages = {17},
series = {FOSE '07}
}

@article{10.1007/s11042-019-08049-3,
author = {Kas, M. and El-merabet, Y. and Ruichek, Y. and Messoussi, R.},
title = {A comprehensive comparative study of handcrafted methods for face recognition LBP-like and non LBP operators},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {1–2},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08049-3},
doi = {10.1007/s11042-019-08049-3},
abstract = {Pattern recognition and computer vision fields experienced the proposal of several architectures and approaches to deal with the demands of real world applications including face recognition. They have almost the same structure, based generally on a series of steps where the main ones are feature extraction and classification. The literature works interessted in face recognition problems, insist on the role of texture description as one of the key elements in face analysis, since it greatly affects recognition accuracy. Therefore, texture feature extraction has gained much attention and became a long-standing research topic thanks to its abilities to efficiently understand the face recognition process, especially in terms of face description. Recently, several literature researches in face application proposed new architectures based on pattern description proved by their discriminative power when extracting the feature information from facial images. These advantages combined with an outstanding performance in many classification applications, allowed the LBP-like descriptors to be one of the most prominent texture description method. Given this period of remarkable evolution, this research work includes a comprehensive analytical study of the face recognition performance of 64 LBP-like and 3 non-LBP texture descriptors recently proposed in the literature. To this end, we adopted a face recognition framework composed of four stages: 1) image pre-processing using gamma correction; 2) feature extraction using texture descriptors; 3) histogram calculation and 4) face recognition and classification based on the simple parameter-free Nearest Neighbors classifier (NN). The conducted comprehensive evaluations and experiments on the challenging and widely used benchmarks ORL, YALE, Extended YALE B and FERET databases presenting different challenges, indicate that a number of evaluated texture descriptors, which are tested for the first time on face recognition task, achieve better or competitive compared to several recent systems reported in face recognition literature.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {375–413},
numpages = {39},
keywords = {Face recognition, Texture descriptor, LBP-like descriptors, Feature extraction, Texture classification, Pre-processing}
}

@article{10.1145/3424308,
author = {Chen, Zhenpeng and Cao, Yanbin and Yao, Huihan and Lu, Xuan and Peng, Xin and Mei, Hong and Liu, Xuanzhe},
title = {Emoji-powered Sentiment and Emotion Detection from Software Developers’ Communication Data},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3424308},
doi = {10.1145/3424308},
abstract = {Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {18},
numpages = {48},
keywords = {Emoji, emotion, sentiment, software engineering}
}

@article{10.1016/j.compind.2019.103184,
author = {Colucci, Domenico and Morra, Lia and Zhang, Xiaoyang and Fissore, Davide and Lamberti, Fabrizio},
title = {An automatic computer vision pipeline for the in-line monitoring of freeze-drying processes},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2019.103184},
doi = {10.1016/j.compind.2019.103184},
journal = {Comput. Ind.},
month = feb,
numpages = {12},
keywords = {Vacuum freeze drying, Real-time monitoring, Object detector, Convolutional neural network, Data augmentation}
}

@inproceedings{10.1145/3281411.3281443,
author = {H\o{}iland-J\o{}rgensen, Toke and Brouer, Jesper Dangaard and Borkmann, Daniel and Fastabend, John and Herbert, Tom and Ahern, David and Miller, David},
title = {The eXpress data path: fast programmable packet processing in the operating system kernel},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281443},
doi = {10.1145/3281411.3281443},
abstract = {Programmable packet processing is increasingly implemented using kernel bypass techniques, where a userspace application takes complete control of the networking hardware to avoid expensive context switches between kernel and userspace. However, as the operating system is bypassed, so are its application isolation and security mechanisms; and well-tested configuration, deployment and management tools cease to function.To overcome this limitation, we present the design of a novel approach to programmable packet processing, called the eXpress Data Path (XDP). In XDP, the operating system kernel itself provides a safe execution environment for custom packet processing applications, executed in device driver context. XDP is part of the mainline Linux kernel and provides a fully integrated solution working in concert with the kernel's networking stack. Applications are written in higher level languages such as C and compiled into custom byte code which the kernel statically analyses for safety, and translates into native instructions.We show that XDP achieves single-core packet processing performance as high as 24 million packets per second, and illustrate the flexibility of the programming model through three example use cases: layer-3 routing, inline DDoS protection and layer-4 load balancing.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {54–66},
numpages = {13},
keywords = {BPF, DPDK, XDP, programmable networking},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1007/978-3-030-34872-4_52,
author = {Baghel, Shikha and Bhattacharjee, Mrinmoy and Prasanna, S. R. M. and Guha, Prithwijit},
title = {Shouted and Normal Speech Classification Using 1D CNN},
year = {2019},
isbn = {978-3-030-34871-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34872-4_52},
doi = {10.1007/978-3-030-34872-4_52},
abstract = {Automatic shouted speech detection systems usually model its spectral characteristics to differentiate it from normal speech. Mostly hand-crafted features have been explored for shouted speech detection. However, many works on audio processing suggest that approaches based on automatic feature learning are more robust than hand-crafted feature engineering. This work re-demonstrates this notion by proposing a 1D-CNN architecture for shouted and normal speech classification task. The CNN learns features from the magnitude spectrum of speech frames. Classification is performed by fully connected layers at later stages of the network. Performance of the proposed architecture is evaluated on three datasets and validated against three existing approaches. As an additional contribution, a discussion of features learned by the CNN kernels is provided with relevant visualizations.},
booktitle = {Pattern Recognition and Machine Intelligence: 8th International Conference, PReMI 2019, Tezpur, India, December 17-20, 2019, Proceedings, Part II},
pages = {472–480},
numpages = {9},
keywords = {Shouted and normal speech classification, Shouted speech detection, 1D CNN, Convolution filter visualization},
location = {Tezpur, India}
}

@inproceedings{10.1007/978-3-030-64243-3_3,
author = {Liu, Yu and Wang, Yuheng and Liu, Haipeng and Zhou, Anfu and Liu, Jianhua and Yang, Ning},
title = {Long-Range Gesture Recognition Using Millimeter Wave Radar},
year = {2020},
isbn = {978-3-030-64242-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64243-3_3},
doi = {10.1007/978-3-030-64243-3_3},
abstract = {Millimeter wave (mmWave) based gesture recognition technology provides a good human computer interaction (HCI) experience. Prior works focus on the close-range gesture recognition, but fall short in range extension, i.e., they are unable to recognize gestures more than one meter away from considerable noise motions. In this paper, we design a long-range gesture recognition model which utilizes a novel data processing method and a customized artificial Convolutional Neural Network (CNN). Firstly, we break down gestures into multiple reflection points and extract their spatial-temporal features which depict gesture details. Secondly, we design a CNN to learn changing patterns of extracted features respectively and output the recognition result. We thoroughly evaluate our proposed system by implementing on a commodity mmWave radar. Besides, we also provide more extensive assessments to demonstrate that the proposed system is practical in several real-world scenarios.},
booktitle = {Green, Pervasive, and Cloud Computing: 15th International Conference, GPC 2020, Xi'an, China, November 13–15, 2020, Proceedings},
pages = {30–44},
numpages = {15},
keywords = {Gesture recognition, Millimeter wave radar, Long-range scenario, Convolutional neural networks},
location = {Xi'an, China}
}

@article{10.1007/s11219-005-4250-1,
author = {Kazman, Rick and Bass, Len and Klein, Mark and Lattanze, Tony and Northrop, Linda},
title = {A Basis for Analyzing Software Architecture Analysis Methods},
year = {2005},
issue_date = {December  2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-005-4250-1},
doi = {10.1007/s11219-005-4250-1},
abstract = {A software architecture is a key asset for any organization that builds complex software-intensive systems. Because of an architecture's central role as a project blueprint, organizations should analyze the architecture before committing resources to it. An analysis helps to ensure that sound architectural decisions are made. Over the past decade a large number of architecture analysis methods have been created, and at least two surveys of these methods have been published. This paper examines the criteria for analyzing architecture analysis methods, and suggests a new set of criteria that focus on the essence of what it means to be an architecture analysis method. These criteria could be used to compare methods, to help understand the suitability of a method, or to improve a method. We then examine two methods--the Architecture Tradeoff Analysis Method and Architecture-level Modifiability Analysis--in light of these criteria, and provide some insight into how these methods can be improved.},
journal = {Software Quality Journal},
month = dec,
pages = {329–355},
numpages = {27},
keywords = {analysis methods, architecture analysis, quality attributes, software architecture}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Knowledge tracing, Self-paced learning, Deep learning, Personalized education},
location = {Hanoi, Vietnam}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {Transfer learning, classification}
}

@inproceedings{10.1145/3386263.3409655,
author = {Yu, Han and Guo, Chao and Chen, Bin and Du, Changxin and Yong, Xiao and Fan, Senhua},
title = {A New Silicon-aware Big Data SoC Timing Analysis Solution: A Case Study of Empyrean University Program},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3409655},
doi = {10.1145/3386263.3409655},
abstract = {With advanced IC process nodes, traditional corner-based timing sign-off methods are facing big challenges. Although STA tools have incorporated more sophisticated models such as AOCV/ POCV/LVF to characterize variation effects, they can still lead to excessive pessimism or incomplete coverage [1]. To consider the effects of variation on reliability, designers need to find a more potent solution. Now Huada Empyrean has raised a smart new approach which can provide a fast timing analysis solution with SPICE accuracy. It can fill the gap between STA and silicon for advanced process nodes, especially for ultra-low-voltage designs that are used in AI/IoT/Blockchain applications. Theories and algorithms supplied by some top universities, which joined a long-term Empyrean University Program, have made great contributions to the R&amp;D process of this solution.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {573–578},
numpages = {6},
keywords = {empyrean university program, process variation, sta, timing sign-off},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.5555/2888619.2888724,
author = {Yanikara, Fatma Selin and Kuhl, Michael E.},
title = {A simulation framework for the comparison of reverse logistic network configurations},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {Reverse logistics networks are designed and implemented by companies to collect products at the end of their useful life from end users in order to remanufacture products or properly recycle materials. In this paper, we present a simulation framework for comparing alternative reverse logistic network configurations based on productivity and sustainability performance metrics. The resulting decision support tool enables the evaluation of user specified system and experimental parameters. An overview of the simulation framework is provided along with an example that illustrates the capabilities and functionality of the tool.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {979–990},
numpages = {12},
location = {Huntington Beach, California},
series = {WSC '15}
}

@article{10.1016/j.comcom.2005.07.017,
author = {Negru, D. and Mehaoua, A. and Hadjadj-aoul, Y. and Berthelot, C.},
title = {Dynamic bandwidth allocation for efficient support of concurrent digital TV and IP multicast services in DVB-T networks},
year = {2006},
issue_date = {March, 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {6},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2005.07.017},
doi = {10.1016/j.comcom.2005.07.017},
abstract = {Convergence of digital video broadcasting networks, from one hand, and IP wireless networks, from the other hand, represents undoubtedly the main evolution in next generation of services combining digital television and interactive Internet applications. This cross-industry synergy is jointly driven by the growing number of mobile devices willing to access the Internet and the large-scale and low-cost deployment of broadband terrestrial digital broadcasting infrastructures (DVB-T). In this paper, we describe a novel dynamic bandwidth allocation algorithm, called iDBMS, for the provisioning of concurrent IP multicast and Digital TV services over a new type of broadband wireless metropolitan area networks that utilises the DVB-T stream in regenerative configurations for creating a multi-service capable infrastructure in the UHF/VHF band. This fast DVB-T metropolitan backbone will permit the seamless inter-connection of multi-technological access networks (i.e. WiFi, xDSL, PSTN, ...) at a regional level by means of a shared broadband DVB-T downlink and point-to-point physical return channels. IP over DVB optimization, asymmetric wireless channels and resource allocation are addressed in this paper. Implementation and performance evaluation of the proposed resource management algorithm iDBMS is also presented in the large-scale context of the IST European project ATHENA.},
journal = {Comput. Commun.},
month = mar,
pages = {741–756},
numpages = {16},
keywords = {DVB, IP, QoS, Resource management}
}

@inproceedings{10.1145/3350546.3352496,
author = {Rafailidis, Dimitrios},
title = {Bayesian Deep Learning with Trust and Distrust in Recommendation Systems},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352496},
doi = {10.1145/3350546.3352496},
abstract = {Exploiting the selections of social friends and foes can efficiently face the data scarcity of user preferences and the cold-start problem. In this paper, we present a Social Deep Pairwise Learning model, namely SDPL. According to the Bayesian Pairwise Ranking criterion, we design a loss function with multiple ranking criteria based on the selections of users, and those in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinearity in user preferences and the social information of trust and distrust relationships by designing a deep learning architecture. In each backpropagation step, we perform social negative sampling to meet the multiple ranking criteria of our loss function. Our experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature, demonstrate the effectiveness of the proposed approach, outperforming other state-of-the art methods. In addition, we show that our deep learning strategy plays an important role in capturing the nonlinear associations between user preferences and the social information of trust and distrust relationships, and demonstrate that our social negative sampling strategy is a key factor in SDPL.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
keywords = {Pairwise learning, collaborative filtering, deep learning, social relationships},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@article{10.1287/isre.2020.0921,
author = {Lee, Gene Moo and He, Shu and Lee, Joowon and Whinston, Andrew B.},
title = {Matching Mobile Applications for Cross-Promotion},
year = {2020},
issue_date = {September 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {3},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.2020.0921},
doi = {10.1287/isre.2020.0921},
abstract = {As the mobile app market grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. Similar to the case of recommender systems, the challenge is how apps can be recommended to the right users and how consumers can find the right apps. This paper studies a new mobile app ad framework, cross-promotion (CP), which is to promote new “target” apps within other “source” apps. With unique random matching experiment data, we empirically test the important determinants of ad effectiveness. We then propose a machine-learning-based framework to optimally match source apps to target apps to improve ad effectiveness in terms of app downloads and postdownload usages. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of CP campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. The paper has important managerial implications because it provides direct guidance to better utilize CP for app developers and to leverage data analytics and machine-learning models for platform managers. It also provides policy implications on the trade-off between utility and privacy in the growing data economy.The mobile applications (apps) market is one of the most successful software markets. As the platform grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. The challenge is how app developers can target the right users with their apps and how consumers can find the apps that fit their needs. Cross-promotion, advertising a mobile app (target app) in another app (source app), is introduced as a new app-promotion framework to alleviate the issue of search costs. In this paper, we model source app user behaviors (downloads and postdownload usages) with respect to different target apps in cross-promotion campaigns. We construct a novel app similarity measure using latent Dirichlet allocation topic modeling on apps’ production descriptions and then analyze how the similarity between the source and target apps influences users’ app download and usage decisions. To estimate the model, we use a unique data set from a large-scale random matching experiment conducted by a major mobile advertising company in Korea. The empirical results show that consumers prefer more diversified apps when they are making download decisions compared with their usage decisions, which is supported by the psychology literature on people’s variety-seeking behavior. Lastly, we propose an app-matching system based on machine-learning models (on app download and usage prediction) and generalized deferred acceptance algorithms. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of cross-promotion campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. This paper has implications on the trade-off between utility and privacy in the growing mobile economy.},
journal = {Info. Sys. Research},
month = sep,
pages = {865–891},
numpages = {27},
keywords = {mobile applications, cross-promotion, matching, search cost, two-sided platform, topic modeling, machine learning, deferred acceptance, algorithm, mobile analytics}
}

@article{10.1155/2017/3902543,
author = {Sun, Fang and Xiao, Dongyue and He, Wei and Li, Ran and Wang, Jintao},
title = {Adaptive Image Compressive Sensing Using Texture Contrast},
year = {2017},
issue_date = {2017},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2017},
issn = {1687-7578},
url = {https://doi.org/10.1155/2017/3902543},
doi = {10.1155/2017/3902543},
abstract = {The traditional image Compressive Sensing (CS) conducts block-wise sampling with the same sampling rate. However, some blocking artifacts often occur due to the varying block sparsity, leading to a low rate-distortion performance. To suppress these blocking artifacts, we propose to adaptively sample each block according to texture features in this paper. With the maximum gradient in 8-connected region of each pixel, we measure the texture variation of each pixel and then compute the texture contrast of each block. According to the distribution of texture contrast, we adaptively set the sampling rate of each block and finally build an image reconstruction model using these block texture contrasts. Experimental results show that our adaptive sampling scheme improves the rate-distortion performance of image CS compared with the existing adaptive schemes and the reconstructed images by our method achieve better visual quality.},
journal = {Int. J. Digit. Multimedia Broadcast.},
month = jan,
numpages = {10}
}

@article{10.1007/s11042-017-5543-7,
author = {Zhou, Yu and Yang, Liu and Li, Leida and Gu, Ke and Tang, Lijuan},
title = {Reduced-reference quality assessment of DIBR-synthesized images based on multi-scale edge intensity similarity},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {16},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5543-7},
doi = {10.1007/s11042-017-5543-7},
abstract = {Depth-image-based-rendering (DIBR) plays an important role in view synthesis for free-viewpoint videos. The warping process in DIBR causes geometric displacement, which distributes intensively around edges, and the subsequent rendering process results in the impairment of edges. Traditional 2D image quality metrics are limited in the quality evaluation of DIBR-synthesized images. In this paper, we present a reduced-reference quality metric for DIBR-synthesized images by only extracting several feature values, namely multi-scale Edge Intensity Similarity (EIS). The original and synthesized images are first downsampled to generate images with different resolutions. Then an edge detection process is conducted on each scale and the edge intensity is calculated. The similarity of the edge intensity between each downsampled original image and the corresponding synthesized image is computed. Finally, the average similarity is calculated as the quality score of the DIBR-synthesized image. Experiments conducted on IRCCyN/IVC DIBR image and video databases demonstrate that the proposed method overall outperforms traditional 2D and existing DIBR-targeted quality metrics.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21033–21052},
numpages = {20},
keywords = {DIBR, Down-sampling, Edge detection, Multi-scale, Quality evaluation, View synthesis}
}

@article{10.1007/s10664-021-10007-3,
author = {Revoredo, Kate and Djurica, Djordje and Mendling, Jan},
title = {A study into the practice of reporting software engineering experiments},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10007-3},
doi = {10.1007/s10664-021-10007-3},
abstract = {It has been argued that reporting software engineering experiments in a standardized way helps researchers find relevant information, understand how experiments were conducted and assess the validity of their results. Various guidelines have been proposed specifically for software engineering experiments. The benefits of such guidelines have often been emphasized, but the actual uptake and practice of reporting have not yet been investigated since the introduction of many of the more recent guidelines. In this research, we utilize a mixed-method study design including sequence analysis techniques for evaluating to which extent papers follow such guidelines. Our study focuses on the four most prominent software engineering journals and the time period from 2000 to 2020. Our results show that many experimental papers miss information suggested by guidelines, that no de facto standard sequence for reporting exists, and that many papers do not cite any guidelines. We discuss these findings and implications for the discipline of experimental software engineering focusing on the review process and the potential to refine and extend guidelines, among others, to account for theory explicitly.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {50},
keywords = {Guideline for software engineering experiments, Controlled experiments, Process mining, Method mining}
}

@inproceedings{10.1145/3385032.3385039,
author = {Bhatia, Kushagra and Mishra, Siba and Sharma, Arpit},
title = {Clustering Glossary Terms Extracted from Large-Sized Software Requirements using FastText},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385039},
doi = {10.1145/3385032.3385039},
abstract = {Specialized terms used in the requirements document should be defined in a glossary. We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents. We use text chunking combined with WordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster. We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {5},
numpages = {11},
keywords = {Clustering, FastText, Glossary, Natural Language Processing, Requirements Engineering, Word Embeddings},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.1016/j.jvcir.2015.04.005,
author = {Liu, Maofu and Liu, Ya and Hu, Huijun and Nie, Liqiang},
title = {Genetic algorithm and mathematical morphology based binarization method for strip steel defect image with non-uniform illumination},
year = {2016},
issue_date = {May 2016},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {37},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2015.04.005},
doi = {10.1016/j.jvcir.2015.04.005},
abstract = {We proposed EOBMM to reduce the non-uniform illumination in strip steel defect image.We put forward BMBGA to decide the threshold value of defect image binarization.We combined EOBMM and BMBGA to present strip steel defect image binarization method.The proposed binarization method is effective and efficiency in real-time environment. In order to precisely extract the image shape feature for the defect detection and classification, the strip steel image needs to firstly be binarized effectively. In this paper, the intelligent information processing, including mathematical morphology and genetic algorithm, is introduced to the strip steel defect image binarization. In order to eliminate the effect of non-uniform illumination and enhance the detailed information of the strip steel defect image, an enhancement operator based on mathematical morphology (EOBMM) is proposed firstly. And then, the binarization method based on genetic algorithm (BMBGA) is applied to the binarization of the strip steel defect image processed by EOBMM. The experiment results show that our method is effective and efficiency in the strip steel defect image binarization and outperforms the traditional image binarization methods, Otsu and Bernsen.},
journal = {J. Vis. Comun. Image Represent.},
month = may,
pages = {70–77},
numpages = {8},
keywords = {EOBMM, Fitness function, Genetic algorithm, Genetic operations, Image binarization, Mathematical morphology, Non-uniform illumination, Strip steel defect image, Top-hat transformation}
}

@article{10.1145/3478094,
author = {Lian, Jie and Yuan, Xu and Li, Ming and Tzeng, Nian-Feng},
title = {Fall Detection via Inaudible Acoustic Sensing},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478094},
doi = {10.1145/3478094},
abstract = {The fall detection system is of critical importance in protecting elders through promptly discovering fall accidents to provide immediate medical assistance, potentially saving elders' lives. This paper aims to develop a novel and lightweight fall detection system by relying solely on a home audio device via inaudible acoustic sensing, to recognize fall occurrences for wide home deployment. In particular, we program the audio device to let its speaker emit 20kHz continuous wave, while utilizing a microphone to record reflected signals for capturing the Doppler shift caused by the fall. Considering interferences from different factors, we first develop a set of solutions for their removal to get clean spectrograms and then apply the power burst curve to locate the time points at which human motions happen. A set of effective features is then extracted from the spectrograms for representing the fall patterns, distinguishable from normal activities. We further apply the Singular Value Decomposition (SVD) and K-mean algorithms to reduce the data feature dimensions and to cluster the data, respectively, before input them to a Hidden Markov Model for training and classification. In the end, our system is implemented and deployed in various environments for evaluation. The experimental results demonstrate that our system can achieve superior performance for detecting fall accidents and is robust to environment changes, i.e., transferable to other environments after training in one environment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {114},
numpages = {21},
keywords = {Device-free, Fall Detection, Hidden Markov Model, Ultrasonic}
}

@inproceedings{10.5555/3242181.3242244,
author = {Balci, Osman and Fujimoto, Richard M. and Goldsman, David and Nance, Richard E. and Zeigler, Bernard P.},
title = {The state of innovation in modeling and simulation: the last 50 years},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Innovation in Modeling and Simulation (M&amp;S) refers to exploiting new ideas, exploiting new technology, and employing out-of-the-box thinking, which lead to the creation of new methodologies, techniques, concepts, frameworks, and software. This paper addresses the following questions: (a) what was the state of the art in M&amp;S 50 years ago, what is it today, how much progress has been made? (b) how much innovation in M&amp;S has been accomplished over the last half a century? (c) what were the obstacles to innovation in M&amp;S? (d) what are some recommendations to promote innovation in M&amp;S? (e) what message should be sent to the funding agencies to encourage innovation in M&amp;S?},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {58},
numpages = {16},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.1007/s11263-020-01374-3,
author = {Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
title = {Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {129},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-020-01374-3},
doi = {10.1007/s11263-020-01374-3},
abstract = {The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10,&nbsp;714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions—one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our .},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {246–266},
numpages = {21},
keywords = {Vision-and-language navigation, Long-range navigation, Spatial memory, Dual attention}
}

@inproceedings{10.1145/3098279.3098564,
author = {Ghosh, Surjya and Ganguly, Niloy and Mitra, Bivas and De, Pradipta},
title = {TapSense: combining self-report patterns and typing characteristics for smartphone based emotion detection},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098564},
doi = {10.1145/3098279.3098564},
abstract = {Typing based communication applications on smartphones, like WhatsApp, can induce emotional exchanges. The effects of an emotion in one session of communication can persist across sessions. In this work, we attempt automatic emotion detection by jointly modeling the typing characteristics, and the persistence of emotion. Typing characteristics, like speed, number of mistakes, special characters used, are inferred from typing sessions. Self reports recording emotion states after typing sessions capture persistence of emotion. We use this data to train a personalized machine learning model for multi-state emotion classification. We implemented an Android based smartphone application, called TapSense, that records typing related metadata, and uses a carefully designed Experience Sampling Method (ESM) to collect emotion self reports. We are able to classify four emotion states - happy, sad, stressed, and relaxed, with an average accuracy (AUCROC) of 84% for a group of 22 participants who installed and used TapSense for 3 weeks.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {2},
numpages = {12},
keywords = {Markov chain, SMOTE, Smartphone typing, emotion detection, emotion persistence, experience sampling method},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3427228.3427259,
author = {Shi, Cong and Wang, Yan and Chen, Yingying and Saxena, Nitesh and Wang, Chen},
title = {WearID: Low-Effort Wearable-Assisted Authentication of Voice Commands via Cross-Domain Comparison without Training},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427259},
doi = {10.1145/3427228.3427259},
abstract = {Due to the open nature of voice input, voice assistant (VA) systems (e.g., Google Home and Amazon Alexa) are vulnerable to various security and privacy leakages (e.g., credit card numbers, passwords), especially when issuing critical user commands involving large purchases, critical calls, etc. Though the existing VA systems may employ voice features to identify users, they are still vulnerable to various acoustic-based attacks (e.g., impersonation, replay, and hidden command attacks). In this work, we propose a training-free voice authentication system, WearID, leveraging the cross-domain speech similarity between the audio domain and the vibration domain to provide enhanced security to the ever-growing deployment of VA systems. In particular, when a user gives a critical command, WearID exploits motion sensors on the user’s wearable device to capture the aerial speech in the vibration domain and verify it with the speech captured in the audio domain via the VA device’s microphone. Compared to existing approaches, our solution is low-effort and privacy-preserving, as it neither requires users’ active inputs (e.g., replying messages/calls) nor to store users’ privacy-sensitive voice samples for training. In addition, our solution exploits the distinct vibration sensing interface and its short sensing range to sound (e.g., 25cm) to verify voice commands. Examining the similarity of the two domains’ data is not trivial. The huge sampling rate gap (e.g., 8000Hz vs. 200Hz) between the audio and vibration domains makes it hard to compare the two domains’ data directly, and even tiny data noises could be magnified and cause authentication failures. To address the challenges, we investigate the complex relationship between the two sensing domains and develop a spectrogram-based algorithm to convert the microphone data into the lower-frequency “ motion sensor data” to facilitate cross-domain comparisons. We further develop a user authentication scheme to verify that the received voice command originates from the legitimate user based on the cross-domain speech similarity of the received voice commands. We report on extensive experiments to evaluate the WearID under various audible and inaudible attacks. The results show WearID can verify voice commands with 99.8% accuracy in the normal situation and detect 97.2% fake voice commands from various attacks, including impersonation/replay attacks and hidden voice/ultrasound attacks.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {829–842},
numpages = {14},
keywords = {Motion Sensor, User Authentication, Voice Assistant Systems},
location = {Austin, USA},
series = {ACSAC '20}
}

@article{10.1007/s10845-021-01870-4,
author = {Wang, Fangjun and Yang, Zhouwang and Huang, Zhangjin and Song, Yanzhi},
title = {A multiple position-based bi-branch model for structural defect inspection},
year = {2021},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {4},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01870-4},
doi = {10.1007/s10845-021-01870-4},
abstract = {Reliable industrial defect inspection is one of the main challenges in manufacturing scenarios, especially for the inspection of structural defects. However, the cost of missing a defect is much higher than the cost of misclassifying a qualified sample, which is seldom emphasized in previous work. Thus, the purpose of our work has two folds: reduce the omission rate of defective samples; classify industrial samples correctly. To that end, in this paper, we first define a position tag for each sample, where samples with the same position tag describe the same product information. We also design the multi-position weighted-resampling (MPWR) method for extracting paired data with identical tags. Then, in order to fully learn from the paired data, we propose a multiple position-based bi-branch (MPB3) neural network architecture to perform similarity measurements and multi-classifications simultaneously. Experimental results demonstrate the effectiveness of our method and generalization capacity to data from unknown tags by comparing with other methods. For example, the proposed method achieves 2.77%/1.00% omission rates and 96.81%/99.03% weighted F-Scores on the SMT defect dataset and the motor brush holder dataset, respectively. In addition, the average running time of the method only needs 9.6&nbsp;ms, which meets requirements of cycle time in manufacturing industries. In conclusion, the omission rate of defective samples can be reduced effectively by the position-based method that consists of MPWR method and MPB3 structure, which greatly improves productivity in real production lines.},
journal = {J. Intell. Manuf.},
month = dec,
pages = {1601–1614},
numpages = {14},
keywords = {Structural defect inspection, Bi-branch, Multi-position, Similarity measurement}
}

@inproceedings{10.1007/11763864_20,
author = {Kakarontzas, George and Stamelos, Ioannis},
title = {A tactic-driven process for developing reusable components},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_20},
doi = {10.1007/11763864_20},
abstract = {True reusability of components assumes that they not only offer the functionality prescribed by their APIs, but also that they conform to a well-defined set of quality attributes so that we know if a component can be successfully reused in a new software product. One of the problems with quality attributes however is that it is hard to identify the characteristics of components that contribute to their emergence. End-user quality attributes are versatile and difficult to predict but their occurrence is not of an accidental nature. In this paper we propose a methodology for the exploration of candidate architectural tactics during component analysis and design for the achievement of desirable quality effects. Our approach is based on executable specifications of components that are augmented with the required tactic-related parameters to form a testbed for quality-driven experimentation. We believe that the proposed approach delivers both reusable components as well as reusable models.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {273–286},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@article{10.1145/1242524.1242529,
author = {Mazeika, Arturas and B\"{o}hlen, Michael H. and Koudas, Nick and Srivastava, Divesh},
title = {Estimating the selectivity of approximate string queries},
year = {2007},
issue_date = {June 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/1242524.1242529},
doi = {10.1145/1242524.1242529},
abstract = {Approximate queries on string data are important due to the prevalence of such data in databases and various conventions and errors in string data. We present the VSol estimator, a novel technique for estimating the selectivity of approximate string queries. The VSol estimator is based on inverse strings and makes the performance of the selectivity estimator independent of the number of strings. To get inverse strings we decompose all database strings into overlapping substrings of length q (q-grams) and then associate each q-gram with its inverse string: the IDs of all strings that contain the q-gram. We use signatures to compress inverse strings, and clustering to group similar signatures.We study our technique analytically and experimentally. The space complexity of our estimator only depends on the number of neighborhoods in the database and the desired estimation error. The time to estimate the selectivity is independent of the number of database strings and linear with respect to the length of query string. We give a detailed empirical performance evaluation of our solution for synthetic and real-world datasets. We show that VSol is effective for large skewed databases of short strings.},
journal = {ACM Trans. Database Syst.},
month = jun,
pages = {12–es},
numpages = {40},
keywords = {Inverse strings, min-wise hash signatures, q-grams}
}

@article{10.1016/j.micpro.2020.103142,
author = {Nouacer, R\'{e}da and Hussein, Mahmoud and Espinoza, Huascar and Ouhammou, Yassine and Ladeira, Matheus and Casti\~{n}eira, Rodrigo},
title = {Towards a framework of key technologies for drones},
year = {2020},
issue_date = {Sep 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2020.103142},
doi = {10.1016/j.micpro.2020.103142},
journal = {Microprocess. Microsyst.},
month = sep,
numpages = {13},
keywords = {Drones, Automation and Control Systems, Composition, Autonomy, Security, Safety, Interoperability}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@article{10.1145/3394602,
author = {Wang, Junjie and Yang, Ye and Menzies, Tim and Wang, Qing},
title = {iSENSE2.0: Improving Completion-aware Crowdtesting Management with Duplicate Tagger and Sanity Checker},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3394602},
doi = {10.1145/3394602},
abstract = {Software engineers get questions of “how much testing is enough” on a regular basis. Existing approaches in software testing management employ experience-, risk-, or value-based analysis to prioritize and manage testing processes. However, very few is applicable to the emerging crowdtesting paradigm to cope with extremely limited information and control over unknown, online crowdworkers. In practice, deciding when to close a crowdtesting task is largely done by experience-based guesswork and frequently results in ineffective crowdtesting. More specifically, it is found that an average of 32% testing cost was wasteful spending in current crowdtesting practice. This article intends to address this challenge by introducing automated decision support for monitoring and determining appropriate time to close crowdtesting tasks.To that end, it first investigates the necessity and feasibility of close prediction of crowdtesting tasks based on an industrial dataset. Next, it proposes a close prediction approach named iSENSE2.0, which applies incremental sampling technique to process crowdtesting reports arriving in chronological order and organizes them into fixed-sized groups as dynamic inputs. Then, a duplicate tagger analyzes the duplicate status of received crowd reports, and a CRC-based (Capture-ReCapture) close estimator generates the close decision based on the dynamic bug arrival status. In addition, a coverage-based sanity checker is designed to reinforce the stability and performance of close prediction. Finally, the evaluation of iSENSE2.0 is conducted on 56,920 reports of 306 crowdtesting tasks from one of the largest crowdtesting platforms. The results show that a median of 100% bugs can be detected with 30% saved cost. The performance of iSENSE2.0 does not demonstrate significant difference with the state-of-the-art approach iSENSE, while the later one relies on the duplicate tag, which is generally considered as time-consuming and tedious to obtain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {24},
numpages = {27},
keywords = {Crowdsourced testing, capture-recapture, close prediction, term coverage, test management}
}

@article{10.1016/j.eswa.2013.10.063,
author = {Chen, Yijun and Wong, Man-Leung and Li, Haibing},
title = {Applying Ant Colony Optimization to configuring stacking ensembles for data mining},
year = {2014},
issue_date = {May, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.10.063},
doi = {10.1016/j.eswa.2013.10.063},
abstract = {An ensemble is a collective decision-making system which applies a strategy to combine the predictions of learned classifiers to generate its prediction of new instances. Early research has proved that ensemble classifiers in most cases can be more accurate than any single component classifier both empirically and theoretically. Though many ensemble approaches are proposed, it is still not an easy task to find a suitable ensemble configuration for a specific dataset. In some early works, the ensemble is selected manually according to the experience of the specialists. Metaheuristic methods can be alternative solutions to find configurations. Ant Colony Optimization (ACO) is one popular approach among metaheuristics. In this work, we propose a new ensemble construction method which applies ACO to the stacking ensemble construction process to generate domain-specific configurations. A number of experiments are performed to compare the proposed approach with some well-known ensemble methods on 18 benchmark data mining datasets. The approach is also applied to learning ensembles for a real-world cost-sensitive data mining problem. The experiment results show that the new approach can generate better stacking ensembles.},
journal = {Expert Syst. Appl.},
month = may,
pages = {2688–2702},
numpages = {15},
keywords = {ACO, Data mining, Direct marketing, Ensemble, Metaheuristics, Stacking}
}

@article{10.1147/rd.446.0851,
author = {Kunkel, S. R. and Eickemeyer, R. J. and Lipasti, M. H. and Mullins, T. J. and O'Krafka, B. and Rosenberg, H. and VanderWiel, S. P. and Vitale, P. L. and Whitley, L. D.},
title = {A performance methodology for commercial servers},
year = {2000},
issue_date = {November 2000},
publisher = {IBM Corp.},
address = {USA},
volume = {44},
number = {6},
issn = {0018-8646},
url = {https://doi.org/10.1147/rd.446.0851},
doi = {10.1147/rd.446.0851},
abstract = {This paper discusses a methodology for analyzing and optimizing the performance of commercial servers. Commercial server workloads are shown to have unique characteristics which expand the elements that must be optimized to achieve good performance and require a unique performance methodology. The steps in the process of server performance optimization are described and include the following: 1. Selection of representative commercial workloads and identification of key characteristics to be evaluated. 2. Collection of performance data. Various instrumentation techniques are discussed in light of the requirements placed by commercial server workloads on the instrumentation. 3. Creation of input data for performance models on the basis of measured workload information. This step in the methodology must overcome the operating environment differences between the instance of the measured system under test and the target system design to be modeled. 4. Creation of performance models. Two general types are described: high-level models and detailed cycle-accurate simulators. These types are applied to model the processor, memory, and I/O system. 5. System performance optimization. The tuning of the operating system and application software is described.Optimization of performance among commercial applications is not simply an exercise in using traces to maximize the processor MIPS. Equally significant are items such as the use of probabilities to reflect future workload characteristics, software tuning, cache miss rate optimization, memory management, and I/O performance. The paper presents techniques for evaluating the performance of each of these key contributors so as to optimize the overall performance and cost/performance of commercial servers.},
journal = {IBM J. Res. Dev.},
month = nov,
pages = {851–872},
numpages = {22}
}

@article{10.1016/j.eswa.2019.05.003,
author = {Garcia, Cleiton dos Santos and Meincheim, Alex and Faria Junior, Elio Ribeiro and Dallagassa, Marcelo Rosano and Sato, Denise Maria Vecino and Carvalho, Deborah Ribeiro and Santos, Eduardo Alves Portela and Scalabrin, Edson Emilio},
title = {Process mining techniques and applications – A systematic mapping study},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.05.003},
doi = {10.1016/j.eswa.2019.05.003},
journal = {Expert Syst. Appl.},
month = nov,
pages = {260–295},
numpages = {36},
keywords = {Process mining, Workflow mining, Process mining applications, Process mining case studies}
}

@article{10.3233/FI-2018-1685,
author = {Sandhu, Jasminder Kaur and Verma, Anil Kumar and Rana, Prashant Singh},
title = {A Novel Framework for Reliable Network Prediction of Small Scale Wireless Sensor Networks (SSWSNs)},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {160},
number = {3},
issn = {0169-2968},
url = {https://doi.org/10.3233/FI-2018-1685},
doi = {10.3233/FI-2018-1685},
abstract = {In Small Scale Wireless Sensor Networks (SSWSNs), reliability is defined as the capability of a network to perform its intended task under certain conditions for a stated time span. There are many tools for modeling and analyzing the reliability of a network. As the intricacy of various networks is increasing, there is a need for many sophisticated methods for reliability analysis. The term reliability is used as an umbrella term to capture various attributes such as safety, availability, security, and ease of use. The existing methods have many shortcomings which include inadequacy of a novel framework and inefficacy to handle scalable networks. This paper presents a novel framework which predicts the overall reliability of the SSWSNs in terms of performance metrics such as, sent packets, received packets, packets forfeit, packet delivery ratio and throughput. This framework includes various phases starting with scenario generation, construction of a dataset, applying ensemble based machine learning techniques to predict the parameters which cannot be calculated. The ensemble model predicts with an optimum accuracy of 99.9% for data flow, 99.9% for the protocol used and 97.6% for the number of nodes. Finally, to check the robustness of the ensemble model 10-fold cross-validation is used. The dataset used in this work is available as a supplement at .},
journal = {Fundam. Inf.},
month = jan,
pages = {303–341},
numpages = {39},
keywords = {Small Scale Wireless Sensor Networks, Reliability, Machine Learning, Network Prediction, Ensemble}
}

@inproceedings{10.1109/ICSE.2019.00097,
author = {Wang, Junjie and Yang, Ye and Krishna, Rahul and Menzies, Tim and Wang, Qing},
title = {iSENSE: completion-aware crowdtesting management},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00097},
doi = {10.1109/ICSE.2019.00097},
abstract = {Crowdtesting has become an effective alternative to traditional testing, especially for mobile applications. However, crowdtesting is hard to manage in nature. Given the complexity of mobile applications and unpredictability of distributed crowdtesting processes, it is difficult to estimate (a) remaining number of bugs yet to be detected or (b) required cost to find those bugs. Experience-based decisions may result in ineffective crowdtesting processes, e.g., there is an average of 32% wasteful spending in current crowdtesting practices.This paper aims at exploring automated decision support to effectively manage crowdtesting processes. It proposes an approach named iSENSE which applies incremental sampling technique to process crowdtesting reports arriving in chronological order, organizes them into fixed-size groups as dynamic inputs, and predicts two test completion indicators in an incremental manner. The two indicators are: 1) total number of bugs predicted with Capture-ReCapture model, and 2) required test cost for achieving certain test objectives predicted with AutoRegressive Integrated Moving Average model. The evaluation of iSENSE is conducted on 46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting platforms in China. Its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi-automation of task closing trade-off analysis. The results show that iSENSE can provide managers with greater awareness of testing progress to achieve cost-effectiveness gains of crowdtesting. Specifically, a median of 100% bugs can be detected with 30% saved cost based on the automated close prediction.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {912–923},
numpages = {12},
keywords = {automated close prediction, crowdtesting, crowdtesting management, test completion},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1016/j.specom.2021.09.004,
author = {Ngo, Thuanvan and Kubo, Rieko and Akagi, Masato},
title = {Increasing speech intelligibility and naturalness in noise based on concepts of modulation spectrum and modulation transfer function},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {135},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.09.004},
doi = {10.1016/j.specom.2021.09.004},
journal = {Speech Commun.},
month = dec,
pages = {11–24},
numpages = {14},
keywords = {Modulation transfer function, Modulation spectrum, Intelligibility}
}

@inproceedings{10.1007/978-3-030-58545-7_8,
author = {Wang, Hu and Wu, Qi and Shen, Chunhua},
title = {Soft Expert Reward Learning for Vision-and-Language Navigation},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_8},
doi = {10.1007/978-3-030-58545-7_8},
abstract = {Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {126–141},
numpages = {16},
keywords = {Soft expert distillation, Self perceiving reward, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@article{10.1016/j.dsp.2019.06.007,
author = {Bj\"{o}rnson, Emil and Sanguinetti, Luca and Wymeersch, Henk and Hoydis, Jakob and Marzetta, Thomas L.},
title = {Massive MIMO is a reality—What is next?},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2019.06.007},
doi = {10.1016/j.dsp.2019.06.007},
journal = {Digit. Signal Process.},
month = nov,
pages = {3–20},
numpages = {18},
keywords = {Massive MIMO, Future directions, Communications, Positioning and radar, Machine learning}
}

@article{10.1016/j.jss.2011.06.004,
author = {Ahmed, Bestoun S. and Zamli, Kamal Z.},
title = {A variable strength interaction test suites generation strategy using Particle Swarm Optimization},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.004},
doi = {10.1016/j.jss.2011.06.004},
abstract = {This paper highlights a novel strategy for generating variable-strength (VS) interaction test suites, called VS Particle Swarm Test Generator (VS-PSTG). As the name suggests, VS-PSTG adopts Particle Swarm Optimization to ensure optimal test size reduction. To determine its efficiency in terms of the size of the generated test suite, VS-PSTG was subjected to well-known benchmark configurations. Comparative results indicate that VS-PSTG gives competitive results as compared to existing strategies. An empirical case study was conducted on a non-trivial software system to show the applicability of the strategy and to determine the effectiveness of the generated test suites to detect faults.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2171–2185},
numpages = {15},
keywords = {Artificial intelligence, Particle Swarm Optimization, Search-based Software Testing, Software testing, Variable-strength interaction}
}

@inproceedings{10.1145/3139958.3140044,
author = {Jiang, Zhe and Li, Yan and Shekhar, Shashi and Rampi, Lian and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity: A Summary of Results},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140044},
doi = {10.1145/3139958.3140044},
abstract = {Class ambiguity refers to the phenomenon whereby samples with similar features belong to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. SEL problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost (finding an optimal zone partition is NP-hard). Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble), and thus cannot effectively minimize class ambiguity. In contrast, our spatial ensemble framework explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {23},
numpages = {10},
keywords = {Spatial classification, class ambiguity, local models, spatial ensemble, spatial heterogeneity},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@article{10.1145/3337798,
author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337798},
doi = {10.1145/3337798},
abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {43},
numpages = {25},
keywords = {Spatial classification, class ambiguity, local models, spatial ensemble, spatial heterogeneity}
}

@article{10.1016/j.asoc.2016.05.015,
author = {Dou, Dongyang and Zhou, Shishuai},
title = {Comparison of four direct classification methods for intelligent fault diagnosis of rotating machinery},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.015},
doi = {10.1016/j.asoc.2016.05.015},
abstract = {Display Omitted A rule-based method was proposed based on MLEM2 and enhanced by a new rule reasoning mechanism.Eight time-domain and five dimensionless frequency-domain parameters were adopted.The proposed method had the ability of feature reduction.The proposed method was an all-rounder compared with KNN, PNN and PSO-SVM as it was very friendly. Condition monitoring of rotating machinery is important to promptly detect early faults, identify potential problems, and prevent complete failure. Four direct classification methods were introduced to diagnose the regular condition, inner race defect, outer race defect, and rolling element defect of rolling bearings. These include the K-Nearest Neighbor algorithm (KNN), Probabilistic Neural Network (PNN), Particle Swarm Optimization optimized Support Vector Machine (PSO-SVM) and a Rule-Based Method (RBM) based on the MLEM2 algorithm and a new Rule Reasoning Mechanism (RRM). All of them can be run on the Fault Decision Table (FDT) containing numerical variables and output fault categories directly. The diagnosis results were discussed in terms of accuracy, time consumption, intelligibility, and maintainability. Especially, the interactions of the systems and human experts were compared in detail. It was concluded that all the four methods can work satisfactorily on accuracy, in an order of the PSO-SVM ranking the first, followed by the RBM that functioned the friendliest. Moreover, the RBM had the ability of feature reduction by itself, and would be most suitable for real-time applications.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {459–468},
numpages = {10},
keywords = {Fault diagnosis, PNN, Rotating machinery, Rule, SVM}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Pi\~{n}ero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Acoustic sensor networks, Urban sound classification, FIWARE, Edge computing}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1016/j.jvcir.2020.102913,
author = {Wang, Yongfang and Ye, Peng and Xia, Yumeng and An, Ping},
title = {A heuristic framework for perceptual saliency prediction},
year = {2020},
issue_date = {Nov 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2020.102913},
doi = {10.1016/j.jvcir.2020.102913},
journal = {J. Vis. Comun. Image Represent.},
month = nov,
numpages = {13},
keywords = {Saliency prediction, Orientation selectivity, Visual acuity, Visual error sensitivity, Free energy principle}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.1007/s11042-017-5483-2,
author = {Wu, Jun and Xia, Zhaoqiang and Li, Huifang and Sun, Kezheng and Gu, Ke and Lu, Hong},
title = {No-reference image quality assessment with center-surround based natural scene statistics},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {16},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5483-2},
doi = {10.1007/s11042-017-5483-2},
abstract = {In this paper, we propose an efficient no-reference image quality assessment (NR-IQA) method dubbed Center-Surround based Blind Image Quality Assessment (CS-BIQA). Our proposed method employs the Difference of Gaussian (DoG) model to decompose images into several frequency bands, considering the center-surround effect and multi-channel attribute of human visual system (HVS). The integrated natural scene statistics (NSS) features can be further derived from all DoG bands. After that, regression models between the integrated features and associated subjective assessment scores are learned on the training dataset. Subsequently, the learned models are used to predict the quality scores of test images. The main contribution of this paper is twofold. Firstly, the empirical distributions of DoG bands of images are proven to be a Gaussian-like distribution. And thus, the NSS features can be employed to represent the perceptual quality of images. Secondly, different types of distortions are observed to affect different frequency components of images. So, the integrated features extracted from multi-frequency bands are employed in CS-BIQA to achieve stronger distinguishable capability of image quality. Excessive experiments are conducted to indicate that our proposed CS-BIQA metric can represent the perceptual characteristics of HVS. The results on popular IQA databases demonstrate that the CS-BIQA metric is competitive with the state-of-the-art relevant IQA metrics. Furthermore, our proposed method has very low computational complexity, making it more suitable for real-time applications.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {20731–20751},
numpages = {21},
keywords = {Center-surround, Difference of Gaussian, Image quality assessment, Natural scene statistics, Support vector regression}
}

@article{10.4018/jdm.2011010102,
author = {Lin, Zhangxi and Xu, Bo and Xu, Yan},
title = {A Study of Open Source Software Development from Control Perspective},
year = {2011},
issue_date = {January 2011},
publisher = {IGI Global},
address = {USA},
volume = {22},
number = {1},
issn = {1063-8016},
url = {https://doi.org/10.4018/jdm.2011010102},
doi = {10.4018/jdm.2011010102},
abstract = {Open source software OSS has achieved great success and exerted significant impact on the software industry. OSS development takes online community as its organizational form, and developers voluntarily work for the project. In the project execution process, control aligns individual behaviors toward the organizational goals via the Internet and becomes critical to the success of OSS projects. This paper investigates the control modes in OSS project communities, and their effects on project performance. Based on a web survey and archival data from OSS projects, it is revealed that three types of control modes, that is, outcome, clanship, and self-control, are effective in an OSS project community. The study contributes to a better understanding of OSS project organizations and processes, and provides advice for OSS development.},
journal = {J. Database Manage.},
month = jan,
pages = {26–42},
numpages = {17},
keywords = {Control, OSS Development, Open Source Software OSS, Project Management, Software Development}
}

@inproceedings{10.1145/3485730.3485948,
author = {Ye, Hanting and Wang, Qing},
title = {SpiderWeb: Enabling Through-Screen Visible Light Communication},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485948},
doi = {10.1145/3485730.3485948},
abstract = {We are now witnessing a trend of realizing full-screen on electronic devices such as smartphones to maximize their screen-to-body ratio for a better user experience. Thus the bezel/narrow-bezel on today's devices to host various line-of-sight sensors would disappear. This trend not only is forcing sensors like the front cameras to be placed under the screen of devices, but also will challenge the deployment of the emerging Visible Light Communication (VLC) technology, a paradigm for the next-generation wireless communication.In this work, we propose the concept of through-screen VLC with photosensors placed under Organic Light-Emitting Diode (OLED) screen. Though being transparent, an OLED screen greatly attenuates the intensity of passing-through light, degrading the efficiency of intensity-based VLC systems. In this paper, we instead exploit the color domain to build SpiderWeb, a through-screen VLC system. For the first time, we observe that an OLED screen introduces a color-pulling effect at photosensors, affecting the decoding of color-based VLC signals. Motivated by this observation and by the structure of spider's web, we design the SWebCSK Color-Shift Keying modulation scheme and a slope-based demodulation method, which can eliminate the color-pulling effect. We prototype SpiderWeb with off-the-shelf hardware and evaluate its performance thoroughly under various scenarios. The results show that compared to existing solutions, our solutions can reduce the bit error rate by two orders of magnitude and can achieve a 3.4x data rate.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {316–328},
numpages = {13},
keywords = {Through-screen VLC, color-pulling effect, transparent OLED screen},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1137/19M1308669,
author = {Brown, Jed and He, Yunhui and MacLachlan, Scott and Menickelly, Matt and Wild, Stefan M.},
title = {Tuning Multigrid Methods with Robust Optimization and Local Fourier Analysis},
year = {2021},
issue_date = {January 2021},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {43},
number = {1},
issn = {1064-8275},
url = {https://doi.org/10.1137/19M1308669},
doi = {10.1137/19M1308669},
abstract = {Local Fourier analysis is a useful  tool for predicting  and analyzing the performance of many efficient
algorithms for the solution of discretized PDEs, such as multigrid and domain decomposition methods. The
crucial aspect  of local Fourier analysis is  that it can be used to minimize an estimate of the
spectral radius of a stationary iteration, or the condition number of a preconditioned system, in terms
of a symbol representation of the algorithm. In practice, this is a “minimax” problem, minimizing with
respect to solver parameters the appropriate measure of solver work, which involves maximizing over the
Fourier frequency. Often, several algorithmic parameters may be determined by local Fourier analysis in
order to obtain efficient algorithms. Analytical solutions to minimax problems are rarely possible beyond
simple problems; the status quo in local Fourier analysis involves grid sampling, which is prohibitively
expensive in high dimensions. In this paper, we propose and explore optimization algorithms to solve
these problems efficiently. Several examples, with known and unknown analytical solutions, are presented
to show the effectiveness of these approaches.},
journal = {SIAM J. Sci. Comput.},
month = jan,
pages = {A109–A138},
numpages = {30},
keywords = {local Fourier analysis, minimax problem, multigrid methods, robust optimization, 47N40, 65M55, 90C26, 49Q10}
}

@article{10.1007/s11042-019-7478-7,
author = {Liu, Yuling},
title = {RETRACTED ARTICLE: Research on multimedia play mode and image optimization based on compensation factor adaptive model},
year = {2020},
issue_date = {Apr 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {13–14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7478-7},
doi = {10.1007/s11042-019-7478-7},
abstract = {When traditional multimedia network video image is compressed and transmitted to compensate, because of the different loss of video image features in the acquisition process, the error of compressed transmission compensation is large and the efficiency is low. Firstly, the NLMS algorithm and the improved NLMS algorithm are analyzed. To solve the problem that the compensation factor in the algorithm is too large due to the severe network shake, the NLMS algorithm is further improved by adaptively adjusting the compensation factor coefficient with the change of the network and the prediction error. The research shows that the multimedia playback mode and the image optimization system software structure based on the compensation factor adaptive model realize the dynamic adaptation of the system to various network conditions and the image optimization function during video playback by adopting the bandwidth adaptive strategy and method.. The conclusion shows that in the multimedia network video image compression transmission, the improved compensation method has higher performance in multimedia network video image optimization and real-time compression transmission compensation, which has certain advantages compared with the traditional compensation method.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {9315–9330},
numpages = {16},
keywords = {Compensation factor self-adaptation, Multimedia playback, Image optimization}
}

@article{10.1145/3457139,
author = {Wu, Fang-jing and Chen, Ying-Jun and Sou, Sok-Ian},
title = {CoCo: Quantifying Correlations between Mobility Traces using Sensor Data from Smartphones},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3457139},
doi = {10.1145/3457139},
abstract = {As mobility is an important key to many applications, this work proposes a location-less model to represent mobility that is used to quantify correlations between mobility traces collected by built-in sensors on smartphones. We analyze the mobility correlations from two aspects: co-direction relationship and co-movement relationship. The former is to quantify the similarity of macroscopic moving directions between mobility traces, whereas the latter is to quantify the similarity of their microscopic vibrations. To verify the merits of the two proposed metrics, an exemplary use case, termed co-mobility detection, is considered to determine if two mobile devices share the same journey on the same mobile entity (e.g., carried by the same person). Comprehensive experiments with diverse combinations of mobility traces are conducted in three different environments with different density of Wi-Fi networks. The experimental results indicate that the proposed metrics can effectively evaluate both the coarse-grained similarity of moving directions and the fine-grained similarity of movement variations along mobility traces. The accuracy of the co-mobility detection algorithm can achieve 90% on average for mobility traces with a duration of 70 s.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {20},
numpages = {22},
keywords = {Cyber-physical systems, mobile sensing, mobility analytics, smart cities, urban computing}
}

@article{10.1145/1883612.1883618,
author = {Nie, Changhai and Leung, Hareton},
title = {A survey of combinatorial testing},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/1883612.1883618},
doi = {10.1145/1883612.1883618},
abstract = {Combinatorial Testing (CT) can detect failures triggered by interactions of parameters in the Software Under Test (SUT) with a covering array test suite generated by some sampling mechanisms. It has been an active field of research in the last twenty years. This article aims to review previous work on CT, highlights the evolution of CT, and identifies important issues, methods, and applications of CT, with the goal of supporting and directing future practice and research in this area. First, we present the basic concepts and notations of CT. Second, we classify the research on CT into the following categories: modeling for CT, test suite generation, constraints, failure diagnosis, prioritization, metric, evaluation, testing procedure and the application of CT. For each of the categories, we survey the motivation, key issues, solutions, and the current state of research. Then, we review the contribution from different research groups, and present the growing trend of CT research. Finally, we recommend directions for future CT research, including: (1) modeling for CT, (2) improving the existing test suite generation algorithm, (3) improving analysis of testing result, (4) exploring the application of CT to different levels of testing and additional types of systems, (5) conducting more empirical studies to fully understand limitations and strengths of CT, and (6) combining CT with other testing techniques.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {11},
numpages = {29},
keywords = {Software testing, combinatorial testing (CT), covering array, test case generation}
}

@inproceedings{10.1145/2627373.2627385,
author = {Li, Shuo and Geva, Robert},
title = {Extract and Extend Parallelism using C/C++ Extension for Array Notation on Multicore and Many-core Platforms: An Empirical Investigation with Quantitative Finance Examples},
year = {2014},
isbn = {9781450329378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627373.2627385},
doi = {10.1145/2627373.2627385},
abstract = {In this paper, we explore the newly introduced array notion syntax extension in recent release of Intel Compiler with a few representative quantitative finance workloads. We will explore the array syntax both as an abstraction tool to allow the user to succinctly express the intended operations and as a performance tool that facilitates the most efficient implementation which can take advantage of the parallel hardware resource such as vector processing units and ever increasing number of processor cores. We specifically look at how these new array style programming capability can help the financial modeler and software developers to extract the parallelism from the numerical algorithm and extend it from multicore host processor to a hybrid of multicore many-core accelerated computing environment.We start with a functional introduction to the C++ array notation syntax that will be used in the subsequent examples. We, then, present background information on a few derivative pricing algorithms in quantitative finance. For each algorithm, we present a scalar program first and take a performance measurement as baseline. As we choose to use the array programming mechanism, we will look at the programming language related issues and postulate what syntax motivate the developer to use then what the alternative syntax are and why some might be more popular than others. Then we look at the performance related issues and look at the code generation on Intel Architecture based multicore and many-core platforms, and investigate mechanism for performance optimization. We conclude the paper by creating a hybrid program that runs both on multicore and many-core environment, concurrently.},
booktitle = {Proceedings of ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {68–75},
numpages = {8},
location = {Edinburgh, United Kingdom},
series = {ARRAY'14}
}

@article{10.1007/s11042-013-1743-y,
author = {Chen, Jian and Chen, Yunzheng and Qin, Dong and Kuo, Yonghong},
title = {An elastic net-based hybrid hypothesis method for compressed video sensing},
year = {2015},
issue_date = {March     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-013-1743-y},
doi = {10.1007/s11042-013-1743-y},
abstract = {Compressed Sensing, an emerging framework for signal processing, can be used in image and video application, especially when available resources at the transmitter side are limited, such as Wireless Multimedia Sensor Networks. For a low-cost and low-power demand, we consider the plain compressive sampling and low sampling rates and propose a Compressed Video Sensing scheme. As a result, most burden of video processing can be shifted to the decoder which employs a hybrid hypothesis prediction method in reconstruction. The Elastic net-based multi-hypothesis mode, one part of the prediction method, combines the multi-hypothesis prediction and the elastic net regression together. And in the process of decoding, either this mode or the single-hypothesis one is implemented based on the threshold which is selected from [1e-11, 1). Both of the prediction modes are carried out in the measurement domain and a residual reconstruction as the final step is executed to accomplish the recovery. According to the performance presented by the simulation results, the proposed multi-hypothesis mode provides a better reconstruction quality than the other multi-hypothesis ones and the proposed scheme outperforms the observed state-of-the-art schemes for compressed-sensing video reconstruction at low sampling rates.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {2085–2108},
numpages = {24},
keywords = {Compressed sensing, Distributed video coding, Elastic net, Hypothesis prediction, Wireless multimedia sensor networks}
}

@article{10.1016/j.future.2019.05.056,
author = {Bagozi, Ada and Bianchini, Devis and De Antonellis, Valeria and Garda, Massimiliano and Marini, Alessandro},
title = {A Relevance-based approach for Big Data Exploration},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.05.056},
doi = {10.1016/j.future.2019.05.056},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {51–69},
numpages = {19},
keywords = {Data exploration, Big data, Multi-dimensional data modelling, Human-In-the-Loop Data Analysis, Industry 4.0, Cyber Physical Systems}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Activity recognition, Distance minimization, Pervasive healthcare, Probability estimation, Smart homes, Support vector machine}
}

@article{10.1007/s10044-016-0578-3,
author = {Sun, Meesun and Cho, Sungzoon},
title = {Obtaining calibrated probability using ROC Binning},
year = {2018},
issue_date = {May 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-016-0578-3},
doi = {10.1007/s10044-016-0578-3},
abstract = {Obtaining calibrated probability, or actual occurrence, is crucial in many real problems because it effectively supports the decision-making process with good assessment of cost and effect. Estimating calibrated probability is a more significant issue in class imbalance and class overlap problems, where direct application of classification algorithms may result in substantial errors. Consequently, several post-processing calibration techniques that aim at improving the probability estimation or the error distribution of existing classification models have been developed. In this underlying context, we propose Receiver Operating Characteristics Binning, a robust method that provides accurate calibrated probabilities that are robust to changes in the prevalence of the positive class by using a combination of True Positive Rate, False Positive Rate, and the prevalence of the positive class. The results of experiments conducted on the real-world UCI dataset indicate that, given a training set in which the positive class proportion is noticeably different from that of the test set, the proposed ROC Binning method outperforms conventional calibration methods.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {307–322},
numpages = {16},
keywords = {Calibrated probability, Calibration method, Class imbalance, Class overlap, Positive proportion, ROC curve}
}

@inproceedings{10.1007/11559573_147,
author = {Gong, Minglun and Langille, Aaron and Gong, Mingwei},
title = {Real-Time image processing using graphics hardware: a performance study},
year = {2005},
isbn = {3540290699},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11559573_147},
doi = {10.1007/11559573_147},
abstract = {Programmable graphics hardware have proven to be a powerful resource for general computing. Previous research has shown that using a GPU for local image processing operations can be much faster than using a CPU. The actual speedup obtained is influenced by many factors. In this paper, we quantify the performance gain that can be achieved by using the GPU for different image processing operations under different conditions. We also compare the strengths and weaknesses of two of the current leaders in mainstream GPUs – ATI's Radeon and nVidia's GeForce FX. Many interesting observations are obtained through the evaluation.},
booktitle = {Proceedings of the Second International Conference on Image Analysis and Recognition},
pages = {1217–1225},
numpages = {9},
location = {Toronto, Canada},
series = {ICIAR'05}
}

@article{10.5555/1349897.1350159,
author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
title = {Software architecture reliability analysis using failure scenarios},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
journal = {J. Syst. Softw.},
month = apr,
pages = {558–575},
numpages = {18},
keywords = {FMEA, Fault trees, Reliability analysis, Scenario-based architectural evaluation}
}

@article{10.1007/s11042-015-2975-9,
author = {Su, Kaixiong and Chen, Jian and Wang, Weixing and Su, Lichao},
title = {Reconstruction algorithm for block-based compressed sensing based on mixed variational inequality},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {23},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-015-2975-9},
doi = {10.1007/s11042-015-2975-9},
abstract = {Block compressed sensing based on mixed variational inequality (BCS-MVI) is proposed to improve the performance of current reconstruction algorithms for block-based compressed sensing. In the measurement phase, an image is sampled block by block. In the recovery period, BCS-MVI takes the sparse regularization of the natural image as prior knowledge and approaches the target function within the entire image through the modified augmented Lagrange method (ALM) and alternating direction method (ADM) of multipliers. Moreover, for the reconstruction problem including two regularization terms, an adaptive weight (\'{z}AW) strategy based on the gray entropy of the initialized image is studied. BCS-MVI achieves an average PSNR gain of 0.5---2.0 dB and an SSIM gain of 0.02---0.05 over previous block-based compressed sensing methods, and the reconstructing time only slightly fluctuates with the sampling rate. The algorithm is suitable for applications in multimedia data processing with fixed transmission delays.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {16417–16438},
numpages = {22},
keywords = {Alternating direction method, Block-based compressed sensing, Image reconstruction, Mixed variational inequality}
}

@inproceedings{10.1145/3371158.3371189,
author = {Pal, Bithika and Jenamani, Mamata},
title = {Personalized Ranking in Collaborative Filtering: Exploiting l-th Order Transitive Relations of Social Ties},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371189},
doi = {10.1145/3371158.3371189},
abstract = {The use of social information in collaborative filtering is highly encouraged, as it can improve the recommendation accuracy by handling the cold start issue. The intuition of social recommendation is to reflect one's personal choice by its social neighbors. Though there exists a considerable amount of studies in this domain, no attention is paid to incorporate the transitive relationships of social ties in the ranking problem. In this paper, we exploit the lth order transitive relations of a user and extend the popular Social Bayesian Personalized Ranking (SBPR) model. The use of transitive relation creates a more granular pairwise ranking of items for a particular user and levels the user's personal choice based on the order of its social neighbors. We implement the model and conduct experiments on two real-world recommendation datasets with different values of l. We show that our model outperforms state-of-the-art pairwise ranking techniques.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {214–218},
numpages = {5},
keywords = {Personalization, Ranking, Recommendation, Social Network},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3474085.3475335,
author = {Zhang, Ji and Song, Jingkuan and Yao, Yazhou and Gao, Lianli},
title = {Curriculum-Based Meta-learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475335},
doi = {10.1145/3474085.3475335},
abstract = {Meta-learning offers an effective solution to learn new concepts with scarce supervision through an episodic training scheme: a series of target-like tasks sampled from base classes are sequentially fed into a meta-learner to extract common knowledge across tasks, which can facilitate the quick acquisition of task-specific knowledge of the target task with few samples. Despite its noticeable improvements, the episodic training strategy samples tasks randomly and uniformly, without considering their hardness and quality, which may not progressively improve the meta-leaner's generalization ability. In this paper, we present a Curriculum-Based Meta-learning (CubMeta) method to train the meta-learner using tasks from easy to hard. Specifically, the framework of CubMeta is in a progressive way, and in each step, we design a module named BrotherNet to establish harder tasks and an effective learning scheme for obtaining an ensemble of stronger meta-learners. In this way, the meta-learner's generalization ability can be progressively improved, and better performance can be obtained even with fewer training tasks. We evaluate our method for few-shot classification on two benchmarks - mini-ImageNet and tiered-ImageNet, where it achieves consistent performance improvements on various meta-learning paradigms.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {curriculum learning, few-shot learning, meta-learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1007/s42979-020-00323-8,
author = {Shafik, Wasswa and Matinkhah, S. Mojtaba and Ghasemzadeh, Mohammad},
title = {Theoretical Understanding of Deep Learning in UAV Biomedical Engineering Technologies Analysis},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00323-8},
doi = {10.1007/s42979-020-00323-8},
abstract = {The unmanned aerial vehicles (UAVs) emerged into a promising research trend within the recurrent year where current and future networks are to use enhanced connectivity in these digital immigrations in different fields like medical, communication, search, and rescue operations among others. The current technologies are using fixed base stations to operate on-site and off-site in the fixed position with its associated problems like poor connectivity. This opens gates for the UAVs technology to be used as a mobile alternative to increase accessibility with a fifth-generation (5G) connectivity that focuses on increased availability and connectivity. There has been less usage of wireless technologies in the medical field. This paper first presents a study on deep learning to medical field application in general, and provides detailed steps that are involved in the multi-armed bandit approach in solving UAV biomedical engineering technologies devices and medical exploration to exploitation dilemma. The paper further presents a detailed description of the bandit network applicability to achieve close optimal medical engineered devices’ performance and efficiency. The simulated results depicted that a multi-armed bandit problem approach can be applied in optimizing the performance of any medical networked device issue compared to the Thompson sampling, Bayesian algorithm, and ε-greedy algorithm. The results obtained further illustrated the optimized utilization of biomedical engineering technologies systems achieving thus close optimal performance on the average period through deep learning of realistic medical situations.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {13},
keywords = {Deep learning, Biomedical technology, Unmanned aerial vehicles}
}

@article{10.1145/2647954,
author = {Schneider, Reinhard and Goswami, Dip and Chakraborty, Samarjit and Bordoloi, Unmesh and Eles, Petru and Peng, Zebo},
title = {Quantifying Notions of Extensibility in FlexRay Schedule Synthesis},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/2647954},
doi = {10.1145/2647954},
abstract = {FlexRay has now become a well-established in-vehicle communication bus at most original equipment manufacturers (OEMs) such as BMW, Audi, and GM. Given the increasing cost of verification and the high degree of crosslinking between components in automotive architectures, an incremental design process is commonly followed. In order to incorporate FlexRay-based designs in such a process, the resulting schedules must be extensible, that is: (i) when messages are added in later iterations, they must preserve deadline guarantees of already scheduled messages, and (ii) they must accommodate as many new messages as possible without changes to existing schedules. Apart from extensible scheduling having not received much attention so far, traditional metrics used for quantifying them cannot be trivially adapted to FlexRay schedules. This is because they do not exploit specific properties of the FlexRay protocol. In this article we, for the first time, introduce new notions of extensibility for FlexRay that capture all the protocol-specific properties. In particular, we focus on the dynamic segment of FlexRay and we present a number of metrics to quantify extensible schedules. Based on the introduced metrics, we propose strategies to synthesize extensible schedules and compare the results of different scheduling algorithms. We demonstrate the applicability of the results with industrial-size case studies and also show that the proposed metrics may also be visually represented, thereby allowing for easy interpretation.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {32},
numpages = {37},
keywords = {FlexRay, automotive, extensibility, schedule synthesis}
}

@article{10.1504/IJEF.2013.058604,
author = {Pandey, Trilok Nath and Jagadev, Alok Kumar and Choudhury, D. and Dehuri, Satchidananda},
title = {Machine learning-based classifiers ensemble for credit risk assessment},
year = {2013},
issue_date = {January 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {3/4},
issn = {1746-0069},
url = {https://doi.org/10.1504/IJEF.2013.058604},
doi = {10.1504/IJEF.2013.058604},
abstract = {Credit risk assessment is acting as a survival weapon in almost every financial institution. It involves deep and sensitive analysis of various financial, social, demographic and other pertinent data provided by the customers and about the customers for building a more accurate and robust electronic finance system. The classification problem is one of the major concerned in the process of analysing gamut of data; however, its complexity has ignited us to use machine learning-based approaches. In this paper, some machine learning algorithms have been studied and compared their effectiveness for credit risk assessment. Further, as an extension of our study, we develop a novel sliding window-based meta-majority voting ensemble learning to improve the prediction accuracy of credit risk assessment problem by properly analysing the underlying samples. The experimental findings draw a clear line between the proposed ensembler and traditional ensemblers. Moreover, the proposed method is very promising vis-\`{a}-vis of individual classifiers.},
journal = {Int. J. Electron. Financ.},
month = jan,
pages = {227–249},
numpages = {23}
}

@inproceedings{10.1145/2623330.2623347,
author = {Zheng, Li and Zeng, Chunqiu and Li, Lei and Jiang, Yexi and Xue, Wei and Li, Jingxuan and Shen, Chao and Zhou, Wubai and Li, Hongtai and Tang, Liang and Li, Tao and Duan, Bing and Lei, Ming and Wang, Pengnian},
title = {Applying data mining techniques to address critical process optimization needs in advanced manufacturing},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623347},
doi = {10.1145/2623330.2623347},
abstract = {Advanced manufacturing such as aerospace, semi-conductor, and flat display device often involves complex production processes, and generates large volume of production data. In general, the production data comes from products with different levels of quality, assembly line with complex flows and equipments, and processing craft with massive controlling parameters. The scale and complexity of data is beyond the analytic power of traditional IT infrastructures. To achieve better manufacturing performance, it is imperative to explore the underlying dependencies of the production data and exploit analytic insights to improve the production process. However, few research and industrial efforts have been reported on providing manufacturers with integrated data analytical solutions to reveal potentials and optimize the production process from data-driven perspectives.In this paper, we design, implement and deploy an integrated solution, named PDP-Miner, which is a data analytics platform customized for process optimization in Plasma Display Panel (PDP) manufacturing. The system utilizes the latest advances in data mining technologies and Big Data infrastructures to create a complete analytical solution. Besides, our proposed system is capable of supporting automatically configuring and scheduling analysis tasks, and balancing heterogeneous computing resources. The system and the analytic strategies can be applied to other advanced manufacturing fields to enable complex data analysis tasks. Since 2013, PDP-Miner has been deployed as the data analysis platform of ChangHong COC. By taking the advantages of our system, the overall PDP yield rate has increased from 91% to 94%. The monthly production is boosted by 10,000 panels, which brings more than 117 million RMB of revenue improvement per year.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1739–1748},
numpages = {10},
keywords = {advanced manufacturing, big data, data mining platform, process optimization},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3351239,
author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
title = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351239},
doi = {10.1145/3351239},
abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {81},
numpages = {24},
keywords = {Acoustic, authentication, biometric, ear canal, echo, wearable devices}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Person re-identification, Action recognition, Gait recognition, Video matching, Temporal sequence matching, Spatio-temporal pyramids, Time shift}
}

@inproceedings{10.1145/3180155.3180235,
author = {Huang, Xin and Zhang, He and Zhou, Xin and Babar, Muhammad Ali and Yang, Song},
title = {Synthesizing qualitative research in software engineering: a critical review},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180235},
doi = {10.1145/3180155.3180235},
abstract = {Synthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confirm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a significant amount of missing knowledge and incomplete understanding of the defined QRS methods in the community. Our effort also identifies an initial set factors that may influence the selection and use of appropriate QRS methods in SE.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1207–1218},
numpages = {12},
keywords = {evidence-based software engineering, qualitative (synthesis) methods, research synthesis, systematic (literature) review},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.csl.2019.04.005,
author = {Jassim, Wissam A. and Zilany, Muhammad S.},
title = {NSQM: A non-intrusive assessment of speech quality using normalized energies of the neurogram},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {58},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2019.04.005},
doi = {10.1016/j.csl.2019.04.005},
journal = {Comput. Speech Lang.},
month = nov,
pages = {260–279},
numpages = {20},
keywords = {Speech quality assessment, PESQ, POLQA, Neurogram, Auditory-nerve model, Discrete Wavelet transform}
}

@article{10.1016/j.patcog.2017.04.024,
author = {Ren, Jianfeng and Jiang, Xudong},
title = {Regularized 2-D complex-log spectral analysis and subspace reliability analysis of micro-Doppler signature for UAV detection},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.04.024},
doi = {10.1016/j.patcog.2017.04.024},
abstract = {The proposed 2-D regularized complex-log-Fourier transform better represents mDS.The proposed subspace reliability analysis better removes unreliable dimensions.The proposed approach demonstrates superior performance for UAV detection. Unmanned aerial vehicle (UAV) has become an important radar target recently because of its wide applications and potential security threats. Traditionally, visual features such as spectrogram were often extracted for human operators to identify the micro-Doppler signature (mDS) of UAVs, i.e. sinusoidal modulation. In this paper, the authors aim to design a system for machine automatic classification of UAVs from other targets, particularly from birds as both UAVs and birds are small and slow-moving radar targets. Most existing mDS representations such as spectrogram, cepstrogram and cadence velocity diagram discard the phase spectrum, and only make use of the magnitude spectrum. Whats more, people often take the logarithm of the spectrum to enlarge the weak mDS but without sufficient care, as noise may be enlarged at the same time. The authors thus propose a regularized 2-D complex-log-Fourier transform to address these problems. Furthermore, the authors propose an object-oriented dimension-reduction technique: subspace reliability analysis, which directly removes the unreliable feature dimensions of two class-conditional covariance matrices in two separate subspaces. On the benchmark dataset, the proposed approach demonstrates better performance than the state-of-the-art approaches. More specifically, the proposed approach significantly reduces the equal error rate of the second best approach, cadence velocity diagram, from 6.68% to 3.27%.},
journal = {Pattern Recogn.},
month = sep,
pages = {225–237},
numpages = {13},
keywords = {2-D regularized complex-log-Fourier transform, Micro-Doppler signature, Radar, Subspace reliability analysis, UAV detection}
}

@article{10.1016/j.parco.2014.07.004,
author = {Coetzee, P. and Leeke, M. and Jarvis, S.},
title = {Towards unified secure on- and off-line analytics at scale},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {10},
issn = {0167-8191},
url = {https://doi.org/10.1016/j.parco.2014.07.004},
doi = {10.1016/j.parco.2014.07.004},
abstract = {A domain specific language and runtime models for on- and off-line data analytics.Detailed analysis of CRUCIBLE's runtime performance in state-of-the-art environments.Development and detailed analysis of a set of runtime models for new environments.Performance comparison with native implementations demonstrating a 14 average performance gap.Formulation of a primitive in the DSL that permits an analytic to be run over multiple data sources. Data scientists have applied various analytic models and techniques to address the oft-cited problems of large volume, high velocity data rates and diversity in semantics. Such approaches have traditionally employed analytic techniques in a streaming or batch processing paradigm. This paper presents CRUCIBLE, a first-in-class framework for the analysis of large-scale datasets that exploits both streaming and batch paradigms in a unified manner. The CRUCIBLE framework includes a domain specific language for describing analyses as a set of communicating sequential processes, a common runtime model for analytic execution in multiple streamed and batch environments, and an approach to automating the management of cell-level security labelling that is applied uniformly across runtimes. This paper shows the applicability of CRUCIBLE to a variety of state-of-the-art analytic environments, and compares a range of runtime models for their scalability and performance against a series of native implementations. The work demonstrates the significant impact of runtime model selection, including improvements of between 2.3 and 480 between runtime models, with an average performance gap of just 14 between CRUCIBLE and a suite of equivalent native implementations.},
journal = {Parallel Comput.},
month = dec,
pages = {738–753},
numpages = {16},
keywords = {Analytics, Data intensive computing, Data science, Domain specific languages, Hadoop, Streaming analysis}
}

@article{10.1287/mnsc.2019.3442,
author = {Abernethy, Margaret A. and Dekker, Henri C. and Grafton, Jennifer},
title = {The Influence of Performance Measurement on the Processual Dynamics of Strategic Change},
year = {2021},
issue_date = {January 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {67},
number = {1},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2019.3442},
doi = {10.1287/mnsc.2019.3442},
abstract = {We draw on a five-year longitudinal data set to investigate the influence of performance measurement in the processual dynamics of strategic change, particularly in enacting effective strategic change. Our model examines the role of performance measurement in driving strategy-consistent operational changes and in ensuring that the desired objectives of the strategic change process are achieved. We investigate these roles for performance measurement over time and empirically document lags between changes in strategic priorities, changes in operational processes, and subsequent changes in firm performance. We find that performance measurement supports the implementation of strategic change by influencing the extent to which changes to operational tasks and activities are made in response to new strategic priorities, as well as influencing the quality and impact of these operational changes, as reflected in improved contemporaneous and future firm performance.This paper was accepted by Suraj Srinivasan, accounting.},
journal = {Manage. Sci.},
month = jan,
pages = {640–659},
numpages = {20},
keywords = {performance measurement, strategic change, firm performance}
}

@article{10.1007/s10664-009-9121-0,
author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9121-0},
doi = {10.1007/s10664-009-9121-0},
abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {250–276},
numpages = {27},
keywords = {Empirical software engineering, Software architecture}
}

@article{10.1016/j.neucom.2018.11.015,
author = {Li, Qiaohong and Lin, Weisi and Gu, Ke and Zhang, Yabin and Fang, Yuming},
title = {Blind image quality assessment based on joint log-contrast statistics},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.015},
doi = {10.1016/j.neucom.2018.11.015},
journal = {Neurocomput.},
month = feb,
pages = {189–198},
numpages = {10},
keywords = {Blind image quality assessment (BIQA), No-reference (NR), Natural scene statistics, Partial least square}
}

@article{10.1287/mnsc.1090.1114,
title = {Index to Volume 55},
year = {2009},
issue_date = {December 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {12},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1114},
doi = {10.1287/mnsc.1090.1114},
journal = {Manage. Sci.},
month = dec,
pages = {2045–2051},
numpages = {7}
}

@article{10.1007/s10619-013-7130-x,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Automatic optimization of stream programs via source program operator graph transformations},
year = {2013},
issue_date = {December  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-013-7130-x},
doi = {10.1007/s10619-013-7130-x},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {543–599},
numpages = {57},
keywords = {Automatic tuning, Code transformation, Data-intensive computing, Performance optimization, Stream processing}
}

@article{10.1016/j.image.2015.04.014,
title = {Spatiotemporal saliency detection based on superpixel-level trajectory},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {38},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2015.04.014},
doi = {10.1016/j.image.2015.04.014},
abstract = {In this paper, we propose a novel spatiotemporal saliency model based on superpixel-level trajectories for saliency detection in videos. The input video is first decomposed into a set of temporally consistent superpixels, on which superpixel-level trajectories are directly generated, and motion histograms at superpixel level as well as frame level are extracted. Based on motion vector fields of multiple successive frames, the inside-outside maps are estimated to roughly indicate whether pixels are inside or outside objects with motion different from background. Then two descriptors, i.e. accumulated motion histogram and trajectory velocity entropy, are exploited to characterize the short-term and long-term temporal features of superpixel-level trajectories. Based on trajectory descriptors and inside-outside maps, superpixel-level trajectory distinctiveness is evaluated and trajectory classification is performed to obtain trajectory-level temporal saliency. Superpixel-level and pixel-level temporal saliency maps are generated in turn by exploiting motion similarity with neighboring superpixels around each trajectory, and color-spatial similarity with neighboring superpixels around each pixel, respectively. Finally, a quality-guided fusion method is proposed to integrate the pixel-level temporal saliency map with the pixel-level spatial saliency map, which is generated based on global contrast and spatial sparsity of superpixels, to generate the pixel-level spatiotemporal saliency map with reasonable quality. Experimental results on two public video datasets demonstrate that the proposed model outperforms the state-of-the-art spatiotemporal saliency models on saliency detection performance. A superpixel-level trajectory based spatiotemporal saliency model is proposed to effectively improve the saliency detection performance on challenging videos.Two trajectory descriptors, i.e. accumulated motion histogram and trajectory velocity entropy, are exploited to handle motion variability of different videos.A novel pipeline, which systematically measures trajectory distinctiveness, trajectory-level, superpixel-level and pixel-level temporal saliency in turn, is proposed to effectively enhance the coherence of temporal saliency through the whole video.A quality-guided fusion method is proposed to reasonably integrate temporal saliency map with spatial saliency map to generate spatiotemporal saliency map.},
journal = {Image Commun.},
month = oct,
pages = {100–114},
numpages = {15}
}

@article{10.1007/s11219-018-9437-3,
author = {Ma, Tao and Ali, Shaukat and Yue, Tao and Elaasar, Maged},
title = {Testing self-healing cyber-physical systems under uncertainty: a fragility-oriented approach},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9437-3},
doi = {10.1007/s11219-018-9437-3},
abstract = {As an essential feature of smart cyber-physical systems (CPSs), self-healing behaviors play a major role in maintaining the normality of CPSs in the presence of faults and uncertainties. It is important to test whether self-healing behaviors can correctly heal faults under uncertainties to ensure their reliability. However, the autonomy of self-healing behaviors and impact of uncertainties make it challenging to conduct such testing. To this end, we devise a fragility-oriented testing approach, which is comprised of two novel algorithms: fragility-oriented testing (FOT) and uncertainty policy optimization (UPO). The two algorithms utilize the fragility, obtained from test executions, to learn the optimal policies for invoking operations and introducing uncertainties, respectively, to effectively detect faults. We evaluated their performance by comparing them against a coverage-oriented testing (COT) algorithm and a random uncertainty generation method (R). The evaluation results showed that the fault detection ability of FOT+UPO was significantly higher than the ones of FOT+R, COT+UPO, and COT+R, in 73 out of 81 cases. In the 73 cases, FOT+UPO detected more than 70% of faults, while the others detected 17% of faults, at the most.},
journal = {Software Quality Journal},
month = jun,
pages = {615–649},
numpages = {35},
keywords = {Cyber-physical systems, Model execution, Reinforcement learning, Self-healing, Uncertainty}
}

@article{10.1007/s00371-019-01768-6,
author = {Sharma, Shallu and Mehra, Rajesh},
title = {Effect of layer-wise fine-tuning in magnification-dependent classification of breast cancer histopathological image},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {9},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-019-01768-6},
doi = {10.1007/s00371-019-01768-6},
abstract = {A large and balanced training data are the foremost requirement in proper convergence of a deep convolutional neural network (CNN). Medical data always suffer from the problem of unbalancing and inadequacy that makes it difficult to train CNN from scratch. It is known that the transfer learning approach provides great potential to deal with inadequate dataset besides the benefit of faster training. The efficient transfer of knowledge from natural images to histopathological images has yet to be achieved. In view of the foregoing, an attempt has been made toward the classification of BreakHis dataset using pre-trained ‘AlexNet’ model with a suitable fine-tuning approach. The effective depth of fine-tuning is also determined at different levels of magnification (40\texttimes{}, 100\texttimes{}, 200\texttimes{} and 400 \texttimes{}). The experimental trials conform that the moderate level of fine-tuning is an optimum choice for the classification of magnification-dependent histology images in contrast to the shallow and deep tuning of the pre-trained network which in turn depends on the size and relative distribution of a dataset. Additionally, the layer-wise fine-tuning approach provides a neck-to-neck performance with the latest state-of-the-art developments.},
journal = {Vis. Comput.},
month = sep,
pages = {1755–1769},
numpages = {15},
keywords = {Transfer learning, Layer-wise fine-tuning, Convolutional neural network, Magnification factor, Histopathological images, Breast cancer}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {Community-based question answering, association rules, classification, latent semantics, question quality}
}

@inproceedings{10.1145/2808492.2808563,
author = {Hou, Xingsong and Wang, Chunli and Qian, Xueming},
title = {Compressive imaging based on coefficients cutting in DLWT domain and approximate message passing},
year = {2015},
isbn = {9781450335287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808492.2808563},
doi = {10.1145/2808492.2808563},
abstract = {In compressive imaging (CI), accurate coefficients recovery is possible when the transform coefficients for an image are sufficiently sparse. However, conventional transforms, such as discrete cosine transform (DCT) and discrete wavelet transform (DWT), can not acquire a sufficiently sparse representation. A large amount of small coefficients indeed bring interference to the the recovery of large ones. This paper aims to improve the recovery performance by accurately reconstructing the large coefficients as many as possible. Thus, a compressive imaging scheme based on coefficients cutting in directional lifting wavelet transform (DLWT) domain and low-complexity iterative Bayesian algorithm is proposed. The proposed scheme is an improved version of our previous work. In this paper, relations between the best-fitted cutoff ratio and sampling rate are further analyzed. Due to the efficient Bayesian recovery algorithm, the proposed method offers better recovery performance with much lower complexity than our previous work. Experimental results show that our method outperforms many state-of-the-art compressive imaging recovery methods.},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service},
articleno = {71},
numpages = {5},
keywords = {DLWT, approximate message passing, coefficients cutting, compressive imaging, tail folding},
location = {Zhangjiajie, Hunan, China},
series = {ICIMCS '15}
}

@article{10.1016/j.jpdc.2019.09.005,
author = {Amah, Tekenate E. and Kamat, Maznah and Abu Bakar, Kamalrulnizam and Moreira, Waldir and Oliveira, Antonio and Batista, Marcos A.},
title = {Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.09.005},
doi = {10.1016/j.jpdc.2019.09.005},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {21–55},
numpages = {35},
keywords = {Opportunistic networks, Contact information overhead, Sensed Data Collection, Wireless sensors, Smart City}
}

@article{10.1016/j.tele.2013.02.004,
author = {Adibi, Sasan},
title = {A low overhead scaled equalized harmonic-based voice authentication system},
year = {2014},
issue_date = {February, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {31},
number = {1},
issn = {0736-5853},
url = {https://doi.org/10.1016/j.tele.2013.02.004},
doi = {10.1016/j.tele.2013.02.004},
abstract = {With the increasing trends of mobile interactions, voice authentication applications are in a higher demand, giving rise to new rounds of research activities. Authentication is an important security mechanism that requires the intended communication parties to present valid credentials to the communication network. In a stronger sense, all the involved parties are required to be authenticated to one another (mutual authentication). In the voice authentication technique described in this paper, the voice characteristics of an intended individual wishing to take part in a communication channel will be classified and processed. This involves a low overhead voice authentication scheme, which features equalization and scaling of the voice frequency harmonics. The performance of this system is discussed in a Labview 8.5 visual development environment, following a complete security analysis.},
journal = {Telemat. Inf.},
month = feb,
pages = {137–152},
numpages = {16},
keywords = {Amplitude equalizing, Delay, Frequency scaling, Harmonic-based voice authentication, Jitter, Labview, Performance, Security, Throughput, UDP, VoIP}
}

@article{10.1016/j.is.2017.04.001,
title = {Extending the framework for mobile health information systems Research},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {69},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2017.04.001},
doi = {10.1016/j.is.2017.04.001},
abstract = {This paper describes findings of a comprehensive content analysis on the current state of M-health research for IS researchers and professionals.The paper identifies eight application categories (themes), ten design issues as well as the stakeholders and development techniques involved.It is anticipated that the findings that reinforce use of design science research, theoretically motivate the central role of M-health IS design. Whilst researchers and professionals recognise that mobile health (M-health) systems offer unprecedented opportunities, most existing work has comprised individual project-based developments in specialised areas. Existing review articles generally utilise medical literature and categories: none investigates M-health from an information systems (IS) design point of view. Identifying application areas, design issues and IS research techniques will demonstrate models, issues, approaches and gaps to inform future research. A comprehensive analysis of the literature from this viewpoint is thus valuable, both for theoretical progression and for guiding real-world innovative system developments.Drawing from key IS and healthcare multidisciplinary journals we analyse recent (20102016) articles concerning M-health application developments and their associated design or development issues, with particular focus on the use of contemporary research methods. Our analysis suggests that M-health is an emerging field to which, although underused, contemporary approaches such as design science research are particularly appropriate. We identify eight application categories, eleven design issues (security, privacy, literacy, accessibility, acceptability, reliability, usability, confidentiality, integrity, knowledge sharing and flexibility) as well as the stakeholders and development techniques involved. This goes beyond previous frameworks, and theoretically integrates the central role of IS design within the sub-field.},
journal = {Inf. Syst.},
month = sep,
pages = {1–24},
numpages = {24}
}

@article{10.1007/s10270-013-0370-4,
author = {Syriani, Eugene and Vangheluwe, Hans and Lashomb, Brian},
title = {T-Core: a framework for custom-built model transformation engines},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0370-4},
doi = {10.1007/s10270-013-0370-4},
abstract = {A large number of model transformation languages and tools have emerged since the early 2000s. A transformation engineer is thus left with too many choices for the language he use to perform a specific transformation task. Furthermore, it is currently not possible to combine or reuse transformations implemented in different languages. We therefore propose T-Core, a framework where primitive transformation constructs can be combined to define and encapsulate reusable model transformation idioms. In this context, the transformation engineer is free to use existing transformation building blocks from an extensible library or define his own transformation units. The proposed primitive transformation operators are the result of deconstructing different existing transformation languages. Reconstructing these languages offers a common basis to compare their expressiveness, provides a framework for inter-operating them, and allows the transformation engineer to design transformations with the most appropriate constructs for the task at hand.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {1215–1243},
numpages = {29},
keywords = {Domain-specific model transformation, Model transformation, Reengineering, Transformation library}
}

@article{10.1016/j.jss.2012.06.025,
author = {Fronza, Ilenia and Sillitti, Alberto and Succi, Giancarlo and Terho, Mikko and Vlasenko, Jelena},
title = {Failure prediction based on log files using Random Indexing and Support Vector Machines},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.06.025},
doi = {10.1016/j.jss.2012.06.025},
abstract = {Research problem: The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures. Contribution: We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs). Method: RI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company. Results: According to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications. Conclusions: Overall, our approach is very reliable in predicting both failures and non-failures.},
journal = {J. Syst. Softw.},
month = jan,
pages = {2–11},
numpages = {10},
keywords = {Event sequence data, Failure prediction, Log files, Random Indexing, Support Vector Machine (SVM)}
}

@article{10.1287/msom.1080.0215,
author = {Masini, Andrea and Van Wassenhove, Luk N.},
title = {ERP Competence-Building Mechanisms: An Exploratory Investigation of Configurations of ERP Adopters in the European and U.S. Manufacturing Sectors},
year = {2009},
issue_date = {Spring 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {11},
number = {2},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.1080.0215},
doi = {10.1287/msom.1080.0215},
abstract = {This paper contributes to the literature on enterprise resource planning (ERP) by pursuing two objectives. First, it identifies configurations of ERP adopters that have similar needs and develop similar competencies. Second, it tests the hypothesis that, to maximize benefits from their ERP projects, organizations should align their ERP competence-building mechanisms with the ERP needs that arise from their operational environment. The analysis of a sample of manufacturing companies that implemented ERP between 1995 and 2001 uncovers four distinct configurations representing different degrees of fit between needs and competence-building mechanisms: the frugal ERP, the extensive business process reengineering (BPR), the adaptive ERP, and the straitjacket. The results support our hypothesis and suggest that the consequences of a misfit between needs and competence-building mechanisms are more severe for companies that operate in complex and dynamic environments and have informal organizational structures than for firms with rigid structures that operate in simple and stable environments.},
journal = {Manufacturing &amp; Service Operations Management},
month = apr,
pages = {274–298},
numpages = {25},
keywords = {cluster analysis, empirical research, enterprise resource planning (ERP), information and communication technology, operations strategy}
}

@book{10.5555/2621967,
author = {Harmon, Paul},
title = {Business Process Change, Third Edition: A Business Process Management Guide for Managers and Process Professionals},
year = {2014},
isbn = {0128003871},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {Business Process Change, 3rd Edition provides a balanced view of the field of business process change. Bestselling author Paul Harmon offers concepts, methods, cases for all aspects and phases of successful business process improvement. Updated and added for this edition is new material on the development of business models and business process architecture development, on integrating decision management models and business rules, on service processes and on dynamic case management, and on integrating various approaches in a broad business process management approach. New to this edition: How to develop business models and business process architecture How to integrate decision management models and business rules New material on service processes and on dynamic case management Learn to integrate various approaches in a broad business process management approach Extensive revision and update addresses Business Process Management Systems, and the integration of process redesign and Six Sigma Learn how all the different process elements fit together in this best first book on business process, now completely updated Tailor the presented methodology, which is based on best practices, to your organization's specific needs Understand the human aspects of process redesign Benefit from all new detailed case studies showing how these methods are implemented}
}

@inproceedings{10.22360/SpringSim.2016.HPC.032,
author = {Polok, Lukas and Smrz, Pavel},
title = {Increasing double precision throughput on NVIDIA Maxwell GPUs},
year = {2016},
isbn = {9781510823181},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.22360/SpringSim.2016.HPC.032},
doi = {10.22360/SpringSim.2016.HPC.032},
abstract = {This paper deals with the impact the architectural changes of modern GPUs have on their use in scientific computing. It particularly focuses on significant drops in the number of double precision functional units in NVIDIA Maxwell architecture. Proposed remedies of the potential negative impact on GPGPU applications that are based on multiple precision arithmetics are discussed. Two new algorithms for fast and precise multiplication and fused multiply add for double precision arithmetics emulation are also presented here.Using these methods, we were able to boost the double precision performance of NVIDIA GTX 980 Ti from 95 GFLOPS up to 286 GFLOPS. The proposed methods are applicable also to other GPUs.},
booktitle = {Proceedings of the 24th High Performance Computing Symposium},
articleno = {20},
numpages = {8},
keywords = {GPGPU, double precision calculation, multiple precision arithmetics},
location = {Pasadena, California},
series = {HPC '16}
}

@inproceedings{10.1145/3079856.3080244,
author = {Venkataramani, Swagath and Ranjan, Ashish and Banerjee, Subarno and Das, Dipankar and Avancha, Sasikanth and Jagannathan, Ashok and Durg, Ajaya and Nagaraj, Dheemanth and Kaul, Bharat and Dubey, Pradeep and Raghunathan, Anand},
title = {ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080244},
doi = {10.1145/3079856.3080244},
abstract = {Deep Neural Networks (DNNs) have demonstrated state-of-the-art performance on a broad range of tasks involving natural language, speech, image, and video processing, and are deployed in many real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and the amount of data they process, both of which are projected to grow in the future. To improve the efficiency of DNNs, we propose ScaleDeep, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage the compute and communication characteristics of DNNs. While several DNN accelerator designs have been proposed in recent years, the key difference is that ScaleDeep primarily targets DNN training, as opposed to only inference or evaluation. The key architectural features from which ScaleDeep derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in computational characteristics (FLOPs and Bytes/FLOP ratio) that manifest at different levels of granularity in DNNs, (ii) a memory hierarchy and 3-tiered interconnect topology that is suited to the memory access and communication patterns in DNNs, (iii) a low-overhead synchronization mechanism based on hardware data-flow trackers, and (iv) methods to map DNNs to the proposed architecture that minimize data movement and improve core utilization through nested pipelining. We have developed a compiler to allow any DNN topology to be programmed onto ScaleDeep, and a detailed architectural simulator to estimate performance and energy. The simulator incorporates timing and power models of ScaleDeep's components based on synthesis to Intel's 14nm technology. We evaluate an embodiment of ScaleDeep with 7032 processing tiles that operates at 600 MHz and has a peak performance of 680 TFLOPs (single precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11 state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M-145.9M weights, including winners from 5 years of the ImageNet competition, ScaleDeep demonstrates 6x-28x speedup at iso-power over the state-of-the-art performance on GPUs.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {13–26},
numpages = {14},
keywords = {Deep Neural Networks, Hardware Accelerators, System Architecture},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@inproceedings{10.5555/3367471.3367606,
author = {Yang, Liang and Chen, Zhiyang and Gu, Junhua and Guo, Yuanfang},
title = {Dual self-paced graph convolutional network: towards reducing attribute distortions induced by topology},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {The success of graph convolutional neural networks (GCNNs) based semi-supervised node classification is credited to the attribute smoothing (propagating) over the topology. However, the attributes may be interfered by the utilization of the topology information. This distortion will induce a certain amount of misclassifications of the nodes, which can be correctly predicted with only the attributes. By analyzing the impact of the edges in attribute propagations, the simple edges, which connect two nodes with similar attributes, should be given priority during the training process compared to the complex ones according to curriculum learning. To reduce the distortions induced by the topology while exploit more potentials of the attribute information, Dual Self-Paced Graph Convolutional Network (DSP-GCN) is proposed in this paper. Specifically, the unlabelled nodes with confidently predicted labels are gradually added into the training set in the node-level self-paced learning, while edges are gradually, from the simple edges to the complex ones, added into the graph during the training process in the edge-level self-paced learning. These two learning strategies are designed to mutually reinforce each other by coupling the selections of the edges and unlabelled nodes. Experimental results of transductive semi-supervised node classification on many real networks indicate that the proposed DSP-GCN has successfully reduced the attribute distortions induced by the topology while it gives superior performances with only one graph convolutional layer.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4062–4069},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1109/TASLP.2021.3101928,
author = {Spagnol, Simone and Miccini, Riccardo and Onofrei, Marius George and Unnthorsson, Runar and Serafin, Stefania},
title = {Estimation of Spectral Notches From Pinna Meshes: Insights From a Simple Computational Model},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3101928},
doi = {10.1109/TASLP.2021.3101928},
abstract = {While previous research on spatial sound perception investigated the physical mechanisms producing the most relevant elevation cues, how spectral notches are generated and related to the individual morphology of the human pinna is still a topic of debate. Correctly modeling these important elevation cues, and in particular the lowest frequency notches, is an essential step for individualizing Head-Related Transfer Functions (HRTFs). In this paper we propose a simple computational model able to predict the center frequencies of pinna notches from ear meshes. We apply such a model to a highly controlled HRTF dataset built with the specific purpose of understanding the contribution of the pinna to the HRTF. Results show that the computational model is able to approximate the lowest frequency notch with improved accuracy with respect to other state-of-the-art methods. By contrast, the model fails to predict higher-order pinna notches correctly. The proposed approximation supplements understanding of the morphology involved in generating spectral notches in experimental HRTFs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2683–2695},
numpages = {13}
}

@inproceedings{10.1145/1529282.1529384,
author = {Lazzarini Lemos, Ot\'{a}vio Augusto and Bajracharya, Sushil and Ossher, Joel and Masiero, Paulo Cesar and Lopes, Cristina},
title = {Applying test-driven code search to the reuse of auxiliary functionality},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529384},
doi = {10.1145/1529282.1529384},
abstract = {Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g. compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and extracted into the workspace. In this paper we introduce the use of test cases as an interface for automating code search and reuse and evaluate its applicability and performance in the reuse of auxiliary functionality. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We present CodeGenie, an Eclipse plugin that performs TDCS using a code search engine called Sourcerer. Our evaluation presents evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {476–482},
numpages = {7},
keywords = {TDD, software reuse, source code search, test-first},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/2674396.2674415,
author = {Roden, Timothy E. and LeGrand, Rob and Fernandez, Raul and Brown, Jacqueline and Deaton, James (Ed) and Ross, Johnny},
title = {Development of a smart insole tracking system for physical therapy and athletics},
year = {2014},
isbn = {9781450327466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674396.2674415},
doi = {10.1145/2674396.2674415},
abstract = {Development of a smart insole tracking system is described. Originally designed for healthcare applications, the system has found applications in both physical therapy and athletic training. The entire system is distributed between insole hardware, mobile device applications that interface with the insoles and a central Internet server for data warehousing and analysis. We describe the development of these components so far including a discussion of custom algorithm development required for the system. The athletic version has been commercialized while the more complex healthcare version is still under development.},
booktitle = {Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {40},
numpages = {6},
keywords = {mobile applications, physical therapy, sensors, tracking},
location = {Rhodes, Greece},
series = {PETRA '14}
}

@article{10.1016/j.jss.2015.11.037,
author = {Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark},
title = {Extracting reusable design decisions for UML-based domain-specific languages},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.11.037},
doi = {10.1016/j.jss.2015.11.037},
abstract = {We conducted an extensive systematic literature review (SLR) based on 8,000+ publications for 2005-2012.We examined 90 UML-based domain-specific languages (DSLs) for their design decisions.We extracted 27 reusable design decisions and 7 characteristic DSL designs into a publicly available design-decision catalog.One third of the documented design decisions can be combined to characterize more than 60% of the reviewed DSLs. When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the DSML itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other DSML projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 DSML projects. These projects were identified-among others-via an extensive systematic literature review (SLR) for the years 2005-2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based DSMLs publicly available. Furthermore, our study offers insights into UML usage (e.g. diagram types) and into the adoption of UML extension techniques (e.g. metamodel extensions, profiles).},
journal = {J. Syst. Softw.},
month = mar,
pages = {140–172},
numpages = {33},
keywords = {Design decision, Design rationale, Domain-specific language, Domain-specific modeling, Model-driven development, Unified modeling language}
}

@article{10.1016/j.micpro.2008.05.002,
author = {Wang, Xiaorui and Lu, Chenyang and Gill, Christopher},
title = {FCS/nORB: A feedback control real-time scheduling service for embedded ORB middleware},
year = {2008},
issue_date = {November, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {32},
number = {8},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2008.05.002},
doi = {10.1016/j.micpro.2008.05.002},
abstract = {Object Request Broker (ORB) middleware has shown promise in meeting the functional and real-time performance requirements of distributed real-time and embedded (DRE) systems. However, existing real-time ORB middleware standards such as RT-CORBA do not adequately address the challenges of (1) managing unpredictable workload, and (2) providing robust performance guarantees portably across different platforms. To overcome this limitation, we have developed software called FCS/nORB that integrates a Feedback Control real-time Scheduling (FCS) service with the nORB small-footprint real-time ORB designed for networked embedded systems. FCS/nORB features feedback control loops that provide real-time performance guarantees by automatically adjusting the rate of remote method invocations transparently to an application. FCS/nORB thus enables real-time applications to be truly portable in terms of real-time performance as well as functionality, without the need for hand tuning. This paper presents the design, implementation, and empirical evaluation of FCS/nORB. Our extensive experiments on a Linux testbed demonstrate that FCS/nORB can provide deadline miss ratio and utilization guarantees in the face of changes in platform and task execution times, while introducing only a small amount of overhead.},
journal = {Microprocess. Microsyst.},
month = nov,
pages = {413–424},
numpages = {12},
keywords = {Feedback control, Middleware, Object request broker, Performance portability, Real-time scheduling}
}

@article{10.1016/j.infsof.2010.11.009,
author = {Lazzarini Lemos, Ot\'{a}vio Augusto and Bajracharya, Sushil and Ossher, Joel and Masiero, Paulo Cesar and Lopes, Cristina},
title = {A test-driven approach to code search and its application to the reuse of auxiliary functionality},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.11.009},
doi = {10.1016/j.infsof.2010.11.009},
abstract = {ContextSoftware developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g., compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and integrated into the workspace. ObjectiveIn this paper we introduce and evaluate the use of test cases as an interface for automating code search and reuse. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We also describe CodeGenie, an Eclipse plugin we have developed that performs TDCS using a code search engine called Sourcerer. MethodOur evaluation consists of two studies: an applicability study with 34 different features that were searched using CodeGenie; and a performance study comparing CodeGenie, Google Code Search, and a manual approach. ResultsBoth studies present evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality. ConclusionThis paper presents an approach to source code search and its application to the reuse of auxiliary functionality. Our exploratory evaluation shows promising results, which motivates the use and further investigation of TDCS.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {294–306},
numpages = {13},
keywords = {Code search, Development tools, Software development, Software reuse, Test-Driven Development}
}

@inproceedings{10.1145/2742647.2742658,
author = {Zhang, Li and Pathak, Parth H. and Wu, Muchen and Zhao, Yixin and Mohapatra, Prasant},
title = {AccelWord: Energy Efficient Hotword Detection through Accelerometer},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742658},
doi = {10.1145/2742647.2742658},
abstract = {Voice control has emerged as a popular method for interacting with smart-devices such as smartphones, smartwatches etc. Popular voice control applications like Siri and Google Now are already used by a large number of smartphone and tablet users. A major challenge in designing a voice control application is that it requires continuous monitoring of user?s voice input through the microphone. Such applications utilize hotwords such as "Okay Google" or "Hi Galaxy" allowing them to distinguish user?s voice command and her other conversations. A voice control application has to continuously listen for hotwords which significantly increases the energy consumption of the smart-devices.To address this energy efficiency problem of voice control, we present AccelWord in this paper. AccelWord is based on the empirical evidence that accelerometer sensors found in today?s mobile devices are sensitive to user?s voice. We also demonstrate that the effect of user?s voice on accelerometer data is rich enough so that it can be used to detect the hotwords spoken by the user. To achieve the goal of low energy cost but high detection accuracy, we combat multiple challenges, e.g. how to extract unique signatures of user?s speaking hotwords only from accelerometer data and how to reduce the interference caused by user?s mobility.We finally implement AccelWord as a standalone application running on Android devices. Comprehensive tests show AccelWord has hotword detection accuracy of 85% in static scenarios and 80% in mobile scenarios. Compared to the microphone based hotword detection applications such as Google Now and Samsung S Voice, AccelWord is 2 times more energy efficient while achieving the accuracy of 98% and 92% in static and mobile scenarios respectively.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {301–315},
numpages = {15},
keywords = {accelerometer, accelword, energy, hotword detection, measurement},
location = {Florence, Italy},
series = {MobiSys '15}
}

@article{10.1007/s11277-017-4154-y,
author = {Kim, Sungryul and Yoo, Younghwan},
title = {MIMO-HFM: A MIMO System with Hyperbolic Frequency Modulation for Underwater Acoustic Communication},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {96},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-4154-y},
doi = {10.1007/s11277-017-4154-y},
abstract = {Reliable transmission and high data rate over underwater acoustic channels are considerably challenging. In this paper, we propose Multiple-Input and Multiple-Output (MIMO) scheme using a Hyperbolic Frequency Modulation (HFM) waveform. Our proposed system combines the advantages of both systems-special multiplexing of MIMO and immunity against Doppler shift of HFM. To increase the spectral efficiency, we employ M-ray HFM and overlapped sub-channels by leveraging the high temporal resolution characteristic. To verify effectiveness of our system, we have designed a theoretically enhanced acoustic simulator, which especially focuses on the reflection phenomenon by utilizing approved reflection loss models. Based on our acoustic simulator, we could verify that our system is robust against for multipath fading and Doppler shifting while keeping the multiplexing benefit of MIMO, while maintaining a very low complexity and system overhead. In addition, the results provide a useful insight for physical layer design in acoustic communication systems.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {103–124},
numpages = {22},
keywords = {Acoustic network simulator, Channel modeling, Data rate enhancement, HFM, MIMO, Reliability, Underwater communication}
}

@inproceedings{10.1145/1048935.1050197,
author = {Deelman, Ewa and Plante, Raymond and Kesselman, Carl and Singh, Gurmeet and Su, Mei-Hui and Greene, Gretchen and Hanisch, Robert and Gaffney, Niall and Volpicelli, Antonio and Annis, James and Sekhri, Vijay and Budavari, Tamas and Nieto-Santisteban, Maria and O'Mullane, William and Bohlender, David and McGlynn, Tom and Rots, Arnold and Pevunova, Olga},
title = {Grid-Based Galaxy Morphology Analysis for the National Virtual Observatory},
year = {2003},
isbn = {1581136951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1048935.1050197},
doi = {10.1145/1048935.1050197},
abstract = {As part of the development of the National Virtual Observatory (NVO), a Data Grid for astronomy, we have developed a prototype science application to explore the dynamical history of galaxy clusters by analyzing the galaxies' morphologies. The purpose of the prototype is to investigate how Grid-based technologies can be used to provide specialized computational services within the NVO environment. In this paper we focus on the key enabling technology components, particularly Chimera and Pegasus which are used to create and manage the computational workflow that must be present to deal with the challenging application requirements. We illustrate how the components interplay with each other and can be driven from a special purpose application portal.},
booktitle = {Proceedings of the 2003 ACM/IEEE Conference on Supercomputing},
pages = {47},
location = {Phoenix, AZ, USA},
series = {SC '03}
}

@article{10.5555/2804890.2804892,
author = {Pauwels, Koen and Hanssens, Dominique M.},
title = {Performance Regimes and Marketing Policy Shifts},
year = {2007},
issue_date = {May 2007},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {26},
number = {3},
issn = {1526-548X},
abstract = {Even in mature markets, managers are expected to improve their brands' performance year after year. When successful, they can expect to continue executing on an established marketing strategy. However, when the results are disappointing, a change or turnaround strategy may be called for to help performance get back on track. In such cases, performance diagnostics are needed to identify turnarounds and to quantify the role of marketing policy shifts in this process. This paper proposes a framework for such a diagnosis and applies several methods to provide converging evidence for two main findings. First, contrary to prevailing beliefs, the performance of brands in mature markets is not always stable. Instead, brands systematically improve or deteriorate their performance outlook in clearly identifiable time windows that are relatively short compared to windows of stability. Second, these shifts in performance regimes are associated with the brand's marketing actions and policy shifts, as opposed to competitive marketing. Promotion-oriented marketing policy shifts are particularly potent in improving a brand's performance outlook.},
journal = {Marketing Science},
month = may,
pages = {293–311},
numpages = {19},
keywords = {advertising, marketing mix, performance improvement, promotion, turnaround strategy}
}

@article{10.1177/0037549715603480,
author = {Peters, Brady},
title = {Integrating acoustic simulation in architectural design workflows},
year = {2015},
issue_date = {9 2015},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {91},
number = {9},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549715603480},
doi = {10.1177/0037549715603480},
abstract = {Sound is an important part of our experience of buildings. However, architects design largely using visually based techniques and largely for visual phenomena. Aiming to address this problem, the research presented in this paper proposes four digital design workflows that integrate acoustic computer simulation into architectural design. These techniques enable architects to design for both visual and acoustic criteria. The goal is to develop rapid and accessible workflows for architects that allow acoustic performance to be tuned as geometry and materials are modified at the scale of the room, and also at the scale of the surface. The discovery and testing of these techniques takes place within the design of the FabPod, a semi-enclosed meeting room situated within an open-plan working environment. The project builds on previous research investigating the design principles, the acoustic performance, and the fabrication methods of hyperboloid surface geometry. Four design workflows were developed: two of these investigate the acoustic performance of the room and use existing acoustic simulation software, and the other two workflows investigate the acoustic performance of the surface and use custom-written scripts to calculate and visualize sound scattering. This paper presents the background to the study, outlines the digital workflows, describes how they integrate acoustic simulation, and shows some of the data produced by these simulations.},
journal = {Simulation},
month = sep,
pages = {787–808},
numpages = {22},
keywords = {acoustic simulation, architectural acoustics, architectural design, computer-aided design, design workflows, performance-based design, simulation visualization}
}

@book{10.5555/2597830,
author = {Shimonski, Robert},
title = {The Wireshark Field Guide: Analyzing and Troubleshooting Network Traffic},
year = {2013},
isbn = {9780124104969},
publisher = {Syngress Publishing},
abstract = {The Wireshark Field Guide provides hackers, pen testers, and network administrators with practical guidance on capturing and interactively browsing computer network traffic. Wireshark is the worlds foremost network protocol analyzer, with a rich feature set that includes deep inspection of hundreds of protocols, live capture, offline analysis and many other features. The Wireshark Field Guide covers the installation, configuration and use of this powerful multi-platform tool. The book give readers the hands-on skills to be more productive with Wireshark as they drill down into the information contained in real-time network traffic. Readers will learn the fundamentals of packet capture and inspection, the use of color codes and filters, deep analysis, including probes and taps, and much more. The Wireshark Field Guide is an indispensable companion for network technicians, operators, and engineers. Learn the fundamentals of using Wireshark in a concise field manual Quickly create functional filters that will allow you to get to work quickly on solving problems Understand the myriad of options and the deep functionality of Wireshark Solve common network problems Learn some advanced features, methods and helpful ways to work more quickly and efficiently Table of Contents Chapter 1: About Wireshark Chapter 2: Installing Wireshark Chapter 3: Configuring a System Chapter 4: Capturing Packets Chapter 5: Color Codes Chapter 6: Filters Chapter 7: Sample Captures Chapter 8: Inspecting Packets Chapter 9: Deep Analysis Chapter 10: Saving Captures}
}

@article{10.1016/j.infsof.2012.06.013,
author = {Ali, Raian and Dalpiaz, Fabiano and Giorgini, Paolo},
title = {Reasoning with contextual requirements: Detecting inconsistency and conflicts},
year = {2013},
issue_date = {January, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.013},
doi = {10.1016/j.infsof.2012.06.013},
abstract = {Context: The environment in which the system operates, its context, is variable. The autonomous ability of a software to adapt to context has to be planned since the requirements analysis stage as a strong mutual influence between requirements and context does exist. On the one hand, context is a main factor to decide whether to activate a requirement, the applicable alternatives to meet an activated requirement as well as their qualities. On the other hand, the system actions to reach requirements could cause changes in the context. Objectives: Modelling the relationship between requirements and context is a complex task and developing error-free models is hard to achieve without an automated support. The main objective of this paper is to develop a set of automated analysis mechanisms to support the requirements engineers to detect and analyze modelling errors in contextual requirements models. Method: We study the analysis of the contextual goal model which is a requirements model that weaves together the variability of both context and requirements. Goal models are used during the early stages of software development and, thus, our analysis detects errors early in the development process. We develop two analysis mechanisms to detect two kinds of modelling errors. The first mechanism concerns the detection of inconsistent specification of contexts in a goal model. The second concerns the detection of conflicting context changes that arise as a consequence of the actions performed by the system to meet different requirements simultaneously. We support our analysis with a CASE tool and provide a systematic process that guides the construction and analysis of contextual goal models. We illustrate and evaluate our framework via a case study on a smart-home system for supporting the life of people having dementia problems. Results: The evaluation showed a significant ability of our analysis mechanisms to detect errors which were not notable by requirements engineers. Moreover, the evaluation showed acceptable performance of these mechanisms when processing up to medium-sized contextual goal models. The modelling constructs which we proposed as an input to enable the analysis were found easy to understand and capture. Conclusions: Our developed analysis for the detection of inconsistency and conflicts in contextual goal models is an essential step for the entire system correctness. It avoids us developing unusable and unwanted functionalities and functionalities which lead to conflicts when they operate together. Further research to improve our analysis to scale with large-sized models and to consider other kinds of errors is still needed.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {35–57},
numpages = {23},
keywords = {Adaptive systems engineering, Consistency and conflicts analysis, Contextual requirements, Goal modelling, Requirements engineering}
}

@article{10.1155/2021/7144635,
author = {Li, Bing and Tuo, Anxie and Kong, Hanyue and Liu, Sujiao and Chen, Jia and Gupta, Suneet Kumar},
title = {Application of Multilayer Perceptron Genetic Algorithm Neural Network in Chinese-English Parallel Corpus Noise Processing},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/7144635},
doi = {10.1155/2021/7144635},
abstract = {This paper uses neural network as a predictive model and genetic algorithm as an online optimization algorithm to simulate the noise processing of Chinese-English parallel corpus. At the same time, according to the powerful random global search mechanism of genetic algorithm, this paper studied the principle and process of noise processing in Chinese-English parallel corpus. Aiming at the task of identifying isolated words for unspecified persons, taking into account the inadequacies of the algorithms in standard genetic algorithms and neural networks, this paper proposes a fast algorithm for training the network using genetic algorithms. Through simulation calculations, different characteristic parameters, the number of training samples, background noise, and whether a specific person affects the recognition result were analyzed and discussed and compared with the traditional dynamic time comparison method. This paper introduces the idea of reinforcement learning, uses different reward mechanisms to solve the inconsistency of loss function and evaluation index measurement methods, and uses different decoding methods to alleviate the problem of exposure bias. It uses various simple genetic operations and the survival of the fittest selection mechanism to guide the learning process and determine the direction of the search, and it can search multiple regions in the solution space at the same time. In addition, it also has the advantage of not being restricted by the restrictive conditions of the search space (such as differentiable, continuous, and unimodal). At the same time, a method of using English subword vectors to initialize the parameters of the translation model is given. The research results show that the neural network recognition method based on genetic algorithm which is given in this paper shows its ability of quickly learning network weights and it is superior to the standard in all aspects. The performance of the algorithm in genetic algorithm and neural network, with high recognition rate and unique application advantages, can achieve a win-win of time and efficiency.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {12}
}

@article{10.1504/ijsnet.2019.102186,
author = {Li, Qiyue and Xu, Heng and Sun, Wei and Li, Jie and Luo, Guojun and Wang, Jianping},
title = {A novel radio map construction method with reduced human efforts for Wi-Fi localisation system},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {31},
number = {2},
issn = {1748-1279},
url = {https://doi.org/10.1504/ijsnet.2019.102186},
doi = {10.1504/ijsnet.2019.102186},
abstract = {Fingerprint based Wi-Fi localisation system often takes a lot of human efforts to measure the received signal strength (RSS) of dense grid in an indoor environment. In this paper, we propose a novel fingerprint database construction method with reduced human effort to obtain optimal length of RSS time series and grid division. We verify the chaotic characteristics of RSS time series, and use phase space reconstruction algorithm to calculate the optimal length of RSS data to be collected at each reference point. Then Gaussian process regression (GPR) for fingerprinting based indoor localisation is used to construct the database with information of limited reference points. The hyper-parameters of GPR is calculated by conjugate gradient descent algorithm. The performance of the proposed radio map construction framework is validated in real indoor environment, and with using Bayesian positioning method, the localisation error mean can be 1.5 m while the construction time of radio map is greatly reduced with ensuring accuracy.},
journal = {Int. J. Sen. Netw.},
month = jan,
pages = {99–110},
numpages = {11},
keywords = {Wi-Fi localisation, indoor, fingerprint, radio map construction, chaotic, Gaussian process regression, GPR}
}

@inproceedings{10.1145/2659021.2659029,
author = {Li, Shuangjiang and Qi, Hairong},
title = {Recursive Low-rank and Sparse Recovery of Surveillance Video using Compressed Sensing},
year = {2014},
isbn = {9781450329255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659021.2659029},
doi = {10.1145/2659021.2659029},
abstract = {This paper focuses on surveillance video processing using Compressed Sensing (CS). The CS measurements are used for recovery of the video frame into a low-rank background component and sparse component that corresponds to the moving object. The spatial and temporal low-rank features of the video frame, e.g., the nonlocal similar patches within the single video frame and the low-rank background component residing in multiple frames, are successfully exploited. We propose rLSDR that consists of three major components. First we develop an efficient single frame CS recovery algorithm, called NLDR, that operates on the nonlocal similarity patches within each frame to solve the low-rank optimization problem with the CS measurements constraint using Douglas-Rachford splitting method. Second, after obtaining a few NLDR recovered frames as training, a fast bilateral random projections (BRP) scheme is adopted for quick low-rank background initialization. Third, rLSDR then incorporates real-time single video frame to recursively recover the sparse component and update the background, where the proposed NLDR algorithm can also be used here for sparse component estimation. Experimental results on standard surveillance videos demonstrate that NLDR performs the best for single frame CS recovery compared with the state-of-the-art and rLSDR could successfully recover the background and sparse object with less resource consumption.},
booktitle = {Proceedings of the International Conference on Distributed Smart Cameras},
articleno = {1},
numpages = {6},
keywords = {Compressed sensing, low-rank approximation, sparse recovery, surveillance video processing},
location = {Venezia Mestre, Italy},
series = {ICDSC '14}
}

@inproceedings{10.1145/1052898.1052902,
author = {Tesanovic, Aleksandra and Amirijoo, Mehdi and Bj\"{o}rk, Mikael and Hansson, J\"{o}rgen},
title = {Empowering configurable QoS management in real-time systems},
year = {2005},
isbn = {1595930426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1052898.1052902},
doi = {10.1145/1052898.1052902},
abstract = {Current Quality of Service (QoS) management methods in real-time systems using feedback control loop lack support for configurability and reusability as they cannot be configured for a target application or reused across different applications. In this paper we present a method for developing reconfigurable feedback-based QoS management for real-time systems, denoted Re-QoS. By combining component-based design with aspect-oriented software development Re-QoS enables successful handling of crosscutting nature of QoS policies, as well as evolutionary design of real-time systems and QoS management architectures. Re-QoS defines a QoS aspect package, which is an implementation of a set of aspects and components that provide a number of different QoS policies. By adding a QoS aspect package to an existing system without QoS guarantees, we are able to use the same system in unpredictable environments where performance guarantees are essential. Furthermore, by exchanging aspects within the QoS aspect package one can efficiently tailor the QoS management of a real-time system based on the application requirements. We demonstrate the usefulness of the concept on a case study of an embedded real-time database system, called COMET. Using the COMET example we show how a real-time database system can be adapted to be used in different applications with distinct QoS needs.},
booktitle = {Proceedings of the 4th International Conference on Aspect-Oriented Software Development},
pages = {39–50},
numpages = {12},
location = {Chicago, Illinois},
series = {AOSD '05}
}

@inproceedings{10.1145/2809695.2809718,
author = {Stisen, Allan and Blunck, Henrik and Bhattacharya, Sourav and Prentow, Thor Siiger and Kj\ae{}rgaard, Mikkel Baun and Dey, Anind and Sonne, Tobias and Jensen, Mads M\o{}ller},
title = {Smart Devices are Different: Assessing and MitigatingMobile Sensing Heterogeneities for Activity Recognition},
year = {2015},
isbn = {9781450336314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809695.2809718},
doi = {10.1145/2809695.2809718},
abstract = {The widespread presence of motion sensors on users' personal mobile devices has spawned a growing research interest in human activity recognition (HAR). However, when deployed at a large-scale, e.g., on multiple devices, the performance of a HAR system is often significantly lower than in reported research results. This is due to variations in training and test device hardware and their operating system characteristics among others. In this paper, we systematically investigate sensor-, device- and workload-specific heterogeneities using 36 smartphones and smartwatches, consisting of 13 different device models from four manufacturers. Furthermore, we conduct experiments with nine users and investigate popular feature representation and classification techniques in HAR research. Our results indicate that on-device sensor and sensor handling heterogeneities impair HAR performances significantly. Moreover, the impairments vary significantly across devices and depends on the type of recognition technique used. We systematically evaluate the effect of mobile sensing heterogeneities on HAR and propose a novel clustering-based mitigation technique suitable for large-scale deployment of HAR, where heterogeneity of devices and their usage scenarios are intrinsic.},
booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
pages = {127–140},
numpages = {14},
keywords = {activity recognition, mobile sensing},
location = {Seoul, South Korea},
series = {SenSys '15}
}

@book{10.5555/2930830,
author = {Menzies, Tim and Kocaguneli, Ekrem and Turhan, Burak and Minku, Leandro and Peters, Fayola},
title = {Sharing Data and Models in Software Engineering},
year = {2014},
isbn = {9780124173071},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Data Science for Software Engineering: Sharing Data and Models presents guidance and procedures for reusing data and models between projects to produce results that are useful and relevant. Starting with a background section of practical lessons and warnings for beginner data scientists for software engineering, this edited volume proceeds to identify critical questions of contemporary software engineering related to data and models. Learn how to adapt data from other organizations to local problems, mine privatized data, prune spurious information, simplify complex results, how to update models for new platforms, and more. Chapters share largely applicable experimental results discussed with the blend of practitioner focused domain expertise, with commentary that highlights the methods that are most useful, and applicable to the widest range of projects. Each chapter is written by a prominent expert and offers a state-of-the-art solution to an identified problem facing data scientists in software engineering. Throughout, the editors share best practices collected from their experience training software engineering students and practitioners to master data science, and highlight the methods that are most useful, and applicable to the widest range of projects. Shares the specific experience of leading researchers and techniques developed to handle data problems in the realm of software engineering Explains how to start a project of data science for software engineering as well as how to identify and avoid likely pitfalls Provides a wide range of useful qualitative and quantitative principles ranging from very simple to cutting edge research Addresses current challenges with software engineering data such as lack of local data, access issues due to data privacy, increasing data quality via cleaning of spurious chunks in data Table of Contents Introduction Data Science 101 Cross company data: Friend or Foe Pruning: Relevancy is the Removal of Irrelevancy Easy Path: Smarter Design Instance Weighting: How not to elaborate on analogies Privacy: Data in Disguise Stability: How to find a silver-bullet model Complexity: How to ensemble multiple models}
}

@inproceedings{10.1145/2030376.2030402,
author = {Porenta, Jernej and Ciglari\v{c}, Mojca},
title = {Empirical comparison of IP reputation databases},
year = {2011},
isbn = {9781450307888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030376.2030402},
doi = {10.1145/2030376.2030402},
abstract = {IP reputation is a common technique to address email spam problem and while there are commercial implementations available, the algorithms behind them are confidential. A few open source implementations (gossip, RepuScore, IP-GroupREP, etc.) are available, but few studies compare their commercial counterparts.For this reason, we have made an empirical comparison of six popular commercial IP reputation databases and three different open-source IP reputation algorithms. We built our own IP reputation database from our email corpus, containing 931,576 email messages from real-time email traffic at an academic ISP. After we processed and classified the corpus, we compared the open-source IP reputation algorithms' results with commercial IP reputation databases by using the Spearman rank correlation coefficient to identify the optimal parameters for open-source algorithms.The results show lower correlation coefficients when the frequency of emails from a single IP is rising. Open-source algorithms performed sufficiently for IP numbers with more than five and less than 50 emails from a single IP, while (surprisingly) the correlation dropped with a higher number of emails from a single IP. For this reason, we believe there should be some additional fine-tuning of open-source algorithms to make them comparable to their commercial counterparts that have IP reputation scores built from many sensors around the world.We also compared commercial IP reputation databases and found mixed correlations between them, which raised many questions regarding the algorithms used for building IP reputation scores. The research also identified the problem of finding a good methodology for comparing IP reputation databases.},
booktitle = {Proceedings of the 8th Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference},
pages = {220–226},
numpages = {7},
keywords = {IP reputation, antispam filtering, email},
location = {Perth, Australia},
series = {CEAS '11}
}

@article{10.4018/jdst.2010040105,
author = {Brightwell, Ron and Camp, William J. and Dosanjh, Sudip and Kelly, Suzanne M. and Levesque, John and Lin, Paul T. and Tipparaju, Vinod and Vaughan, Courtenay T. and Tomkins, James L.},
title = {The Red Storm Architecture and Early Experiences with Multi-Core Processors},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {2},
issn = {1947-3532},
url = {https://doi.org/10.4018/jdst.2010040105},
doi = {10.4018/jdst.2010040105},
abstract = {The Red Storm architecture, which was conceived by Sandia National Laboratories and implemented by Cray, Inc., has become the basis for most successful line of commercial supercomputers in history. The success of the Red Storm architecture is due largely to the ability to effectively and efficiently solve a wide range of science and engineering problems. The Cray XT series of machines that embody the Red Storm architecture have allowed for unprecedented scaling and performance of parallel applications spanning many areas of scientific computing. This paper describes the fundamental characteristics of the architecture and its implementation that have enabled this success, even through successive generations of hardware and software.},
journal = {Int. J. Distrib. Syst. Technol.},
month = apr,
pages = {74–93},
numpages = {20},
keywords = {High-Performance Computing, Large-Scale Computers, Massively Parallel Processor, Multiprocessing Systems, Parallel Processing Systems, Supercomputers}
}

@article{10.1147/JRD.2013.2243551,
author = {Biem, A. and Feng, H. and Riabov, A. V. and Turaga, D. S.},
title = {Real-time analysis and management of big time-series data},
year = {2013},
issue_date = {May/July 2013},
publisher = {IBM Corp.},
address = {USA},
volume = {57},
number = {3–4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2013.2243551},
doi = {10.1147/JRD.2013.2243551},
abstract = {The ability to process and analyze large volumes of time-series data is in increasing demand in various domains including health care, finance, energy and utilities, transportation, and cybersecurity. Despite the broad use of time-series data worldwide, the design of a system to easily manage, analyze, and visualize large multidimensional time series, with dimensions on the order of hundreds of thousands, is still a challenging endeavor. This paper describes the Streaming Time-Series Analysis and Management (STAM) system as a solution to this problem. STAM provides the capability to glean actionable information from continuously changing time series with thousands of dimensions, in real time. STAM exploits the IBM InfoSphere® Streams platform and allows for general-purpose large-scale time-series analytics for applications including anomaly detection, modeling, smoothing, forecasting, and tracking. In addition, the system provides user-friendly tools for managing, deploying, and initiating analytics on large-scale data streams of interest, and provides a web-based graphical visualization interface that allows highlighting of events of interest with interactive menus. In this paper, we describe the system and illustrate its use in a large-scale system-monitoring application.},
journal = {IBM J. Res. Dev.},
month = may,
articleno = {1},
numpages = {1}
}

@article{10.1016/j.robot.2016.10.013,
author = {St-Onge, David and Brches, Pierre-Yves and Sharf, Inna and Reeves, Nicolas and Rekleitis, Ioannis and Abouzakhm, Patrick and Girdhar, Yogesh and Harmat, Adam and Dudek, Gregory and Gigure, Philippe},
title = {Control, localization and human interaction with an autonomous lighter-than-air performer},
year = {2017},
issue_date = {February 2017},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {88},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2016.10.013},
doi = {10.1016/j.robot.2016.10.013},
abstract = {Due to the recent technological progress, HumanRobotInteraction (HRI) has become a major field of research in both engineering and artistic realms, particularly so in the last decade. The mainstream interests are, however, extremely diverse: challenges are continuously shifting, the evolution of robot skills, as well as the advances in methods for understanding their environment radically impact the design and implementation of research prototypes. When directly deployed in a public installation or artistic performances, robots help foster the next level of understanding in HRI. To this effect, this paper presents a successful interdisciplinary art-science-technology project, the Aerostabiles, leading to a new way of conducting HRI research. The project consists of developing a mechatronic, intelligent platform embodied in multiple geometric blimps cubes that hover and move in the air. The artistic context of this project required a number of advances in engineering on the aspects of localization and control systems, flight dynamics, as well as interaction strategies, and their evolution through periods of collective activities called researchcreation residencies. These events involve artists, engineers, and performers working in close collaboration, sometimes, over several weeks at a time. They generate fruitful exchanges between all researchers, but most of all, they present a unique and creative way to direct and focus the robotics development. This paper represents an overview of the technical contributions from a range of expertise through the artistic drive of the Aerostabiles project. Presents an art-science-technology project for HRI research in the artistic realm.Details the dynamics, with effects of its added mass and vision-based localization.Experiments with controlled trajectories of the blimp for evoking emotions.Denes a sonar-based sphere of intimacy and a vision-based perplexing features scanner.Uses of electromyography sensors to interpret performers moods.},
journal = {Robot. Auton. Syst.},
month = feb,
pages = {165–186},
numpages = {22},
keywords = {Airship, Dynamic modeling, Humanrobot interaction, Mobile robotics, Robotic art, Robotic blimp, Theater}
}

@article{10.1017/S0269888909990099,
title = {From the journals…},
year = {2009},
issue_date = {December 2009},
publisher = {Cambridge University Press},
address = {USA},
volume = {24},
number = {4},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888909990099},
doi = {10.1017/S0269888909990099},
journal = {Knowl. Eng. Rev.},
month = dec,
pages = {417–437},
numpages = {21}
}

@article{10.1016/j.comnet.2019.07.013,
author = {Pimpinella, Andrea and Redondi, Alessandro E.C. and Cesana, Matteo},
title = {Walk this way! An IoT-based urban routing system for smart cities},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.07.013},
doi = {10.1016/j.comnet.2019.07.013},
journal = {Comput. Netw.},
month = oct,
numpages = {12},
keywords = {Internet of things, Smart cities, Urban routing, Spatial interpolation, Temporal forecasting}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Brain connectivity, Classification, Data mining, Mutual information, Sleep EEG}
}

@article{10.5555/3288647.3288714,
author = {Alshanqiti, Abdullah and Heckel, Reiko and Kehrer, Timo},
title = {Inferring visual contracts from Java programs},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {0928-8910},
abstract = {Visual contracts model the operation of components or services by pre- and post-conditions formalised as graph transformation rules. They provide a precise intuitive notation to support testing, understanding and analysis of software. Their detailed specification of internal data states and transformations, referred to as deep behavioural modelling, is an error-prone activity. In this paper we propose a dynamic approach to reverse engineering visual contracts from Java based on tracing the execution of Java operations. The resulting contracts give an accurate description of the observed object transformations, their effects and preconditions in terms of object structures, parameter and attribute values, and their generalised specification by universally quantified (multi) objects, patterns, and invariants. While this paper focusses on the fundamental technique rather than a particular application, we explore potential uses in our evaluation, including in program understanding, review of test reports and debugging.},
journal = {Automated Software Engg.},
month = dec,
pages = {745–784},
numpages = {40},
keywords = {Dynamic analysis, Graph transformation, Model extraction, Reverse engineering, Specification mining, Visual contracts}
}

@article{10.1016/j.knosys.2012.11.016,
author = {Kazemi, S. M. R. and Hadavandi, Esmaeil and Mehmanpazir, Farhad and Nakhostin, Mohammad Masoud},
title = {A hybrid intelligent approach for modeling brand choice and constructing a market response simulator},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2012.11.016},
doi = {10.1016/j.knosys.2012.11.016},
abstract = {Brand choice models, used for describing the process of choosing mutually exclusive alternatives, attracted a large amount of attention from researchers in the early of the recent decade. These models can be used as a market response simulator for simulating marketing strategies and assessing how changes in marketing variables such as pricing and promotions will influence consumer behavior and thus perform 'what-if' simulations. So a reliable and relevant brand choice model can be very useful and effective, which, in fact, represents a worthwhile opportunity to improve the efficiency of the marketing decisions. In this paper we offer a new approach by integrating of Probabilistic Neural Network (PNN) and Data preprocessing for brand choice modeling and constructing a market response simulator. This approach, called Preprocessed-Probabilistic Neural Network (PPNN), consists of two main stages. First, a robust Genetic Based Instance Selection Model (GBIS) employed to look for a representative data subset of instances in training data set. The second stage ends up with a relevant brand choice model, using a probabilistic neural network trained by Dynamic Decay Adjustment Algorithm (DDA). The evaluation process is carried out using the same data set been used in literature for modeling individual consumer choices in a retail coffee market. The evaluation results show that the offered approach outperforms all previous methods, so it can be considered as an effective tool for consumer behavior modeling and simulation.},
journal = {Know.-Based Syst.},
month = mar,
pages = {101–110},
numpages = {10},
keywords = {Brand choice, Dynamic decay algorithm, Instance selection, Market response simulator, Probabilistic neural networks}
}

@article{10.1109/TASLP.2018.2886743,
author = {Dietzen, Thomas and Spriet, Ann and Tirry, Wouter and Doclo, Simon and Moonen, Marc and van Waterschoot, Toon},
title = {Comparative Analysis of Generalized Sidelobe Cancellation and Multi-Channel Linear Prediction for Speech Dereverberation and Noise Reduction},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2886743},
doi = {10.1109/TASLP.2018.2886743},
abstract = {For blind speech dereverberation, two frameworks are commonly used: on the one hand, the multi-channel linear prediction MCLP framework, and on the other hand, data-dependent beamforming, e.g., the generalized sidelobe canceler GSC framework. The MCLP framework is designed to perform deconvolution and hence has gained increased prominence in blind speech dereverberation. The GSC framework is commonly used for noise reduction, but may be applied for dereverberation as well. In previous work, we have shown that for the noiseless case, MCLP and the GSC yield in theory mathematically equivalent results in terms of dereverberation. In this paper, we assume additional coherent as well as incoherent-noise components and formally analyze and compare both frameworks in terms of dereverberation and noise reduction performance. Both the theoretical analysis and time domain simulation results demonstrate that unlike the GSC, MCLP expectably shows limited performance in terms of noise reduction, while both perform equally well in terms of dereverberation, provided that the GSC blocking matrix achieves complete blocking of the early reverberant-speech component and sufficiently many microphones are available. In case of incomplete blocking, however, the GSC performs inferior to MCLP in terms of dereverberation, as shown in short-time Fourier transform domain simulations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {544–558},
numpages = {15}
}

@inproceedings{10.1145/256562.256650,
author = {Kleijnen, Jack P. C. and Bettonvil, Bert and Van Groenendaal, Willem J. H.},
title = {Validation of trace-driven simulation models: regression analysis revisited},
year = {1996},
isbn = {0780333837},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1145/256562.256650},
doi = {10.1145/256562.256650},
abstract = {For the validation of trace-driven simulation models this paper recommends a simple statistical test that uses elementary regression analysis in a novel way. This test concerns a (joint) null-hypothesis: the outputs of the simulated and the real systems have the same means and the same variances. Technically, the differences between simulated and real outputs are regressed on their sums, and the resulting slope and intercept are tested to be zero. This paper further proves that it is wrong to use a naive test that regresses the simulation outputs on the real outcomes, and hypothesizes that the resulting regression line gives a 45 /spl deg/ line through the origin. The new and the old tests are investigated in Monte Carlo experiments with inventory systems. The conclusion is that the new test has the correct type I error probability, whereas the old test (falsely) rejects a valid simulation model substantially more often than the nominal alpha level. The power of the new test increases, as the simulation model deviates more from the real system.},
booktitle = {Proceedings of the 28th Conference on Winter Simulation},
pages = {352–359},
numpages = {8},
location = {Coronado, California, USA},
series = {WSC '96}
}

@inproceedings{10.1145/2851553.2851569,
author = {Hork\'{y}, Vojt\v{e}ch and Kotr\v{c}, Jaroslav and Libi\v{c}, Peter and T\r{u}ma, Petr},
title = {Analysis of Overhead in Dynamic Java Performance Monitoring},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851569},
doi = {10.1145/2851553.2851569},
abstract = {In production environments, runtime performance monitoring is often limited to logging of high level events. More detailed measurements, such as method level tracing, tend to be avoided because their overhead can disrupt execution. This limits the information available to developers when solving performance issues at code level. One approach that reduces the measurement disruptions is dynamic performance monitoring, where the measurement instrumentation is inserted and removed as needed. Such selective monitoring naturally reduces the aggregate overhead, but also introduces transient overhead artefacts related to insertion and removal of instrumentation. We experimentally analyze this overhead in Java, focusing in particular on the measurement accuracy, the character of the transient overhead, and the longevity of the overhead artefacts.Among other results, we show that dynamic monitoring requires time from seconds to minutes to deliver stable measurements, that the instrumentation can both slow down and speed up the execution, and that the overhead artefacts can persist beyond the monitoring period.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {275–286},
numpages = {12},
keywords = {dynamic instrumentation, java, performance measurement overhead},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/2038642.2038683,
author = {Broy, Manfred and Chakraborty, Samarjit and Goswami, Dip and Ramesh, S. and Satpathy, M. and Resmerita, Stefan and Pree, Wolfgang},
title = {Cross-layer analysis, testing and verification of automotive control software},
year = {2011},
isbn = {9781450307147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038642.2038683},
doi = {10.1145/2038642.2038683},
abstract = {Automotive architectures today consist of up to 100 electronic control units (ECUs) that communicate via one or more FlexRay and CAN buses. Multiple control applications - like cruise control, brake control, etc. are specified as Simulink/Stateflow models, from which code is generated and mapped onto the different ECUs. In addition, scheduling policies and parameters, both for the ECUs and the buses, need to be specified. Code generation/optimization from the Simulink/Stateflow models, task partitioning and mapping decisions, as well as the parameters chosen for the schedulers all of these impact the execution times and timing behaviour of the control tasks and control messages. These in turn affect control performance, such as stability and steady-/transient-state behaviour. This paper discusses different aspects of this multi-layered design flow and the associated research challenges. The emphasis is on model-based code generation, analysis, testing and verification of control software for automotive architectures, as well as on architecture or platform configuration to ensure that the required control performance requirements are satisfied.},
booktitle = {Proceedings of the Ninth ACM International Conference on Embedded Software},
pages = {263–272},
numpages = {10},
keywords = {automotive control systems, model-based code-generation, model-based testing and verification},
location = {Taipei, Taiwan},
series = {EMSOFT '11}
}

@article{10.1007/s10851-021-01035-1,
author = {Pascal, Barbara and Vaiter, Samuel and Pustelnik, Nelly and Abry, Patrice},
title = {Automated Data-Driven Selection of the Hyperparameters for Total-Variation-Based Texture Segmentation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {63},
number = {7},
issn = {0924-9907},
url = {https://doi.org/10.1007/s10851-021-01035-1},
doi = {10.1007/s10851-021-01035-1},
abstract = {Penalized least squares are widely used in signal and image processing. Yet, it suffers from a major limitation since it requires fine-tuning of the regularization parameters. Under assumptions on the noise probability distribution, Stein-based approaches provide unbiased estimator of the quadratic risk. The Generalized Stein Unbiased Risk Estimator is revisited to handle correlated Gaussian noise without requiring to invert the covariance matrix. Then, in order to avoid expansive grid search, it is necessary to design algorithmic scheme minimizing the quadratic risk with respect to regularization parameters. This work extends the Stein’s Unbiased GrAdient estimator of the Risk of Deledalle et al. (SIAM J Imaging Sci 7(4):2448–2487, 2014) to the case of correlated Gaussian noise, deriving a general automatic tuning of regularization parameters. First, the theoretical asymptotic unbiasedness of the gradient estimator is demonstrated in the case of general correlated Gaussian noise. Then, the proposed parameter selection strategy is particularized to fractal texture segmentation, where problem formulation naturally entails inter-scale and spatially correlated noise. Numerical assessment is provided, as well as discussion of the practical issues.},
journal = {J. Math. Imaging Vis.},
month = sep,
pages = {923–952},
numpages = {30},
keywords = {Regularization parameters tuning, SURE, Estimation, Gaussian noise, Texture, segmentation, Algorithmic differentiation}
}

@article{10.1007/s10916-009-9381-7,
author = {Ayd\i{}n, Serap and Sarao\u{g}lu, Hamdi Melih and Kara, Sad\i{}k},
title = {Singular Spectrum Analysis of Sleep EEG in Insomnia},
year = {2011},
issue_date = {Aug 2011},
publisher = {Plenum Press},
address = {USA},
volume = {35},
number = {4},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-009-9381-7},
doi = {10.1007/s10916-009-9381-7},
abstract = {In the present study, the Singular Spectrum Analysis (SSA) is applied to sleep EEG segments collected from healthy volunteers and patients diagnosed by either psycho physiological insomnia or paradoxical insomnia. Then, the resulting singular spectra computed for both C3 and C4 recordings are assigned as the features to the Artificial Neural Network (ANN) architectures for EEG classification in diagnose. In tests, singular spectrum of particular sleep stages such as awake, REM, stage1 and stage2, are considered. Three clinical groups are successfully classified by using one hidden layer ANN architecture with respect to their singular spectra. The results show that the SSA can be applied to sleep EEG series to support the clinical findings in insomnia if ten trials are available for the specific sleep stages. In conclusion, the SSA can detect the oscillatory variations on sleep EEG. Therefore, different sleep stages meet different singular spectra. In addition, different healthy conditions generate different singular spectra for each sleep stage. In summary, the SSA can be proposed for EEG discrimination to support the clinical findings for psycho-psychological disorders.},
journal = {J. Med. Syst.},
month = aug,
pages = {457–461},
numpages = {5},
keywords = {EEG classification, Singular Spectrum Analysis, Sleep EEG}
}

@article{10.1016/j.neucom.2016.07.040,
author = {Maurya, Chandresh Kumar and Toshniwal, Durga and Vijendran Venkoparao, Gopalan},
title = {Online sparse class imbalance learning on big data},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {216},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.07.040},
doi = {10.1016/j.neucom.2016.07.040},
abstract = {Class imbalance learning is the study of problems in which some classes appear more frequently than the others. Most existing works that study this problem assume data set to be dense and do not exploit the rich structure of the data. One such structure is the sparsity. In the present work, we focus on solving the class imbalance problem under the sparsity assumption. More specifically, a well-known Gmean metric for class imbalance learning problem in binary classification setting has been maximized, which results in a non-convex loss function. Convex relaxation techniques are used to convert the non-convex problem to the convex problem. The problem formulation in the present work uses L1 regularized proximal learning framework and is solved via accelerated-stochastic-proximal gradient descent algorithm. Our aim in the paper is to show: (i) the application of proximal algorithms to solve real world problems (class imbalance); (ii) how it scales to Big data; and (iii) how it outperforms some recently proposed algorithms in terms of Gmean, F-measure and Mistake rate on several benchmark data sets.},
journal = {Neurocomput.},
month = dec,
pages = {250–260},
numpages = {11},
keywords = {Big data, Class imbalance learning, Online learning, Proximal algorithm}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@book{10.5555/2785650,
author = {Kotu, Vijay and Deshpande, Bala},
title = {Predictive Analytics and Data Mining: Concepts and Practice with RapidMiner},
year = {2014},
isbn = {0128014601},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Put Predictive Analytics into Action Learn the basics of Predictive Analysis and Data Mining through an easy to understand conceptual framework and immediately practice the concepts learned using the open source RapidMiner tool. Whether you are brand new to Data Mining or working on your tenth project, this book will show you how to analyze data, uncover hidden patterns and relationships to aid important decisions and predictions. Data Mining has become an essential tool for any enterprise that collects, stores and processes data as part of its operations. This book is ideal for business users, data analysts, business analysts, business intelligence and data warehousing professionals and for anyone who wants to learn Data Mining. Youll be able to: 1. Gain the necessary knowledge of different data mining techniques, so that you can select the right technique for a given data problem and create a general purpose analytics process. 2. Get up and running fast with more than two dozen commonly used powerful algorithms for predictive analytics using practical use cases. 3. Implement a simple step-by-step process for predicting an outcome or discovering hidden relationships from the data using RapidMiner, an open source GUI based data mining tool Predictive analytics and Data Mining techniques covered: Exploratory Data Analysis, Visualization, Decision trees, Rule induction, k-Nearest Neighbors, Nave Bayesian, Artificial Neural Networks, Support Vector machines, Ensemble models, Bagging, Boosting, Random Forests, Linear regression, Logistic regression, Association analysis using Apriori and FP Growth, K-Means clustering, Density based clustering, Self Organizing Maps, Text Mining, Time series forecasting, Anomaly detection and Feature selection. Implementation files can be downloaded from the book companion site at www.LearnPredictiveAnalytics.com Demystifies data mining concepts with easy to understand language Shows how to get up and running fast with 20 commonly used powerful techniques for predictive analysis Explains the process of using open source RapidMiner toolsDiscusses a simple 5 step process for implementing algorithms that can be used for performing predictive analytics Includes practical use cases and examples}
}

@article{10.1007/s11235-020-00684-8,
author = {Qian, Qing and Cui, Yunhe and Wang, Hongxia and Deng, Mingsen},
title = {REPAIR: fragile watermarking for encrypted speech authentication with recovery ability},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {3},
issn = {1018-4864},
url = {https://doi.org/10.1007/s11235-020-00684-8},
doi = {10.1007/s11235-020-00684-8},
abstract = {Confidentiality and integrity are fundamental requirements when transmitting and storing data. In order to guarantee the confidentiality and integrity of speech signal, we present a novel wateRmarking scheme for Encrypted sPeech AuthenticatIon with Recovery (REPAIR) scheme. In REPAIR, an encryption algorithm is first designed based on a hyper-chaotic method to improve the confidentiality of the original speech by encryption. Subsequently, a watermark generation and embedding algorithm is proposed to generate and embed the check bits and compression bits. Afterwards, a content authentication and tampering recovery algorithm is introduced to locate and recover the tampered speech frames. Meanwhile, a speech decryption algorithm is also presented to decrypt the encrypted speech. Analysis and experimental results demonstrate that REPAIR can detect and locate synchronization attacks and de-synchronization attacks without using the auxiliary synchronous code. Additionally, REPAIR can also recover the tampered content with high quality.},
journal = {Telecommun. Syst.},
month = nov,
pages = {273–289},
numpages = {17},
keywords = {Digital speech, Authentication, Confidentiality, Integrity, Recovery, Encryption, Watermarking}
}

@article{10.1504/IJIPT.2016.079546,
author = {Balakrishnan, Senthil Murugan and Sangaiah, Arun Kumar},
title = {Aspect-oriented middleware framework for resolving service discovery issues in Internet of Things},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2/3},
issn = {1743-8209},
url = {https://doi.org/10.1504/IJIPT.2016.079546},
doi = {10.1504/IJIPT.2016.079546},
abstract = {Internet of Things IoT is a model of future internet and pervasive computing which have its own challenges derived from the internet in terms of scalability, undefined topology and so on. The proposed work aims to resolve the challenges posed by IoT in service discovery functionality. Considering the pervasive and context dependent nature of IoT the planned work bases its development strategy using an aspect oriented software development methodology. The novelty lies in achieving high degree configuration and customisability by selecting subset of middleware functionality depending on the need. The performance is compared with MUSIC pervasive computing middleware and Android built-in configuration. The result reveals 6.5% decrease on average boot up and reconfiguration time for smart phones and 11.6% percentage decrease in Android tablets. In the context of boot up and reconfiguration time the middleware brings out 5% decrease for smart phones and 3% for Android tablets when compared with MUSIC middleware. The middleware shows 6.3% and 4% reduction in execution time of applications on smart phones and tablets when assessed with MUSIC middleware.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {Android tablets, IoT, Spring AOP, aspect-oriented middleware, boot up time, internet of things, pervasive computing, reconfiguration time, service discovery, smartphones}
}

@inproceedings{10.5555/1874620.1874903,
author = {Fern\'{a}ndez Villena, Jorge and Ciuprina, Gabriela and Ioan, Daniel and Silveira, Luis Miguel},
title = {On the efficient reduction of complete EM based parametric models},
year = {2009},
isbn = {9783981080155},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Due to higher integration and increasing frequency based effects, full Electromagnetic Models (EM) are needed for accurate prediction of the real behavior of integrated passives and interconnects. Furthermore, these structures are subject to parametric effects due to small variations of the geometric and physical properties of the inherent materials and manufacturing process. Accuracy requirements lead to huge models, which are expensive to simulate and this cost is increased when parameters and their effects are taken into account. This paper presents a complete procedure for efficient reduction of realistic, hierarchy aware, EM based parametric models. Knowledge of the structure of the problem is explicitly exploited using domain partitioning and novel electromagnetic connector modeling techniques to generate a hierarchical representation. This enables the efficient use of block parametric model order reduction techniques to generate block-wise compressed models that satisfy overall requirements, and provide accurate approximations of the complete EM behaviour, which are cheap to evaluate and simulate.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {1172–1177},
numpages = {6},
location = {Nice, France},
series = {DATE '09}
}

@article{10.1017/S026988890700121X,
title = {From the journals …},
year = {2007},
issue_date = {September 2007},
publisher = {Cambridge University Press},
address = {USA},
volume = {22},
number = {3},
issn = {0269-8889},
url = {https://doi.org/10.1017/S026988890700121X},
doi = {10.1017/S026988890700121X},
journal = {Knowl. Eng. Rev.},
month = sep,
pages = {297–314},
numpages = {18}
}

@inproceedings{10.1145/2462456.2464450,
author = {Liu, Kaikai and Liu, Xinxin and Li, Xiaolin},
title = {Guoguo: enabling fine-grained indoor localization via smartphone},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464450},
doi = {10.1145/2462456.2464450},
abstract = {Using smartphones for accurate indoor localization opens a new frontier of mobile services, offering enormous opportunities to enhance users' experiences in indoor environments. Despite significant efforts on indoor localization in both academia and industry in the past two decades, highly accurate and practical smartphone-based indoor localization remains an open problem. To enable indoor location-based services (ILBS), there are several stringent requirements for an indoor localization system: highly accurate that can differentiate massive users' locations (foot-level); no additional hardware components or extensions on users' smartphones; scalable to massive concurrent users. Current GPS, Radio RSS (e.g. WiFi, Bluetooth, ZigBee), or Fingerprinting based solutions can only achieve meter-level or room-level accuracy. In this paper, we propose a practical and accurate solution that fills the long-lasting gap of smartphone-based indoor localization. Specifically, we design and implement an indoor localization ecosystem Guoguo. Guoguo consists of an anchor network with a coordination protocol to transmit modulated localization beacons using high-band acoustic signals, a realtime processing app in a smartphone, and a backend server for indoor contexts and location-based services. We further propose approaches to improve its coverage, accuracy, and location update rate with low-power consumption. Our prototype shows centimeter-level localization accuracy in an office and classroom environment. Such precise indoor localization is expected to have high impact in the future ILBS and our daily activities.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {235–248},
numpages = {14},
keywords = {acoustic signal, anchor network, localization, smartphone},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.5555/1765571.1765583,
author = {Liu, Shih-Hsi and Bryant, Barrett R. and Auguston, Mikhail and Gray, Jeff and Raje, Rajeev and Tuceryan, Mihran},
title = {A component-based approach for constructing high-confidence distributed real-time and embedded systems},
year = {2005},
isbn = {9783540711551},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In applying Component-Based Software Engineering (CBSE) techniques to the domain of Distributed Real-time and Embedded (DRE) Systems, there are five critical challenges: 1) discovery of relevant components and resources, 2) specification and modeling of components, 3) exploration and elimination of design assembly options, 4) automated generation of heterogeneous component bridges, and 5) validation of context-related embedded systems. To address these challenges, this paper introduces four core techniques to facilitate high-confidence DRE system construction from components: 1) A component and resource discovery technique promotes component searching based on rich and precise descriptions of components and context; 2) A timed colored Petri Net-based modeling toolkit enables design and analysis on DRE systems, as well as reduces unnecessary later work by eliminating infeasible design options; 3) A formal specification language describes all specifications consistently and automatically generates component bridges for seamless system integration; and 4) A grammar-based formalism specifies context behaviors and validates integrated systems using sufficient context-related test cases. The success of these ongoing techniques may not only accelerate the software development pace and reduce unnecessary development cost, but also facilitate high-confidence DRE system construction using different formalisms over the entire software life-cycle.},
booktitle = {Proceedings of the 12th Monterey Conference on Reliable Systems on Unreliable Networked Platforms},
pages = {225–247},
numpages = {23},
location = {Laguna Beach, CA, 2005}
}

@article{10.1147/JRD.2015.2408911,
author = {Cavalin, P. R. and Gatti, M. A. C. and Moraes, T. G. P. and Oliveira, F. S. and Pinhanez, C. S. and Rademaker, A. and De Paula, R. A.},
title = {A scalable architecture for real-time analysis of microblogging data},
year = {2015},
issue_date = {March/May 2015},
publisher = {IBM Corp.},
address = {USA},
volume = {59},
number = {2–3},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2015.2408911},
doi = {10.1147/JRD.2015.2408911},
abstract = {As events take place in the real world, e.g., sports games and marketing campaigns, people react and interact on online social networks (OSNs), especially microblog services such as Twitter, generating a large stream of data. Analyzing this data presents an opportunity for researchers and companies to better understand human behavior (both on the network and in real life) during the event's lifespan. Designing automated systems to conduct these analyses in fractions of minutes (or even seconds) is subjected to many challenges: the volume of data is large, the number of posts in future events cannot be predicted, and the system need to be always available and running smoothly to avoid information loss and delays on delivering the analytics results. In this paper, we present a scalable architecture for real-time analysis of microblogging data, with the ability to deal with large volumes of posts, by considering modular parallel workflows. This architecture, which has been implemented on the IBM InfoSphere Streams platform, was tested on a real-world use case to conduct sentiment analysis of Twitter posts during the games of the 2013 F\'{e}d\'{e}ration Internationale de Football Association (FIFA) Confederations Cup, and the system has successfully coped with the challenges of this task.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {16:1–16:10},
numpages = {10}
}

@inproceedings{10.5555/800289.811266,
author = {Lord, R. J.},
title = {Probabilistic budgeting: One practical experience},
year = {1977},
publisher = {Winter Simulation Conference},
abstract = {Probabilistic budgeting has been recommended in the accounting literature for nearly fifteen years as a basis for enabling organizations to better plan for and cope with uncertainty. Unfortunately, while it has been argued that the information contained in a probabilistic budget is essential for improved planning and management neither the feasibility or relevance of probabilistic budgets has been demonstrated.An attempt to develop a probabilistic budget utilizing Monte-Carlo Simulation is reported in this paper. It describes how the model of a small transport firm was developed and how the data necessary to support the model was gathered. It then discusses the apparent implications of this probabilistic budget for the firm, for the practical application of simulation techniques to the budget development process, and for the broader field of planning and management control.},
booktitle = {Proceedings of the 9th Conference on Winter Simulation - Volume 2},
pages = {608–616},
numpages = {9},
location = {Gaitersburg, MD},
series = {WSC '77}
}

@inproceedings{10.1007/978-3-642-41338-4_12,
author = {Tallevi-Diotallevi, Simone and Kotoulas, Spyros and Foschini, Luca and L\'{e}cu\'{e}, Freddy and Corradi, Antonio},
title = {Real-Time Urban Monitoring in Dublin Using Semantic and Stream Technologies},
year = {2013},
isbn = {9783642413377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41338-4_12},
doi = {10.1007/978-3-642-41338-4_12},
abstract = {Several sources of information, from people, systems, things, are already available in most modern cities. Processing these continuous flows of information and capturing insight poses unique technical challenges that span from response time constraints to data heterogeneity, in terms of format and throughput. To tackle these problems, we focus on a novel prototype to ease real-time monitoring and decision-making processes for the City of Dublin with three main original technical aspects: (i) an extension to SPARQL to support efficient querying of heterogeneous streams; (ii) a query execution framework and runtime environment based on IBM InfoSphere Streams, a high-performance, industrial strength, stream processing engine; (iii) a hybrid RDFS reasoner, optimized for our stream processing execution framework. Our approach has been validated with real data collected on the field, as shown in our Dublin City video demonstration. Results indicate that real-time processing of city information streams based on semantic technologies is indeed not only possible, but also efficient, scalable and low-latency.},
booktitle = {Proceedings of the 12th International Semantic Web Conference - Part II},
pages = {178–194},
numpages = {17},
series = {ISWC '13}
}

@article{10.1016/j.jengtecman.2011.09.012,
author = {Zhu, Qinghua and Sarkis, Joseph and Lai, Kee-Hung},
title = {Green supply chain management innovation diffusion and its relationship to organizational improvement: An ecological modernization perspective},
year = {2012},
issue_date = {January, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {1},
issn = {0923-4748},
url = {https://doi.org/10.1016/j.jengtecman.2011.09.012},
doi = {10.1016/j.jengtecman.2011.09.012},
abstract = {Drawing on diffusion of innovation and ecological modernization theories, we identify three types of industrial manufacturers, namely early adopters, followers, and laggards, based on the adoption of green supply chain management (GSCM) practices among Chinese manufacturers. Test results indicate that differences exist between the three types of GSCM adopters in terms of their environmental, operational, and economic performance. Understanding how Chinese manufacturers adopt GSCM practices and if this adoption affects their performance contributes theoretical advancement to the diffusion of innovation theory. Practically, the results provide managerial insights for manufacturers to benchmark for environmental management practices and performance improvement.},
journal = {J. Eng. Technol. Manag.},
month = jan,
pages = {168–185},
numpages = {18},
keywords = {C12, C83, Cluster analysis, Diffusion of innovation theory, Empirical taxonomy, Environmental issues, Q56, Supply chain management}
}

@article{10.1016/j.infsof.2010.05.002,
author = {Ortiz, Guadalupe and Prado, Alfonso Garc\'{\i}a De},
title = {Improving device-aware Web services and their mobile clients through an aspect-oriented, model-driven approach},
year = {2010},
issue_date = {October, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.002},
doi = {10.1016/j.infsof.2010.05.002},
abstract = {Context: Mobile devices have become an essential element in our daily lives, even for connecting to the Internet. Consequently, Web services have become extremely important when offering services through the Internet. However, current Web services are very inflexible as regards their invocation from different types of device, especially if we consider the need for them to be adaptable when being invoked from mobile devices. Objective: In this paper, we provide an approach for the creation of flexible Web services which can be invoked transparently from different device types and which return subsequent responses, as well as providing the client's adaptation as a result of the particular device characteristics and end-user preferences in a completely decoupled way. Method: Aspect-Oriented Programming and model-driven development have been used to reduce both the impact of service and client code adaptation for multiple devices as well as to facilitate the developer's task. Results: A model-driven methodology can be followed from system models to code, providing the Web service developer with the option of marking which services should be adapted to mobile devices in the UML models, and obtaining the decoupled adaptation code automatically from the models. Conclusion: We can conclude that the approach presented in this paper provides us with the possibility of following the development of mobile-aware Web services in an integrated platform, benefiting from the use of aspect-oriented techniques not only for maintaining device-related code completely decoupled from the main functionality one, but also allowing a modularized non-intrusive adaptation of mobile clients to the specific device characteristics as well as to final user preferences.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1080–1093},
numpages = {14},
keywords = {Aspect-oriented software development, Mobile devices, Model-driven development, Web service}
}

@inproceedings{10.1145/2207676.2208331,
author = {Gupta, Sidhant and Morris, Daniel and Patel, Shwetak and Tan, Desney},
title = {SoundWave: using the doppler effect to sense gestures},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208331},
doi = {10.1145/2207676.2208331},
abstract = {Gesture is becoming an increasingly popular means of interacting with computers. However, it is still relatively costly to deploy robust gesture recognition sensors in existing mobile platforms. We present SoundWave, a technique that leverages the speaker and microphone already embedded in most commodity devices to sense in-air gestures around the device. To do this, we generate an inaudible tone, which gets frequency-shifted when it reflects off moving objects like the hand. We measure this shift with the microphone to infer various gestures. In this note, we describe the phenomena and detection algorithm, demonstrate a variety of gestures, and present an informal evaluation on the robustness of this approach across different devices and people.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1911–1914},
numpages = {4},
keywords = {doppler, in-air gesture sensing, interaction technique},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@article{10.1016/j.neucom.2016.09.082,
author = {Ge, Hao and Li, Jianhua and Li, Shenghong and Jiang, Wen and Wang, Yifan},
title = {A novel parallel framework for pursuit learning schemes},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {228},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.082},
doi = {10.1016/j.neucom.2016.09.082},
abstract = {Parallel operation of learning automata (LA), which is proposed by Thathachar and Arvind, is a promising mechanism that can reduce the computational burden without compromising accuracy. However, as far as we know, this parallel mechanism has not been widely used due to two reasons: one is the fact that the environment can response to multi-actions simultaneously are few, the other is the relatively slow speed of the learning process.In this paper, a novel parallel framework is presented to reduce the number of required interactions between the incorporated pursuit LA and the environment by introducing decentralized learning and centralized fusion. The philosophy is to learn various aspects of the problem at hand by taking advantage of the diverse exploration of decentralized learning and summarize the common knowledge learned by centralized fusion. Simulations are conducted to verify the effectiveness of our framework and demonstrate its outperforming. The proposed framework is further applied to the stochastic point location problem and obtains an attractive performance.},
journal = {Neurocomput.},
month = mar,
pages = {198–204},
numpages = {7},
keywords = {Centralized fusion, Decentralized learning, Learning automata, Learning speed, Parallel framework}
}

@inproceedings{10.1145/2534645.2534649,
author = {Coetzee, Peter and Jarvis, Stephen},
title = {CRUCIBLE: towards unified secure on- and off-line analytics at scale},
year = {2013},
isbn = {9781450325066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534645.2534649},
doi = {10.1145/2534645.2534649},
abstract = {The burgeoning field of data science benefits from the application of a variety of analytic models and techniques to the oft-cited problems of large volume, high velocity data rates, and significant variety in data structure and semantics. Many approaches make use of common analytic techniques in either a streaming or batch processing paradigm.This paper presents progress in developing a framework for the analysis of large-scale datasets using both of these pools of techniques in a unified manner. This includes: (1) a Domain Specific Language (DSL) for describing analyses as a set of Communicating Sequential Processes, fully integrated with the Java type system, including an Integrated Development Environment (IDE) and a compiler which builds idiomatic Java; (2) a runtime model for execution of an analytic in both streaming and batch environments; and (3) a novel approach to automated management of cell-level security labels, applied uniformly across all runtimes.The paper concludes with a demonstration of the successful use of this system with a sample workload developed in (1), and an analysis of the performance characteristics of each of the runtimes described in (2).},
booktitle = {Proceedings of the 2013 International Workshop on Data-Intensive Scalable Computing Systems},
pages = {43–48},
numpages = {6},
location = {Denver, Colorado},
series = {DISCS-2013}
}

@article{10.1016/j.vlsi.2009.09.001,
author = {Barros, Manuel and Guilherme, Jorge and Horta, Nuno},
title = {Analog circuits optimization based on evolutionary computation techniques},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {1},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2009.09.001},
doi = {10.1016/j.vlsi.2009.09.001},
abstract = {This paper presents a new design automation tool, based on a modified genetic algorithm kernel, in order to improve efficiency on the analog IC design cycle. The proposed approach combines a robust optimization with corner analysis, machine learning techniques and distributed processing capability able to deal with multi-objective and constrained optimization problems. The resulting optimization tool and the improvement in design productivity is demonstrated for the design of CMOS operational amplifiers.},
journal = {Integr. VLSI J.},
month = jan,
pages = {136–155},
numpages = {20},
keywords = {Analog integrated circuit synthesis, Design automation, Evolutionary optimization, Learning strategies}
}

@article{10.1287/isre.1120.0419,
author = {Zablah, Alex R. and Bellenger, Danny N. and Straub, Detmar W. and Johnston, Wesley J.},
title = {Performance Implications of CRM Technology Use: A Multilevel Field Study of Business Customers and Their Providers in the Telecommunications Industry},
year = {2012},
issue_date = {06 2012},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {23},
number = {2},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.1120.0419},
doi = {10.1287/isre.1120.0419},
abstract = {Extant research is equivocal about the organizational performance effects of customer relationship management (CRM) technology use, with some studies reporting positive effects and other studies reporting no effects at all. The present research effort posits that these mixed findings may potentially be explained by two factors: (1) CRM technology use may have different effects on different customers, and (2) different CRM tools may have different performance consequences. This study investigates this possibility by building on relationship marketing and management theory to propose and test a model of the customer-and firm-level consequences of the organizational use of CRM interaction support and customer prioritization tools. The results of data analysis of 295 customer firms nested within 10 provider firms reveal that firm use of CRM interaction support tools is positively related to customers' relationship perceptions, regardless of customer account size. In contrast, the data indicate that use of CRM prioritization tools appears to have positive effects on a firm's larger customers and negative effects on smaller customers. The results also suggest that when considered at an aggregate level, customer perceptions of the exchange relationship are predictive of organizational performance and that the association between these two variables is significant for larger customer accounts but insignificant for smaller accounts. Overall, the study's results help explain some of the inconsistent findings reported in the literature regarding the performance implications of CRM technology use and suggest that use of the technology may serve to enhance organizational performance, at least over the short term.},
journal = {Info. Sys. Research},
month = jun,
pages = {418–435},
numpages = {18},
keywords = {CRM, CRM technology, customer relationship management, multilevel modeling, relationship investment, relationship marketing and management}
}

@inproceedings{10.1145/2973750.2973775,
author = {Zhang, Huanle and Du, Wan and Zhou, Pengfei and Li, Mo and Mohapatra, Prasant},
title = {DopEnc: acoustic-based encounter profiling using smartphones},
year = {2016},
isbn = {9781450342261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973750.2973775},
doi = {10.1145/2973750.2973775},
abstract = {This paper presents DopEnc, an acoustic-based encounter profiling system on smartphones. DopEnc can automatically identify the persons that users interact with in the context of encountering. DopEnc performs encounter profiling in two major steps: (1) Doppler profiling to detect that two persons approach and stop in front of each other via an effective trajectory, and (2) voice profiling to confirm that they are thereafter engaged in an interactive conversation. DopEnc is further extended to support parallel acoustic exploration of many users by incorporating a unique multiple access scheme within the limited inaudible acoustic frequency band. All implementation of DopEnc is based on commodity sensors like speakers, microphones and accelerometers integrated on commercial-off-the-shelf smartphones. We evaluate DopEnc with detailed experiments and a real use-case study of 11 participants. Overall DopEnc achieves an accuracy of 6.9% false positive and 9.7% false negative in real usage.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
pages = {294–307},
numpages = {14},
keywords = {acoustic signals, doppler effect, encounter profiling, multiple access, voice profiling},
location = {New York City, New York},
series = {MobiCom '16}
}

@article{10.1147/JRD.2014.2376112,
author = {Sinharoy, B. and Van Norstrand, J. A. and Eickemeyer, R. J. and Le, H. Q. and Leenstra, J. and Nguyen, D. Q. and Konigsburg, B. and Ward, K. and Brown, M. D. and Moreira, J. E. and Levitan, D. and Tung, S. and Hrusecky, D. and Bishop, J. W. and Gschwind, M. and Boersma, M. and Kroener, M. and Kaltenbach, M. and Karkhanis, T. and Fernsler, K. M.},
title = {IBM POWER8 processor core microarchitecture},
year = {2015},
issue_date = {January/February 2015},
publisher = {IBM Corp.},
address = {USA},
volume = {59},
number = {1},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2014.2376112},
doi = {10.1147/JRD.2014.2376112},
abstract = {The POWER8™ processor is the latest RISC (Reduced Instruction Set Computer) microprocessor from IBM. It is fabricated using the company's 22-nm Silicon on Insulator (SOI) technology with 15 layers of metal, and it has been designed to significantly improve both single-thread performance and single-core throughput over its predecessor, the POWER7® processor. The rate of increase in processor frequency enabled by new silicon technology advancements has decreased dramatically in recent generations, as compared to the historic trend. This has caused many processor designs in the industry to show very little improvement in either single-thread or single-core performance, and, instead, larger numbers of cores are primarily pursued in each generation. Going against this industry trend, the POWER8 processor relies on a much improved core and nest microarchitecture to achieve approximately one-and-a-half times the single-thread performance and twice the single-core throughput of the POWER7 processor in several commercial applications. Combined with a 50% increase in the number of cores (from 8 in the POWER7 processor to 12 in the POWER8 processor), the result is a processor that leads the industry in performance for enterprise workloads. This paper describes the core microarchitecture innovations made in the POWER8 processor that resulted in these significant performance benefits.},
journal = {IBM J. Res. Dev.},
month = jan,
pages = {2:1–2:21},
numpages = {21}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Data stream classification, Ensemble learning, Recurring concept drift, Unlabeled data}
}

@inproceedings{10.5555/1762146.1762166,
author = {D'Alberto, Paolo and P\"{u}schel, Markus and Franchetti, Franz},
title = {performance/energy optimization of dsp transforms on the XScale processor},
year = {2007},
isbn = {9783540693376},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The XScale processor family provides user-controllable independent configuration of CPU, bus, and memory frequencies. This feature introduces another handle for the code optimization with respect to energy consumption or runtime performance. We quantify the effect of frequency configurations on both performance and energy for three signal processing transforms: the discrete Fourier transform (DFT), finite impulse response (FIR) filters, and the Walsh-Hadamard Transform (WHT).To do this, we use SPIRAL, a program generation and optimization system for signal processing transforms. For a given transform to be implemented, SPIRAL searches over different algorithms to find the best match to the given platform with respect to the chosen performance metric (usually runtime). In this paper we use SPIRAL to generate implementations for different frequency configurations and optimize for runtime and physically measured energy consumption. In doing so we show that first, each transform achieves best performance/energy consumption for a different system configuration; second, the best code depends on the chosen configuration, problem size and algorithm; third, the fastest implementation is not always the most energy efficient; fourth, we introduce dynamic (i.e., during execution) reconfiguration in order to further improve performance/energy. Finally, we benchmark SPIRAL generated code against Intel's vendor library routines. We show competitive results as well as 20% performance improvements or energy reduction for selected transforms and problem sizes.},
booktitle = {Proceedings of the 2nd International Conference on High Performance Embedded Architectures and Compilers},
pages = {201–214},
numpages = {14},
location = {Ghent, Belgium},
series = {HiPEAC'07}
}

@inproceedings{10.5555/782010.782022,
author = {Kunz, Thomas and Seuren, Michiel F. H.},
title = {Fast detection of communication patterns in distributed executions},
year = {1997},
publisher = {IBM Press},
abstract = {Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.},
booktitle = {Proceedings of the 1997 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {12},
location = {Toronto, Ontario, Canada},
series = {CASCON '97}
}

@article{10.1007/s11633-021-1299-7,
author = {Ramalepa, Larona Pitso and Jamisola, Rodrigo S.},
title = {A Review on Cooperative Robotic Arms with Mobile or Drones Bases},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {4},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-021-1299-7},
doi = {10.1007/s11633-021-1299-7},
abstract = {This review paper focuses on cooperative robotic arms with mobile or drone bases performing cooperative tasks. This is because cooperative robots are often used as risk-reduction tools to human life. For example, they are used to explore dangerous places such as minefields and disarm explosives. Drones can be used to perform tasks such as aerial photography, military and defense missions, agricultural surveys, etc. The bases of the cooperative robotic arms can be stationary, mobile (ground), or drones. Cooperative manipulators allow faster performance of assigned tasks because of the available “extra hand”. Furthermore, a mobile base increases the reachable ground workspace of cooperative manipulators while a drone base drastically increases this workspace to include the aerial space. The papers in this review are chosen to extensively cover a wide variety of cooperative manipulation tasks and industries that use them. In cooperative manipulation, avoiding self-collision is one of the most important tasks to be performed. In addition, path planning and formation control can be challenging because of the increased number of components to be coordinated.},
journal = {Int. J. Autom. Comput.},
month = aug,
pages = {536–555},
numpages = {20},
keywords = {Cooperative arms, mobile manipulator, aerial manipulator, mobile base, drone base, cooperative tasks}
}

@article{10.1177/1094342017718068,
author = {Videau, Brice and Pouget, Kevin and Genovese, Luigi and Deutsch, Thierry and Komatitsch, Dimitri and Desprez, Fr\'{e}d\'{e}ric and M\'{e}haut, Jean-Fran\c{c}ois},
title = {BOAST},
year = {2018},
issue_date = {1 2018},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {32},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342017718068},
doi = {10.1177/1094342017718068},
abstract = {The portability of real high-performance computing HPC applications on new platforms is an open and very delicate problem. Especially, the performance portability of the underlying computing kernels is problematic as they need to be tuned for each and every platform the application encounters. This article presents BOAST, a metaprogramming framework dedicated to computing kernels. BOAST allows the description of a kernel and its possible optimizations using a domain-specific language. BOAST runtime will then compare the different versions'performance as well as verify their exactness. BOAST is applied to three use cases: a Laplace kernel in OpenCL and two HPC applications BigDFT electronic density computation and SPECFEM3D seismic and wave propagation.},
journal = {Int. J. High Perform. Comput. Appl.},
month = jan,
pages = {28–44},
numpages = {17},
keywords = {Code generation, autotuning, genericity, high-performance computing, nonregression, portability, productivity and software design, testing}
}

@article{10.1145/3377871,
author = {Besta, Maciej and Fischer, Marc and Ben-Nun, Tal and Stanojevic, Dimitri and Licht, Johannes De Fine and Hoefler, Torsten},
title = {Substream-Centric Maximum Matchings on FPGA},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3377871},
doi = {10.1145/3377871},
abstract = {Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4\texttimes{} speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = apr,
articleno = {8},
numpages = {33},
keywords = {Graph computations, energy-efficient graph processing, streaming graph processing}
}

@inproceedings{10.5555/3061436.3061442,
author = {Budiu, Mihai and Isaacs, Rebecca and Murray, Derek and Plotkin, Gordon and Barham, Paul and Al-Kiswany, Samer and Boshmaf, Yazan and Luo, Qingzhou and Andoni, Alexandr},
title = {Interacting with large distributed datasets using sketch},
year = {2016},
isbn = {9783038680062},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {We present Sketch, a library and a distributed runtime for building interactive tools for exploring large datasets, distributed across multiple machines. We have built several applications using Sketch; here we describe a billion-row spreadsheet, and a distributed-systems performance analyzer. Sketch applications allow interactive and responsive exploration of complex distributed datasets, scaling effectively to use large computational resources.},
booktitle = {Proceedings of the 16th Eurographics Symposium on Parallel Graphics and Visualization},
pages = {31–43},
numpages = {13},
location = {Groningen, The Netherlands},
series = {EGPGV '16}
}

@article{10.1109/90.759314,
author = {Hobson, Richard F. and Wong, P. S.},
title = {A parallel embedded-processor architecture for ATM reassembly},
year = {1999},
issue_date = {Feb. 1999},
publisher = {IEEE Press},
volume = {7},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/90.759314},
doi = {10.1109/90.759314},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {23–37},
numpages = {15},
keywords = {ATM, embedded systems, medium access control, segmentation and reassembly}
}

@article{10.1016/j.specom.2012.11.002,
author = {Nam, Kyoung Won and Ji, Yoon Sang and Han, Jonghee and Lee, Sangmin and Kim, Dongwook and Hong, Sung Hwa and Jang, Dong Pyo and Kim, In Young},
title = {Clinical evaluation of the performance of a blind source separation algorithm combining beamforming and independent component analysis in hearing aid use},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {4},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.11.002},
doi = {10.1016/j.specom.2012.11.002},
abstract = {There have been several reports on improved blind source separation algorithms that combine beamforming and independent component analysis. However, none of the prior reports verified the clinical efficacy of such combinational algorithms in real hearing aid situations. In the current study, we evaluated the clinical efficacy of such a combinational algorithm using the mean opinion score and speech recognition threshold tests in various types of real-world hearing aid situations involving environmental noise. Parameters of the testing algorithm were adjusted to match the geometric specifications of the real behind-the-ear type hearing aid housing. The study included 15 normal-hearing volunteers and 15 hearing-impaired patients. Experimental results demonstrated that the testing algorithm improved the speech intelligibility of all of the participants in noisy environments, and the clinical efficacy of the combinational algorithm was superior to either the beamforming or independent component analysis algorithms alone. Despite the computational complexity of the testing algorithm, our experimental results and the rapid enhancement of hardware technology indicate that the testing algorithm has the potential to be applied to real hearing aids in the near future, thereby improving the speech intelligibility of hearing-impaired patients in noisy environments.},
journal = {Speech Commun.},
month = may,
pages = {544–552},
numpages = {9},
keywords = {BSS, BTE, Beamforming, CDMA, CIC, DHA, DOA, FD, Hearing aid, ICA, IRB, ITC, ITE, Independent component analysis, L/R/H, MOS, MVDR, NRR, Noise reduction, SIMO, SMC, SNR, SRT, TD, TIMIT, fMRI}
}

@article{10.1016/j.sigpro.2015.08.019,
author = {Zhong, Zhen and Zhang, Baoju and Durrani, Tariq S. and Xiao, Shuifang},
title = {Nonlinear signal processing for vocal folds damage detection based on heterogeneous sensor network},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2015.08.019},
doi = {10.1016/j.sigpro.2015.08.019},
abstract = {Heterogeneous sensor network-based medical decision making could facilitate the patient diagnosis process. In this paper, we present an intelligent approach for vocal folds damage detection based on patient's vowel voices using heterogeneous sensor network. Based on human voice samples and Hidden Markov Model, we show that transformed voice samples (linearly combined samples) follow Gaussian distribution, further we demonstrate that a type-2 fuzzy membership function (MF), i.e., a Gaussian MF with uncertain mean, is most appropriate to model the transformed voices samples, which motivates us to use a nonlinear signal processing technique, interval type-2 fuzzy logic systems, to handle this problem. We also apply Short-Time-Fourier-Transform (STFT) and Singular-Value-Decomposition (SVD) to the vowel voice samples, and observe that the power decay rate could be used as an identifier in vocal folds damage detection. Two fuzzy classifiers, a Bayesian classifier and a linear classifier, are designed for vocal folds damage detection based on human vowel voices /a:/ and /i:/ only, and the fuzzy classifiers are compared against the Bayesian classifier and linear classifier. Simulation results show that an interval type-2 fuzzy classifier performs the best of the four classifiers. HighlightsHeterogeneous sensor network-based medical decision making could facilitate the patient diagnosis process.Two fuzzy classifiers, a Bayesian classifier and a linear classifier, are designed for vocal folds damage detection.Interval type-2 fuzzy classifier performs the best of the four classifiers.},
journal = {Signal Process.},
month = sep,
pages = {125–133},
numpages = {9},
keywords = {Bayesian classifier, Heterogeneous sensor network, Interval type-2 fuzzy logic systems, Short-time-Fourier-transform, Singular-value decomposition, Vocal folds}
}

@article{10.1145/1515693.1516680,
author = {Madnick, Stuart E. and Wang, Richard Y. and Lee, Yang W. and Zhu, Hongwei},
title = {Overview and Framework for Data and Information Quality Research},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1516680},
doi = {10.1145/1515693.1516680},
abstract = {Awareness of data and information quality issues has grown rapidly in light of the critical role played by the quality of information in our data-intensive, knowledge-based economy. Research in the past two decades has produced a large body of data quality knowledge and has expanded our ability to solve many data and information quality problems. In this article, we present an overview of the evolution and current landscape of data and information quality research. We introduce a framework to characterize the research along two dimensions: topics and methods. Representative papers are cited for purposes of illustrating the issues addressed and the methods used. We also identify and discuss challenges to be addressed in future research.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {2},
numpages = {22},
keywords = {Data quality, information quality, research methods, research topics}
}

@article{10.1504/IJAIP.2009.026765,
author = {Koshizen, Takamasa and Kon, Motohri and Raynolds, Carson and Aihara, Kazuyuki},
title = {Habituation detection with Allen-Cahn boundary generation},
year = {2009},
issue_date = {June 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {1},
number = {4},
issn = {1755-0386},
url = {https://doi.org/10.1504/IJAIP.2009.026765},
doi = {10.1504/IJAIP.2009.026765},
abstract = {We describe a new habituation system that makes use of Partial Differential Equations (PDEs) to deal with time-inconsistent patterns. With respect to our detection system, the Allen-Cahn (AC) equation is used to model a boundary which evolves over time. As a result, the AC could outperform the traditional techniques even real-world dataset. Thus, it leads to a boundary computation for robust detection system.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jun,
pages = {463–487},
numpages = {25},
keywords = {Allen-Cahn equation, PDEs, boundary growth, habituation, partial differential equations, pattern categorisation, robust detection systems, time inconsistency, user modelling}
}

@article{10.1145/1086519.1086526,
author = {Caspi, P. and Sangiovanni-Vincentelli, A. and Almeida, L. and Benveniste, A. and Bouyssounouse, B. and Buttazzo, G. and Crnkovic, I. and Damm, W. and Engblom, J. and Folher, G. and Garcia-Valls, M. and Kopetz, H. and Lakhnech, Y. and Laroussinie, F. and Lavagno, L. and Lipari, G. and Maraninchi, F. and Peti, Ph. and Puente, J. de la and Scaife, N. and Sifakis, J. and de Simone, R. and Torngren, M. and Ver\'{\i}ssimo, P. and Wellings, A. J. and Wilhelm, R. and Willemse, T. and Yi, W.},
title = {Guidelines for a graduate curriculum on embedded software and systems},
year = {2005},
issue_date = {August 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/1086519.1086526},
doi = {10.1145/1086519.1086526},
abstract = {The design of embedded real-time systems requires skills from multiple specific disciplines, including, but not limited to, control, computer science, and electronics. This often involves experts from differing backgrounds, who do not recognize that they address similar, if not identical, issues from complementary angles. Design methodologies are lacking in rigor and discipline so that demonstrating correctness of an embedded design, if at all possible, is a very expensive proposition that may delay significantly the introduction of a critical product. While the economic importance of embedded systems is widely acknowledged, academia has not paid enough attention to the education of a community of high-quality embedded system designers, an obvious difficulty being the need of interdisciplinarity in a period where specialization has been the target of most education systems. This paper presents the reflections that took place in the European Network of Excellence Artist leading us to propose principles and structured contents for building curricula on embedded software and systems.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
pages = {587–611},
numpages = {25},
keywords = {Graduate curriculum, architecture and design, control, distributed systems, embedded systems, extrafunctional properties, labs, real-time}
}

@inbook{10.5555/2172290.2172301,
author = {Te\v{s}anovi\'{c}, Aleksandra and Amirijoo, Mehdi and Hansson, J\"{o}rgen},
title = {Providing configurable qos management in real-time systems with qos aspect packages},
year = {2006},
isbn = {3540488901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Current quality of service (QoS) management approaches in real-time systems lack support for configurability and reusability as they cannot be configured for a target application or reused across many applications. In this paper we present the concept of a QoS aspect package that enables developing configurable QoS management for real-time systems. A QoS aspect package represents both the specification and the implementation of a set of aspects and components that provide a number of QoS policies. A QoS aspect package enables upgrades of already existing systems to support QoS performance assurance by adding aspects and components from the package. Furthermore, a family of real-time systems can easily be developed by adding aspects from the QoS aspect package into an existing system configuration. We illustrate the way a family of real-time database systems is developed using the QoS aspect package with a case study of an embedded real-time database system, called COMET. Our experiments with the COMET database have shown that it is indeed possible to design a real-time system without QoS management and then with a reasonable effort add the QoS dimension to the system using a QoS aspect package.},
booktitle = {Transactions on Aspect-Oriented Software Development II},
pages = {256–288},
numpages = {33}
}

@article{10.5555/2882774.2882775,
author = {Goldenberg, Jacob and Lowengart, Oded and Shapira, Daniel},
title = {Zooming In: Self-Emergence of Movements in New Product Growth},
year = {2009},
issue_date = {March 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {28},
number = {2},
issn = {1526-548X},
abstract = {In this paper, we propose an individual-level approach to diffusion and growth models. By zooming in, we refer to the unit of analysis, which is a single consumer instead of segments or markets and the use of granular sales data daily instead of smoothed e.g., annual data as is more commonly used in the literature. By analyzing the high volatility of daily data, we show how changes in sales patterns can self-emerge as a direct consequence of the stochastic nature of the process. Our contention is that the fluctuations observed in more granular data are not noise, but rather consist of accurate measurement and contain valuable information. By stepping into the noise-like data and treating it as information, we generated better short-term predictions even at very early stages of the penetration process. Using a Kalman-Filter-based tracker, we demonstrate how movements can be traced and how predictions can be significantly improved. We propose that for such tasks, daily data with high volatility offer more insights than do smoothed annual data.},
journal = {Marketing Science},
month = mar,
pages = {274–292},
numpages = {19},
keywords = {adoption, agent base modeling, diffusion, forecasting, growth process, innovation, new product, penetration, sales movements, social networks, takeoff}
}

@inproceedings{10.5555/1161734.1161738,
author = {Kiviat, Philip J.},
title = {A view from the beginning: when does a description become a taxonomy},
year = {2004},
isbn = {0780387864},
publisher = {Winter Simulation Conference},
abstract = {This paper describes the author's career leading up to the publication of his 1969 paper Digital Computer Simulation: Computer Programming Languages, how it influenced the paper, and why the paper has endured as a taxonomy for discrete-event simulation programming languages.},
booktitle = {Proceedings of the 36th Conference on Winter Simulation},
pages = {4–6},
numpages = {3},
location = {Washington, D.C.},
series = {WSC '04}
}

@article{10.1016/j.neucom.2007.08.001,
author = {Pichevar, Ramin and Rouat, Jean},
title = {Monophonic sound source separation with an unsupervised network of spiking neurones},
year = {2007},
issue_date = {December, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {1–3},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2007.08.001},
doi = {10.1016/j.neucom.2007.08.001},
abstract = {We incorporate auditory-based features into an unconventional pattern classification system, consisting of a network of spiking neurones with dynamical and multiplicative synapses. Although the network does not need any training and is autonomous, the analysis is dynamic and capable of extracting multiple features and maps. The neural network allows computing a binary mask that acts as a dynamic switch on a speech vocoder made of an FIR gammatone analysis/synthesis bank of 256 filters. We report experiments on separation of speech from various intruding sounds (siren, telephone bell, speech, etc.) and compare our approach to other techniques by using the log spectral distortion (LSD) metric.},
journal = {Neurocomput.},
month = dec,
pages = {109–120},
numpages = {12},
keywords = {Amplitude modulation, Auditory maps, Auditory scene analysis, Neurones, Source separation, Speech enhancement, Spikes}
}

@article{10.1145/1409060.1409096,
author = {Patney, Anjul and Owens, John D.},
title = {Real-time Reyes-style adaptive surface subdivision},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/1409060.1409096},
doi = {10.1145/1409060.1409096},
abstract = {We present a GPU based implementation of Reyes-style adaptive surface subdivision, known in Reyes terminology as the Bound/Split and Dice stages. The performance of this task is important for the Reyes pipeline to map efficiently to graphics hardware, but its recursive nature and irregular and unbounded memory requirements present a challenge to an efficient implementation. Our solution begins by characterizing Reyes subdivision as a work queue with irregular computation, targeted to a massively parallel GPU. We propose efficient solutions to these general problems by casting our solution in terms of the fundamental primitives of prefix-sum and reduction, often encountered in parallel and GPGPU environments.Our results indicate that real-time Reyes subdivision can indeed be obtained on today's GPUs. We are able to subdivide a complex model to subpixel accuracy within 15 ms. Our measured performance is several times better than that of Pixar's RenderMan. Our implementation scales well with the input size and depth of subdivision. We also address concerns of memory size and bandwidth, and analyze the feasibility of conventional ideas on screen-space buckets.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {143},
numpages = {8},
keywords = {GPGPU, Reyes, adaptive surface subdivision, graphics hardware}
}

@article{10.1145/2898354,
author = {Andreetta, Christian and B\'{e}got, Vivien and Berthold, Jost and Elsman, Martin and Henglein, Fritz and Henriksen, Troels and Nordfang, Maj-Britt and Oancea, Cosmin E.},
title = {FinPar: A Parallel Financial Benchmark},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/2898354},
doi = {10.1145/2898354},
abstract = {Commodity many-core hardware is now mainstream, but parallel programming models are still lagging behind in efficiently utilizing the application parallelism. There are (at least) two principal reasons for this. First, real-world programs often take the form of a deeply nested composition of parallel operators, but mapping the available parallelism to the hardware requires a set of transformations that are tedious to do by hand and beyond the capability of the common user. Second, the best optimization strategy, such as what to parallelize and what to efficiently sequentialize, is often sensitive to the input dataset and therefore requires multiple code versions that are optimized differently, which also raises maintainability problems.This article presents three array-based applications from the financial domain that are suitable for gpgpu execution. Common benchmark-design practice has been to provide the same code for the sequential and parallel versions that are optimized for only one class of datasets. In comparison, we document (1) all available parallelism via nested map-reduce functional combinators, in a simple Haskell implementation that closely resembles the original code structure, (2) the invariants and code transformations that govern the main trade-offs of a data-sensitive optimization space, and (3) report target cpu and multiversion gpgpu code together with an evaluation that demonstrates optimization trade-offs and other difficulties. We believe that this work provides useful insight into the language constructs and compiler infrastructure capable of expressing and optimizing such applications, and we report in-progress work in this direction.},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {18},
numpages = {27},
keywords = {Data-parallel functional language, fission, fusion, strength reduction}
}

@inproceedings{10.1145/1457515.1409096,
author = {Patney, Anjul and Owens, John D.},
title = {Real-time Reyes-style adaptive surface subdivision},
year = {2008},
isbn = {9781450318310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1457515.1409096},
doi = {10.1145/1457515.1409096},
abstract = {We present a GPU based implementation of Reyes-style adaptive surface subdivision, known in Reyes terminology as the Bound/Split and Dice stages. The performance of this task is important for the Reyes pipeline to map efficiently to graphics hardware, but its recursive nature and irregular and unbounded memory requirements present a challenge to an efficient implementation. Our solution begins by characterizing Reyes subdivision as a work queue with irregular computation, targeted to a massively parallel GPU. We propose efficient solutions to these general problems by casting our solution in terms of the fundamental primitives of prefix-sum and reduction, often encountered in parallel and GPGPU environments.Our results indicate that real-time Reyes subdivision can indeed be obtained on today's GPUs. We are able to subdivide a complex model to subpixel accuracy within 15 ms. Our measured performance is several times better than that of Pixar's RenderMan. Our implementation scales well with the input size and depth of subdivision. We also address concerns of memory size and bandwidth, and analyze the feasibility of conventional ideas on screen-space buckets.},
booktitle = {ACM SIGGRAPH Asia 2008 Papers},
articleno = {143},
numpages = {8},
keywords = {GPGPU, Reyes, adaptive surface subdivision, graphics hardware},
location = {Singapore},
series = {SIGGRAPH Asia '08}
}

@book{10.5555/2785635,
author = {Rosing, Mark von and Scheel, Henrik von and Scheer, August-Wilhelm},
title = {The Complete Business Process Handbook: Body of Knowledge from Process Modeling to BPM, Volume I},
year = {2014},
isbn = {0127999590},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {This is the most comprehensive body of knowledge on business processes. Written as a practical guide for Executives, Practitioners, Managers and Students by the authorities that have shaped the way we think and work with process today. This book is one of tree volumes, representing the most comprehensive body of knowledge publised on business process. Second volume uniquely bridging theory with how BPM is applied today with the most extensive information on extended BPM. The third volume explores award winning real-life examples of leading business process practices and how it can be replaced to your advantage. Covering what Practitioners, Managers, Executives and Students need to know about Key Features Learn what Business Process is and how to get started Comprehensive historical process evolution In-depth look at the Process Anatomy, Semantics and Ontology Find out how to link Strategy to Operation with value driven BPM Uncover how to establish a way of Thinking, Working, Modelling and Implementation Explore comprehensive Frameworks, Methods and Approaches How to build BPM competencies and establish a Center of Excellence Discover how to apply Social BPM, Sustainable and Evidence based BPM Learn how Value &amp; Performance Measurement and Management Learn how to roll-out and deploy process Explore how to enable Process Owners, Roles and Knowledge Workers Discover how to Process and Application Modelling Uncover Process Lifecycle, Maturity, Alignment and Continuous Improvement Practical continuous improvement with the way of Governance Future BPM trends that will affect business Explore the BPM Body of Knowledge}
}

@proceedings{10.1145/2998392,
title = {SCALA 2016: Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
year = {2016},
isbn = {9781450346481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1109/TASLP.2016.2627186,
author = {Abeber, Jakob and Frieler, Klaus and Cano, Estefania and Pfleiderer, Martin and Zaddach, Wolf-Georg and Abesser, Jakob and Frieler, Klaus and Cano, Estefan\'{\i}a and Pfleiderer, Martin and Zaddach, Wolf-Georg},
title = {Score-Informed Analysis of Tuning, Intonation, Pitch Modulation, and Dynamics in Jazz Solos},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2627186},
doi = {10.1109/TASLP.2016.2627186},
abstract = {Both the collection and analysis of large music repertoires constitute major challenges within musicological disciplines such as jazz research. Automatic methods of music analysis based on audio signal processing have the potential to assist researchers and to accelerate the transcription and analysis of music recordings significantly. In this paper, we propose a framework for analyzing improvised monophonic solos in multi-instrumental jazz recordings with special focus on reed and brass instruments. The analysis algorithms rely on prior score-information, which is taken from high quality manual solo transcriptions. Following an initial solo and accompaniment source separation, we propose algorithms for tone-wise extraction of fundamental frequency and intensity contours. Based on this fine-grained representation of recorded jazz solos, we perform several exploratory experiments motivated by questions relating to jazz research in order to analyze the use of expressive stylistic devices such as intonation, pitch modulation, and dynamics in jazz solos. The results show that a score-informed audio analysis of jazz recordings can provide valuable insights into the individual stylistic characteristics of jazz musicians.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {168–177},
numpages = {10}
}

@article{10.1155/ASP.2005.3003,
author = {Kates, James M. and Arehart, Kathryn Hoberg},
title = {Multichannel dynamic-range compression using digital frequency warping},
year = {2005},
issue_date = {1 January 2005},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2005},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP.2005.3003},
doi = {10.1155/ASP.2005.3003},
abstract = {A multichannel dynamic-range compressor system using digital frequency warping is described. A frequency-warped filter is realized by replacing the filter unit delays with all-pass filters. The appropriate design of the frequency warping gives a nonuniform frequency representation very close to the auditory Bark scale. The warped compressor is shown to have substantially reduced group delay in comparison with a conventional design having comparable frequency resolution. The warped compressor, however, has more delay at low than at high frequencies, which can lead to perceptible changes in the signal. The detection threshold for the compressor group delay was determined as a function of the number of all-pass filter sections in cascade needed for a detectible change in signal quality. The test signals included clicks, vowels, and speech, and results are presented for both normal-hearing and hearing-impaired subjects. Thresholds for clicks are lower than thresholds for vowels, and hearing-impaired subjects have higher thresholds than normal-hearing listeners. A frequency-warped compressor using a cascade of 31 all-pass filter sections offers a combination of low overall delay, good frequency resolution, and imperceptible frequency-dependent delay effects for most listening conditions.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {3003–3014},
numpages = {12},
keywords = {delay perception, dynamic-range compression, frequency warping, hearing aids}
}

@book{10.5555/1971969,
author = {Tupper, Charles},
title = {Data Architecture: From Zen to Reality},
year = {2011},
isbn = {0123851262},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Data is an expensive and expansive asset. Information capture has forced storage capacity from megabytes to terabytes, exabytes and, pretty soon, zetabytes of data. So the need for accessible storage space for this data is great. To make this huge amount of data usable and relevant, it needs to be organized effectively. Database Base Management Systems, such as Oracle, IBM's DB2, and Microsoft SqlServer are used often, but these are being enhanced continuously and auxiliary tools are being developed every week; there needs to be a fundamental starting point for it all. That stating point is Data Architecture, the blueprint for organizing and structuring of information for services, service providers, and the consumers of that data. Data Architecture: From Zen to Reality explains the principles underlying data architecture, how data evolves with organizations, and the challenges organizations face in structuring and managing their data. It also discusses proven methods and technologies to solve the complex issues dealing with data. The book uses a holistic approach to the field of data architecture by covering the various applied areas of data, including data modelling and data model management, data quality , data governance, enterprise information management, database design, data warehousing, and warehouse design. This book is a core resource for anyone emplacing, customizing or aligning data management systems, taking the Zen-like idea of data architecture to an attainable reality. Presents fundamental concepts of enterprise architecture with definitions and real-world applications and scenariosTeaches data managers and planners about the challenges of building a data architecture roadmap, structuring the right team, and building a long term set of solutions Includes the detail needed to illustrate how the fundamental principles are used in current business practice}
}

@inbook{10.1145/234286.1057822,
author = {Nance, Richard E.},
title = {A history of discrete event simulation programming languages},
year = {1996},
isbn = {0201895021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/234286.1057822},
abstract = {The history of simulation programming languages is organized as a progression in periods of similar developments. The five periods, spanning 1955--1986, are labeled: The Period of Search (1955--1960); The Advent (1961--1965); The Formative Period (1966--1970); The Expansion Period (1971--1978); and The Period of Consolidation and Regeneration (1979--1986). The focus is on recognizing the people and places that have made important contributions in addition to the nature of the contribution. A balance between comprehensive and in-depth treatment has been reached by providing more detailed description of those languages that have or have had major use. Over 30 languages are mentioned, and numerous variations are described. A concluding summary notes the concepts and techniques either originating with simulation programming languages or given significant visibility by them.},
booktitle = {History of Programming Languages---II},
pages = {369–427},
numpages = {59}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@book{10.5555/2155698,
author = {Martin, Grant and Bailey, Brian and Piziali, Andrew},
title = {ESL Design and Verification: A Prescription for Electronic System Level Methodology},
year = {2007},
isbn = {9780080488837},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Visit the authors companion site! http: - Includes interactive forum with the authors! Electronic System Level (ESL) design has mainstreamed --- it is now an established approach at most of the worlds leading system-on-chip (SoC) design companies and is being used increasingly in system design. From its genesis as an algorithm modeling methodology with no links to implementation, ESL is evolving into a set of complementary methodologies that enable embedded system design, verification and debug through to the hardware and software implementation of custom SoC, system-on-FPGA, system-on-board, and entire multi-board systems. This book arises from experience the authors have gained from years of work as industry practitioners in the Electronic System Level design area; they have seen SLD or ESL go through many stages and false starts, and have observed that the shift in design methodologies to ESL is finally occurring. This is partly because of ESL technologies themselves are stabilizing on a useful set of languages being standardized (SystemC is the most notable), and use models are being identified that are beginning to get real adoption. ESL DESIGN &amp; VERIFICATION offers a true prescriptive guide to ESL that reviews its past and outlines the best practices of today. Table of Contents CHAPTER 1: WHAT IS ESL CHAPTER 2: TAXONOMY AND DEFINITIONS FOR THE ELECTRONIC SYSTEM LEVEL CHAPTER 3: EVOLUTION OF ESL DEVELOPMENT CHAPTER 4: WHAT ARE THE ENABLERS OF ESL CHAPTER 5: ESL FLOW CHAPTER 6: SPECIFICATIONS AND MODELING CHAPTER 7: PRE-PARTITIONING ANALYSIS CHAPTER 8: PARTITIONING CHAPTER 9: POST-PARTITIONING ANALYSIS AND DEBUG CHAPTER 10: POST-PARTITIONING VERIFICATION CHAPTER 11: HARDWARE IMPLEMENTATION CHAPTER 12: SOFTWARE IMPLEMENTATION CHAPTER 13: USE OF ESL FOR IMPLEMENTATION VERIFICATION CHAPTER 14: RESEARCH, EMERGING AND FUTURE PROSPECTS APPENDIX: LIST OF ACRONYMS * Provides broad, comprehensive coverage not available in any other such book * Massive global appeal with an internationally recognised author team * Crammed full of state of the art content from notable industry experts}
}

@book{10.5555/2103614,
author = {Hwu, Wen-mei W.},
title = {GPU Computing Gems Jade Edition},
year = {2011},
isbn = {9780123859631},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {This is the second volume of Morgan Kaufmanns GPU Computing Gems, offering an all-new set of insights, ideas, and practical hands-on skills from researchers and developers worldwide. Each chapter gives you a window into the work being performed across a variety of application domains, and the opportunity to witness the impact of parallel GPU computing on the efficiency of scientific research. GPU Computing Gems: Jade Edition showcases the latest research solutions with GPGPU and CUDA, including: Improving memory access patterns for cellular automata using CUDA Large-scale gas turbine simulations on GPU clusters Identifying and mitigating credit risk using large-scale economic capital simulations GPU-powered MATLAB acceleration with Jacket Biologically-inspired machine vision An efficient CUDA algorithm for the maximum network flow problem 30 more chapters of innovative GPU computing ideas, written to be accessible to researchers from any industry GPU Computing Gems: Jade Edition contains 100% new material covering a variety of application domains: algorithms and data structures, engineering, interactive physics for games, computational finance, and programming tools. This second volume of GPU Computing Gems offers 100% new material of interest across industry, including finance, medicine, imaging, engineering, gaming, environmental science, green computing, and more Covers new tools and frameworks for productive GPU computing application development and offers immediate benefit to researchers developing improved programming environments for GPUs Even more hands-on, proven techniques demonstrating how general purpose GPU computing is changing scientific research Distills the best practices of the community of CUDA programmers; each chapter provides insights and ideas as well as hands on skills applicable to a variety of fields Table of Contents Part 1: Parallel Algorithms and Data Structures - Paulius Micikevicius, NVIDIA 1 Large-Scale GPU Search 2 Edge v. Node Parallelism for Graph Centrality Metrics 3 Optimizing parallel prefix operations for the Fermi architecture 4 Building an Efficient Hash Table on the GPU 5 An Efficient CUDA Algorithm for the Maximum Network Flow Problem 6 On Improved Memory Access Patterns for Cellular Automata Using CUDA 7 Fast Minimum Spanning Tree Computation on Large Graphs 8 Fast in-place sorting with CUDA based on bitonic sort Part 2: Numerical Algorithms - Frank Jargstorff, NVIDIA 9 Interval Arithmetic in CUDA 10 Approximating the erfinv Function 11 A Hybrid Method for Solving Tridiagonal Systems on the GPU 12 LU Decomposition in CULA 13 GPU Accelerated Derivative-free Optimization Part 3: Engineering Simulation - Peng Wang, NVIDIA 14 Large-scale gas turbine simulations on GPU clusters 15 GPU acceleration of rarefied gas dynamic simulations 16 Assembly of Finite Element Methods on Graphics Processors 17 CUDA implementation of Vertex-Centered, Finite Volume CFD methods on Unstructured Grids with Flow Control Applications 18 Solving Wave Equations on Unstructured Geometries 19 Fast electromagnetic integral equation solvers on graphics processing units (GPUs) Part 4: Interactive Physics for Games and Engineering Simulation - Richard Tonge, NVIDIA 20 Solving Large Multi-Body Dynamics Problems on the GPU 21 Implicit FEM Solver in CUDA 22 Real-time Adaptive GPU multi-agent path planning Part 5: Computational Finance - Thomas Bradley, NVIDIA 23 High performance finite difference PDE solvers on GPUs for financial option pricing 24 Identifying and Mitigating Credit Risk using Large-scale Economic Capital Simulations 25 Financial Market Value-at-Risk Estimation using the Monte Carlo Method Part 6: Programming Tools and Techniques - Cliff Wooley, NVIDIA 26 Thrust: A Productivity-Oriented Library for CUDA 27 GPU Scripting and Code Generation with PyCUDA 28 Jacket: GPU Powered MATLAB Acceleration 29 Accelerating Development and Execution Speed with Just In Time GPU Code Generation 30 GPU Application Development, Debugging, and Performance Tuning with GPU Ocelot 31 Abstraction for AoS and SoA Layout in C++ 32 Processing Device Arrays with C++ Metaprogramming 33 GPU Metaprogramming: A Case Study in Biologically-Inspired Machine Vision 34 A Hybridization Methodology for High-Performance Linear Algebra Software for GPUs 35 Dynamic Load Balancing using Work-Stealing 36 Applying software-managed caching and CPUGPU task scheduling for accelerating dynamic workloads}
}

@techreport{10.5555/974969,
author = {Yung, Robert},
title = {Evaluation of a Commercial Microprocessor},
year = {1998},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate Division of the University of California, Berkeley.}
}

@inproceedings{10.1145/3064176.3064195,
author = {Chothia, Zaheer and Liagouris, John and Dimitrova, Desislava and Roscoe, Timothy},
title = {Online Reconstruction of Structural Information from Datacenter Logs},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064195},
doi = {10.1145/3064176.3064195},
abstract = {Well-run datacenter application architectures are heavily instrumented to provide detailed traces of messages and remote invocations. Reconstructing user sessions, call graphs, transaction trees, and other structural information from these messages, a process known as sessionization, is the foundation for a variety of diagnostic, profiling, and monitoring tasks essential to the operation of the datacenter.We present the design and implementation of a system which processes log streams at gigabits per second and reconstructs user sessions comprising millions of transactions per second in real time with modest compute resources, while dealing with clock skew, message loss, and other real-world phenomena that make such a task challenging. Our system is based on the Timely Dataflow framework for low latency, data-parallel computation, and we demonstrate its utility with a number of use-cases and traces from a large, operational, mission-critical enterprise data center.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {344–358},
numpages = {15},
keywords = {Data Parallelism, Resource Attribution, Sessionization, Streaming Log Analytics, Trace Trees},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@book{10.5555/2331269,
author = {Barry, Peter and Crowley, Patrick},
title = {Modern Embedded Computing: Designing Connected, Pervasive, Media-Rich Systems},
year = {2012},
isbn = {0123914906},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Modern embedded systems are used for connected, media-rich, and highly integrated handheld devices such as mobile phones, digital cameras, and MP3 players. All of these embedded systems require networking, graphic user interfaces, and integration with PCs, as opposed to traditional embedded processors that can perform only limited functions for industrial applications. While most books focus on these controllers, Modern Embedded Computing provides a thorough understanding of the platform architecture of modern embedded computing systems that drive mobile devices. The book offers a comprehensive view of developing a framework for embedded systems-on-chips. Examples feature the Intel Atom processor, which is used in high-end mobile devices such as e-readers, Internet-enabled TVs, tablets, and net books. Beginning with a discussion of embedded platform architecture and Intel Atom-specific architecture, modular chapters cover system boot-up, operating systems, power optimization, graphics and multi-media, connectivity, and platform tuning. Companion lab materials compliment the chapters, offering hands-on embedded design experience.Learn embedded systems design with the Intel Atom Processor, based on the dominant PC chip architecture. Examples use Atom and offer comparisons to other platformsDesign embedded processors for systems that support gaming, in-vehicle infotainment, medical records retrieval, point-of-sale purchasing, networking, digital storage, and many more retail, consumer and industrial applicationsExplore companion lab materials online that offer hands-on embedded design experience}
}

@article{10.1016/S0140-3664(97)00020-0,
author = {Al-Saqabi, Khaled and Sarwar, Syed and Saleh, Kassem},
title = {Distributed gang scheduling in networks of heterogenous workstations},
year = {1997},
issue_date = {July, 1997},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {20},
number = {5},
issn = {0140-3664},
url = {https://doi.org/10.1016/S0140-3664(97)00020-0},
doi = {10.1016/S0140-3664(97)00020-0},
abstract = {The wide availability of workstation networks and the rapid evolution of workstation technology is a motivation for investigating methods of harnessing the full power of such systems. Individual workstations are not usually effectively utilized by their owners. Owners may be willing to lend the processing power of their workstations if used in an unobtrusive way. The ability to effectively borrow the idle cycles of the workstations in a network and efficiently schedule parallel application programs concurrently onto those idle workstations is the topic of this paper. In this paper, we present a distributed scheduling algorithm that will track the available workstations, i.e. workstations not used by their owners, in networks and act upon those workstations by scheduling processes of parallel applications onto them. Our scheduling objectives are minimizing the average Turn Around Time (TAT) of the scheduled applications and maintaining fairness among scheduled applications by granting each application all the resources it requires. Moreover, scheduling solutions are narrowed to those that produce a responsive and scalable scheduling algorithm.},
journal = {Comput. Commun.},
month = jul,
pages = {338–348},
numpages = {11},
keywords = {Concurrent processing, Dynamic scheduling, Heterogenous distributed systems, Migrations, Workstation networks}
}

@proceedings{10.1145/2986012,
title = {Onward! 2016: Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1155/ASP.2005.2938,
author = {Natarajan, Ajay and Hansen, John H. L. and Arehart, Kathryn Hoberg and Rossi-Katz, Jessica},
title = {An auditory-masking-threshold-based noise suppression algorithm GMMSE-AMT[ERB] for listeners with sensorineural hearing loss},
year = {2005},
issue_date = {1 January 2005},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2005},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP.2005.2938},
doi = {10.1155/ASP.2005.2938},
abstract = {This study describes a new noise suppression scheme for hearing aid applications based on the auditory masking threshold (AMT) in conjunction with a modified generalized minimum mean square error estimator (GMMSE) for individual subjects with hearing loss. The representation of cochlear frequency resolution is achieved in terms of auditory filter equivalent rectangular bandwidths (ERBs). Estimation of AMT and spreading functions for masking are implemented in two ways: with normal auditory thresholds and normal auditory filter bandwidths (GMMSE-AMT[ERB]-NH) and with elevated thresholds and broader auditory filters characteristic of cochlear hearing loss (GMMSE-AMT[ERB]-HI). Evaluation is performed using speech corpora with objective quality measures (segmental SNR, Itakura-Saito), along with formal listener evaluations of speech quality rating and intelligibility. While no measurable changes in intelligibility occurred, evaluations showed quality improvement with both algorithm implementations. However, the customized formulation based on individual hearing losses was similar in performance to the formulation based on the normal auditory system.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {2938–2953},
numpages = {16},
keywords = {auditory masking threshold, equivalent rectangular bandwidth, generalized minimum mean square estimation, hearing impaired, normal hearing}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@article{10.1155/2010/540159,
author = {Brodtkorb, Andre R. and Dyken, Christopher and Hagen, Trond R. and Hjelmervik, Jon M. and Storaasli, Olaf O.},
title = {State-of-the-art in heterogeneous computing},
year = {2010},
issue_date = {January 2010},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {1},
url = {https://doi.org/10.1155/2010/540159},
doi = {10.1155/2010/540159},
abstract = {Node level heterogeneous architectures have become attractive during the last decade for several reasons: compared to traditional symmetric CPUs, they offer high peak performance and are energy and/or cost efficient. With the increase of fine-grained parallelism in high-performance computing, as well as the introduction of parallelism in workstations, there is an acute need for a good overview and understanding of these architectures. We give an overview of the state-of-the-art in heterogeneous computing, focusing on three commonly found architectures: the Cell Broadband Engine Architecture, graphics processing units (GPUs), and field programmable gate arrays (FPGAs). We present a review of hardware, available software tools, and an overview of state-of-the-art techniques and algorithms. Furthermore, we present a qualitative and quantitative comparison of the architectures, and give our view on the future of heterogeneous computing.},
journal = {Sci. Program.},
month = jan,
pages = {1–33},
numpages = {33},
keywords = {Power-efficient architectures, energy and power consumption, microprocessor performance, parallel computer architecture, stream or vector architectures}
}

@article{10.1016/j.comnet.2007.05.003,
author = {Norden, Samphel and Guo, Katherine},
title = {Support for resilient Peer-to-Peer gaming},
year = {2007},
issue_date = {October, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {51},
number = {14},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2007.05.003},
doi = {10.1016/j.comnet.2007.05.003},
abstract = {In areas such as Massively-Multiplayer Online Games (MMOGs), the conventional centralized server model does not scale with the sheer number of simultaneous clients that need to be supported. P2P architectures are increasingly being considered as replacements for traditional client-server architectures in MMOGs. A distributed P2P architecture that uses ''Coordinator'' nodes for handling smaller groups of players has been shown to be especially effective in supporting MMOGs. However, the drawback of moving from centralized to distributed architectures is the loss of control, and more specifically the increase in the vulnerability of the system as a whole to compromises. There has been no prior work on handling the specific case when the Coordinator itself is compromised and cheats, a scenario akin to cheating conducted by the network. We address this problem by proposing an architecture that is resilient to Coordinator compromises and demonstrate the effectiveness of this architecture. We believe that this is an essential step towards enabling a widespread deployment of P2P-based MMOGs.},
journal = {Comput. Netw.},
month = oct,
pages = {4212–4233},
numpages = {22},
keywords = {Cheating, Multiplayer, Overlay networks, P2P games, P2P networks}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@article{10.1145/1124153.1124154,
author = {Keromytis, Angelos D. and Wright, Jason L. and Raadt, Theo De and Burnside, Matthew},
title = {Cryptography as an operating system service: A case study},
year = {2006},
issue_date = {February 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/1124153.1124154},
doi = {10.1145/1124153.1124154},
abstract = {Cryptographic transformations are a fundamental building block in many security applications and protocols. To improve performance, several vendors market hardware accelerator cards. However, until now no operating system provided a mechanism that allowed both uniform and efficient use of this new type of resource.We present the OpenBSD Cryptographic Framework (OCF), a service virtualization layer implemented inside the operating system kernel, that provides uniform access to accelerator functionality by hiding card-specific details behind a carefully designed API. We evaluate the impact of the OCF in a variety of benchmarks, measuring overall system performance, application throughput and latency, and aggregate throughput when multiple applications make use of it.We conclude that the OCF is extremely efficient in utilizing cryptographic accelerator functionality, attaining 95% of the theoretical peak device performance and over 800 Mbps aggregate throughput using 3DES. We believe that this validates our decision to opt for ease of use by applications and kernel components through a uniform API and for seamless support for new accelerators. Furthermore, our evaluation points to several bottlenecks in system and operating system design: data copying between user and kernel modes, PCI bus signaling inefficiency, protocols that use small data units, and single-threaded applications. We identify some of these limitations through a set of measurements focusing on application-layer cryptographic protocols such as SSL. We offer several suggestions for improvements and directions for future work. We provide experimental evidence of the effectiveness of a new approach which we call operating system shortcutting. Shortcutting can improve the performance of application-layer cryptographic protocols by 27% with very small changes to the kernel.},
journal = {ACM Trans. Comput. Syst.},
month = feb,
pages = {1–38},
numpages = {38},
keywords = {Encryption, authentication, cryptographic protocols, digital signatures, hash functions}
}

@book{10.5555/2671171,
author = {Lee, Gary},
title = {Cloud Networking: Understanding Cloud-based Data Center Networks},
year = {2014},
isbn = {0128007281},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Networking: Understanding Cloud-Based Data Center Networks explains the evolution of established networking technologies into distributed, cloud-based networks. Starting with an overview of cloud technologies, the book explains how cloud data center networksleverage distributed systems for network virtualization, storage networking, and software-defined networking. The author offers insider perspective to key components that make a cloud network possible such as switch fabric technology and data center networking standards. The final chapters look ahead to developments in architectures, fabric technology, interconnections, and more. By the end of the book, readers will understand core networking technologies and how they're used in a cloud data center. Understand existing and emerging networking technologies that combine to form cloud data center networks Explains the evolution of data centers from enterprise to private and public cloud networks Reviews network virtualization standards for multi-tenant data center environments Includes cutting-edge detail on the latest switch fabric technologies from the networking team in Intel}
}

@book{10.5555/1971981,
author = {West, Matthew},
title = {Developing High Quality Data Models},
year = {2011},
isbn = {9780123751065},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {A multitude of problems is likely to arise when developing data models. With dozens of attributes and millions of rows, data modelers are always in danger of inconsistency and inaccuracy. The development of the data model itself could result in difficulties presenting accurate data. The need to improve data models begins with getting it right in the first place. Using real-world examples, Developing High Quality Data Models walks the reader through identifying a number of data modeling principles and analysis techniques that enable the development of data models that both meet business requirements and have a consistent basis. The reader is presented with a variety of generic data model patterns that both exemplify the principles and techniques discussed and build upon one another to give a powerful and integrated generic data model. This model has wide applicability across many disciplines in government and industry, including but not limited to energy exploration, healthcare, telecommunications, transportation, military defense, transportation, and more. * Uses a number of common data model patterns to explain how to develop data models over a wide scope in a way that is consistent and of high quality *Offers generic data model templates that are reusable in many applications and are fundamental for developing more specific templates *Develops ideas for creating consistent approaches to high quality data models}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {bipartite networks, community detection, graphon clustering, regularization of random graphs, spectral clustering, stochastic block model, sub-Gaussian biclustering}
}

@inproceedings{10.5555/510378.510612,
author = {Nance, Richard E.},
title = {Keynote address: simulation education: past reflections and future directions},
year = {2000},
isbn = {0780365828},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The results of two surveys of persons concerned with simulation education in the 1974-76 timeframe are compared with the results of a 1997 workshop entitled, "What Makes a Modeling and Simulation Professional?" Analysis of these two samplings, separated in time by over 20 years and admittedly with differing objectives and under dissimilar conditions, is used to identify persistent issues, beliefs or convictions regarding the needs for professionals. The intent is to establish a departure point for further discussion of simulation education.},
booktitle = {Proceedings of the 32nd Conference on Winter Simulation},
pages = {1595–1601},
numpages = {7},
location = {Orlando, Florida},
series = {WSC '00}
}

@book{10.5555/2930834,
author = {Ljubuncic, Igor},
title = {Problem-solving in High Performance Computing: A Situational Awareness Approach with Linux},
year = {2015},
isbn = {9780128010648},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Problem-Solving in High Performance Computing: A Situational Awareness Approach with Linux focuses on understanding giant computing grids as cohesive systems. Unlike other titles on general problem-solving or system administration, this book offers a cohesive approach to complex, layered environments, highlighting the difference between standalone system troubleshooting and complex problem-solving in large, mission critical environments, and addressing the pitfalls of information overload, micro, and macro symptoms, also including methods for managing problems in large computing ecosystems. The authors offer perspective gained from years of developing Intel-based systems that lead the industry in the number of hosts, software tools, and licenses used in chip design. The book offers unique, real-life examples that emphasize the magnitude and operational complexity of high performance computer systems. Provides insider perspectives on challenges in high performance environments with thousands of servers, millions of cores, distributed data centers, and petabytes of shared data Covers analysis, troubleshooting, and system optimization, from initial diagnostics to deep dives into kernel crash dumps Presents macro principles that appeal to a wide range of users and various real-life, complex problems Includes examples from 24/7 mission-critical environments with specific HPC operational constraints Table of Contents Identifying Problems Beginning an Investigation First Level Debugging and Analysis System Internals Systematic Troubleshooting Analyzing Crashed Applications Solving Problems Monitoring and Prevention Implementing Safe Policies Fine-tuning System Performance Summary and Conclusions}
}

@inproceedings{10.1145/324898.325002,
author = {Roeckel, Michael W. and Rivoir, Robert H. and Gibson, Ronald E. and Linder, Stephen P.},
title = {Simulation environments for the design and test of an intelligent controller for autonomous underwater vehicles},
year = {1999},
isbn = {0780357809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/324898.325002},
doi = {10.1145/324898.325002},
booktitle = {Proceedings of the 31st Conference on Winter Simulation: Simulation---a Bridge to the Future - Volume 2},
pages = {1088–1093},
numpages = {6},
location = {Phoenix, Arizona, USA},
series = {WSC '99}
}

@article{10.1162/089976601750541804,
author = {Heinz, Michael G. and Colburn, H. Steven and Carney, Laurel H.},
title = {Evaluating Auditory Performance Limits: I. One-Parameter Discrimination Using a Computational Model for the Auditory Nerve},
year = {2001},
issue_date = {October 2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {13},
number = {10},
issn = {0899-7667},
url = {https://doi.org/10.1162/089976601750541804},
doi = {10.1162/089976601750541804},
abstract = {A method for calculating psychophysical performance limits based on stochastic neural responses is introduced and compared to previous analytical methods for evaluating auditory discrimination of tone frequency and level. The method uses signal detection theory and a computational model for a population of auditory nerve (AN) fiber responses. The use of computational models allows predictions to be made over a wider parameter range and with more complete descriptions of AN responses than in analytical models. Performance based on AN discharge times (all-information) is compared to performance based only on discharge counts (rate-place). After the method is verified over the range of parameters for which previous analytical models are applicable, the parameter space is then extended. For example, a computational model of AN activity that extends to high frequencies is used to explore the common belief that rate-place information is responsible for frequency encoding at high frequencies due to the rolloff in AN phase locking above 2 kHz. This rolloff is thought to eliminate temporal information at high frequencies. Contrary to this belief, results of this analysis show that rate-place predictions for frequency discrimination are inconsistent with human performance in the dependence on frequency for high frequencies and that there is significant temporal information in the AN up to at least 10 kHz. In fact, the all-information predictions match the functional dependence of human performance on frequency, although optimal performance is much better than human performance. The use of computational AN models in this study provides new constraints on hypotheses of neural encoding of frequency in the auditory system; however, the method is limited to simple tasks with deterministic stimuli. A companion article in this issue ("Evaluating Auditory Performance Limits: II") describes an extension of this approach to more complex tasks that include random variation of one parameter, for example, random-level variation, which is often used in psychophysics to test neural encoding hypotheses.},
journal = {Neural Comput.},
month = oct,
pages = {2273–2316},
numpages = {44}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@techreport{10.5555/974958,
author = {Sun Microsystems Laboratories Staff},
title = {Fiscal 1996 Project Portfolio Report},
year = {1996},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {A summary of the significant accomplishments of Sun Microsystems Laboratories for the Fiscal Year ending June 30, 1996.}
}

@book{10.5555/2809012,
author = {Leito, Paulo and Karnouskos, Stamatis},
title = {Industrial Agents: Emerging Applications of Software Agents in Industry},
year = {2015},
isbn = {0128003413},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Industrial Agents explains how multi-agent systems improve collaborative networks to offer dynamic service changes, customization, improved quality and reliability, and flexible infrastructure. Learn how these platforms can offer distributed intelligent management and control functions with communication, cooperation and synchronization capabilities, and also provide for the behavior specifications of the smart components of the system. The book offers not only an introduction to industrial agents, but also clarifies and positions the vision, on-going efforts, example applications, assessment and roadmap applicable to multiple industries. This edited work is guided and co-authored by leaders of the IEEE Technical Committee on Industrial Agents who represent both academic and industry perspectives and share the latest research along with their hands-on experiences prototyping and deploying industrial agents in industrial scenarios.Learn how new scientific approaches and technologies aggregate resources such next generation intelligent systems, manual workplaces and information and material flow systemGain insight from experts presenting the latest academic and industry research on multi-agent systemsExplore multiple case studies and example applications showing industrial agents in a variety of scenariosUnderstand implementations across the enterprise, from low-level control systems to autonomous and collaborative management units}
}

@inproceedings{10.5555/324493.324625,
author = {LaMaire, O. R. and White, W. W.},
title = {The contribution to performance of instruction set usage in System/370},
year = {1986},
isbn = {0818647434},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of 1986 ACM Fall Joint Computer Conference},
pages = {665–674},
numpages = {10},
location = {Dallas, Texas, USA},
series = {ACM '86}
}

@book{10.5555/1999263,
author = {Hennessy, John L. and Patterson, David A.},
title = {Computer Architecture, Fifth Edition: A Quantitative Approach},
year = {2011},
isbn = {012383872X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {5th},
abstract = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the "cloud" are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change. Updated to cover the mobile computing revolutionEmphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms.Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next")Includes three review appendices in the printed text. Additional reference appendices are available online.Includes updated Case Studies and completely new exercises.}
}

@book{10.5555/2633591,
author = {Luisi, James},
title = {Pragmatic Enterprise Architecture: Strategies to Transform Information Systems in the Era of Big Data},
year = {2014},
isbn = {0128002050},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Pragmatic Enterprise Architecture is a practical hands-on instruction manual for enterprise architects. This book prepares you to better engage IT, management, and business users by equipping you with the tools and knowledge you need to address the most common enterprise architecture challenges. You will come away with a pragmatic understanding of and approach to enterprise architecture and actionable ideas to transform your enterprise. Experienced enterprise architect James V. Luisi generously shares life cycle architectures, transaction path analysis frameworks, and more so you can save time, energy, and resources on your next big project. As an enterprise architect, you must have relatable frameworks and excellent communication skills to do your job. You must actively engage and support a large enterprise involving a hundred architectural disciplines with a modest number of subject matter experts across business, information systems, control systems, and operations architecture. They must achieve their mission using the influence of ideas and business benefits expressed in simple terms so that any audience can understand what to do and why. Pragmatic Enterprise Architecture gives you the tools to accomplish your goals in less time with fewer resources. Expand your Enterprise Architecture skills so you can do more in less time with less money with the priceless tips presented Understand the cost of creating new Enterprise Architecture disciplines and contrast those costs to letting them go unmanaged Includes 10 life cycle architectures so that you can properly assess the ROI of performing activities such as outsourcing, insourcing, restructuring, mergers and acquisitions, and more Complete appendix of eight transaction path analysis frameworks provide DBA guidelines for proper physical database design}
}

@book{10.5555/2755633,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML, Third Edition: The Systems Modeling Language},
year = {2014},
isbn = {0128002026},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {A Practical Guide to SysML, Third Edition, fully updated for SysML version 1.4, provides a comprehensive and practical guide for modeling systems with SysML. With their unique perspective as leading contributors to the language, Friedenthal, Moore, and Steiner provide a full description of the language along with a quick reference guide and practical examples to help you use SysML. The book begins with guidance on the most commonly used features to help you get started quickly. Part 1 explains the benefits of a model-based approach, providing an overview of the language and how to apply SysML to model systems. Part 2 includes a comprehensive description of SysML that provides a detailed understanding that can serve as a foundation for modeling with SysML, and as a reference for practitioners. Part 3 includes methods for applying model-based systems engineering using SysML to specify and design systems, and how these methods can help manage complexity. Part 4 deals with topics related to transitioning MBSE practice into your organization, including integration of the system model with other engineering models, and strategies for adoption of MBSE. Learn how and why to deploy MBSE in your organization with an introduction to systems and model-based systems engineering Use SysML to describe systems with this general overview and a detailed description of the Systems Modeling Language Review practical examples of MBSE methodologies to understand their application to specifying and designing a system Includes comprehensive modeling notation tables as an appendix that can be used as a standalone reference}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@proceedings{10.1145/3132787,
title = {SA '17: SIGGRAPH Asia 2017 Mobile Graphics &amp; Interactive Applications},
year = {2017},
isbn = {9781450354103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Mobile Graphics and Interactive Applications will offer attendees the opportunity to explore the opportunities and challenges of mobile applications relevant to the global graphics community.The program will cover the development, technology, and marketing of mobile graphics and interactive applications. It will especially highlight novel uses of graphics and interactivity on mobile devices. Attendees can expect to be exposed to the latest in mobile graphics and interactive applications through expert keynote talks, paper presentations, panel discussions, industry case studies, and hands-on demonstrations.},
location = {Bangkok, Thailand}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@book{10.5555/2843494,
author = {Fisher, Joseph A. and Faraboschi, Paolo and Young, Cliff},
title = {Embedded Computing: A VLIW Approach to Architecture, Compilers and Tools},
year = {2005},
isbn = {9780080477541},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The fact that there are more embedded computers than general-purpose computers and that we are impacted by hundreds of them every day is no longer news. What is news is that their increasing performance requirements, complexity and capabilities demand a new approach to their design. Fisher, Faraboschi, and Young describe a new age of embedded computing design, in which the processor is central, making the approach radically distinct from contemporary practices of embedded systems design. They demonstrate why it is essential to take a computing-centric and system-design approach to the traditional elements of nonprogrammable components, peripherals, interconnects and buses. These elements must be unified in a system design with high-performance processor architectures, microarchitectures and compilers, and with the compilation tools, debuggers and simulators needed for application development. In this landmark text, the authors apply their expertise in highly interdisciplinary hardwaresoftware development and VLIW processors to illustrate this change in embedded computing. VLIW architectures have long been a popular choice in embedded systems design, and while VLIW is a running theme throughout the book, embedded computing is the core topic. Embedded Computing examines both in a book filled with fact and opinion based on the authors many years of R&amp;D experience. Complemented by a unique, professional-quality embedded tool-chain on the authors website, http: Combines technical depth with real-world experience Comprehensively explains the differences between general purpose computing systems and embedded systems at the hardware, software, tools and operating system levels. Uses concrete examples to explain and motivate the trade-offs. Table of Contents Preface Chapter 1: An Introduction to Embedded Processing Chapter 2: An Overview of VLIW and ILP Chapter 3: An Overview of ISA Design Chapter 4: Architectural Structures in ISA design Chapter 5: Microarchitecture Design Chapter 6: System Design and Simulation Chapter 7: Embedded Compiling and Toolchains Chapter 8: Compiling for VLIWs and ILP Chapter 9: The Run-time System Chapter 10: Application Design and Customization Chapter 11: Application Areas Appendix A: The VEX System Appendix B: Glossary Appendix C: Bibliography}
}

@book{10.5555/2028568,
author = {Lund, Arnie},
title = {User Experience Management: Essential Skills for Leading Effective UX Teams},
year = {2011},
isbn = {9780123854964},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The role of UX manager is of vital importance -- it means leading a productive team, influencing businesses to adopt user-centered design, and delivering valuable products customers. Few UX professionals who find themselves in management positions have formal training in management. More often than not they are promoted to a management position after having proven themselves as an effective and successful practitioner.Yet as important as the position of manager is to the advancement of the field there are no books that specifically address the needs of user experience managers. Though information is available on the Web, nothing ties that advice together in the way a manager would need to integrate it in their work. User Experience Management speaks directly to the UX manager and to the unique challenges one may face. It outlines the robust framework for how to be an effective UX manager, from creating a team, to orchestrating product development, to ensuring UX is not compromised, to achieving company buy-in on results. This acts as a checklist readers can use to make sure they have covered the bases as they think about how to build their own user experience programs. Written by an experienced UX manager, and containing testamonials from many leading managers in the field, managers both current and aspiring will find this an invaluable reference loaded with ideas and techniques for managing user experience. *Gives a UX leadership boot-camp from putting together a winning team, to giving them a driving focus, to acting as their spokesman, to handling difficult situations *Full of practical advice and experiences for managers and leaders in virtually any area of the user experience field *Contains best practices, real-world stories, and insights from UX leaders at IBM, Microsoft, SAP, and many more! Table of Contents Chapter 1- Introduction Chapter2- Building the Team Chapter 3- Creating Your Team Chapter 4- Equipping Your Team Chapter 5- Focusing the Team Chapter 6- Creating A High-Performing Team Chapter 7- Communication and Collaboration Chapter 8- Transforming the Organization Chapter 9- Evangelizing UX Chapter 10- Conclusion}
}

@book{10.5555/2480824,
author = {Gaster, Benedict and Howes, Lee and Kaeli, David R. and Mistry, Perhaad and Schaa, Dana},
title = {Heterogeneous Computing with OpenCL: Revised OpenCL 1.2 Edition},
year = {2012},
isbn = {9780124055209},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Heterogeneous Computing with OpenCL teaches OpenCL and parallel programming for complex systems that may include a variety of device architectures: multi-core CPUs, GPUs, and fully-integrated Accelerated Processing Units (APUs) such as AMD Fusion technology. Designed to work on multiple platforms and with wide industry support, OpenCL will help you more effectively program for a heterogeneous future. Written by leaders in the parallel computing and OpenCL communities, this book will give you hands-on OpenCL experience to address a range of fundamental parallel algorithms. The authors explore memory spaces, optimization techniques, graphics interoperability, extensions, and debugging and profiling. Intended to support a parallel programming course, Heterogeneous Computing with OpenCL includes detailed examples throughout, plus additional online exercises and other supporting materials. Explains principles and strategies to learn parallel programming with OpenCL, from understanding the four abstraction models to thoroughly testing and debugging complete applications. Covers image processing, web plugins, particle simulations, video editing, performance optimization, and more. Shows how OpenCL maps to an example target architecture and explains some of the tradeoffs associated with mapping to various architectures Addresses a range of fundamental programming techniques, with multiple examples and case studies that demonstrate OpenCL extensions for a variety of hardware platforms Table of Contents Introduction to Parallel Programming Introduction to OpenCL OpenCL Device Architectures Basic OpenCL Examples Understanding OpenCLs Concurrency and Execution Model Dissecting a CPUGPU OpenCL Implementation Data Management OpenCL Case Study: Convolution OpenCL Case Study: Histogram OpenCL Case Study: Mixed Particle Simulation OpenCL Extensions Foreign Lands: Plugging OpenCL In OpenCL Profiling and Debugging Performance Optimization of an Image Analysis Application}
}

@book{10.5555/1479757,
author = {Azad, Tariq},
title = {Securing Citrix Presentation Server in the Enterprise},
year = {2008},
isbn = {9780080569987},
publisher = {Syngress Publishing},
abstract = {Citrix Presentation Server allows remote users to work off a network server as if they weren't remote. That means: Incredibly fast access to data and applications for users, no third party VPN connection, and no latency issues. All of these features make Citrix Presentation Server a great tool for increasing access and productivity for remote users. Unfortunately, these same features make Citrix just as dangerous to the network it's running on. By definition, Citrix is granting remote users direct access to corporate servers ..achieving this type of access is also the holy grail for malicious hackers. To compromise a server running Citrix Presentation Server, a hacker need not penetrate a heavily defended corporate or government server. They can simply compromise the far more vulnerable laptop, remote office, or home office of any computer connected to that server by Citrix Presentation Server. All of this makes Citrix Presentation Server a high-value target for malicious hackers. And although it is a high-value target, Citrix Presentation Servers and remote workstations are often relatively easily hacked, because they are often times deployed by overworked system administrators who haven't even configured the most basic security features offered by Citrix. "The problem, in other words, isn't a lack of options for securing Citrix instances; the problem is that administrators aren't using them." (eWeek, October 2007). In support of this assertion Security researcher Petko D. Petkov, aka "pdp", said in an Oct. 4 posting that his recent testing of Citrix gateways led him to "tons" of "wide-open" Citrix instances, including 10 on government domains and four on military domains. * The most comprehensive book published for system administrators providing step-by-step instructions for a secure Citrix Presentation Server. * Special chapter by Security researcher Petko D. Petkov'aka "pdp detailing tactics used by malicious hackers to compromise Citrix Presentation Servers. * Companion Web site contains custom Citrix scripts for administrators to install, configure, and troubleshoot Citrix Presentation Server.}
}

@book{10.5555/2974976,
author = {Douglass, Bruce Powel},
title = {Agile Systems Engineering},
year = {2015},
isbn = {9780128023495},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Agile Systems Engineering presents a vision of systems engineering where precise specification of requirements, structure, and behavior meet larger concerns as such as safety, security, reliability, and performance in an agile engineering context. World-renown author and speaker Dr. Bruce Powel Douglass incorporates agile methods and model-based systems engineering (MBSE) to define the properties of entire systems while avoiding errors that can occur when using traditional textual specifications. Dr. Douglass covers the lifecycle of systems development, including requirements, analysis, design, and the handoff to specific engineering disciplines. Throughout, Dr. Douglass couples agile methods with SysML and MBSE to arm system engineers with the conceptual and methodological tools they need to avoid specification defects and improve system quality while simultaneously reducing the effort and cost of systems engineering. Identifies how the concepts and techniques of agile methods can be effectively applied in systems engineering context Shows how to perform model-based functional analysis and tie these analyses back to system requirements and stakeholder needs, and forward to system architecture and interface definition Provides a means by which the quality and correctness of systems engineering data can be assured (before the entire system is built!) Explains agile system architectural specification and allocation of functionality to system components Details how to transition engineering specification data to downstream engineers with no loss of fidelity Includes detailed examples from across industries taken through their stages, including the "Waldo" industrial exoskeleton as a complex system Table of Contents What is Model-Based Systems Engineering What are Agile Methods and Why Should I Care The importance of Agile methods Agile Stakeholder Requirements Engineering Agile Systems Requirements Definition and Analysis System Architectural Analysis and Trade Studies Agile Systems Architectural Design The Handoff to Downstream Engineering Appendix A. T-Wrecks Stakeholder Requirements Appendix B. T-Wrecks System Requirements}
}

@inbook{10.5555/2167748.2167761,
author = {Wojna, Arkadiusz},
title = {Analogy-based reasoning in classifier construction},
year = {2005},
isbn = {3540298304},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Analogy-based reasoning methods in machine learning make it possible to reason about properties of objects on the basis of similarities between objects. A specific similarity based method is the k nearest neighbors (k-nn) classification algorithm. In the k-nn algorithm, a decision about a new object x is inferred on the basis of a fixed number k of the objects most similar to x in a given set of examples. The primary contribution of the dissertation is the introduction of two new classification models based on the k-nn algorithm.The first model is a hybrid combination of the k-nn algorithm with rule induction. The proposed combination uses minimal consistent rules defined by local reducts of a set of examples. To make this combination possible the model of minimal consistent rules is generalized to a metric-dependent form. An effective polynomial algorithm implementing the classification model based on minimal consistent rules has been proposed by Bazan. We modify this algorithm in such a way that after addition of the modified algorithm to the k-nn algorithm the increase of the computation time is inconsiderable. For some tested classification problems the combined model was significantly more accurate than the classical k-nn classification algorithm.For many real-life problems it is impossible to induce relevant global mathematical models from available sets of examples. The second model proposed in the dissertation is a method for dealing with such sets based on locally induced metrics. This method adapts the notion of similarity to the properties of a given test object. It makes it possible to select the correct decision in specific fragments of the space of objects. The method with local metrics improved significantly the classification accuracy of methods with global models in the hardest tested problems.The important issues of quality and efficiency of the k-nn based methods are a similarity measure and the performance time in searching for the most similar objects in a given set of examples, respectively. In this dissertation both issues are studied in detail and some significant improvements are proposed for the similarity measures and for the search methods found in the literature.},
booktitle = {Transactions on Rough Sets IV},
pages = {277–374},
numpages = {98}
}

@book{10.5555/1526227,
author = {Gregg, Michael and Seagren, Eric and Orebaugh, Angela and Jonkman, Matt and Marty, Raffael},
title = {How to Cheat at Configuring Open Source Security Tools},
year = {2007},
isbn = {9780080553566},
publisher = {Syngress Publishing},
abstract = {The Perfect Reference for the Multitasked SysAdmin This is the perfect guide if network security tools is not your specialty. It is the perfect introduction to managing an infrastructure with freely available, and powerful, Open Source tools. Learn how to test and audit your systems using products like Snort and Wireshark and some of the add-ons available for both. In addition, learn handy techniques for network troubleshooting and protecting the perimeter. * Take Inventory See how taking an inventory of the devices on your network must be repeated regularly to ensure that the inventory remains accurate. * Use Nmap Learn how Nmap has more features and options than any other free scanner. * Implement Firewalls Use netfilter to perform firewall logic and see how SmoothWall can turn a PC into a dedicated firewall appliance that is completely configurable. * Perform Basic Hardening Put an IT security policy in place so that you have a concrete set of standards against which to measure. * Install and Configure Snort and Wireshark Explore the feature set of these powerful tools, as well as their pitfalls and other security considerations. * Explore Snort Add-Ons Use tools like Oinkmaster to automatically keep Snort signature files current. * Troubleshoot Network Problems See how to reporting on bandwidth usage and other metrics and to use data collection methods like sniffing, NetFlow, and SNMP. * Learn Defensive Monitoring Considerations See how to define your wireless network boundaries, and monitor to know if they're being exceeded and watch for unauthorized traffic on your network. *Covers the top 10 most popular open source security tools including Snort, Nessus, Wireshark, Nmap, and Kismet *Companion Web site contains dozens of working scripts and tools for readers *Follows Syngress' proven "How to Cheat" pedagogy providing readers with everything they need and nothing they don't}
}

@book{10.5555/2755638,
author = {Sherman, Rick},
title = {Business Intelligence Guidebook: From Data Integration to Analytics},
year = {2014},
isbn = {012411461X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Between the high-level concepts of business intelligence and the nitty-gritty instructions for using vendors' tools lies the essential, yet poorly-understood layer of architecture, design and process. Without this knowledge, Big Data is belittled - projects flounder, are late and go over budget. Business Intelligence Guidebook: From Data Integration to Analytics shines a bright light on an often neglected topic, arming you with the knowledge you need to design rock-solid business intelligence and data integration processes. Practicing consultant and adjunct BI professor Rick Sherman takes the guesswork out of creating systems that are cost-effective, reusable and essential for transforming raw data into valuable information for business decision-makers. After reading this book, you will be able to design the overall architecture for functioning business intelligence systems with the supporting data warehousing and data-integration applications. You will have the information you need to get a project launched, developed, managed and delivered on time and on budget - turning the deluge of data into actionable information that fuels business knowledge. Finally, you'll give your career a boost by demonstrating an essential knowledge that puts corporate BI projects on a fast-track to success. Provides practical guidelines for building successful BI, DW and data integration solutions. Explains underlying BI, DW and data integration design, architecture and processes in clear, accessible language. Includes the complete project development lifecycle that can be applied at large enterprises as well as at small to medium-sized businesses Describes best practices and pragmatic approaches so readers can put them into action. Companion website includes templates and examples, further discussion of key topics, instructor materials, and references to trusted industry sources.}
}

@inproceedings{10.1145/76738.76746,
author = {Balci, O.},
title = {How to assess the acceptability and credibility of simulation results},
year = {1989},
isbn = {0911801588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/76738.76746},
doi = {10.1145/76738.76746},
abstract = {The purpose of this paper is to present a comprehensive life cycle of a simulation study and guide the simulationist in conducting 10 processes, 10 phases, and 13 credibility assessment stages of the life cycle. The guidelines assist the simulation practitioners in: formulating the problem; investigating solution techniques and the system under study; formulating, representing, and programming the simulation model; designing experiments; experimenting; redefining the model; and presenting the simulation results. The guidelines also assist the practitioners in: formulated problem verification, feasibility assessment of simulation, system and objectives definition verification, model qualification, communicative model verification, programmed model verification, experiment design verification, data validation, model validation, and presentation verification. The practitioners can follow the guidelines presented herein and significantly increase their chance of being successful in conducting a simulation study.},
booktitle = {Proceedings of the 21st Conference on Winter Simulation},
pages = {62–71},
numpages = {10},
location = {Washington, D.C., USA},
series = {WSC '89}
}

@book{10.5555/1543376,
author = {Jacob, Bruce and Ng, Spencer and Wang, David},
title = {Memory Systems: Cache, DRAM, Disk},
year = {2007},
isbn = {0123797519},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Is your memory hierarchy stopping your microprocessor from performing at the high level it should be? Memory Systems: Cache, DRAM, Disk shows you how to resolve this problem. The book tells you everything you need to know about the logical design and operation, physical design and operation, performance characteristics and resulting design trade-offs, and the energy consumption of modern memory hierarchies. You learn how to to tackle the challenging optimization problems that result from the side-effects that can appear at any point in the entire hierarchy.As a result you will be able to design and emulate the entire memory hierarchy. . Understand all levels of the system hierarchy -Xcache, DRAM, and disk. . Evaluate the system-level effects of all design choices. . Model performance and energy consumption for each component in the memory hierarchy.}
}

@book{10.5555/2385466,
author = {McCool, Michael and Reinders, James and Robison, Arch},
title = {Structured Parallel Programming: Patterns for Efficient Computation},
year = {2012},
isbn = {9780123914439},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Programming is now parallel programming. Much as structured programming revolutionized traditional serial programming decades ago, a new kind of structured programming, based on patterns, is relevant to parallel programming today. Parallel computing experts and industry insiders Michael McCool, Arch Robison, and James Reinders describe how to design and implement maintainable and efficient parallel algorithms using a pattern-based approach. They present both theory and practice, and give detailed concrete examples using multiple programming models. Examples are primarily given using two of the most popular and cutting edge programming models for parallel programming: Threading Building Blocks, and Cilk Plus. These architecture-independent models enable easy integration into existing applications, preserve investments in existing code, and speed the development of parallel applications. Examples from realistic contexts illustrate patterns and themes in parallel algorithm design that are widely applicable regardless of implementation technology. The patterns-based approach offers structure and insight that developers can apply to a variety of parallel programming models Develops a composable, structured, scalable, and machine-independent approach to parallel computing Includes detailed examples in both Cilk Plus and the latest Threading Building Blocks, which support a wide variety of computers Table of Contents 1. Introduction 2. Map 3. Collectives 4. Data reorganization 5. Fork-join 6. Examples 7. Further Reading}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@book{10.5555/2331379,
author = {Das, Sajal K. and Kant, Krishna and Zhang, Nan},
title = {Handbook on Securing Cyber-Physical Critical Infrastructure},
year = {2012},
isbn = {9780124158153},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The worldwide reach of the Internet allows malicious cyber criminals to coordinate and launch attacks on both cyber and cyber-physical infrastructure from anywhere in the world. This purpose of this handbook is to introduce the theoretical foundations and practical solution techniques for securing critical cyber and physical infrastructures as well as their underlying computing and communication architectures and systems. Examples of such infrastructures include utility networks (e.g., electrical power grids), ground transportation systems (automotives, roads, bridges and tunnels), airports and air traffic control systems, wired and wireless communication and sensor networks, systems for storing and distributing water and food supplies, medical and healthcare delivery systems, as well as financial, banking and commercial transaction assets. The handbook focus mostly on the scientific foundations and engineering techniques - while also addressing the proper integration of policies and access control mechanisms, for example, how human-developed policies can be properly enforced by an automated system. *Addresses the technical challenges facing design of secure infrastructures by providing examples of problems and solutions from a wide variety of internal and external attack scenarios *Includes contributions from leading researchers and practitioners in relevant application areas such as smart power grid, intelligent transportation systems, healthcare industry and so on. *Loaded with examples of real world problems and pathways to solutions utilizing specific tools and techniques described in detail throughout Table of Contents Introduction: Securing Cyber-Physical Infrastructures--An Overview Part 1: Theoretical Foundations of Security Chapter 1: Security and Vulnerability of Cyber-Physical Infrastructure Networks: A Control-Theoretic Approach Chapter 2: Game Theory for Infrastructure Security - The Power of Intent-Based Adversary Models Chapter 3: An Analytical Framework for Cyber-Physical Networks Chapter 4: Evolution of Widely Spreading Worms and Countermeasures : Epidemic Theory and Application Part 2: Security for Wireless Mobile Networks Chapter 5: Mobile Wireless Network Security Chapter 6: Robust Wireless Infrastructure against Jamming Attacks Chapter 7: Security for Mobile Ad Hoc Networks Chapter 8: Defending against Identity-Based Attacks in Wireless Networks Part 3: Security for Sensor Networks Chapter 9: Efficient and Distributed Access Control for Sensor Networks Chapter 10: Defending against Physical Attacks in Wireless Sensor Networks Chapter 11: Node Compromise Detection in Wireless Sensor Networks Part 4: Platform Security Chapter 12: Hardware and Security: Vulnerabilities and Solutions Chapter 13: Languages and Security: Safer Software Through Language and Compiler Techniques Part 5: Cloud Computing and Data Security Chapter 14: Protecting Data in Outsourcing Scenarios Chapter 15: Data Security in Cloud Computing Chapter 16: Secure Mobile Cloud Computing Chapter 17: Relation Privacy Preservation in Online Social Networks Part 6: Event Monitoring and Situation Awareness Chapter 18: Distributed Network and System Monitoring for Securing Cyber-Physical Infrastructure Chapter 19: Discovering and Tracking Patterns of Interest in Security Sensor Streams Chapter 20: Pervasive Sensing and Monitoring for Situational Awareness Chapter 21: Sense and Response Systems for Crisis Management Part 7. Policy Issues in Security Management Chapter 22: Managing and Securing Critical Infrastructure -- A Semantic Policy and Trust-Driven Approach Chapter 23: Policies, Access Control, and Formal Methods Chapter 24: Formal Analysis of Policy based Security Con gurations in Enterprise Networks Part 8: Security Issues in Real-World Systems Chapter 25: Security and Privacy in the Smart Grid Chapter 26: Cyber-physical Security of Automotive Information Technology Chapter¿27: Security and Privacy for Mobile Healthcare (m-Health) Systems Chapter¿28: Security and Robustness in the Internet Infrastructure Chapter¿29: Emergency Vehicular Networks Chapter¿30: Security Issues in VoIP Telecommunication Networks}
}

@book{10.5555/2901596,
author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio},
title = {Data Analysis in the Cloud: Models, Techniques and Applications},
year = {2015},
isbn = {0128028815},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Data Analysis in the Cloud introduces and discusses models, methods, techniques, and systems to analyze the large number of digital data sources available on the Internet using the computing and storage facilities of the cloud. Coverage includes scalable data mining and knowledge discovery techniques together with cloud computing concepts, models, and systems. Specific sections focus on map-reduce and NoSQL models. The book also includes techniques for conducting high-performance distributed analysis of large data on clouds. Finally, the book examines research trends such as Big Data pervasive computing, data-intensive exascale computing, and massive social network analysis.Introduces data analysis techniques and cloud computing conceptsDescribes cloud-based models and systems for Big Data analyticsProvides examples of the state-of-the-art in cloud data analysisExplains how to develop large-scale data mining applications on cloudsOutlines the main research trends in the area of scalable Big Data analysis}
}

@book{10.5555/2597859,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML: The Systems Modeling Language},
year = {2011},
isbn = {9780123852076},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {A general purpose graphical modeling language used to specify, analyze, and design systems that may include hardware, software, and personnel, SysML is now being adopted by companies across a broad range of industries, including aerospace and defense, automotive, and IT system developers. This book is the bestselling, authoritative guide to SysML for systems and software engineers, providing a comprehensive and practical resource for modeling systems with SysML. Fully updated to cover newly released version 1.3, it includes a full description of the modeling language along with a quick reference guide, and shows how an organization or project can transition to model-based systems engineering using SysML, with considerations for processes, methods, tools, and training. Numerous examples to help readers understand how SysML can be used in practice, while reference material facilitates studying for the OMG Systems Modeling Professional (OCSMP) Certification Program, designed to test candidates knowledge of SysML and their ability to use models to represent real-world systems. Authoritative and comprehensive guide to understanding and implementing SysML A quick reference guide, including language descriptions and practical examples Application of model-based methodologies to solve complex system problems Guidance on transitioning to model-based systems engineering using SysML Preparation guide for OMG Certified Systems Modeling Professional (OCSMP) Table of Contents Part I Introduction Systems Engineering Overview Model-Based Systems Engineering3 SysML Language Overview SysML Language Overview Part II Language Description SysML Language Architecture Organizing the Model with Packages Modeling Structure with Blocks Modeling Constraints with Parametrics Modeling Flow-Based Behavior with Activities Modeling Message-Based Behavior with Interactions Modeling Event-Based Behavior with State Machines Modeling Functionality with Use Cases Modeling Text-Based Requirements and their Relationship to Design Modeling Cross-Cutting Relationships with Allocations Customizing SysML for Specific Domains Part III Modeling Examples Water Distiller Example Using Functional Analysis Residential Security System Example Using the Object-Oriented Systems Engineering Method Part IV Transitioning to Model-Based Systems Engineering Integrating SysML into a Systems Development Environment Deploying SysML into an Organization APPENDIXES A-1 SysML Reference Guide A-2 Cross Reference Guide to the OMG Systems Modeling Professional Certification Program (OCSMP) - NEW}
}

@book{10.5555/1477660,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML: Systems Modeling Language},
year = {2008},
isbn = {9780080558363},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This book provides a comprehensive and practical guide for modeling systems with SysML. It includes a full description of the language along with a quick reference guide, and shows how the language can be applied to specify, analyze, and design systems. It contains examples to help readers understand how SysML can be used in practice. The book also includes guidance on how an organization or project can transition to model based systems engineering using SysML, with considerations for processes, methods, tools, and training. *The authoritative guide for understanding and applying SysML *Authored by the foremost experts on the language *Language description, examples, and quick reference guide included}
}

@book{10.5555/2843495,
author = {Wile, Bruce and Goss, John and Roesner, Wolfgang},
title = {Comprehensive Functional Verification: The Complete Industry Cycle},
year = {2005},
isbn = {9780080476643},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {One of the biggest challenges in chip and system design is determining whether the hardware works correctly. That is the job of functional verification engineers and they are the audience for this comprehensive text from three top industry professionals. As designs increase in complexity, so has the value of verification engineers within the hardware design team. In fact, the need for skilled verification engineers has grown dramatically--functional verification now consumes between 40 and 70% of a project's labor, and about half its cost. Currently there are very few books on verification for engineers, and none that cover the subject as comprehensively as this text. A key strength of this book is that it describes the entire verification cycle and details each stage. The organization of the book follows the cycle, demonstrating how functional verification engages all aspects of the overall design effort and how individual cycle stages relate to the larger design process. Throughout the text, the authors leverage their 35 plus years experience in functional verification, providing examples and case studies, and focusing on the skills, methods, and tools needed to complete each verification task. Additionally, the major vendors (Mentor Graphics, Cadence Design Systems, Verisity, and Synopsys) have implemented key examples from the text and made these available on line, so that the reader can test out the methods described in the text. * Comprehensive overview of the complete verification cycle * Combines industry experience with a strong emphasis on functional verification fundamentals * Includes real-world case studies and downloadable software implementations of key examples from the major vendors (Mentor Graphics, Cadence Design Systems, Verisity, and Synopsys) Table of Contents Part I: Introduction to Verification Chapter 1: Verification in the Chip Design Process Chapter 2: Verification Flow Chapter 3: Fundamentals of Simulation Based Verification Chapter 4: The Verification Plan Part II: Simulation-Based Verification Chapter 5: HDLs and Simulation Engines Chapter 6: Creating Environments Chapter 7: Strategies for Simulation-based Stimulus Generation Chapter 8: Strategies for Results Checking in Chapter 9: Pervasive Function Verification Chapter 10: Re-Use Strategies and System Simulation Part III: Formal Verification Chapter 11 Introduction to Formal Verification Chapter 12 Using Formal Verification Part IV: Comprehensive Verification Chapter 13: Completing the Verification Cycle Chapter 14: Advanced Verification Techniques Part V: Case Studies Chapter 15: Case Studies Glossary References}
}

@book{10.5555/1564780,
author = {Hauck, Scott and DeHon, Andre},
title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation},
year = {2007},
isbn = {9780080556017},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The main characteristic of Reconfigurable Computing is the presence of hardware that can be reconfigured to implement specific functionality more suitable for specially tailored hardware than on a simple uniprocessor. Reconfigurable computing systems join microprocessors and programmable hardware in order to take advantage of the combined strengths of hardware and software and have been used in applications ranging from embedded systems to high performance computing. Many of the fundamental theories have been identified and used by the Hardware/Software Co-Design research field. Although the same background ideas are shared in both areas, they have different goals and use different approaches.This book is intended as an introduction to the entire range of issues important to reconfigurable computing, using FPGAs as the context, or "computing vehicles" to implement this powerful technology. It will take a reader with a background in the basics of digital design and software programming and provide them with the knowledge needed to be an effective designer or researcher in this rapidly evolving field. · Treatment of FPGAs as computing vehicles rather than glue-logic or ASIC substitutes · Views of FPGA programming beyond Verilog/VHDL · Broad set of case studies demonstrating how to use FPGAs in novel and efficient ways}
}

@techreport{10.5555/886462,
author = {Lawrence J. Prinzel, III and Alan T., Pope and Frederick G., Freeman and Mark W., Scerbo and Peter J., Mikulka},
title = {Empirical Analysis of EEG and ERPs for Pyschophysiological Adaptive Task Allocation},
year = {2001},
publisher = {NASA Langley Technical Report Server},
abstract = {The present study was designed to test the efficacy of using Electroencephalogram (EEG) and Event-Related Potentials (ERPs) for making task allocation decisions. Thirty-six participants were randomly assigned to an experimental, yoked, or control group condition. Under the experimental condition, a tracking task was switched between task modes based upon the participant''s EEG. The results showed that the use of adaptive aiding improved performance and lowered subjective workload under negative feedback as predicted. Additionally, participants in the adaptive group had significantly lower RMSE and NASA-TLX ratings than participants in either the yoked or control group conditions. Furthermore, the amplitudes of the N1 and P3 ERP components were significantly larger under the experimental group condition than under either the yoked or control group conditions. These results are discussed in terms of the implications for adaptive automation design.}
}

@article{10.1145/359327.359337,
author = {Case, Richard P. and Padegs, Andris},
title = {Architecture of the IBM system/370},
year = {1978},
issue_date = {Jan. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/359327.359337},
doi = {10.1145/359327.359337},
abstract = {This paper discusses the design considerations for the architectural extensions that distinguish System/370 from System/360. It comments on some experiences with the original objectives for System/360 and on the efforts to achieve them, and it describes the reasons and objectives for extending the architecture. It covers virtual storage, program control, data-manipulation instructions, timing facilities, multiprocessing, debugging and monitoring, error handling, and input/output operations. A final section tabulates some of the important parameters of the various IBM machines which implement the architecture.},
journal = {Commun. ACM},
month = jan,
pages = {73–96},
numpages = {24},
keywords = {architecture, computer systems, error handling, instruction sets, virtual storage}
}

@book{10.5555/2500962,
author = {Sheikh, Nauman},
title = {Implementing Analytics: A Blueprint for Design, Development, and Adoption},
year = {2013},
isbn = {9780124016811},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Implementing Analytics demystifies the concept, technology and application of analytics and breaks its implementation down to repeatable and manageable steps, making it possible for widespread adoption across all functions of an organization. Implementing Analytics simplifies and helps democratize a very specialized discipline to foster business efficiency and innovation without investing in multi-million dollar technology and manpower. A technology agnostic methodology that breaks down complex tasks like model design and tuning and emphasizes business decisions rather than the technology behind analytics. Simplifies the understanding of analytics from a technical and functional perspective and shows a wide array of problems that can be tackled using existing technology Provides a detailed step by step approach to identify opportunities, extract requirements, design variables and build and test models. It further explains the business decision strategies to use analytics models and provides an overview for governance and tuning Helps formalize analytics projects from staffing, technology and implementation perspectives Emphasizes machine learning and data mining over statistics and shows how the role of a Data Scientist can be broken down and still deliver the value by building a robust development process Table of Contents 1. Introduction 2. What is Analytics 3. Analytics Project Lifecycle 4. Analytics Project Business Case 5. Analytics Project Architecture 6. Analytics Project Team 7. Analytics Project Development Methodology 8. Existing Technology 9. Specialized Databases 10. Statistical Tools 11. Scoring and Rating Engine 12. Strategy Design Tool}
}

@book{10.5555/1481619,
author = {Wright, Craig S.},
title = {The IT Regulatory and Standards Compliance Handbook: How to Survive Information Systems Audit and Assessments},
year = {2008},
isbn = {9780080560175},
publisher = {Syngress Publishing},
abstract = {This book provides comprehensive methodology, enabling the staff charged with an IT security audit to create a sound framework, allowing them to meet the challenges of compliance in a way that aligns with both business and technical needs. This "roadmap" provides a way of interpreting complex, often confusing, compliance requirements within the larger scope of an organization's overall needs. Key Features: * The ulitmate guide to making an effective security policy and controls that enable monitoring and testing against them * The most comprehensive IT compliance template available, giving detailed information on testing all your IT security, policy and governance requirements * A guide to meeting the minimum standard, whether you are planning to meet ISO 27001, PCI-DSS, HIPPA, FISCAM, COBIT or any other IT compliance requirement * Both technical staff responsible for securing and auditing information systems and auditors who desire to demonstrate their technical expertise will gain the knowledge, skills and abilities to apply basic risk analysis techniques and to conduct a technical audit of essential information systems from this book * This technically based, practical guide to information systems audit and assessment will show how the process can be used to meet myriad compliance issues}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@book{10.5555/1543446,
author = {Stanger, James},
title = {How to Cheat at Securing Linux},
year = {2007},
isbn = {9780080558684},
publisher = {Syngress Publishing},
abstract = {Are you one of the millions of SysAdmins running a Linux server who can't find a current book on Linux security Well .this is the book for you. How to Cheat at Securing Linux Servers is designed to help you deploy a Linux system on the Internet in a variety of security roles. This book provides practical instructions and pointers concerning the open source security tools that we use every day. This book shows you how to use your Linux skills to provide the most important security services such as encryption, authentication, access control, and logging. While writing the book, the authors had the following three-part structure in mind: locking down the network, securing data passing across the network, and protecting the network perimeter with firewalls, DMZs, and VPNs. The Perfect Reference for the Multitasked SysAdmin * Discover Why "Measure Twice, Cut Once" Applies to Securing Linux * Complete Coverage of Hardening the Operating System, Implementing an Intrusion Detection System, and Defending Databases * Short on Theory, History, and Technical Data that Is Not Helpful in Performing Your Job}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@book{10.5555/2086747,
author = {Laszewski, Tom and Nauduri, Prakash},
title = {Migrating to the Cloud: Oracle Client/Server Modernization},
year = {2011},
isbn = {9781597496476},
publisher = {Syngress Publishing},
edition = {1st},
abstract = {Whether your company is planning on database migration, desktop application migration, or has IT infrastructure consolidation projects, this book gives you all the resources youll need. It gives you recommendations on tools, strategy and best practices and serves as a guide as you plan, determine effort and budget, design, execute and roll your modern Oracle system out to production. Focusing on Oracle grid relational database technology and Oracle Fusion Middleware as the target cloud-based architecture, your company can gain organizational efficiency, agility, increase innovation and reduce IT Total Cost of Ownership (TCO) by moving to service-oriented, Web-based cloud architectures. Focuses on Oracle architecture, Middleware and COTS business applications Explains the tools and technologies necessary for your legacy migration Gives useful information about various strategies, migration methodologies and efficient plans for executing migration projects Table of Contents Introduction Chapter 1: Migrating to the Cloud: ClientServer Migrations to the Oracle Cloud Chapter 2: Identifying the Level of Effort and Cost Chapter 3: Methodology and Design Chapter 4: Relational Migration Tools Chapter 5: Database Schema and Data Migration Chapter 6: Database Stored Object Migration Chapter 7: Application MigrationPorting Due to Database Migration Chapter 8: Migrating Applications to the Cloud Chapter 9: Service Enablement of ClientServer Applications Chapter 10: Oracle Database Cloud Infrastructure Planning and Implementation Chapter 11: Sybase Migrations from a Systems Integrator Perspective, and Case Study Chapter 12: Application Migration: Oracle Forms to Oracle Application Development Framework 11g Chapter 13: Application Migration: PowerBuilder to Oracle APEX Chapter 14: Challenges and Emerging Trends}
}

@book{10.5555/2821563,
author = {McGilvray, Danette},
title = {Executing Data Quality Projects: Ten Steps to Quality Data and Trusted InformationTM},
year = {2008},
isbn = {9780080558394},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Information is currency. Recent studies show that data quality problems are costing businesses billions of dollars each year, with poor data linked to waste and inefficiency, damaged credibility among customers and suppliers, and an organizational inability to make sound decisions. In this important and timely new book, Danette McGilvray presents her Ten Steps approach to information quality, a proven method for both understanding and creating information quality in the enterprise. Her trademarked approach-in which she has trained Fortune 500 clients and hundreds of workshop attendees-applies to all types of data and to all types of organizations. * Includes numerous templates, detailed examples, and practical advice for executing every step of the Ten Steps approach. * Allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, and best practices. * A companion Web site includes links to numerous data quality resources, including many of the planning and information-gathering templates featured in the text, quick summaries of key ideas from the Ten Step methodology, and other tools and information available online. Table of Contents Introduction The Reason for This Book Intended Audiences Structure of This Book How to Use This Book Acknowledgements Chapter 1 Overview Impact of Information and Data Quality About the Methodology Approaches to Data Quality in Projects Engaging Management Chapter 2 Key Concepts Introduction Framework for Information Quality (FIQ) Information Life Cycle Data Quality Dimensions Business Impact Techniques Data Categories Data Specifications Data Governance and Stewardship The Information and Data Quality Improvement Cycle The Ten Steps Process Best Practices and Guidelines Chapter 3 The Ten Steps 1. Define Business Need and Approach 2. Analyze Information Environment 3. Assess Data Quality 4. Assess Business Impact 5. Identify Root Causes 6. Develop Improvement Plans 7. Prevent Future Data Errors 8. Correct Current Data Errors 9. Implement Controls 10. Communicate Actions and Results Chapter 4 Structuring Your Project Projects and The Ten Steps Data Quality Project Roles Project Timing Chapter 5 Other Techniques and Tools Introduction Information Life Cycle Approaches Capture Data Analyze and Document Results Metrics Data Quality Tools The Ten Steps and Six Sigma Chapter 6 A Few Final Words Appendix Quick References Framework for Information Quality POSMAD Interaction Matrix Detail POSMAD Phases and Activities Data Quality Dimensions Business Impact Techniques The Ten Steps Overview Definitions of Data Categories}
}

@book{10.5555/2523262,
author = {Jeffers, James and Reinders, James},
title = {Intel Xeon Phi Coprocessor High Performance Programming},
year = {2013},
isbn = {9780124104143},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Authors Jim Jeffers and James Reinders spent two years helping educate customers about the prototype and pre-production hardware before Intel introduced the first Intel Xeon Phi coprocessor. They have distilled their own experiences coupled with insights from many expert customers, Intel Field Engineers, Application Engineers and Technical Consulting Engineers, to create this authoritative first book on the essentials of programming for this new architecture and these new products. This book is useful even before you ever touch a system with an Intel Xeon Phi coprocessor. To ensure that your applications run at maximum efficiency, the authors emphasize key techniques for programming any modern parallel computing system whether based on Intel Xeon processors, Intel Xeon Phi coprocessors, or other high performance microprocessors. Applying these techniques will generally increase your program performance on any system, and better prepare you for Intel Xeon Phi coprocessors and the Intel MIC architecture. A practical guide to the essentials of the Intel Xeon Phi coprocessor Presents best practices for portable, high-performance computing and a familiar and proven threaded, scalar-vector programming model Includes simple but informative code examples that explain the unique aspects of this new highly parallel and high performance computational product Covers wide vectors, many cores, many threads and high bandwidth cachememory architecture Table of Contents 1. Introduction 2. High Performance examples 3. Benchmarking Apps 4. Real-world Situations 5. Lots of Data (Vectors) 6. Lots of Tasks (not Threads) 7. Processing Parallelism 8. Coprocessor Architecture 9. Coprocessor System Software 10. Linux on the Coprocessor 11. Math Library 12. MPI 13. Profiling 14. Summary}
}

@book{10.5555/1537180,
author = {Harley, David and Bechtel, Ken and Blanchard, Michael and Diemer, Henk K. and Lee, Andrew and Muttik, Igor and Zdrnja, Bojan},
title = {AVIEN Malware Defense Guide for the Enterprise},
year = {2007},
isbn = {9780080558660},
publisher = {Syngress Publishing},
abstract = {Members of AVIEN (the Anti-Virus Information Exchange Network) have been setting agendas in malware management for several years: they led the way on generic filtering at the gateway, and in the sharing of information about new threats at a speed that even anti-virus companies were hard-pressed to match. AVIEN members represent the best-protected large organizations in the world, and millions of users. When they talk, security vendors listen: so should you. AVIENs sister organization AVIEWS is an invaluable meeting ground between the security vendors and researchers who know most about malicious code and anti-malware technology, and the top security administrators of AVIEN who use those technologies in real life. This new book uniquely combines the knowledge of these two groups of experts. Anyone who is responsible for the security of business information systems should be aware of this major addition to security literature. * Customer Power takes up the theme of the sometimes stormy relationship between the antivirus industry and its customers, and tries to dispel some common myths. It then considers the roles of the independent researcher, the vendor-employed specialist, and the corporate security specialist. * Stalkers on Your Desktop considers the thorny issue of malware nomenclature and then takes a brief historical look at how we got here, before expanding on some of the malware-related problems we face today. * A Tangled Web discusses threats and countermeasures in the context of the World Wide Web. * Big Bad Bots tackles bots and botnets, arguably Public Cyber-Enemy Number One. * Cr me de la CyberCrime takes readers into the underworld of old-school virus writing, criminal business models, and predicting future malware hotspots. * Defense in Depth takes a broad look at DiD in the enterprise, and looks at some specific tools and technologies. * Perilous Outsorcery offers sound advice on how to avoid the perils and pitfalls of outsourcing, incorporating a few horrible examples of how not to do it. * Education in Education offers some insights into user education from an educationalists perspective, and looks at various aspects of security in schools and other educational establishments. * DIY Malware Analysis is a hands-on, hands-dirty approach to security management, considering malware analysis and forensics techniques and tools. * Antivirus Evaluation &amp; Testing continues the D-I-Y theme, discussing at length some of the thorny issues around the evaluation and testing of antimalware software. * AVIEN &amp; AVIEWS: the Future looks at future developments in AVIEN and AVIEWS. .}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@book{10.5555/2843514,
author = {Wang, Laung-Terng and Chang, Yao-Wen and Cheng, Kwang-Ting (Tim)},
title = {Electronic Design Automation: Synthesis, Verification, and Test},
year = {2009},
isbn = {9780080922003},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This book provides broad and comprehensive coverage of the entire EDA flow. EDA/VLSI practitioners and researchers in need of fluency in an "adjacent" field will find this an invaluable reference to the basic EDA concepts, principles, data structures, algorithms, and architectures for the design, verification, and test of VLSI circuits. Anyone who needs to learn the concepts, principles, data structures, algorithms, and architectures of the EDA flow will benefit from this book. Covers complete spectrum of the EDA flow, from ESL design modeling to logic/test synthesis, verification, physical design, and test - helps EDA newcomers to get "up-and-running" quickly Includes comprehensive coverage of EDA concepts, principles, data structures, algorithms, and architectures - helps all readers improve their VLSI design competence Contains latest advancements not yet available in other books, including Test compression, ESL design modeling, large-scale floorplanning, placement, routing, synthesis of clock and power/ground networks - helps readers to design/develop testable chips or products Includes industry best-practices wherever appropriate in most chapters - helps readers avoid costly mistakes Table of Contents Chapter 1: Introduction Chapter 2: Fundamentals of CMOS Design Chapter 3: Design for Testability Chapter 4: Fundamentals of Algorithms Chapter 5: Electronic System-Level Design and High-Level Synthesis Chapter 6: Logic Synthesis in a Nutshell Chapter 7: Test Synthesis Chapter 8: Logic and Circuit Simulation Chapter 9: Functional Verification Chapter 10: Floorplanning Chapter 11: Placement Chapter 12: Global and Detailed Routing Chapter 13: Synthesis of Clock and Power/Ground Networks Chapter 14: Fault Simulation and Test Generation.}
}

@book{10.5555/1523275,
author = {Wayner, Peter},
title = {Disappearing Cryptography: Information Hiding: Steganography &amp; Watermarking},
year = {2008},
isbn = {9780080922706},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3},
abstract = {Cryptology is the practice of hiding digital information by means of various obfuscatory and steganographic techniques. The application of said techniques facilitates message confidentiality and sender/receiver identity authentication, and helps to ensure the integrity and security of computer passwords, ATM card information, digital signatures, DVD and HDDVD content, and electronic commerce. Cryptography is also central to digital rights management (DRM), a group of techniques for technologically controlling the use of copyrighted material that is being widely implemented and deployed at the behest of corporations that own and create revenue from the hundreds of thousands of mini-transactions that take place daily on programs like iTunes. This new edition of our best-selling book on cryptography and information hiding delineates a number of different methods to hide information in all types of digital media files. These methods include encryption, compression, data embedding and watermarking, data mimicry, and scrambling. During the last 5 years, the continued advancement and exponential increase of computer processing power have enhanced the efficacy and scope of electronic espionage and content appropriation. Therefore, this edition has amended and expanded outdated sections in accordance with new dangers, and includes 5 completely new chapters that introduce newer more sophisticated and refined cryptographic algorithms and techniques (such as fingerprinting, synchronization, and quantization) capable of withstanding the evolved forms of attack. Each chapter is divided into sections, first providing an introduction and high-level summary for those who wish to understand the concepts without wading through technical explanations, and then presenting concrete examples and greater detail for those who want to write their own programs. This combination of practicality and theory allows programmers and system designers to not only implement tried and true encryption procedures, but also consider probable future developments in their designs, thus fulfilling the need for preemptive caution that is becoming ever more explicit as the transference of digital media escalates. * Includes 5 completely new chapters that delineate the most current and sophisticated cryptographic algorithms, allowing readers to protect their information against even the most evolved electronic attacks. * Conceptual tutelage in conjunction with detailed mathematical directives allows the reader to not only understand encryption procedures, but also to write programs which anticipate future security developments in their design. * Grants the reader access to online source code which can be used to directly implement proven cryptographic procedures such as data mimicry and reversible grammar generation into their own work.}
}

@techreport{10.1145/2594496,
author = {Atchison, William F. and Conte, Samuel D. and Hamblen, John W. and Hull, Thomas E. and Keenan, Thomas A. and Kehl, William B. and McCluskey, Edward J. and Navarro, Silvio O. and Rheinboldt, Werner C. and Schweppe, Earl J. and Viavant, William and Young, David M.},
title = {ACM Recommended Curricula for Computer Science and Information Processing Programs in Colleges and Universities, 1968-1981},
year = {1981},
isbn = {0897910583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {When we enter the twenty-first century, computers will have influenced our lives more than any other technology known to civilization. The need for knowledge about computers, computer technology, and the theoretical aspects of computers has grown exponentially. Since its founding in 1947, the Association for Computing Machinery has attempted to address this need for computer knowledge.The study of computers is an endeavor demanding careful, thorough, and organized development. Recognizing the need for a comprehensive model curriculum for the growing number of institutions offering computer science courses, ACM, in the mid-sixties, established the Curriculum Committee on Computer Science to make recommendations and provide guidelines to institutions. Chaired by Dr. William Atchison of the University of Maryland, this dedicated group of forward-thinking people published preliminary recommendations in September 1965. With financial assistance from the National Science Foundation, they published their final report, "Recommendations for Academic Programs in Computer Science," in Communications of the ACM in March 1968. This curriculum, known as "Curriculum '68," formed the basis of formal computer science study in colleges and universities for the next ten years.}
}

@techreport{10.1145/2594501,
author = {Ashenhurst, R. L.},
title = {ACM Curricula Recommendations for Information Systems},
year = {1983},
isbn = {089791 1180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This report contains curriculum recommendations prepared by the ACM Curriculum Committee on Computer Education for Management.}
}

@book{10.5555/2843512,
author = {Rossi, Francesca and van Beek, Peter and Walsh, Toby},
title = {Handbook of Constraint Programming},
year = {2006},
isbn = {9780080463803},
publisher = {Elsevier Science Inc.},
address = {USA},
abstract = {Constraint programming is a powerful paradigm for solving combinatorial search problems that draws on a wide range of techniques from artificial intelligence, computer science, databases, programming languages, and operations research. Constraint programming is currently applied with success to many domains, such as scheduling, planning, vehicle routing, configuration, networks, and bioinformatics. The aim of this handbook is to capture the full breadth and depth of the constraint programming field and to be encyclopedic in its scope and coverage. While there are several excellent books on constraint programming, such books necessarily focus on the main notions and techniques and cannot cover also extensions, applications, and languages. The handbook gives a reasonably complete coverage of all these lines of work, based on constraint programming, so that a reader can have a rather precise idea of the whole field and its potential. Of course each line of work is dealt with in a survey-like style, where some details may be neglected in favor of coverage. However, the extensive bibliography of each chapter will help the interested readers to find suitable sources for the missing details. Each chapter of the handbook is intended to be a self-contained survey of a topic, and is written by one or more authors who are leading researchers in the area. The intended audience of the handbook is researchers, graduate students, higher-year undergraduates and practitioners who wish to learn about the state-of-the-art in constraint programming. No prior knowledge about the field is necessary to be able to read the chapters and gather useful knowledge. Researchers from other fields should find in this handbook an effective way to learn about constraint programming and to possibly use some of the constraint programming concepts and techniques in their work, thus providing a means for a fruitful cross-fertilization among different research areas. The handbook is organized in two parts. The first part covers the basic foundations of constraint programming, including the history, the notion of constraint propagation, basic search methods, global constraints, tractability and computational complexity, and important issues in modeling a problem as a constraint problem. The second part covers constraint languages and solver, several useful extensions to the basic framework (such as interval constraints, structured domains, and distributed CSPs), and successful application areas for constraint programming. - Covers the whole field of constraint programming - Survey-style chapters - Five chapters on applications Table of Contents Foreword (Ugo Montanari) Part I : Foundations Chapter 1. Introduction (Francesca Rossi, Peter van Beek, Toby Walsh) Chapter 2. Constraint Satisfaction: An Emerging Paradigm (Eugene C. Freuder, Alan K. Mackworth) Chapter 3. Constraint Propagation (Christian Bessiere) Chapter 4. Backtracking Search Algorithms (Peter van Beek) Chapter 5. Local Search Methods (Holger H. Hoos, Edward Tsang) Chapter 6. Global Constraints (Willem-Jan van Hoeve, Irit Katriel) Chapter 7. Tractable Structures for CSPs (Rina Dechter) Chapter 8. The Complexity of Constraint Languages (David Cohen, Peter Jeavons) Chapter 9. Soft Constraints (Pedro Meseguer, Francesca Rossi, Thomas Schiex) Chapter 10. Symmetry in Constraint Programming (Ian P. Gent, Karen E. Petrie, Jean-Francois Puget) Chapter 11. Modelling (Barbara M. Smith) Part II : Extensions, Languages, and Applications Chapter 12. Constraint Logic Programming (Kim Marriott, Peter J. Stuckey, Mark Wallace) Chapter 13. Constraints in Procedural and Concurrent Languages (Thom Fruehwirth, Laurent Michel, Christian Schulte) Chapter 14. Finite Domain Constraint Programming Systems (Christian Schulte, Mats Carlsson) Chapter 15. Operations Research Methods in Constraint Programming (John Hooker) Chapter 16. Continuous and Interval Constraints(Frederic Benhamou, Laurent Granvilliers) Chapter 17. Constraints over Structured Domains (Carmen Gervet) Chapter 18. Randomness and Structure (Carla Gomes, Toby Walsh) Chapter 19. Temporal CSPs (Manolis Koubarakis) Chapter 20. Distributed Constraint Programming (Boi Faltings) Chapter 21. Uncertainty and Change (Kenneth N. Brown, Ian Miguel) Chapter 22. Constraint-Based Scheduling and Planning (Philippe Baptiste, Philippe Laborie, Claude Le Pape, Wim Nuijten) Chapter 23. Vehicle Routing (Philip Kilby, Paul Shaw) Chapter 24. Configuration (Ulrich Junker) Chapter 25. Constraint Applications in Networks (Helmut Simonis) Chapter 26. Bioinformatics and Constraints (Rolf Backofen, David Gilbert)}
}

@book{10.5555/2974978,
author = {Rabinowitz, Harold and Vogel, Suzanne},
title = {The Manual of Scientific Style: A Guide for Authors, Editors, and Researchers},
year = {2009},
isbn = {9780080557960},
publisher = {Academic Press, Inc.},
address = {USA},
abstract = {Much like the Chicago Manual of Style, The Manual of Scientific Style addresses all stylistic matters in the relevant disciplines of physical and biological science, medicine, health, and technology. It presents consistent guidelines for text, data, and graphics, providing a comprehensive and authoritative style manual that can be used by the professional scientist, science editor, general editor, science writer, and researcher. Scientific disciplines treated independently, with notes where variances occur in the same linguistic areas Organization and directives designed to assist readers in finding the precise usage rule or convention A focus on American usage in rules and formulations with noted differences between American and British usage Differences in the various levels of scientific discourse addressed in a variety of settings in which science writing appears Instruction and guidance on the means of improving clarity, precision, and effectiveness of science writing, from its most technical to its most popular Table of Contents Part I. General Style Manuscript Preparation General Style Units of Measurement Citation of References Presentation of Data and Figures Part II. References, Citations and Quotations Standards for Clear and Proper Attribution Standard Citation Formats Text Sources Audiovisual Media Electronic Sources Part III. Style Issues for Specific Disciplines Mathematics Physics Chemistry Earth and Environmental Science Life Science Medicine Civil, Mechanical and Electrical Engineering Computer Science and Information Science Appendices Scientific Organizations and Publications: Standard Abbreviations Classification Schemes in Science and Technology Standard Abbreviation Dictionary Difficult and Troublesome Terms and Words Comparative Standards for Shared Terms and Conventions BibliographyIndex}
}

@book{10.5555/983652,
author = {Sloss, Andrew and Symes, Dominic and Wright, Chris},
title = {ARM System Developer's Guide: Designing and Optimizing System Software},
year = {2004},
isbn = {1558608745},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA}
}

@book{10.5555/1095599,
author = {Organick, Elliott I.},
title = {The multics system: an examination of its structure},
year = {1972},
isbn = {0262150123},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This volume provides an overview of the Multics system developed at M.I.T.--a time-shared, general purpose utility like system with third-generation software. The advantage that this new system has over its predecessors lies in its expanded capacity to manipulate and file information on several levels and to police and control access to data in its various files. On the invitation of M.I.T.'s Project MAC, Elliott Organick developed over a period of years an explanation of the workings, concepts, and mechanisms of the Multics system. This book is a result of that effort, and is approved by the Computer Systems Research Group of Project MAC.In keeping with his reputation as a writer able to explain technical ideas in the computer field clearly and precisely, the author develops an exceptionally lucid description of the Multics system, particularly in the area of "how it works." His stated purpose is to serve the expected needs of designers, and to help them "to gain confidence that they are really able to exploit the system fully, as they design increasingly larger programs and subsystems."The chapter sequence was planned to build an understanding of increasingly larger entities. From segments and the addressing of segments, the discussion extends to ways in which procedure segments may link dynamically to one another and to data segments. Subsequent chapters are devoted to how Multics provides for the solution of problems, the file system organization and services, and the segment management functions of the Multics file system and how the user may employ these facilities to advantage. Ultimately, the author builds a picture of the life of a process in coexistence with other processes, and suggests ways to model or construct subsystems that are far more complex than could be implemented using predecessor computer facilities.This volume is intended for the moderately well informed computer user accustomed to predecessor systems and familiar with some of the Multics overview literature. While not intended as a definitive work on this living, ever-changing system, the book nevertheless reflects Multics as it has been first implemented, and should reveal its flavor, structure and power for some time to come.}
}

