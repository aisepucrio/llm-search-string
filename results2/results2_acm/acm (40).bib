@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {tooling roadmap, tool support, state of practice, spl in practice},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Variability, Software Product Line, Repository, Optimisation, Metrics, Energy Efficiency, Clafer},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Service-oriented architecture}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Software Product Line, Quality Attributes, Empirical Software Engineering},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {GORE, Goal models, MAS-PL, Internet of Things, Evolution, Software product line}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1109/SPLC.2011.20,
author = {Siegmund, Norbert and Rosenmuller, Marko and Kastner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable Prediction of Non-functional Properties in Software Product Lines},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.20},
doi = {10.1109/SPLC.2011.20},
abstract = {A software product line is a family of related software products, typically, generated from a set of common assets. Users can select features to derive a product that fulfills their needs. Often, users expect a product to have specific non-functional properties, such as a small footprint or a minimum response time. Because a product line can contain millions of products, it is usually not feasible to generate and measure non-functional properties for each possible product of a product line. Hence, we propose an approach to predict a product's non-functional properties, based on the product's feature selection. To this end, we generate and measure a small set of products, and by comparing the measurements, we approximate each feature's non-functional properties. By aggregating the approximations of selected features, we predict the product's properties. Our technique is independent of the implementation approach and language. We show how already little domain knowledge can improve predictions and discuss trade-offs regarding accuracy and the required number of measurements. Although our approach is in general applicable for quantifiable non-functional properties, we evaluate it for the non-functional property footprint. With nine case studies, we demonstrate that our approach usually predicts the footprint with an accuracy of 98% and an accuracy of over 99% if feature interactions are known.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {160–169},
numpages = {10},
keywords = {software product lines, predicition, non-functional properties, measurement, SPL Conqueror},
series = {SPLC '11}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Software product lines, SPL Conqueror, Prediction, Non-functional properties, Measurement}
}

@article{10.1007/s00766-013-0165-8,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Dynamic decision models for staged software product line configuration},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0165-8},
doi = {10.1007/s00766-013-0165-8},
abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.},
journal = {Requir. Eng.},
month = jun,
pages = {187–212},
numpages = {26},
keywords = {Utility elicitation, Stakeholder preferences, Software product lines, Feature models}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {type theory, relational algebra, quality attributes, higher-order simple predicate logic, core assets},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1016/j.infsof.2015.09.004,
author = {Souza Neto, Pl\'{a}cido A. and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A.},
title = {Designing service-based applications in the presence of non-functional properties},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.004},
doi = {10.1016/j.infsof.2015.09.004},
abstract = {ContextThe development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. ObjectiveThis paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. MethodOur analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. ResultsThis paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. ConclusionThis systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {84–105},
numpages = {22},
keywords = {Systematic mapping, Service-based software process, Non-functional requirements}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Software product line, Refactoring, Reengineering, Legacy system, Evolution}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@article{10.1016/j.infsof.2012.07.017,
author = {Ghezzi, Carlo and Molzam Sharifloo, Amir},
title = {Model-based verification of quantitative non-functional properties for software product lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.017},
doi = {10.1016/j.infsof.2012.07.017},
abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {508–524},
numpages = {17},
keywords = {Software product lines, Quality analysis, Probabilistic model checking, Parametric verification, Non-functional requirements}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@article{10.4018/ijismd.2014070103,
author = {Lotz, Alex and Ingl\'{e}s-Romero, Juan F. and Stampfer, Dennis and Lutz, Matthias and Vicente-Chicote, Cristina and Schlegel, Christian},
title = {Towards a Stepwise Variability Management Process for Complex Systems: A Robotics Perspective},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014070103},
doi = {10.4018/ijismd.2014070103},
abstract = {Complex systems are executed in environments with a huge number of potential situations and contingencies, therefore a mechanism is required to express dynamic variability at design-time that can be efficiently resolved in the application at run-time based on the then available information. We present an approach for dynamic variability modeling and its exploitation at run-time. It supports different developer roles and allows the separation of two different kinds of dynamic variability at design-time: (i) variability related to the system operation, and (ii) variability associated with QoS. The former provides robustness to contingencies, maintaining a high success rate in task fulfillment. The latter focuses on the quality of the application execution (defined in terms of non-functional properties like safety or task efficiency) under changing situations and limited resources. The authors also discuss different alternatives for the run-time integration of the two variability management mechanisms, and show real-world robotic examples to illustrate them.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {55–74},
numpages = {20},
keywords = {Variability Management, VML, SmartTCL, Service Robotics, Modeling Run-Time Variability}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Software architecture, Feature models, Dynamic variability, Dynamic Software Product Lines}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {software product line, repository mining, feature revision, feature location, benchmark extractor},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.5555/2022115.2022129,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Software product line evolution with cardinality-based feature models},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are widely used for modelling variability present in a Software Product Line family. We propose using cardinality-based feature models and clonable features to model and manage the evolution of the structural variability present in pervasive systems, composed by a large variety of heterogeneous devices. The use of clonable features increases the expressiveness of feature models, but also greatly increases the complexity of the resulting configurations. So, supporting the evolution of product configurations becomes an intractable task to do it manually. In this paper, we propose a model driven development process to propagate changes made in an evolved feature model, into existing configurations. Furthermore, our process allows us to calculate the effort needed to perform the evolution changes in the customized products. To do this, we have defined two operators, one to calculate the differences between two configurations and another to create a new configuration from a previous one. Finally, we validate our approach, showing that by using our tool support we can generate new configurations for a family of products with thousands of cloned features.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {102–118},
numpages = {17},
keywords = {software product lines, feature models, evolution},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {uniform random sampling, t-wise sampling, software product lines, decision propagation, configurable system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {software product lines, mitigation, measurement, energy, consumption},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {variability, software product line, numerical, model, attribute, NFQA},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {software product line, software configuration, satisfiability solving, product configuration, knownledge compilation, feature models, decision models, configurable system, binary decision diagrams, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {uml, software product line, software performance engineering, model transformation, marte},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.compind.2006.09.004,
author = {Boucher, Xavier and Bonjour, Eric and Grabot, Bernard},
title = {Formalisation and use of competencies for industrial performance optimisation: A survey},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {2},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2006.09.004},
doi = {10.1016/j.compind.2006.09.004},
abstract = {For many years, industrial performance has been implicitly considered as deriving from the optimisation of technological and material resources (machines, inventories, etc.), made possible by centralized organisations. The topical requirements for reactive and flexible industrial systems have progressively reintroduced the human workforce as the main source of industrial performance. Making this paradigm operational requires the identification and careful formalisation of the link between human resource and industrial performance, through concepts like skills, competencies or know-how. This paper provides a general survey of the formalisation and integration of competence-oriented concepts within enterprise information systems and decision systems, aiming at providing new methods and tools for performance management.},
journal = {Comput. Ind.},
month = feb,
pages = {98–117},
numpages = {20},
keywords = {Performance, Information and decision systems, Enterprise modelling, Competence model}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {variability-aware analysis, variability management, software variation, software product lines, software evolution, software configuration management, regression analysis, product-line analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {variability modeling, empirical study, constraint solving, architecture synthesis, allocation problem},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/11946441_74,
author = {Bonelli, Andreas and Franchetti, Franz and Lorenz, Juergen and P\"{u}schel, Markus and Ueberhuber, Christoph W.},
title = {Automatic performance optimization of the discrete fourier transform on distributed memory computers},
year = {2006},
isbn = {3540680675},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11946441_74},
doi = {10.1007/11946441_74},
abstract = {This paper introduces a formal framework for automatically generating performance optimized implementations of the discrete Fourier transform (DFT) for distributed memory computers. The framework is implemented as part of the program generation and optimization system Spiral. DFT algorithms are represented as mathematical formulas in Spiral's internal language SPL. Using a tagging mechanism and formula rewriting, we extend Spiral to automatically generate parallelized formulas. Using the same mechanism, we enable the generation of rescaling DFT algorithms, which redistribute the data in intermediate steps to fewer processors to reduce communication overhead. It is a novel feature of these methods that the redistribution steps are merged with the communication steps of the algorithm to avoid additional communication overhead. Among the possible alternative algorithms, Spiral's search mechanism now determines the fastest for a given platform, effectively generating adapted code without human intervention. Experiments with DFT MPI programs generated by Spiral show performance gains of up to 30% due to rescaling. Further, our generated programs compare favorably with Fftw-MPI 2.1.5.},
booktitle = {Proceedings of the 4th International Conference on Parallel and Distributed Processing and Applications},
pages = {818–832},
numpages = {15},
location = {Sorrento, Italy},
series = {ISPA'06}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {variability modeling, product lines, language design, feature model, expressiveness, automated analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {software product line, reference architecture, SPL design method},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {timed automata, reconfiguration decisions, dynamic software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {state machine, modeling language, dynamic software product lines, constraint programming, IoT},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {self-adaptation, on-line learning, knowledge},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Tools, Software product line engineering, Controlled experiment}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {product line, feature modeling, core assets, case study},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Software product line engineering, Optimal feature selection, Indicator-based evolutionary algorithm (IBEA), Differential evolutionary algorithm (DE)}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {product lines, multi-criteria evaluation, feature model, consistency check, configuration, automation systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {mobile systems, autonomic computing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {state space reduction, feature models, contexts, adaptive systems, DSPL},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/11554844_7,
author = {Zhang, Weishan and Jarzabek, Stan},
title = {Reuse without compromising performance: industrial experience from RPG software product line for mobile devices},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_7},
doi = {10.1007/11554844_7},
abstract = {It is often believed that reusable solutions, being generic, must necessarily compromise performance. In this paper, we consider a family of Role-Playing Games (RPGs). We analyzed similarities and differences among four RPGs. By applying a reuse technique of XVCL, we built an RPG product line architecture (RPG-PLA) from which we could derive any of the four RPGs. We built into the RPG-PLA a number of performance optimization strategies that could benefit any of the four (and possibly other similar) RPGs. By comparing the original vs. the new RPGs derived from the RPG-PLA, we demonstrated that reuse allowed us to achieve improved performance, both speed and memory utilization, as compared to each game developed individually. At the same time, our solution facilitated rapid development of new games, for new mobile devices, as well as ease of evolving with new features the RPG-PLA and custom games already in use.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {57–69},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {relational database management system, quality attributes, product line architecture, enterprise applications, data oriented approach, data intensiveness, core asset development},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Self adaptation, Software product lines, Feature model, Service composition}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {variability management, software product line (SPL), service-oriented architecture (SOA), service variability, process family, non-functional properties, feature modeling, QoS aggregation},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {variability modeling, software product line teaching, software engineering teaching, Software product lines}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Utility Elicitation, Software Product Line, Economic Value, Decision Process, Configuration Process}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Software product line, Quality attributes assessment, Product configuration, Non-functional requirement (NFR) framework, Feature model, Analytic hierarchical process (AHP)}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Software Product Line, Software Development, Semantic Web, Method Oriented Architecture MOA, Method Engineering}
}

@article{10.1007/s10619-013-7130-x,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Automatic optimization of stream programs via source program operator graph transformations},
year = {2013},
issue_date = {December  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-013-7130-x},
doi = {10.1007/s10619-013-7130-x},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {543–599},
numpages = {57},
keywords = {Stream processing, Performance optimization, Data-intensive computing, Code transformation, Automatic tuning}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Models, Classification, Framework, Software product lines, Feature model}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Systematic literature review, Product configuration, Extended product line}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Software product line, Feature facets, Case study, Bitcoin-wallet, Marlin, Feature location}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Framework, Systematic mapping study, Collaborative configuration, Product lines}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Systematic literature review, Product line engineering, Multi product lines, Large-scale systems}
}

@inproceedings{10.1145/3131473.3131486,
author = {Kazdaridis, Giannis and Keranidis, Stratos and Symeonidis, Polychronis and Dias, Paulo Sousa and Gon\c{c}alves, Pedro and Loureiro, Bruno and Gjanci, Petrika and Petrioli, Chiara},
title = {EVERUN: Enabling Power Consumption Monitoring in Underwater Networking Platforms},
year = {2017},
isbn = {9781450351478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131473.3131486},
doi = {10.1145/3131473.3131486},
abstract = {The energy restricted nature of underwater sensor networks directly affects the expected lifetime of autonomous deployments and limits the capabilities for long term underwater monitoring. Towards the goal of developing energy-efficient protocols and algorithms, researchers and equipment vendors require in-depth understanding of the power consumption characteristics of underwater hardware when deployed in-field. In this work, we introduce the EVERUN power monitoring framework, consisting of hardware and software components that were integrated with real equipment of the SUNRISE testbed facilities. Through the execution of a wide set of experiments under realistic conditions, we highlighted the limitations of model-based energy evaluation tools and characterized the energy efficiency performance of key protocols and mechanisms. The accuracy of the collected power data, along with the interesting derived findings, verified the applicability of our approach in evaluating the energy efficiency performance of proposed solutions.},
booktitle = {Proceedings of the 11th Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {83–90},
numpages = {8},
keywords = {underwater networking, testbed experimentation, power consumption monitorin, energy efficiency},
location = {Snowbird, Utah, USA},
series = {WiNTECH '17}
}

@inproceedings{10.1145/3417113.3423000,
author = {de Macedo, Jo\~{a}o and Alo\'{\i}sio, Jo\~{a}o and Gon\c{c}alves, Nelson and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Energy wars - Chrome vs. Firefox: which browser is more energy efficient?},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3423000},
doi = {10.1145/3417113.3423000},
abstract = {This paper presents a preliminary study on the energy consumption of two popular web browsers. In order to properly measure the energy consumption of both environments, we simulate the usage of various applications, which the goal to mimic typical user interactions and usage.Our preliminary results show interesting findings based on observation, such as what type of interactions generate high peaks of energy consumption, and which browser is overall the most efficient. Our goal with this preliminary study is to show to users how very different the efficiency of web browsers can be, and may serve with advances in this area of study.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {159–165},
numpages = {7},
keywords = {web browsers, green software, energy efficiency},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2188286.2188347,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Hirundo: a mechanism for automated production of optimized data stream graphs},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188347},
doi = {10.1145/2188286.2188347},
abstract = {Stream programs have to be crafted carefully to maximize the performance gain that can be obtained from stream processing environments. Manual fine tuning of a stream program is a very difficult process which requires considerable amount of programmer time and expertise. In this paper we present Hirundo, which is a mechanism for automatically generating optimized stream programs that are tailored for the environment they run. Hirundo analyzes, identifies the structure of a stream program, and transforms it to many different sample programs with same semantics using the notions of Tri-Operator Transformation, Transformer Blocks, and Operator Blocks Fusion. Then it uses empirical optimization information to identify a small subset of generated sample programs that could deliver high performance. It runs the selected sample programs in the run-time environment for a short period of time to obtain their performance information. Hirundo utilizes these information to output a ranked list of optimized stream programs that are tailored for a particular run-time environment. Hirundo has been developed using Python as a prototype application for optimizing SPADE programs, which run on System S stream processing run-time. Using three example real world stream processing applications we demonstrate effectiveness of our approach, and discuss how well it generalizes for automatic stream program performance optimization.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {335–346},
numpages = {12},
keywords = {stream processing, scalability, performance optimization, fault tolerance, data-intensive computing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@article{10.1007/s10270-015-0459-z,
author = {S\'{a}nchez, Ana B. and Segura, Sergio and Parejo, Jos\'{e} A. and Ruiz-Cort\'{e}s, Antonio},
title = {Variability testing in the wild: the Drupal case study},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0459-z},
doi = {10.1007/s10270-015-0459-z},
abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {173–194},
numpages = {22},
keywords = {Variability-intensive systems, Variability testing, Test case selection, Test case prioritization, Non-functional properties, Automated testing}
}

@inproceedings{10.1145/3238147.3240466,
author = {Cashman, Mikaela and Cohen, Myra B. and Ranjan, Priya and Cottingham, Robert W.},
title = {Navigating the maze: the impact of configurability in bioinformatics software},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240466},
doi = {10.1145/3238147.3240466},
abstract = {The bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating automated techniques to interact with bioinformatics software. We believe these will generalize to other types of scientific software.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {757–767},
numpages = {11},
keywords = {software testing, configurability, bioinformatics},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.1016/j.vlsi.2018.02.013,
author = {Stamelakos, Ioannis and Xydis, Sotirios and Palermo, Gianluca and Silvano, Cristina},
title = {Workload- and process-variation aware voltage/frequency tuning for energy efficient performance sustainability of NTC manycores},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2018.02.013},
doi = {10.1016/j.vlsi.2018.02.013},
journal = {Integr. VLSI J.},
month = mar,
pages = {252–262},
numpages = {11},
keywords = {Variability, Energy efficiency, Low power, Manycore architectures, Near-threshold computing}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.5555/3288338.3288341,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Finding correlations of features affecting energy consumption and performance of web servers using the HADAS eco-assistant},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {100},
number = {11},
issn = {0010-485X},
abstract = {The impact of energy consumption on the environment and the economy is raising awareness of "green" software engineering. HADAS is an eco-assistant that makes developers aware of the influence of their designs and implementations on the energy consumption and performance of the final product. In this paper, we extend HADAS to better support the requirements of users: researchers, automatically dumping the energy-consumption of different software solutions; and developers, who want to perform a sustainability analysis of different software solutions. This analysis has been extended by adding Pearson's chi-squared differentials and Bootstrapping statistics, to automatically check the significance of correlations of the energy consumption, or the execution time, with any other variable (e.g., the number of users) that can influence the selection of a particular eco-efficient configuration. We have evaluated our approach by performing a sustainability analysis of the most common web servers (i.e. PHP servers) using the time and energy data measured with the Watts Up? Pro tool previously dumped in HADAS. We show how HADAS helps web server providers to make a trade-off between energy consumption and execution time, allowing them to sell different server configurations with different costs without modifying the hardware.},
journal = {Computing},
month = nov,
pages = {1155–1173},
numpages = {19},
keywords = {Web servers, Performance, Linux, Energy efficiency, 97K80, 68U35, 68N30, 68M20}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {software product lines, modeling principles, Feature models},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Tertiary study, Systematic literature review, Trends in software reuse, Software reuse}
}

@inproceedings{10.1145/3338906.3338928,
author = {Shahin, Ramy and Chechik, Marsha and Salay, Rick},
title = {Lifting Datalog-based analyses to software product lines},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338928},
doi = {10.1145/3338906.3338928},
abstract = {Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ”lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\'{e} Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {39–49},
numpages = {11},
keywords = {Souffl'{e}, Software Product Lines, Program Analysis, Pointer Analysis, Lifting, Doop, Datalog},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1007/978-3-030-86130-8_44,
author = {Feng, Zhigang and Song, Xiaoqin and Lei, Lei},
title = {Efficient Concurrent Transmission Scheme for Wireless Ad Hoc Networks: A Joint Optimization Approach},
year = {2021},
isbn = {978-3-030-86129-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86130-8_44},
doi = {10.1007/978-3-030-86130-8_44},
abstract = {In this paper, we focus on the joint optimization of scheduling and power control to achieve effective concurrent transmission. Under constraints, we describe the optimization problem as a scheduling problem based on power control. In order to solve the complexity problem, we have determined the best power control strategy in the large network area. Using the dual-link communication architecture, we decompose the optimization problem into two sub-problems, namely power control and link scheduling. By solving two sub-problems, we have determined the best link scheduling and power control mechanism suitable for the actual network environment. We realize efficient concurrent transmission based on the optimal solution obtained by joint optimization to effectively utilize network resources and improve energy efficiency. The simulation results prove the effectiveness of the scheme. Compared with existing solutions, the joint optimization solution has obvious advantages in terms of network throughput and energy consumption.},
booktitle = {Wireless Algorithms, Systems, and Applications: 16th International Conference, WASA 2021, Nanjing, China, June 25–27, 2021, Proceedings, Part II},
pages = {563–574},
numpages = {12},
keywords = {Dual-link structure, Energy consumption, Link scheduling, Power control, Concurrent transmission},
location = {Nanjing, China}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {User Requirements Notation, Software product lines, Scenario-based requirements engineering, Goal-based requirements engineering, Feature interactions, Aspect-oriented modeling}
}

@article{10.1007/s11276-018-1718-z,
author = {Mukhlif, Fadhil and Noordin, Kamarul Ariffin Bin and Mansoor, Ali Mohammed and Kasirun, Zarinah Mohd},
title = {Green transmission for C-RAN based on SWIPT in 5G: a review},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1718-z},
doi = {10.1007/s11276-018-1718-z},
abstract = {C-RAN is a promising new design for the next generation, an important aspect of it in the energy efficiency consideration. Hence, it is considering an innovative candidate to use it as an alternative cellular network instead of the traditional. Investigation green transmission of mobile cloud radio access networks based on SWIPT for 5G cellular networks. Especially, with considering SWIPT as a future solution for increasing the lifetime of end-user battery’s, that’s mean this technique will improving energy efficiency (EE). Addressing SWIPT into C-RAN is a challenging and it is needed to developing a new algorithm to use it on the cellular network with many trying to ensure the success of the system performance. C-RAN as a network and SWIPT as a promising technique with the suggesting green wireless network are discussed besides the importance of energy efficiency for the next generation. Furthermore, there was a study on fifth enabling technologies that can be used for 5G with emphasis on two of them (C-RAN and energy efficiency). Lastly, research challenges and future direction that require substantial research efforts are summarized.},
journal = {Wirel. Netw.},
month = jul,
pages = {2621–2649},
numpages = {29},
keywords = {MIMO, Power splitting, Time switching, Information decoding (ID), Energy harvesting (EH), Cloud radio access network, Power transfer, Green transmission}
}

@article{10.1016/j.future.2015.05.017,
title = {Allocating resources for customizable multi-tenant applications in clouds using dynamic feature placement},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.05.017},
doi = {10.1016/j.future.2015.05.017},
abstract = {Multi-tenancy, where multiple end users make use of the same application instance, is often used in clouds to reduce hosting costs. A disadvantage of multi-tenancy is however that it makes it difficult to create customizable applications, as all end users use the same application instance. In this article, we describe an approach for the development and management of highly customizable multi-tenant cloud applications. We apply software product line engineering techniques to cloud applications, and use an approach where applications are composed of multiple interacting components, referred to as application features. Using this approach, multiple features can be shared between different applications. Allocating resources for these feature-based applications is complex, as relations between components must be taken into account, and is referred to as the feature placement problem.In this article, we describe dynamic feature placement algorithms that minimize migrations between subsequent invocations, and evaluate them in dynamic scenarios where applications are added and removed throughout the evaluation scenario. We find that the developed algorithm achieves a low cost, while resulting in few resource migrations. In our evaluations, we observe that adding migration-awareness to the management algorithms reduces the number of instance migrations by more than 77 % and reduces the load moved between instances by more than 96 % when compared to a static management approach. Despite this reduction in number of migrations, a cost that is on average less than 3 % more than the optimal cost is achieved. We model customizable SaaS applications using feature modeling.A dynamic, migration-aware management approach is presented.Two ILP-based algorithms and a heuristic algorithm are compared.The dynamic algorithms reduce migrations and remain within 3% of the optimal cost.},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {63–76},
numpages = {14}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@article{10.1007/s10922-013-9265-5,
author = {Moens, Hendrik and Truyen, Eddy and Walraven, Stefan and Joosen, Wouter and Dhoedt, Bart and De Turck, Filip},
title = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
year = {2014},
issue_date = {October   2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9265-5},
doi = {10.1007/s10922-013-9265-5},
abstract = {Cloud computing technologies can be used to more flexibly provision application resources. By exploiting multi-tenancy, instances can be shared between users, lowering the cost of providing applications. A weakness of current cloud offerings however, is the difficulty of creating customizable applications that retain these advantages. In this article, we define a feature-based cloud resource management model, making use of Software Product Line Engineering techniques, where applications are composed of feature instances using a service-oriented architecture. We focus on how resources can be allocated in a cost-effective way within this model, a problem which we refer to as the feature placement problem. A formal description of this problem, that can be used to allocate resources in a cost-effective way, is provided. We take both the cost of failure to place features, and the cost of using servers into account, making it possible to take energy costs or the cost of public cloud infrastructure into consideration during the placement calculation. Four algorithms that can be used to solve the feature placement problem are defined. We evaluate the algorithm solutions, comparing them with the optimal solution determined using an integer linear problem solver, and evaluating the execution times of the algorithms, making use of both generated inputs and a use case based on three applications. We show that, using our approach a higher degree of multi-tenancy can be achieved, and that for the considered scenarios, taking the relationships between features into account and using application-oriented placement performs 25---40 % better than a purely feature-oriented placement.},
journal = {J. Netw. Syst. Manage.},
month = oct,
pages = {517–558},
numpages = {42},
keywords = {SPLE, Distributed computing, Cloud computing, Application placement}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@article{10.1007/s10270-016-0569-2,
author = {Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Lochau, Malte and Meinicke, Jens and Saake, Gunter},
title = {Effective product-line testing using similarity-based product prioritization},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0569-2},
doi = {10.1007/s10270-016-0569-2},
abstract = {A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {499–521},
numpages = {23},
keywords = {Test-case prioritization, Software product lines, Product-line testing, Model-based testing, Combinatorial interaction testing}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Structural coupling, Structural cohesion, Software product lines, Software measurement, Fuji, Feature-oriented programming}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {variability, software architecture, reconfiguration, SPL, Quality attributes},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.5555/2492708.2492954,
author = {Benini, Luca and Flamand, Eric and Fuin, Didier and Melpignano, Diego},
title = {P2012: building an ecosystem for a scalable, modular and high-efficiency embedded computing accelerator},
year = {2012},
isbn = {9783981080186},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {P2012 is an area- and power-efficient many-core computing fabric based on multiple globally asynchronous, locally synchronous (GALS) clusters supporting aggressive fine-grained power, reliability and variability management. Clusters feature up to 16 processors and one control processor with independent instruction streams sharing a multi-banked L1 data memory, a multi-channel DMA engine, and specialized hardware for synchronization and scheduling. P2012 achieves extreme area and energy efficiency by supporting domain-specific acceleration at the processor and cluster level through the addition of dedicated HW IPs. P2012 can run standard OpenCL and OpenMP parallel codes well as proprietary Native Programming Model (NPM) SW components that provide the highest level of control on application-to-resource mapping. In Q3 2011 the P2012 SW Development Kit (SDK) has been made available to a community of R&amp;D users; it includes full OpenCL and NPM development environments. The first P2012 SoC prototype in 28nm CMOS will sample in Q4 2012, featuring four clusters and delivering 80GOPS (with single precision floating point support) in 15.2mm2 with 2W power consumption.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {983–987},
numpages = {5},
location = {Dresden, Germany},
series = {DATE '12}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@article{10.1016/j.infsof.2012.06.012,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Architectural evolution of FamiWare using cardinality-based feature models},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.012},
doi = {10.1016/j.infsof.2012.06.012},
abstract = {Context: Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems. Objective: We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems. Method: FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components. Results: Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one. Conclusion: Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {563–580},
numpages = {18},
keywords = {Software Product Lines, Middleware family, Feature Models, Evolution}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Software product lines, Scientific workflows, Feature models, Composition}
}

@article{10.1023/A:1019791213967,
author = {Fornaciari, William and Sciuto, Donatella and Silvano, Cristina and Zaccaria, Vittorio},
title = {A Sensitivity-Based Design Space Exploration Methodology for Embedded Systems},
year = {2002},
issue_date = {September 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {1–2},
issn = {0929-5585},
url = {https://doi.org/10.1023/A:1019791213967},
doi = {10.1023/A:1019791213967},
abstract = {In this paper, we propose a system-level design methodology for the efficient exploration of the architectural parameters of the memory sub-systems, from the energy-delay joint perspective. The aim is to find the best configuration of the memory hierarchy without performing the exhaustive analysis of the parameters space. The target system architecture includes the processor, separated instruction and data caches, the main memory, and the system buses. To achieve a fast convergence toward the near-optimal configuration, the proposed methodology adopts an iterative local-search algorithm based on the sensitivity analysis of the cost function with respect to the tuning parameters of the memory sub-system architecture. The exploration strategy is based on the Energy-Delay Product (EDP) metric taking into consideration both performance and energy constraints. The effectiveness of the proposed methodology has been demonstrated through the design space exploration of a real-world case study: the optimization of the memory hierarchy of a MicroSPARC2-based system executing the set of Mediabench benchmarks for multimedia applications. Experimental results have shown an optimization speedup of 2 orders of magnitude with respect to the full search approach, while the near-optimal system-level configuration is characterized by a distance from the optimal full search configuration in the range of 2%.},
journal = {Des. Autom. Embedded Syst.},
month = sep,
pages = {7–33},
numpages = {27},
keywords = {power and performance optimization, Design space exploration}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {variability, feature modelling, experience report, e-commerce, automated testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Reverse engineering, Multi-objective evolutionary algorithms, Feature models, Empirical evaluation}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Software engineering, Evolution, Automation, Automated production systems}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3251104,
author = {Langdon, William B. and Petke, Justyna and White, David R.},
title = {Session details: Genetic Improvement 2015 Workshop},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251104},
doi = {10.1145/3251104},
abstract = {It is our great pleasure to welcome you to the first international workshop on the Genetic Improvement of Software -- GI-2015, held at GECCO'15. Our goal was to bring together research from across the globe to exchange ideas on using optimisation techniques, particularly evolutionary computation such as genetic programming, to improve existing software. We invited short position papers to encourage the discussion of new ideas and recent work in addition to longer and more concrete submissions. The call for participation invited GI work on automatic bug-fixing; improving functionality; improving non-functional properties such as efficiency, memory and energy consumption; "plastic surgery" by transplanting functionality from other existing code to host software; and automatically specialising generic software for dedicated tasks. As you will see, we have accepted papers in most of these areas as well as papers on improving the nascent genetic improvement tools in use, improving parallel code, GI's relationship with software product lines (SPL), improving security and GI for embedded systems.We had submissions from Asia, Europe and both North and South America. They were exactly evenly split between full-length submissions (8) and two page position papers (8).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1007/978-3-642-54804-8_1,
author = {Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Daum, Marcus and Klein, Joachim and M\"{a}rcker, Steffen and Wunderlich, Sascha},
title = {Probabilistic Model Checking and Non-standard Multi-objective Reasoning},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_1},
doi = {10.1007/978-3-642-54804-8_1},
abstract = {Probabilistic model checking is a well-established method for the automated quantitative system analysis. It has been used in various application areas such as coordination algorithms for distributed systems, communication and multimedia protocols, biological systems, resilient systems or security. In this paper, we report on the experiences we made in inter-disciplinary research projects where we contribute with formal methods for the analysis of hardware and software systems. Many performance measures that have been identified as highly relevant by the respective domain experts refer to multiple objectives and require a good balance between two or more cost or reward functions, such as energy and utility. The formalization of these performance measures requires several concepts like quantiles, conditional probabilities and expectations and ratios of cost or reward functions that are not supported by state-ofthe- art probabilistic model checkers. We report on our current work in this direction, including applications in the field of software product line verification.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Software product lines, Search-based testing, Performance testing, Feature models, Evolutionary algorithms, Automated analysis}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.jss.2014.12.041,
author = {Pascual, Gustavo G. and Lopez-Herrejon, Roberto E. and Pinto, M\'{o}nica and Fuentes, Lidia and Egyed, Alexander},
title = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.041},
doi = {10.1016/j.jss.2014.12.041},
abstract = {Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
journal = {J. Syst. Softw.},
month = may,
pages = {392–411},
numpages = {20},
keywords = {Evolutionary algorithms, Dynamic reconfiguration, DSPL}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.5555/2485288.2485437,
author = {Ayad, Gasser and Acquaviva, Andrea and Macii, Enrico and Sahbi, Brahim and Lemaire, Romain},
title = {HW-SW integration for energy-efficient/variability-aware computing},
year = {2013},
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Recent trends in embedded system architectures brought a rapid shift towards multicore, heterogeneous and reconfigurable platforms. This imposes a large effort for programmers to develop their applications to efficiently exploit the underlying architecture. In addition, process variability issues lead to performance and power uncertainties, impacting expected quality of service and energy efficiency of the running software. In particular, variability may lead to sub-optimal runtime task allocation.In this paper we present a holistic approach to tackle these issues exploiting high level HW/SW modeling to customize the runtime library. The customization introduces variability awareness in task allocation decisions, with the final purpose of optimizing a given objective: Execution time, power consumption, or overall energy consumption.We present a complete walkthrough, from top-level modeling down to variability-aware execution using a parallelized computational kernel running on a next generation, NoC based, heterogeneous multicore simulation platform.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {607–611},
numpages = {5},
location = {Grenoble, France},
series = {DATE '13}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@article{10.1007/s11276-014-0838-3,
author = {Yang, Meng and Kim, Donghyun and Li, Deying and Chen, Wenping and Tokuta, Alade O.},
title = {Maximum lifetime suspect monitoring on the street with battery-powered camera sensors},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-014-0838-3},
doi = {10.1007/s11276-014-0838-3},
abstract = {A camera sensor network is a sensor network of a group of camera sensors and is being deployed for various surveillance and monitoring applications. In this paper, we propose a new surveillance model for camera sensor network, namely half-view model, which requires a camera sensor network to capture the face image of any object if it moves forward to pass over an area of interest. Based on this new surveillance model, we introduce a new sleep-wakeup scheduling problem in camera sensor network, namely the maximum lifetime half-view barrier-coverage (MaxL-HV-BC) problem, whose goal is to find an on-off schedule of battery-operated camera sensors such that the continuous time duration providing half-view barrier-coverage over an area of interest is maximized. We develop a strategy to check if a region is half-view covered by a given set of camera sensors, and use this strategy to design two new heuristic algorithms for MaxL-HV-BC. We also conduct simulations to compare the average performance of the proposed algorithms with a trivial solution as well as the theoretical upper bound.},
journal = {Wirel. Netw.},
month = may,
pages = {1093–1107},
numpages = {15},
keywords = {Scheduling, Maximum lifetime, Half-view coverage, Energy-efficiency, Camera sensor networks, Barrier coverage}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {software product lines, runtime adaption, interoperability},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@inproceedings{10.1007/978-3-662-45234-9_10,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kit, Micha\l{} and Marek, Luk\'{a}\v{s} and T\r{u}ma, Petr},
title = {Towards Performance-Aware Engineering of Autonomic Component Ensembles},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_10},
doi = {10.1007/978-3-662-45234-9_10},
abstract = {Ensembles of autonomic components are a novel software engineering paradigm for development of open-ended distributed highly dynamic software systems e.g. smart cyber-physical systems. Recent research centered around the concept of ensemble-based systems resulted in design and development models that aim to systematize and simplify the engineering process of autonomic components and their ensembles. These methods highlight the importance of covering both the functional concepts and the non-functional properties, specifically performance-related aspects of the future systems. In this paper we propose an integration of the emerging techniques for performance assessment and awareness into different stages of the development process. Our goal is to aid both designers and developers of autonomic component ensembles with methods providing performance awareness throughout the entire development life cycle including runtime.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {131–146},
numpages = {16},
keywords = {performance engineering, ensemble-based systems, component systems}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining energy-aware commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {software product lines, multi-objective optimization},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Variability, Test case prioritization, Highly-configurable systems, Automated software testing}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Systems of systems, Systematic literature review, Requirements monitoring}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {scalability, performance modeling, optimization, distributed system, deep learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2377836.2377842,
author = {Gamez, Nadia and Romero, Daniel and Fuentes, Lidia and Rouvoy, Romain and Duchien, Laurence},
title = {Constraint-based self-adaptation of wireless sensor networks},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377836.2377842},
doi = {10.1145/2377836.2377842},
abstract = {In recent years, the Wireless Sensor Networks (WSNs) have become a useful mechanism to monitor physical phenomena in environments. The sensors that make part of these long-lived networks have to be reconfigured according to context changes in order to preserve the operation of the network. Such reconfigurations require to consider the distributed nature of the sensor nodes as well as their resource scarceness. Therefore, self-adaptations for WSNs have special requirements comparing with traditional information systems. In particular, the reconfiguration of the WSN requires a trade-off between critical dimensions for this kind of networks and devices, such as resource consumption or reconfiguration cost. Thus, in this paper, we propose to exploit Constraint-Satisfaction Problem (CSP) techniques in order to find a suitable configuration for self-adapting WSNs, modelled using a Dynamic Software Product Line (DSPL), when the context changes. We exploit CSP modeling to find a compromise between contradictory dimensions. To illustrate our approach, we use an Intelligent Transportation System scenario. This case study enables us to show the advantages of obtaining suitable and optimized configurations for self-adapting WSNs.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
pages = {20–27},
numpages = {8},
keywords = {wireless sensor networks, self-adaptation, dynamic software product lines, constraint-satisfaction problem},
location = {Bertinoro, Italy},
series = {WAS4FI-Mashups '12}
}

@inproceedings{10.5555/1947545.1947579,
author = {Schlegel, Christian and Steck, Andreas and Brugali, Davide and Knoll, Alois},
title = {Design abstraction and processes in robotics: from code-driven to model-driven engineering},
year = {2010},
isbn = {3642173187},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Advanced software engineering is the key factor in the design of future complex cognitive robots. It will decide about their robustness, (run-time) adaptivity, cost-effectiveness and usability.We present a novel overall vision of a model-driven engineering approach for robotics that fuses strategies for robustness by design and robustness by adaptation. It enables rigid definitions of quality-of-service, re-configurability and physics-based simulation as well as for seamless system level integration of disparate technologies and resource awareness.We report on steps towards implementing this idea driven by a first robotics meta-model with first explications of non-functional properties. A model-driven toolchain provides the model transformation and code generation steps. It also provides design time analysis of resource parameters (e.g. schedulability analysis of realtime tasks) as step towards resource awareness in the development of integrated robotic systems.},
booktitle = {Proceedings of the Second International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {324–335},
numpages = {12},
location = {Darmstadt, Germany},
series = {SIMPAR'10}
}

@inproceedings{10.1145/2742647.2742658,
author = {Zhang, Li and Pathak, Parth H. and Wu, Muchen and Zhao, Yixin and Mohapatra, Prasant},
title = {AccelWord: Energy Efficient Hotword Detection through Accelerometer},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742658},
doi = {10.1145/2742647.2742658},
abstract = {Voice control has emerged as a popular method for interacting with smart-devices such as smartphones, smartwatches etc. Popular voice control applications like Siri and Google Now are already used by a large number of smartphone and tablet users. A major challenge in designing a voice control application is that it requires continuous monitoring of user?s voice input through the microphone. Such applications utilize hotwords such as "Okay Google" or "Hi Galaxy" allowing them to distinguish user?s voice command and her other conversations. A voice control application has to continuously listen for hotwords which significantly increases the energy consumption of the smart-devices.To address this energy efficiency problem of voice control, we present AccelWord in this paper. AccelWord is based on the empirical evidence that accelerometer sensors found in today?s mobile devices are sensitive to user?s voice. We also demonstrate that the effect of user?s voice on accelerometer data is rich enough so that it can be used to detect the hotwords spoken by the user. To achieve the goal of low energy cost but high detection accuracy, we combat multiple challenges, e.g. how to extract unique signatures of user?s speaking hotwords only from accelerometer data and how to reduce the interference caused by user?s mobility.We finally implement AccelWord as a standalone application running on Android devices. Comprehensive tests show AccelWord has hotword detection accuracy of 85% in static scenarios and 80% in mobile scenarios. Compared to the microphone based hotword detection applications such as Google Now and Samsung S Voice, AccelWord is 2 times more energy efficient while achieving the accuracy of 98% and 92% in static and mobile scenarios respectively.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {301–315},
numpages = {15},
keywords = {measurement, hotword detection, energy, accelword, accelerometer},
location = {Florence, Italy},
series = {MobiSys '15}
}

@article{10.4018/IJCSSA.2015070103,
author = {Kolagari, Ramin Tavakoli and Chen, DeJiu and Lanusse, Agnes and Librino, Renato and L\"{o}nn, Henrik and Mahmud, Nidhal and Mraidha, Chokri and Reiser, Mark-Oliver and Torchiaro, Sandra and Tucci-Piergiovanni, Sara and W\"{a}gemann, Tobias and Yakymets, Nataliya},
title = {Model-Based Analysis and Engineering of Automotive Architectures with EAST-ADL: Revisited},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015070103},
doi = {10.4018/IJCSSA.2015070103},
abstract = {Modern cars have turned into complex high-technology products, subject to strict safety and timing requirements, in a short time span. This evolution has translated into development processes that are not as efficient, flexible and agile as they could or should be. This paper presents the main aspects and capabilities of a rich model-based design framework, founded on EAST-ADL. EAST-ADL is an architecture description language specific to the automotive domain and complemented by a methodology compliant with the functional safety standard for the automotive domain ISO26262. The language and the methodology are used to develop an information model in the sense of a conceptual model, providing the engineer the basis for specifying the various aspects of the system. Inconsistencies, redundancies, and partly even missing system description aspects can be found automaticlally by advanced analyses and optimization capabilities to effectively improve development processes of modern cars.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jul,
pages = {25–70},
numpages = {46},
keywords = {Timing Modelling, Software Product Lines, Optimization, Model-Based Software Development, ISO 26262, Functional Safety, EAST-ADL, Dependability, Automotive Software Development, AUTOSAR}
}

@inproceedings{10.1145/2897937.2897972,
author = {Choi, Young-kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {A quantitative analysis on microarchitectures of modern CPU-FPGA platforms},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2897972},
doi = {10.1145/2897937.2897972},
abstract = {CPU-FPGA heterogeneous acceleration platforms have shown great potential for continued performance and energy efficiency improvement for modern data centers, and have captured great attention from both academia and industry. However, it is nontrivial for users to choose the right platform among various PCIe and QPI based CPU-FPGA platforms from different vendors. This paper aims to find out what microarchitectural characteristics affect the performance, and how. We conduct our quantitative comparison and in-depth analysis on two representative platforms: QPI-based Intel-Altera HARP with coherent shared memory, and PCIe-based Alpha Data board with private device memory. We provide multiple insights for both application developers and platform designers.},
booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
articleno = {109},
numpages = {6},
location = {Austin, Texas},
series = {DAC '16}
}

@article{10.1007/s11219-012-9185-8,
author = {Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan},
title = {Foreword to the special issue on quality engineering for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9185-8},
doi = {10.1007/s11219-012-9185-8},
journal = {Software Quality Journal},
month = sep,
pages = {421–424},
numpages = {4}
}

@article{10.1016/j.jnca.2018.03.021,
title = {Resource management in cellular base stations powered by renewable energy sources},
year = {2018},
issue_date = {June 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {112},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2018.03.021},
doi = {10.1016/j.jnca.2018.03.021},
abstract = {This paper aims to consolidate the work carried out in making base station (BS) green and energy efficient by integrating renewable energy sources (RES). Clean and green technologies are mandatory for reduction of carbon footprint in future cellular networks. RES, especially solar and wind, are emerging as a viable alternate to fossil fuel based energy, which is the main cause of climate pollution. With advances in technologies, renewable energy is making inroads into all sectors including information and communication technologies (ICT). The main contributors of energy consumption in ICT sector are data centers' and cellular networks'. In cellular networks the BS is the main consumer of energy, mostly powered by the utility and a diesel generator. This energy comes at a significant operating cost as well as the environmental cost in terms of harmful greenhouse gas (GHG) emissions. Recent research shows that powering BSs with renewable energy is technically feasible. Although installation cost of energy from non-renewable fuel is still lower than RES, optimized use of the two sources can yield the best results. This paper presents a comprehensive overview of resource management in cellular BSs powered by RES and an in-depth analysis of power consumption optimization in order to reduce both cost and GHGs. Renewable energy sources are not only feasible for a stand-alone or off-grid BSs, but also feasible for on-grid BSs. This paper covers different aspects of optimization in cellular networks to provide reader with a holistic view of concepts, directions, and advancements in renewable energy based systems incorporated in cellular communications. Energy management strategies are studied in the realm of smart grids and other technologies, increasing the possibilities for energy efficiency further by employing schemes such as energy cooperation. Finally, the paper supports the move towards green communication in order to contribute positively towards climate change.},
journal = {J. Netw. Comput. Appl.},
month = jun,
pages = {1–17},
numpages = {17}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {software maintenance, software evolution, services, serviceoriented architecture, service-oriented systems, cloudbased systems, cloud computing, SOA}
}

@article{10.1109/TASLP.2018.2889927,
author = {Zhu, Qiaoxi and Coleman, Philip and Qiu, Xiaojun and Wu, Ming and Yang, Jun and Burnett, Ian},
title = {Robust Personal Audio Geometry Optimization in the SVD-Based Modal Domain},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2889927},
doi = {10.1109/TASLP.2018.2889927},
abstract = {Personal audio generates sound zones in a shared space to provide private and personalized listening experiences with minimized interference between consumers. Regularization has been commonly used to increase the robustness of such systems against potential perturbations in the sound reproduction. However, the performance is limited by the system geometry such as the number and location of the loudspeakers and controlled zones. This paper proposes a geometry optimization method to find the most geometrically robust approach for personal audio amongst all available candidate system placements. The proposed method aims to approach the most “natural” sound reproduction so that the solo control of the listening zone coincidently accompanies the preferred quiet zone. Being formulated in the SVD-based modal domain, the method is demonstrated by applications in three typical personal audio optimizations, i.e., the acoustic contrast control, the pressure matching, and the planarity control. Simulation results show that the proposed method can obtain the system geometry with better avoidance of “occlusion,” improved robustness to regularization, and improved broadband equalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {610–620},
numpages = {11}
}

@article{10.1007/s11219-012-9193-8,
author = {Mets\"{a}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
title = {Using aspects for testing of embedded software: experiences from two industrial case studies},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9193-8},
doi = {10.1007/s11219-012-9193-8},
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry.},
journal = {Software Quality Journal},
month = jun,
pages = {185–213},
numpages = {29},
keywords = {Software testing, Embedded software, Case studies, Aspect-oriented programming}
}

@article{10.1007/s11219-011-9146-7,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {A systematic review of quality attributes and measures for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9146-7},
doi = {10.1007/s11219-011-9146-7},
abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e.g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92% of the measures evaluate attributes that are related to maintainability. In addition, 67% of the measures are used during the design phase of Domain Engineering, and 56% are applied to evaluate the product line architecture. However, only 25% of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {425–486},
numpages = {62},
keywords = {Systematic literature review, Software product lines, Quality attributes, Quality, Measures}
}

@inproceedings{10.1145/3205651.3208239,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Multi-objective feature selection for EEG classification with multi-level parallelism on heterogeneous CPU-GPU clusters},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208239},
doi = {10.1145/3205651.3208239},
abstract = {The present trend in the development of computer architectures that offer improvements in both performance and energy efficiency has provided clusters with interconnected nodes including multiple multi-core microprocessors and accelerators. In these so-called heterogeneous computers, the applications can take advantage of different parallelism levels according to the characteristics of the architectures in the platform. Thus, the applications should be properly programmed to reach good efficiencies, not only with respect to the achieved speedups but also taking into account the issues related to energy consumption. In this paper we provide a multi-objective evolutionary algorithm for feature selection in electroencephalogram (EEG) classification, which can take advantage of parallelism at multiple levels: among the CPU-GPU nodes interconnected in the cluster (through message-passing), and inside these nodes (through shared-memory thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The procedure has been experimentally evaluated in performance and energy consumption and shows statistically significant benefits for feature selection: speedups of up to 73 requiring only a 6% of the energy consumed by the sequential code.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1862–1869},
numpages = {8},
keywords = {subpopulation-based genetic algorithm, parallel programming, heterogeneous platform, energy-aware computing, distributed master-worker procedure, EEG multi-objective feature selection},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {software product lines, feature-oriented programming, energy consumption},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {variation-points, variants, variability, on-line, off-line training, goals, context, autonomic elements, MAPE-K},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {sustainability dimensions, software sustainability, empirical study},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.5555/2820656.2820658,
author = {Lago, Patricia},
title = {Challenges and opportunities for sustainable software},
year = {2015},
publisher = {IEEE Press},
abstract = {With the increasing role played by software in supporting our society, its sustainability and environmental impact have become major factors in the development and operation of software-intensive systems. Myths and beliefs hide the real truth behind Green IT: IT is energy-inefficient because software is developed to make it so -- intentionally or not. But how far are we from being able to control software energy-efficiency? What makes software greener? How can we transform measuring software energy consumption in a general practice? What architectural design decisions will result in more sustainable systems? How can we ensure that new-generation software will be both cloud-ready and environmental-friendly? and How can we make evident the economic and social impact of developing software with 'energy in mind'? These are a few of the challenges ahead for a more sustainable digital society. This talk will discuss them, hence drawing directions for exciting challenges, promising opportunities, and ultimately inspiring research.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {1–2},
numpages = {2},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Software reuse, Software library, Search-based software engineering, Multi-objective optimization}
}

@inproceedings{10.1145/2749469.2750397,
author = {Akin, Berkin and Franchetti, Franz and Hoe, James C.},
title = {Data reorganization in memory using 3D-stacked DRAM},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750397},
doi = {10.1145/2749469.2750397},
abstract = {In this paper we focus on common data reorganization operations such as shuffle, pack/unpack, swap, transpose, and layout transformations. Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy. This paper presents a two pronged approach for efficient data reorganization, which combines (i) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and (ii) a mathematical framework that is used to represent and optimize the reorganization operations.We evaluate our proposed system through two major use cases. First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads. Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package. We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {131–143},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@inproceedings{10.1145/2228360.2228568,
author = {Melpignano, Diego and Benini, Luca and Flamand, Eric and Jego, Bruno and Lepley, Thierry and Haugou, Germain and Clermidy, Fabien and Dutoit, Denis},
title = {Platform 2012, a many-core computing accelerator for embedded SoCs: performance evaluation of visual analytics applications},
year = {2012},
isbn = {9781450311991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2228360.2228568},
doi = {10.1145/2228360.2228568},
abstract = {P2012 is an area- and power-efficient many-core computing accelerator based on multiple globally asynchronous, locally synchronous processor clusters. Each cluster features up to 16 processors with independent instruction streams sharing a multi-banked one-cycle access L1 data memory, a multi-channel DMA engine and specialized hardware for synchronization and aggressive power management. P2012 is 3D stacking ready and can be customized to achieve extreme area and energy efficiency by adding domain-specific HW IPs to the cluster. The first P2012 SoC prototype in 28nm CMOS will sample in Q3, featuring four 16-processor clusters, a 1MB L2 memory and delivering 80GOPS (with 32 bit single precision floating point support) in 18mm2 with 2W power consumption (worst-case). P2012 can run standard OpenCL™ and proprietary Native Programming Model SW components to achieve the highest level of control on application-to-resource mapping. A dedicated version of the OpenCV vision library is provided in the P2012 SW Development Kit to enable visual analytics acceleration. This paper will discuss preliminary performance measurements of common feature extraction and tracking algorithms, parallelized on P2012, versus sequential execution on ARM CPUs.},
booktitle = {Proceedings of the 49th Annual Design Automation Conference},
pages = {1137–1142},
numpages = {6},
keywords = {process aware, many-core, low-power, feature extraction, computer vision, SoC, 3D stacking},
location = {San Francisco, California},
series = {DAC '12}
}

@article{10.1147/rd.492.0437,
author = {Lorenz, J. and Kral, S. and Franchetti, F. and Ueberhuber, C. W.},
title = {Vectorization techniques for the Blue Gene/L double FPU},
year = {2005},
issue_date = {March 2005},
publisher = {IBM Corp.},
address = {USA},
volume = {49},
number = {2},
issn = {0018-8646},
url = {https://doi.org/10.1147/rd.492.0437},
doi = {10.1147/rd.492.0437},
abstract = {This paper presents vectorization techniques tailored to meet the specifics of the two-way single-instruction multiple-data (SIMD) double-precision floating-point unit (FPU), which is a core element of the node application-specific integrated circuit (ASIC) chips of the IBM 360-teraflops Blue Gene®/L supercomputer. This paper focuses on the general-purpose basic-block vectorization and optimization methods as they are incorporated in the Vienna MAP vectorizer and optimizer. The innovative technologies presented here, which have consistently delivered superior performance and portability across a wide range of platforms, were carried over to prototypes of Blue Gene/L and joined with the automatic performance-tuning system known as Fastest Fourier Transform in the West (FFTW). FFTW performance-optimization facilities working with the compiler technologies presented in this paper are able to produce vectorized fast Fourier transform (FFT) codes that are tuned automatically to single Blue Gene/L processors and are up to 80% faster than the best-performing scalar FFT codes generated by FFTW.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {437–446},
numpages = {10}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1007/s00165-017-0441-3,
author = {Str\"{u}ber, D. and Rubin, J. and Arendt, T. and Chechik, M. and Taentzer, G. and Pl\"{o}ger, J.},
title = {Variability-based model transformation: formal foundation and application},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0441-3},
doi = {10.1007/s00165-017-0441-3},
abstract = {Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {133–162},
numpages = {30},
keywords = {Category theory, Variability, Graph transformation, Model transformation}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {vulnerability, variability model, software architecture, feature model, exploit, docker-container, attack scenarios},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Software quality, Measurement, Mapping study, Design-time quality attributes}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/MICRO.2010.15,
author = {Watkins, Matthew A. and Albonesi, David H.},
title = {ReMAP: A Reconfigurable Heterogeneous Multicore Architecture},
year = {2010},
isbn = {9780769542997},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2010.15},
doi = {10.1109/MICRO.2010.15},
abstract = {This paper presents ReMAP, a reconfigurable architecture geared towards accelerating and parallelizing applications within a heterogeneous CMP. In ReMAP, threads share a common reconfigurable fabric that can be configured for individual thread computation or fine-grained communication with integrated computation. The architecture supports both fine-grained point-to-point communication for pipeline parallelization and fine-grained barrier synchronization. The combination of communication and configurable computation within ReMAP provides the unique ability to perform customized computation while data is transferred between cores, and to execute custom global functions after barrier synchronization. ReMAP demonstrates significantly higher performance and energy efficiency compared to hard-wired communication-only mechanisms, and over what can ideally be achieved by allocating the fabric area to additional or more powerful cores.},
booktitle = {Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {497–508},
numpages = {12},
series = {MICRO '43}
}

@article{10.1016/j.future.2019.04.032,
author = {Mousa, Afaf and Bentahar, Jamal and Alam, Omar},
title = {Context-aware composite SaaS using feature model},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.032},
doi = {10.1016/j.future.2019.04.032},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {376–390},
numpages = {15}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Meta-heuristics, Genetic Algorithms, Evolutionary Algorithms, Systematic Mapping, Hyper-heuristics, Search-based Software Testing}
}

@inproceedings{10.1007/978-3-030-10801-4_1,
author = {A\ss{}mann, Uwe and Grzelak, Dominik and Mey, Johannes and Pukhkaiev, Dmytro and Sch\"{o}ne, Ren\'{e} and Werner, Christopher and P\"{u}schel, Georg},
title = {Cross-Layer Adaptation in Multi-layer Autonomic Systems (Invited Talk)},
year = {2019},
isbn = {978-3-030-10800-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-10801-4_1},
doi = {10.1007/978-3-030-10801-4_1},
abstract = {This work presents a new reference architecture for multi-layer autonomic systems called context-controlled autonomic controllers (ConAC). Usually, the principle of multiple system layers contradicts the principle of a global adaptation strategy, because system layers are considered to be black boxes. The presented architecture relies on an explicit context model, so a simple change of contexts can consistently vary the adaptation strategies for all layers. This reveals that explicit context modeling enables consistent meta-adaptation in multi-layer autonomic systems. The paper presents two application areas for the ConAC architecture, robotic co-working and energy-adaptive servers, but many other multi-layered system designs should benefit from it.},
booktitle = {SOFSEM 2019: Theory and Practice of Computer Science: 45th International Conference on Current Trends in Theory and Practice of Computer Science, Nov\'{y} Smokovec, Slovakia, January 27-30, 2019, Proceedings},
pages = {1–20},
numpages = {20},
location = {Nov\'{y} Smokovec, Slovakia}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2485922.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating big data with high-throughput, energy-efficient data partitioning},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485944},
doi = {10.1145/2485922.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {249–260},
numpages = {12},
keywords = {streaming data, specialized functional unit, microarchitecture, data partitioning, accelerator},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Variability, Systematic literature review, Service-based systems, Quality attributes}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1147/JRD.2010.2050539,
author = {Molloy, C. and Iqbal, M.},
title = {Improving data-center efficiency for a smarter planet},
year = {2010},
issue_date = {July 2010},
publisher = {IBM Corp.},
address = {USA},
volume = {54},
number = {4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2010.2050539},
doi = {10.1147/JRD.2010.2050539},
abstract = {In 2009, IBM launched its Smarter Planet™ initiative, which is based on the paradigm that virtually any physical object, process, or system can be instrumented, interconnected, and infused with intelligence. Because of the increased demand for information technology (IT) to facilitate Smarter Planet solutions, physical data centers have become interconnected, instrumented, and intelligent (i.e., adaptable, scalable, energy efficient, and cost effective). "Green data centers," which are discussed in this paper, are those that make use of facilities and IT integration, resulting in lower energy costs, reduced carbon footprint, and reduced demand for power, space, and cooling resources. This paper reviews energy-efficiency strategies that are being incorporated in eight million square feet of data-center space that IBM operates in support of its customers. These strategies include both a dynamic infrastructure IT initiative and an energy-efficiencies pillar of the IBM New Enterprise Data Center initiative. For example, compared to nonmodular designs, implementation of a modular design to optimize expandable data centers will better match the IT demand with the data-center energy supply. This high-level overview describes the integration of these strategies to create a class of leading-edge IBM data centers and a plan for the optimization of existing data centers.},
journal = {IBM J. Res. Dev.},
month = jul,
pages = {388–395},
numpages = {8}
}

@inbook{10.5555/1985668.1985673,
author = {Kazhamiakin, Raman and Benbernou, Salima and Baresi, Luciano and Plebani, Pierluigi and Uhlig, Maike and Barais, Olivier},
title = {Adaptation of service-based systems},
year = {2010},
isbn = {3642175988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Service Research Challenges and Solutions for the Future Internet: S-Cube - towards Engineering, Managing and Adapting Service-Based Systems},
pages = {117–156},
numpages = {40}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: symbolic execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {Symbolic execution, Java, GraalVM, Compiler optimizations},
location = {Virtual, UK},
series = {MPLR '20}
}

@inproceedings{10.1145/3377024.3377036,
author = {Sprey, Joshua and Sundermann, Chico and Krieter, Sebastian and Nieke, Michael and Mauro, Jacopo and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SMT-based variability analyses in FeatureIDE},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377036},
doi = {10.1145/3377024.3377036},
abstract = {Handling configurable systems with thousands of configuration options is a challenging problem in research and industry. One of the most common approaches to manage the configuration options of large systems is variability modelling. The verification and configuration process of large variability models is manually infeasible. Hence, they are usually assisted by automated analyses based on solving satisfiability problems (SAT). Recent advances in satisfiability modulo theories (SMT) could prove SMT solvers as a viable alternative to SAT solvers. However, SMT solvers are typically not utilized for variability analyses. A comparison for SAT and SMT could help to estimate SMT solvers potential for the automated analysis. We integrated two SMT solvers into FeatureIDE and compared them against a SAT solver on analyses for feature models, configurations, and realization artifacts. We give an overview of all variability analyses in FeatureIDE and present the results of our empirical evaluation for over 122 systems. We observed that SMT solvers are generally faster in generating explanations of unsatisfiable requests. However, the evaluated SAT solver outperformed SMT solvers for other analyses.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {9},
keywords = {variability analysis, preprocessor analysis, feature models, feature model analysis, feature attributes, configuration analysis, attribute optimization, SMT analysis, SMT, SAT vs SMT, SAT analysis, SAT},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2212736.2212739,
author = {Takeuchi, Mikio and Makino, Yuki and Kawachiya, Kiyokuni and Horii, Hiroshi and Suzumura, Toyotaro and Suganuma, Toshio and Onodera, Tamiya},
title = {Compiling X10 to Java},
year = {2011},
isbn = {9781450307703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212736.2212739},
doi = {10.1145/2212736.2212739},
abstract = {X10 is a new programming language for improving the software productivity in the multicore era by making parallel/distributed programming easier. X10 programs are compiled into C++ or Java source code, but X10 supports various features not supported directly in Java. To implement them efficiently in Java, new compilation techniques are needed.This paper discusses problems in translating X10-specific functions to Java and provides our solutions. By using appropriate implementations, sequential execution performance has been improved by about 5 times making it comparable to native Java. The parallel execution performance has also been improved and the gap from Java Fork/Join performance is about 3 times when run at a single place. Initial evaluation of distributed execution shows good scalability. Most of the results in this paper have already been incorporated in X10 release 2.1.2.Many of the compilation techniques described in this paper can be useful for implementing other programming languages targeted for Java or other managed environments.},
booktitle = {Proceedings of the 2011 ACM SIGPLAN X10 Workshop},
articleno = {3},
numpages = {10},
keywords = {optimization, evaluation, code generation, X10, Java},
location = {San Jose, California},
series = {X10 '11}
}

@inproceedings{10.1145/2162049.2162071,
author = {Cardoso, Jo\~{a}o M.P. and Carvalho, Tiago and Coutinho, Jos\'{e} G.F. and Luk, Wayne and Nobre, Ricardo and Diniz, Pedro and Petrov, Zlatko},
title = {LARA: an aspect-oriented programming language for embedded systems},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162071},
doi = {10.1145/2162049.2162071},
abstract = {The development of applications for high-performance embedded systems is typically a long and error-prone process. In addition to the required functions, developers must consider various and often conflicting non-functional application requirements such as performance and energy efficiency. The complexity of this process is exacerbated by the multitude of target architectures and the associated retargetable mapping tools. This paper introduces an As-pect-Oriented Programming (AOP) approach that conveys domain knowledge and non-functional requirements to optimizers and mapping tools. We describe a novel AOP language, LARA, which allows the specification of compi-lation strategies to enable efficient generation of software code and hardware cores for alternative target architectures. We illustrate the use of LARA for code instrumentation and analysis, and for guiding the application of compiler and hardware synthesis optimizations. An important LARA feature is its capability to deal with different join points, action models, and attributes, and to generate an aspect intermediate representation. We present examples of our aspect-oriented hardware/software design flow for mapping real-life application codes to embedded platforms based on Field Programmable Gate Array (FPGA) technology.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {179–190},
numpages = {12},
keywords = {reconfigurable computing, embedded systems, domain-specific languages, compilers, aspect-oriented programming, FPGAs},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Reuse Hierarchies, Model-Driven Engineering, Model Reuse, Model Interfaces, Delaying of Decisions},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.scico.2019.102344,
author = {Basile, Davide and ter Beek, Maurice H. and Degano, Pierpaolo and Legay, Axel and Ferrari, Gian-Luigi and Gnesi, Stefania and Di Giandomenico, Felicita},
title = {Controller synthesis of service contracts with variability},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {187},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.102344},
doi = {10.1016/j.scico.2019.102344},
journal = {Sci. Comput. Program.},
month = feb,
numpages = {23},
keywords = {Behavioural variability, Variability, Service orchestrations, Contract automata, Supervisory control theory}
}

@inproceedings{10.5555/1927661.1927724,
author = {Si, Xiaojie and Zhang, Xuyun and Dou, Wanchun},
title = {A novel local optimization method for QoS-aware web service composition},
year = {2010},
isbn = {3642165141},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {QoS-aware web service selection has become a hot-spot research topic in the domain of web service composition. In previous works, the multiple tasks recruited in a composite schema are usually considered of equal importance. However, it is unreasonable for each task to have the absolutely same weight in certain circumstances. Hence, it is a great challenge to mine the weights among different tasks to reflect customers' partial preferences. In view of this challenge, a novel local optimization method is presented in this paper, which is based on a two-hierarchy weight, i.e., weight of task's criteria and weight of tasks. Finally, a case study is demonstrated to validate the feasibility of our proposal.},
booktitle = {Proceedings of the 2010 International Conference on Web Information Systems and Mining},
pages = {402–409},
numpages = {8},
keywords = {weight, service composition, local optimization, QoS},
location = {Sanya, China},
series = {WISM'10}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Systematic mapping study, Software-intensive systems, Literature survey, Embedded systems, Automotive systems, Automotive software engineering}
}

@inproceedings{10.1007/978-3-319-29339-4_21,
author = {Elibol, Ercan and Calderon, Juan and Llofriu, Martin and Quintero, Carlos and Moreno, Wilfrido and Weitzenfeld, Alfredo},
title = {Power Usage Reduction of Humanoid Standing Process Using Q-Learning},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_21},
doi = {10.1007/978-3-319-29339-4_21},
abstract = {An important area of research in humanoid robots is energy consumption, as it limits autonomy, and can harm task performance. This work focuses on power aware motion planning. Its principal aim is to find joint trajectories to allow for a humanoid robot to go from crouch to stand position while minimizing power consumption. Q-Learning (QL) is used to search for optimal joint paths subject to angular position and torque restrictions. A planar model of the humanoid is used, which interacts with QL during a simulated offline learning phase. The best joint trajectories found during learning are then executed by a physical humanoid robot, the Aldebaran NAO. Position, velocity, acceleration, and current of the humanoid system are measured to evaluate energy, mechanical power, and Center of Mass (CoM) in order to estimate the performance of the new trajectory which yield a considerable reduction in power consumption.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {251–263},
numpages = {13},
keywords = {Humanoid, Dynamic modeling, Energy analysis, Optimization, Q-learning},
location = {Hefei, China}
}

@article{10.1007/s10270-017-0625-6,
author = {Str\"{u}ber, Daniel and Acre?Oaie, Vlad and Pl\"{o}ger, Jennifer},
title = {Model clone detection for rule-based model transformation languages},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0625-6},
doi = {10.1007/s10270-017-0625-6},
abstract = {Cloning is a convenient mechanism to enable reuse across and within software artifacts. On the downside, it is also a practice related to severe long-term maintainability impediments, thus generating a need to identify clones in affected artifacts. A large variety of clone detection techniques have been proposed for programming and modeling languages; yet no specific ones have emerged for model transformation languages. In this paper, we explore clone detection for rule-based model transformation languages, including graph-based ones, such as Henshin, and hybrid ones, such as ATL. We introduce use cases for such techniques in the context of constructive and analytical quality assurance, and a set of key requirements we derived from these use cases. To address these requirements, we describe our customization of existing model clone detection techniques: We consider eScan, an a-priori-based technique, ConQAT, a heuristic technique, and a hybrid technique based on a combination of eScan and ConQAT. We compare these techniques in a comprehensive experimental evaluation, based on three realistic Henshin rule sets, and a comprehensive body of examples from the ATL transformation zoo. Our results indicate that our customization of ConQAT enables the efficient detection of the considered clones, without sacrificing accuracy. With our contributions, we present the first evidence on the usefulness of model clone detection for the quality assurance of model transformations and pave the way for future research efforts at the intersection of model clone detection and model transformation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {995–1016},
numpages = {22},
keywords = {Quality assurance, Model transformation, Model clone detection, Henshin, ATL}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@article{10.1016/j.jss.2012.08.031,
author = {Kulesza, Uir\'{a} and Soares, S\'{e}Rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F. and Delicato, Fl\'{a}Via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot\'{a}Vio and Mendon\c{c}A, Nabor and Batista, Thais and Bonif\'{a}Cio, Rodrigo and Cacho, N\'{e}Lio and Silva, Lyrene and Von Staa, Arndt and Silveira, F\'{a}Bio and Valente, Marco T\'{u}Lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec\'{\i}Lia},
title = {The crosscutting impact of the AOSD Brazilian research community},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.08.031},
doi = {10.1016/j.jss.2012.08.031},
abstract = {Background: Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil. Aims: In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community. Method: Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results: Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.},
journal = {J. Syst. Softw.},
month = apr,
pages = {905–933},
numpages = {29},
keywords = {Research impact, Modularity, Aspect-Oriented Software Development}
}

@article{10.1016/j.jss.2010.02.017,
author = {White, J. and Benavides, D. and Schmidt, D. C. and Trinidad, P. and Dougherty, B. and Ruiz-Cortes, A.},
title = {Automated diagnosis of feature model configurations},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.02.017},
doi = {10.1016/j.jss.2010.02.017},
abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1094–1107},
numpages = {14},
keywords = {Software product-lines, Optimization, Diagnosis, Constraint satisfaction, Configuration}
}

@inproceedings{10.1007/978-3-030-95391-1_37,
author = {Zheng, Xuda and Zhang, Chi and Duan, Keqiang and Wu, Weiguo and Yan, Jie},
title = {SLA: A Cache Algorithm for&nbsp;SSD-SMR Storage System with&nbsp;Minimum RMWs},
year = {2021},
isbn = {978-3-030-95390-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95391-1_37},
doi = {10.1007/978-3-030-95391-1_37},
abstract = {To satisfy the low-cost and massive data storage requirements imposed by data growth, Shingled Magnetic Recording (SMR) disks are extensively employed in the area of high-density storage. The primary drawback of SMR disks is the write amplification issue caused by sequential write constraints, which is becoming more prominent in the field of non-cold archives. Although a hybrid storage system comprised of Solid State Disks (SSDs) and SMRs may alleviate the aforementioned issue, current SSD cache replacement algorithms are still limited to the management method of Least Recently Used (LRU). The LRU queue, on the other hand, is ineffective in reducing the triggers of Read-Modify-Write (RMW), which is a critical factor for the performance of SMR disks. In this paper, we propose a new SMR Locality-Aware (SLA) algorithm based on a band-based management method. SLA adopts the Dual Locality Compare (DLC) strategy to solve the hit rate reduction problem caused by the traditional band-based management method, as well as the Relatively Clean Band First (RCBF) strategy to further minimize the number of RMW operations. Experiments indicate that, compared to the MSOT method, the SLA algorithm can maintain a similar hit rate as the LRU, while reducing the number of RMWs by 77.2% and the SMR disk write time by 95.1%.},
booktitle = {Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part III},
pages = {587–601},
numpages = {15},
keywords = {Spatial locality, Hybrid storage system, Replacement algorithm, Shingled Magnetic Recording}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/1854273.1854284,
author = {Watkins, Matthew A. and Albonesi, David H.},
title = {Dynamically managed multithreaded reconfigurable architectures for chip multiprocessors},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854284},
doi = {10.1145/1854273.1854284},
abstract = {Prior work has demonstrated that reconfigurable logic can significantly benefit certain applications. However, reconfigurable architectures have traditionally suffered from high area overhead and limited application coverage. We present a dynamically managed multithreaded reconfigurable architecture consisting of multiple clusters of shared reconfigurable fabrics that greatly reduces the area overhead of reconfigurability while still offering the same power efficiency and performance benefits. Like other shared SMT and CMP resources, the dynamic partitioning of the reconfigurable resource among sharing threads, along with the co-scheduling of threads among different reconfigurable clusters, must be intelligently managed for the full benefits of the shared fabrics to be realized.We propose a number of sophisticated dynamic management approaches, including the application of machine learning, multithreaded phase-based management, and stability detection. Overall, we show that, with our dynamic management policies, multithreaded reconfigurable fabrics can achieve better energy\texttimes{}delay2, at far less area and power, than providing each core with a much larger private fabric. Moreover, our approach achieves dramatically higher performance and energy-efficiency for particular workloads compared to what can be ideally achieved by allocating the fabric area to additional cores.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {41–52},
numpages = {12},
keywords = {shared resource management, reconfigurable architecture},
location = {Vienna, Austria},
series = {PACT '10}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@inproceedings{10.5555/1762146.1762166,
author = {D'Alberto, Paolo and P\"{u}schel, Markus and Franchetti, Franz},
title = {performance/energy optimization of dsp transforms on the XScale processor},
year = {2007},
isbn = {9783540693376},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The XScale processor family provides user-controllable independent configuration of CPU, bus, and memory frequencies. This feature introduces another handle for the code optimization with respect to energy consumption or runtime performance. We quantify the effect of frequency configurations on both performance and energy for three signal processing transforms: the discrete Fourier transform (DFT), finite impulse response (FIR) filters, and the Walsh-Hadamard Transform (WHT).To do this, we use SPIRAL, a program generation and optimization system for signal processing transforms. For a given transform to be implemented, SPIRAL searches over different algorithms to find the best match to the given platform with respect to the chosen performance metric (usually runtime). In this paper we use SPIRAL to generate implementations for different frequency configurations and optimize for runtime and physically measured energy consumption. In doing so we show that first, each transform achieves best performance/energy consumption for a different system configuration; second, the best code depends on the chosen configuration, problem size and algorithm; third, the fastest implementation is not always the most energy efficient; fourth, we introduce dynamic (i.e., during execution) reconfiguration in order to further improve performance/energy. Finally, we benchmark SPIRAL generated code against Intel's vendor library routines. We show competitive results as well as 20% performance improvements or energy reduction for selected transforms and problem sizes.},
booktitle = {Proceedings of the 2nd International Conference on High Performance Embedded Architectures and Compilers},
pages = {201–214},
numpages = {14},
location = {Ghent, Belgium},
series = {HiPEAC'07}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1007/s10270-018-00712-x,
author = {Bencomo, Nelly and G\"{o}tz, Sebastian and Song, Hui},
title = {Models@run.time: a guided tour of the state of the art and research challenges},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00712-x},
doi = {10.1007/s10270-018-00712-x},
abstract = {More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing attention. Given the prolific results during these years, the current outcomes need to be sorted and classified. Furthermore, many gaps need to be categorized in order to further develop the research topic by experts of the research area but also newcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of the research line. To make the discussion more concrete, a taxonomy is defined and used to compare the main approaches and research outcomes in the area during the last decade and including ancestor research initiatives. We identified and classified 275 papers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding research challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct and visualize data.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {3049–3082},
numpages = {34},
keywords = {Systematic literature review, Self-reflection, Models@run.time, Causal connection}
}

@inproceedings{10.1007/978-3-030-59003-1_10,
author = {G\'{o}mez, Paola and Casallas, Rubby and Roncancio, Claudia},
title = {Automatic Schema Generation for Document-Oriented Systems},
year = {2020},
isbn = {978-3-030-59002-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59003-1_10},
doi = {10.1007/978-3-030-59003-1_10},
abstract = {Popular document-oriented systems store JSON-like data (e.g. MongoDB). Such data formats combine the flexibility of semi-structured models and traditional data structures like records and arrays. This allows numerous structuring possibilities even for simple data. The data structure choice is important as it impacts many aspects such as memory footprint, data access performances and programming complexity. Our work aims at helping users in selecting data structuring from a set of automatically generated alternatives. These alternatives can be analyzed considering complexity metrics, query requirements and best practices using such “schemaless” databases. Our approach for “schema” generation has been inspired from Software Product Lines strategies based on feature models. From a UML class diagram that represents user’s data, we generate automatically a feature model that implicitly contains the structure alternatives with their variations and common points. This feature model satisfies document-oriented constraints so as user constraints reflecting good practices or particular needs. It leads to a set of data structuring alternatives to be considered by the user for his operational choices.},
booktitle = {Database and Expert Systems Applications: 31st International Conference, DEXA 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings, Part I},
pages = {152–163},
numpages = {12},
keywords = {NoSQL, Document-oriented systems, Variability, Feature models},
location = {Bratislava, Slovakia}
}

@article{10.1007/s00450-014-0273-9,
author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M\"{a}rtin, Lukas and Vogel-Heuser, Birgit},
title = {Design for future: managed software evolution},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3–4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0273-9},
doi = {10.1007/s00450-014-0273-9},
abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
journal = {Comput. Sci.},
month = aug,
pages = {321–331},
numpages = {11},
keywords = {maintenance and operation, Software life cycle, Legacy systems, Knowledge carrying software, Design, Co-evolution}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@article{10.1016/j.mejo.2013.03.016,
author = {Russo, Patrice and Yengui, Firas and Pillonnet, Gael and Taupin, Sophie and Abouchi, Nacer},
title = {Dynamic voltage scaling for series hybrid amplifiers},
year = {2013},
issue_date = {September, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {44},
number = {9},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2013.03.016},
doi = {10.1016/j.mejo.2013.03.016},
abstract = {We present an optimization of the voltage scaling algorithm in low power audio class-G amplifier for headphones application to allow longer playback time. The optimization approach minimizes the voltage difference between the internal audio amplifier power supply and its output signal over a large range of operating conditions. The modeling is based on a behavioral model enabling accurate and rapid evaluation of efficiency and audio quality with realistic input stimuli. The model validated in practice is used to optimize the voltage scaling using only few power supply levels. Thanks to a global search algorithm followed by a local one, the optimization gives the better parameters for voltage scaling algorithm while keeping a good audio quality. The proposed configuration increases the efficiency up to 48% at nominal operation.},
journal = {Microelectron. J.},
month = sep,
pages = {753–763},
numpages = {11},
keywords = {Optimization, Hybrid amplifier, Dynamic voltage scaling, Class-G/H, Audio amplifier}
}

@inproceedings{10.5555/2874916.2874975,
author = {Mittal, Saurabh and Ruth, Mark and Pratt, Annabelle and Lunacek, Monte and Krishnamurthy, Dheepak and Jones, Wesley},
title = {A system-of-systems approach for integrated energy systems modeling and simulation},
year = {2015},
isbn = {9781510810594},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Energy systems integration combines energy carriers, including electricity, with infrastructures, to maximize efficiency and minimize waste. In order to study systems at a variety of physical scales---from individual buildings to distribution systems---interconnected through these energy infrastructures, NREL is developing an Integrated Energy System Model (IESM), with an initial focus on the electricity system. Today's electricity grid is the most complex system ever built---and the future grid is likely to be even more complex because it will incorporate distributed energy resources (DERs) such as wind, solar, and various other sources of generation and energy storage. The complexity is further augmented by the possible evolution to new retail market structures that would provide incentives to owners of DERs to support the grid. The IESM can be used to understand and test the impact of new retail market structures and technologies such as DERs, demand-response equipment, and energy management systems on the system's ability to provide reliable electricity to all customers. The IESM is composed of a power flow simulator (GridLAB-D), building and appliance models including home energy management system implemented using either GAMS or Pyomo, a market layer, and is able to include hardware-in-the-loop simulation (testing appliances such as air conditioners, dishwashers, etc.). The IESM is a system-of-systems (SoS) simulator wherein the constituent systems are brought together in a virtual testbed. We will describe an SoS approach for developing a distributed simulation environment. We will elaborate on the methodology and the control mechanisms used in the co-simulation illustrated by a case study.},
booktitle = {Proceedings of the Conference on Summer Computer Simulation},
pages = {1–10},
numpages = {10},
keywords = {system-of-systems, smart grid, optimization, integrated energy systems, discrete-event simulation, co-simulation, GridLAB-D, DEVS},
location = {Chicago, Illinois},
series = {SummerSim '15}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {survey, scalability, product requirements, product line, performance, literature review, configuration},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {performance testing, performance documentation, performance awareness, javadoc, java},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2491246.2491250,
author = {Zhang, Xi and Ansari, Junaid and Arya, Manish and M\"{a}h\"{o}nen, Petri},
title = {Exploring parallelization for medium access schemes on many-core software defined radio architecture},
year = {2013},
isbn = {9781450321815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491246.2491250},
doi = {10.1145/2491246.2491250},
abstract = {As multi-standard devices and high speed communication standards are emerging, timeliness requirements and flexibility for both baseband modem and medium access schemes are becoming essential. Software Defined Radios (SDRs), in this context, aim at offering the desired flexibility while satisfying the real-time constraints. An SDR architecture consisting of many-core homogeneous computing elements provides easy protocol implementation, a high level of portability and extension possibilities. It does not require architecture specific program code which is needed by the popular heterogeneous SDR architectures. Therefore, in this paper, we explore how a homogeneous SDR architecture is used for efficient realization and execution of Medium Access Control (MAC) protocols. In particular, we investigate the performance of two broad classes of MAC schemes on the Platform 2012 (P2012) many-core programmable computing fabric. We provide a toolchain which utilizes the characteristics of P2012 for MAC parallelization, runtime scheduling, and execution. Our results indicate that by using the supporting toolchain, reconfigurable MAC implementations are able to exploit the computational power offered by the platform and adhere to the timeliness constraints. Computationally intensive algorithms for MAC layer parameter optimization show an improvement of up to 85% in the convergence time as compared to using a single-core architecture.},
booktitle = {Proceedings of the Second Workshop on Software Radio Implementation Forum},
pages = {37–44},
numpages = {8},
keywords = {sdr platform, parallelization, many-core, mac},
location = {Hong Kong, China},
series = {SRIF '13}
}

@inproceedings{10.5555/3297765.3297769,
author = {Trapp, Matthias and Pasewaldt, Sebastian and D\"{u}rschmid, Tobias and Semmo, Amir and D\"{o}llner, J\"{u}rgen},
title = {Teaching image-processing programming for mobile devices: a software development perspective},
year = {2018},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In this paper we present a concept of a research course that teaches students in image processing as a building block of mobile applications. Our goal with this course is to teach theoretical foundations, practical skills in software development as well as scientific working principles to qualify graduates to start as fully-valued software developers or researchers. The course includes teaching and learning focused on the nature of small team research and development as encountered in the creative industries dealing with computer graphics, computer animation and game development. We discuss our curriculum design and issues in conducting undergraduate and graduate research that we have identified through four iterations of the course. Joint scientific demonstrations and publications of the students and their supervisors as well as quantitative and qualitative evaluation by students underline the success of the proposed concept. In particular, we observed that developing using a common software framework helps the students to jump start their course projects, while industry software processes such as branching coupled with a three-tier breakdown of project features helps them to structure and assess their progress.},
booktitle = {Proceedings of the 39th Annual European Association for Computer Graphics Conference: Education Papers},
pages = {17–24},
numpages = {8},
location = {Delft, The Netherlands},
series = {EG-EDU '18}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.5555/1515915.1515947,
author = {Delicato, Fl\'{a}via Coimbra and Pires, Paulo F. and Pirmez, Luci and da Costa Carmo, Luiz Fernando Rust},
title = {A flexible middleware system for wireless sensor networks},
year = {2003},
isbn = {3540403175},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The current wireless sensor networks (WSN) are assumed to be designed for specific applications, having data communication protocols strongly coupled to applications. The future WSNs are envisioned as comprising of heterogeneous devices assisting to a large range of applications. To achieve this goal, a flexible middleware layer is needed, separating application specific features from the data communication protocol, while allowing applications to influence the WSN behavior for energy efficiency. We propose a service-based middleware system for WSNs. In our proposal, sensor nodes are service providers and applications are clients of such services. Our main goal is to enable an interoperability layer among applications and sensor networks, among different sensors in a WSN and eventually among different WSN spread all over the world.},
booktitle = {Proceedings of the ACM/IFIP/USENIX 2003 International Conference on Middleware},
pages = {474–492},
numpages = {19},
location = {Rio de Janeiro, Brazil},
series = {Middleware '03}
}

@article{10.1016/j.jss.2018.07.014,
author = {Makki, Majid and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter},
title = {A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.014},
doi = {10.1016/j.jss.2018.07.014},
journal = {J. Syst. Softw.},
month = oct,
pages = {423–438},
numpages = {16},
keywords = {Software quality, Functional customization, Workflow automation, Software-as-a-Service, Multi-tenancy}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {program synthesis, exploitation, data only attacks, block oriented programming, binary analysis},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.1007/s10586-014-0397-5,
author = {Tan, Yujuan and Yan, Zhichao and Feng, Dan and He, Xubin and Zou, Qiang and Yang, Lei},
title = {De-Frag: an efficient scheme to improve deduplication performance via reducing data placement de-linearization},
year = {2015},
issue_date = {Mar 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-014-0397-5},
doi = {10.1007/s10586-014-0397-5},
abstract = {Data deduplication has become a commodity in large-scale storage systems, especially in data backup and archival systems. However, due to the removal of redundant data, data deduplication de-linearizes data placement and forces the data chunks of the same data object to be divided into multiple separate units. In our preliminary study, we found that the de-linearization of data placement compromises the data spatial locality that is used to improve data read performance, deduplication throughput and deduplication efficiency in some deduplication approaches, which significantly affects deduplication performance and makes some deduplication approaches become less effective. In this paper, we first analyze the negative effect of data placement de-linearization to deduplication performance, and then propose an effective approach called De-Frag to reduce the de-linearization of data placement. The key idea of De-Frag is to choose some redundant data to be written to the disks rather than be removed. It quantifies the spatial locality of each chunk group by spatial locality level (SPL for short) and writes the redundant chunks to disks when SPL value is smaller than a preset value, thus to reduce the de-linearization of data placement and enhance the spatial locality. As shown in our experimental results driven by real world datasets, De-Frag effectively enhances data spatial locality and improves deduplication throughput, deduplication efficiency, and data read performance, at the cost of slightly lower compression ratios.},
journal = {Cluster Computing},
month = mar,
pages = {79–92},
numpages = {14},
keywords = {Spatial locality, Data placement de-linearization, Data deduplication}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2593770.2593781,
author = {Ramaswamy, Arunkumar and Monsuez, Bruno and Tapus, Adriana},
title = {Model-driven software development approaches in robotics research},
year = {2014},
isbn = {9781450328494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593770.2593781},
doi = {10.1145/2593770.2593781},
abstract = {Recently, there is an encouraging trend in adopting model-driven engineering approaches for software development in robotics research. In this paper, currently available model-driven techniques in robotics are analyzed with respect to the domain-specific requirements. A conceptual overview of our software development approach called 'Self Adaptive Framework for Robotic Systems (SafeRobots)' is explained and we also try to position our approach within this model ecosystem.},
booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
pages = {43–48},
numpages = {6},
keywords = {Robotics, Model-driven software development},
location = {Hyderabad, India},
series = {MiSE 2014}
}

@article{10.1016/j.comnet.2013.02.025,
author = {Apel, Sven and Von Rhein, Alexander and Th\"{u}m, Thomas and K\"{a}stner, Christian},
title = {Feature-interaction detection based on feature-based specifications},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {12},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2013.02.025},
doi = {10.1016/j.comnet.2013.02.025},
abstract = {Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification.},
journal = {Comput. Netw.},
month = aug,
pages = {2399–2409},
numpages = {11},
keywords = {Software product lines, Modularity, Feature-based specification, Feature orientation, Feature interaction}
}

@inproceedings{10.1007/978-3-642-36949-0_15,
author = {Membarth, Richard and Hannig, Frank and Teich, J\"{u}rgen and K\"{o}rner, Mario and Eckert, Wieland},
title = {Mastering software variant explosion for GPU accelerators},
year = {2012},
isbn = {9783642369483},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36949-0_15},
doi = {10.1007/978-3-642-36949-0_15},
abstract = {Mapping algorithms in an efficient way to the target hardware poses a challenge for algorithm designers. This is particular true for heterogeneous systems hosting accelerators like graphics cards. While algorithm developers have profound knowledge of the application domain, they often lack detailed insight into the underlying hardware of accelerators in order to exploit the provided processing power. Therefore, this paper introduces a rule-based, domain-specific optimization engine for generating the most appropriate code variant for different Graphics Processing Unit (GPU) accelerators. The optimization engine relies on knowledge fused from the application domain and the target architecture. The optimization engine is embedded into a framework that allows to design imaging algorithms in a Domain-Specific Language (DSL). We show that this allows to have one common description of an algorithm in the DSL and select the optimal target code variant for different GPU accelerators and target languages like CUDA and OpenCL.},
booktitle = {Proceedings of the 18th International Conference on Parallel Processing Workshops},
pages = {123–132},
numpages = {10},
location = {Rhodes Island, Greece},
series = {Euro-Par'12}
}

@article{10.1016/j.datak.2021.101929,
author = {Ali, Mughees and Khan, Saif Ur Rehman and Hussain, Shahid},
title = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {136},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101929},
doi = {10.1016/j.datak.2021.101929},
journal = {Data Knowl. Eng.},
month = nov,
numpages = {19},
keywords = {Self-adaptive mobile apps, Smartphone applications, Mobile applications, Self-adaptation}
}

@article{10.1504/IJWET.2015.069359,
author = {Berkane, Mohamed Lamine and Seinturier, Lionel and Boufaida, Mahmoud},
title = {Using variability modelling and design patterns for self-adaptive system engineering: application to smart-home},
year = {2015},
issue_date = {May 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2015.069359},
doi = {10.1504/IJWET.2015.069359},
abstract = {Adaptability is an increasingly important requirement for many systems, in particular for those that are deployed in dynamically changing environments. The purpose is to let the systems react and adapt autonomously to changing executing conditions without human intervention. Due to the large number of variability decisions e.g., user needs, environment characteristics and the current lack of reusable adaptation expertise, it becomes increasingly difficult to build a system that satisfies all the requirements and constraints that might arise during its lifetime. In this paper, we propose an approach for developing policies for self-adaptive systems at multiple levels of abstraction. This approach is the first that allows the combination of variability with feature model and reusability with design pattern into a single solution for product derivation that gives strong support to develop self-adaptive systems in a modular way. We demonstrate the feasibility of the proposed approach with a use case based on a smart home scenario.},
journal = {Int. J. Web Eng. Technol.},
month = may,
pages = {65–93},
numpages = {29}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1016/j.cie.2016.09.023,
author = {Addo-Tenkorang, Richard and Helo, Petri T.},
title = {Big data applications in operations/supply-chain management},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.09.023},
doi = {10.1016/j.cie.2016.09.023},
abstract = {Harnessing optimum value from industrial data increased in the last two decades.A detailed review of "big data" application in operations/SC management processes.Proposed (Value-adding - V5) framework for operation/SC management. PurposeBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of "big data." Therefore, this paper attempts to thoroughly investigate "big data," its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of "big data" and its extension into "big data II"/IoT-value-adding perspectives by proposing a value-adding framework. Methodology/research approachThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of "big data" applications between 2010 and 2015. Findings/resultsThe four main attributes or factors identified with "big data" include - big data development sources (Variety - V1), big data acquisition (Velocity - V2), big data storage (Volume - V3), and finally big data analysis (Veracity - V4). However, the study of "big data" has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding - V5) - "Big Data cloud computing perspective/Internet of Things (IoT)". Hence, the four Vs of "big data" is now expanded into five Vs. Originality/value of researchThis paper presents original literature review research discussing "big data" issues, trends and perspectives in operations/supply-chain management in order to propose "Big data II" (IoT - Value-adding) framework. This proposed framework is supposed or assumed to be an extension of "big data" in a value-adding perspective, thus proposing that "big data" be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.},
journal = {Comput. Ind. Eng.},
month = nov,
pages = {528–543},
numpages = {16},
keywords = {Operations/supply-chain management, Master database management, Internet of Things (IoT), Cloud computing, Big data - applications and analysis}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Digital library, Replication, Threats to validity, Software engineering, Systematic literature review, Tertiary study}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {system architecture, safety analysis, cyber-physical system},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1016/j.future.2018.05.023,
author = {Guerrera, Danilo and Maffia, Antonio and Burkhart, Helmar},
title = {Reproducible stencil compiler benchmarks using prova!      },
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.05.023},
doi = {10.1016/j.future.2018.05.023},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {933–946},
numpages = {14},
keywords = {Roofline, Performance engineering, Stencil, HPC, Reproducible research, Reproducibility}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Spanish community, Research trends, Systematic review, Search-based software engineering}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {data stream processing, complex event processing, Event processing}
}

@article{10.1016/j.comnet.2019.07.013,
author = {Pimpinella, Andrea and Redondi, Alessandro E.C. and Cesana, Matteo},
title = {Walk this way! An IoT-based urban routing system for smart cities},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.07.013},
doi = {10.1016/j.comnet.2019.07.013},
journal = {Comput. Netw.},
month = oct,
numpages = {12},
keywords = {Temporal forecasting, Spatial interpolation, Urban routing, Smart cities, Internet of things}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@article{10.1007/s11257-015-9159-1,
author = {Dim, Eyal and Kuflik, Tsvi and Reinhartz-Berger, Iris},
title = {When user modeling intersects software engineering: the info-bead user modeling approach},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-015-9159-1},
doi = {10.1007/s11257-015-9159-1},
abstract = {User models (UMs) allow systems to provide personalized services to their users. Nowadays, UMs are developed ad-hoc, as part of specific applications, thus requiring repetitive development efforts. In this paper, we propose the info-bead user modeling approach, which is based on ideas taken from software engineering in general and component-based software development in particular. The basic standalone unit, the info-bead, represents a single user attribute within time-tagged information-items. An info-bead encapsulates an inference process that uses data received from sensors or other info-beads and yields an information-item value. Having standard interfaces, info-beads can be linked, thus creating info-pendants. Both info-beads and info-pendants can be assembled as needed into complex and abstract user models (UMs) and group models (GMs). The goal of the suggested approach is to ease the modeling process and to allow reuse of info beads developed for one UM in other UMs that need the same information. In order to assess the reusability and collaboration capabilities of the info-bead user modeling approach, we developed a prototype tool that enables UM designers, who are not necessarily software developers, to easily select and integrate info-beads for constructing UMs and GMs. We further demonstrated the use of the approach in a museum environment, for modeling of assistive technology ontology and for user modeling in various specific domains. Finally, we analyzed and assessed the characteristics of the approach with respect to existing generic user modeling criteria.},
journal = {User Modeling and User-Adapted Interaction},
month = aug,
pages = {189–229},
numpages = {41},
keywords = {User modeling tool, User modeling software engineering, User model reusability, User model, Info-pendant, Info-bead, Group model, Component-based user model}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {type checking, product-line analysis, fuji, feature-oriented programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {systematic literature review, non-functional requirements, model-based integration testing, assessment types},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Systematic mapping study, Software component, Component-based software engineering, Component-based software development}
}

@inproceedings{10.1007/978-3-030-37734-2_62,
author = {Choi, Jung-Woo},
title = {Real-Time Demonstration of Personal Audio and 3D Audio Rendering Using Line Array Systems},
year = {2020},
isbn = {978-3-030-37733-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37734-2_62},
doi = {10.1007/978-3-030-37734-2_62},
abstract = {Control of sound fields using array loudspeakers has been attempted in many practical areas, such as 3D audio, active noise control, and personal audio. In this work, we demonstrate two real-time sound field control systems involving a line array of loudspeakers. The first one, a personal audio system, aims to reproduce two independent sound zones with different audio programs at the same time. By suppressing acoustic interference between two sound zones, the personal audio system allows users at different locations to enjoy independent sounds. In the second demonstration, active control of spatial audio scene is presented. It has been found that the interaction between the radiation from a sound source and surrounding environment is linked with many perceptual cues of spaciousness. Especially, the perceived stage width and distance are strongly related to the interaural cross-correlation and direct-to-reverberation ratio, which can be easily manipulated by changing the directivity of a loudspeaker array. The smooth transition of spaciousness is demonstrated by changing the shapes of multiple beam patterns radiated from the line array.},
booktitle = {MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part II},
pages = {734–738},
numpages = {5},
keywords = {Spatial audio, Personal audio, Sound field control},
location = {Daejeon, Korea (Republic of)}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@article{10.1007/s10270-020-00823-4,
author = {Kretschmer, Roland and Khelladi, Djamel Eddine and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Consistent change propagation within models},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00823-4},
doi = {10.1007/s10270-020-00823-4},
abstract = {Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so, developers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts of the same model and/or even other models, possibly created and maintained by other developers. The consequences are incomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing inconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete changes in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As developers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not yet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be done in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identifies actual repair sequences that developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach meets the workflow of how developers handle changes by always computing the sequence of repairs resulting from the change propagation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {539–555},
numpages = {17},
keywords = {Consistency detection, Change propagation, Inconsistency repair, Model-driven engineering}
}

@inproceedings{10.1145/3319008.3319015,
author = {Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
title = {A Review of Meta-ethnographies in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319015},
doi = {10.1145/3319008.3319015},
abstract = {Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {systematic (literature) review, qualitative research synthesis, meta-ethnography},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1007/s00450-011-0202-0,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {A quality driven extension to the QVT-relations transformation language},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-011-0202-0},
doi = {10.1007/s00450-011-0202-0},
abstract = {An emerging approach to software development is Model Driven Software Development 				(MDSD). It shifts the focus from source code to models, aims at cost reduction, risk 				mitigation, and eases the engineering of complex applications. System models can be 				used in the early development stages to verify certain relevant properties, such as 				performance, before source code is available and problems become hard and costly to 				solve. The present status of Model Driven Engineering (MDE) is still far from this 				ideal situation. A well-known problem is feedback provisioning, which arises when 				different solutions for the same design problem exist. An approach for feedback 				provisioning automation leverages model transformations, which glue together models 				in an MDSD setting, encapsulate the design rationale, and promote knowledge reuse 				and solutions otherwise available only to experienced engineers. In this article we 				present QVTR2, our solution to the feedback problem. 					QVTR2 is an extension of the QVT-Relations language 				with constructs to express design alternatives, their impact on non-functional 				metrics, and how to evaluate them and guide the engineers in the selection of the 				most appropriate solution. We demonstrate the effectiveness of our solution by using 				the QVTR2 engine to perform a modified version of the 				standard UML-to-RDBMS transformation in the 				context of a real e-commerce application, and by showing how we can guide a 				non-expert engineer in the selection of a solution that satisfies given performance 				requirements.},
journal = {Comput. Sci.},
month = feb,
pages = {1–20},
numpages = {20},
keywords = {Model transformations, Model driven software development, Model driven quality prediction, Feedback provisioning}
}

@inproceedings{10.1145/3361525.3361544,
author = {Ni, Xiang and Schneider, Scott and Pavuluri, Raju and Kaus, Jonathan and Wu, Kun-Lung},
title = {Automating Multi-level Performance Elastic Components for IBM Streams},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361544},
doi = {10.1145/3361525.3361544},
abstract = {Streaming applications exhibit abundant opportunities for pipeline parallelism, data parallelism and task parallelism. Prior work in IBM Streams introduced an elastic threading model that sought the best performance by automatically tuning the number of threads. In this paper, we introduce the ability to automatically discover where that threading model is profitable. However this introduces a new challenge: we have separate performance elastic mechanisms that are designed with different objectives, leading to potential negative interactions and unintended performance degradation. We present our experiences in overcoming these challenges by showing how to coordinate separate but interfering elasticity mechanisms to maxmize performance gains with stable and fast parallelism exploitation. We first describe an elastic performance mechanism that automatically adapts different threading models to different regions of an application. We then show a coherent ecosystem for coordinating this threading model elasticty with thread count elasticity. This system is an online, stable multi-level elastic coordination scheme that adapts different regions of a streaming application to different threading models and number of threads. We implemented this multi-level coordination scheme in IBM Streams and demonstrated that it (a) scales to over a hundred threads; (b) can improve performance by an order of magnitude on two different processor architectures when an application can benefit from multiple threading models; and (c) achieves performance comparable to hand-optimized applications but with much fewer threads.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {163–175},
numpages = {13},
keywords = {runtime, elastic scheduling, Stream processing},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1007/978-3-030-27455-9_4,
author = {Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Farah, Paulo Roberto and Vergilio, Silvia Regina and Guizzo, Giovani},
title = {A Review of Ten Years of the Symposium on Search-Based Software Engineering},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_4},
doi = {10.1007/978-3-030-27455-9_4},
abstract = {The year 2018 marked the tenth anniversary of the Symposium on Search Based Software Engineering (SSBSE). In order to better understand the characteristics and evolution of papers published in SSBSE, this work reports results from a mapping study targeting the ten proceedings of SSBSE. Our goal is to identify and to analyze authorship collaborations, the impact and relevance of SSBSE in terms of citations, the software engineering areas commonly studied as well as the new problems recently solved, the computational intelligence techniques preferred by authors and the rigour of experiments conducted in the papers. Besides this analysis, we list some recommendations to new authors who envisage to publish their work in SSBSE. Despite of existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {42–57},
numpages = {16},
keywords = {Bibliometric analysis, SBSE, Systematic mapping},
location = {Tallinn, Estonia}
}

@article{10.1109/92.920819,
author = {Kim, Suhwan and Papaefthymiou, Marios C.},
title = {True single-phase adiabatic circuitry},
year = {2001},
issue_date = {Feb. 2001},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {9},
number = {1},
issn = {1063-8210},
url = {https://doi.org/10.1109/92.920819},
doi = {10.1109/92.920819},
abstract = {Dynamic logic families that rely on energy recovery to achieve low energy dissipation control the flow of data through gate cascades using multiphase clocks. Consequently, they typically use multiple clock generators and can exhibit increased energy consumption on their clock distribution networks. Moreover, they are not attractive for high-speed design due to their high complexity and clock skew management problems. In this paper, we present TSEL, the first energy-recovering (a.k.a. adiabatic) logic family that operates with a single-phase sinusoidal clocking scheme. We also present SCAL, a source-coupled variant of TSEL with improved supply voltage scalability and energy efficiency. Optimal performance under any operating conditions is achieved in SCAL using a tunable current source in each gate. TSEL and SCAL outperform previous adiabatic logic families in terms of energy efficiency and operating speed. In layout-based simulations with 0.5 /spl mu/m standard CMOS process parameters, 8-bit carry-lookahead adders (CLAs) in TSEL and SCAL function correctly for operating frequencies exceeding 200 MHz. In comparison with corresponding CLAs in alternative logic styles that operate at minimum supply voltages, CLAs designed in our single-phase adiabatic logic families are more energy efficient across a broad range of operating frequencies. Specifically, for clock rates ranging from 10 to 200 MHz, our andbit SCAL CLAs are 1.5 to 2.5 times more energy efficient than corresponding adders developed in PAL and 2N2P and 2.0 to 5.0 times less dissipative than their purely combinational or pipelined CMOS counterparts.},
journal = {IEEE Trans. Very Large Scale Integr. Syst.},
month = feb,
pages = {52–64},
numpages = {13},
keywords = {true single-phase clocking, low-energy computing, energy recovery logic, dynamic circuits, carry-lookahead adder, adiabatic circuits}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {software product lines, product-line analysis, PLA model},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Decision-making, Cloud service selection, Cloud computing}
}

@inproceedings{10.1007/978-3-319-26844-6_32,
author = {Brink, Christopher and Heisig, Philipp and Sachweh, Sabine},
title = {Using Cross-Dependencies During Configuration of System Families},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_32},
doi = {10.1007/978-3-319-26844-6_32},
abstract = {Nowadays, the automotive industry uses software product lines to support the management and maintenance of software variants. However, the development of mechatronic systems includes not merely software, but also other system parts like operating system, hardware or even mechanical parts. We call a combination of these system parts a system family SF. This combination raises the question how different variable system parts can be modeled and used for a combined configuration in a flexible way. We argue that a modeling process should combine all of these system parts, while the product configuration has to consider dependencies between them. Based on our previous work, we address this question and discuss dependencies between different system parts.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {439–452},
numpages = {14},
keywords = {Systems, System families, Product lines, Hardware/software, Feature models, Dependencies},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {Well-formed preferences, S-AHP method, Lexicographic order, Conditional preferences, Comparative preferences, AHP method}
}

@inproceedings{10.1145/2307636.2307676,
author = {Borchert, Christoph and Lohmann, Daniel and Spinczyk, Olaf},
title = {CiAO/IP: a highly configurable aspect-oriented IP stack},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307676},
doi = {10.1145/2307636.2307676},
abstract = {Internet protocols are constantly gaining relevance for the domain of mobile and embedded systems. However, building complex network protocol stacks for small resource-constrained devices is more than just porting a reference implementation. Due to the cost pressure in this area especially the memory footprint has to be minimized. Therefore, embedded TCP/IP implementations tend to be statically configurable with respect to the concrete application scenario. This paper describes our software engineering approach for building CiAO/IP - a tailorable TCP/IP stack for small embedded systems, which pushes the limits of static configurability while retaining source code maintainability. Our evaluation results show that CiAO/IP thereby outperforms both lwIP and uIP in terms of code size (up to 90% less than uIP), throughput (up to 20% higher than lwIP), energy consumption (at least 40% lower than uIP) and, most importantly, tailorability.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {435–448},
numpages = {14},
keywords = {tcp/ip, operating systems, network protocol stacks, internet protocol, embedded systems, aspectc++, aspect-oriented programming, aop},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.5555/3320516.3320904,
author = {Wenzel, Sigrid and Peter, Tim and Stoldt, Johannes and Schlegel, Andreas and Uhlig, Tobias and J\'{o}svai, J\'{a}nos},
title = {Considering energy in the simulation of manufacturing systems},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {In recent years, environmental aspects became one of the key interests in manufacturing. Accordingly, simulation studies had to include factors like energy or emissions. This paper aims to provide a comprehensive introduction to the state of the art in modeling of energy and emissions in simulation of manufacturing systems. We review existing literature to develop a landscape of common approaches and best practices. Typical goals and objectives of the reviewed simulation projects are summarized. Furthermore, we will evaluate the structure and life cycle phases of the examined manufacturing systems and look into the requirements and implementation of respective simulation studies. Finally, we will discuss open questions and future trends in this field of research.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3275–3286},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1007/978-3-642-30982-3_7,
author = {Petriu, Dorina C. and Alhaj, Mohammad and Tawhid, Rasha},
title = {Software performance modeling},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_7},
doi = {10.1007/978-3-642-30982-3_7},
abstract = {Ideally, a software development methodology should include both the ability to specify non-functional requirements and to analyze them starting early in the lifecycle; the goal is to verify whether the system under development would be able to meet such requirements. This chapter considers quantitative performance analysis of UML software models annotated with performance attributes according to the standard "UML Profile for Modeling and Analysis of Real-Time and Embedded Systems" (MARTE). The chapter describes a model transformation chain named PUMA (Performance by Unified Model Analysis) that enables the integration of performance analysis in a UML-based software development process, by automating the derivation of performance models from UML+MARTE software models, and by facilitating the interoperability of UML tools and performance tools. PUMA uses an intermediate model called "Core Scenario Model" (CSM) to bridge the gap between different kinds of software models accepted as input and different kinds of performance models generated as output. Transformation principles are described for transforming two kinds of UML behaviour representation (sequence and activity diagrams) into two kinds of performance models (Layered Queueing Networks and stochastic Petri nets). Next, PUMA extensions are described for two classes of software systems: service-oriented architecture (SOA) and software product lines (SPL).},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {219–262},
numpages = {44},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {HADAS eco-assistant, Green plugin, Cyber–physical systems, Energy consumption}
}

@article{10.1155/2021/8822786,
author = {Zhao, Yiqing and Prabhu, M. and Ahmed, Ramyar Rzgar and Sahu, Anoop Kumar and Bruneo, Dario},
title = {Research Trends and Performance of IIoT Communication Network-Architectural Layers of Petrochemical Industry 4.0 for Coping with Circular Economy},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/8822786},
doi = {10.1155/2021/8822786},
abstract = {In the present era, many Petrochemical Industries (PIs) are driven energetically due to IIoT (Industrial Internet of Things) Communication Networks/Architectural Layers (CNs/ALs), abbreviated as PI4.0-CNs/ALs. PI4.0 fruitfully participated to achieve the Circular Economy (CE) by speeding the reutilization, recovery, and recycling of scrap materials by minimizing cost, unproductive operations, energy consumption, emission of flue gases, etc. Recently, it has been ascertained that the identification and measurement of Research Trends (RTs) of CNs-ALs help the PI4.0 to build the future CE. In addressing the said research challenge, the objective of this research dossier is turned towards inculcating into future PI4.0 researchers the RTs of CNs/ALs of PI4.0, so that the researches can be organized over the very weak and moderately performing CNs-ALs to hike the future CE. To materialize the RTs of PI4-CNs/ALs, the authors conducted the Systematic Literature Survey (SLS) focusing on PI4.0-CNs/ALs, i.e., Internet of Things (IoTs), Cyber Physical System (CPS), Virtual Reality (VR), Integration (I), Data Optimization (DO), Enterprise Resource Planning (ERP), Plant Control (PC), Data and Analytics (DA), Network (N), and Information and Data Management (IDM). The authors searched three hundred two research documents, wherein two hundred seventy-five research manuscripts qualified as RQ2. Next, the authors collected the DOIs/URLs corresponding to each CN-AL and explored the Sum of Digit Scoring System (SDSS) to summarize the DOIs/URLs of PI4.0-CNs/ALs. The RTs of DO have been determined as excellent and stronger over 2007-2017 than residue CNs/ALs. Eventually, the authors advised scholars to focus on the research areas of very weak and moderately weak performing CNs/ALs in order to attain future CE.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {32}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Software Product Lines, Recommenders, Product-Line Configuration, Personalized Recommendations},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {object-oriented modeling, comparison criteria, case study, aspect-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1007/978-3-642-54804-8_7,
author = {Kowal, Matthias and Schaefer, Ina and Tribastone, Mirco},
title = {Family-Based Performance Analysis of Variant-Rich Software Systems},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_7},
doi = {10.1007/978-3-642-54804-8_7},
abstract = {We study models of software systems with variants that stem from a specific choice of configuration parameters with a direct impact on performance properties. Using UML activity diagrams with quantitative annotations, we model such systems as a product line. The efficiency of a product-based evaluation is typically low because each product must be analyzed in isolation, making difficult the re-use of computations across variants. Here, we propose a family-based approach based on symbolic computation. A numerical assessment on large activity diagrams shows that this approach can be up to three orders of magnitude faster than product-based analysis in large models, thus enabling computationally efficient explorations of large parameter spaces.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {94–108},
numpages = {15}
}

@inproceedings{10.5555/2045753.2045804,
author = {Zhou, Jingang and Zhao, Dazhe and Liu, Jiren},
title = {A domain specific language for interactive enterprise application development},
year = {2011},
isbn = {9783642239816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web-based enterprise applications (EAs) have become the mainstream for business systems; however, there are enormous challenges for EAs development to meet the software quality and delivery deadline. In this paper, we propose a domain specific language, called WL4EA, which combines components with generative reuse and targets for popular application frameworks (or platform) and supports high interactivity. With WL4EA, an EA can be declaratively specified as some sets of entities, views, business objects, and data access objects. Such language elements will be composed according to known EA architecture and patterns. Such a DSL and code generation can lower the development complexity and error proneness and improve efficiency.},
booktitle = {Proceedings of the 2011 International Conference on Web Information Systems and Mining - Volume Part II},
pages = {351–360},
numpages = {10},
keywords = {web Application, generative programming, enterprise application, domain specific language},
location = {Taiyuan, China},
series = {WISM'11}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {xUML, real-time, embedded, UML, TLA+, MDE, MDD, MDA, MARTE, ADL, AADL}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/1768904.1768924,
author = {Regnell, Bj\"{o}rn and H\"{o}st, Martin and Svensson, Richard Berntsson},
title = {A quality performance model for cost-benefit analysis of non-functional requirements applied to the mobile handset domain},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In market-driven requirements engineering for platform-based development of embedded systems such as mobile phones, it is crucial to market success to find the right balance among competing quality aspects (aka nonfunctional requirements). This paper presents a conceptual model that incorporates quality as a dimension in addition to the cost and value dimensions used in prioritisation approaches for functional requirements. The model aims at supporting discussion and decision-making in early requirements engineering related to activities such as roadmapping, release planning and platform scoping. The feasibility and relevance of the model is initially validated through interviews with requirements experts in six cases that represent important areas in the mobile handset domain. The validation suggests that the model is relevant and feasible for this particular domain.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {277–291},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@article{10.1007/s10664-015-9364-x,
author = {Passos, Leonardo and Teixeira, Leopoldo and Dintzner, Nicolas and Apel, Sven and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof and Borba, Paulo and Guo, Jianmei},
title = {Coevolution of variability models and related software artifacts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9364-x},
doi = {10.1007/s10664-015-9364-x},
abstract = {Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1744–1793},
numpages = {50},
keywords = {Variability, Software product lines, Patterns, Linux, Evolution}
}

@inproceedings{10.5555/2487336.2487363,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {trade-off analysis, model-driven engineering, embedded systems, design space exploration, UML},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@article{10.1186/s13638-020-01795-1,
author = {Cox, Bert and Van der Perre, Liesbet and Wielandt, Stijn and Ottoy, Geoffrey and De Strycker, Lieven},
title = {High precision hybrid RF and ultrasonic chirp-based ranging for low-power IoT nodes},
year = {2020},
issue_date = {Nov 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-020-01795-1},
doi = {10.1186/s13638-020-01795-1},
abstract = {Hybrid acoustic-RF systems offer excellent ranging accuracy, yet they typically come at a power consumption that is too high to meet the energy constraints of mobile IoT nodes. We combine pulse compression and synchronized wake-ups to achieve a ranging solution that limits the active time of the nodes to 1 ms. Hence, an ultra low-power consumption of 9.015 µW for a single measurement is achieved. The operation time is estimated on 8.5 years on a CR2032 coin cell battery at a 1 Hz update rate, which is over 250 times larger than state-of-the-art RF-based positioning systems. Measurements based on a proof-of-concept hardware platform show median distance error values below 10 cm. Both simulations and measurements demonstrate that the accuracy is reduced at low signal-to-noise ratios and when reflections occur. We introduce three methods that enhance the distance measurements at a low extra processing power cost. Hence, we validate in realistic environments that the centimeter accuracy can be obtained within the energy budget of mobile devices and IoT nodes. The proposed hybrid signal ranging system can be extended to perform accurate, low-power indoor positioning.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = sep,
numpages = {24},
keywords = {Acoustic signal processing, Pulse compression, Ultra low-power electronics, Hybrid signaling, Ranging}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.jss.2012.10.013,
author = {Nakagawa, Elisa Y. and Antonino, Pablo O. and Becker, Martin and Maldonado, Jos\'{e} C. and Storf, Holger and Villela, Karina B. and Rombach, Dieter},
title = {Relevance and perspectives of AAL in Brazil},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.10.013},
doi = {10.1016/j.jss.2012.10.013},
abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
journal = {J. Syst. Softw.},
month = apr,
pages = {985–996},
numpages = {12},
keywords = {Reference architecture, Population aging, Ambient Assisted Living (AAL), AAL platform}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inbook{10.5555/1793854.1793863,
author = {Liu, Chunjian Robin and Gibbs, Celina and Coady, Yvonne},
title = {Safe and sound evolution with SONAR: sustainable optimization and navigation with aspects for system-wide reconciliation},
year = {2007},
isbn = {3540770410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditional diagnostic and optimization techniques typically rely on static instrumentation of a small portion of an overall system. Unfortunately, solely static and localized approaches are simply no longer sustainable in the evolution of today's complex and dynamic systems. Sustainable Optimization and Navigation with Aspects for system-wide Reconciliation is a fluid and unified framework that enables stakeholders to explore and adapt meaningful entities that are otherwise spread across predefined abstraction boundaries. Through a combination of Aspect-Oriented Programming, Extensible Markup Language, and management tools such as Java Management Extensions, SONAR can comprehensively coalesce scattered artifacts--enabling evolution to be more inclusive of system-wide considerations by supporting both iterative and interactive practices. We believe this system-wide approach promotes the application of safe and sound principles in system evolution. This paper presents SONAR's model, examples of its concrete manifestation, and an overview of its associated costs and benefits. Case studies demonstrate how SONAR can be used to accurately identify performance bottlenecks and effectively evolve systems by optimizing behaviour, even at runtime.},
booktitle = {Transactions on Aspect-Oriented Software Development IV},
pages = {163–190},
numpages = {28}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/2517208.2517228,
author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}schel, Markus},
title = {Spiral in scala: towards the systematic construction of generators for performance libraries},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517228},
doi = {10.1145/2517208.2517228},
abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {125–134},
numpages = {10},
keywords = {synthesis, selective precomputation, scalar replacement, data representation, abstraction over staging},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.4108/eai.25-10-2016.2266615,
author = {Distefano, Salvatore and Scarpa, Marco},
title = {Quantitative assessment of workflow performance through PH reduction},
year = {2017},
isbn = {9781631901416},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/eai.25-10-2016.2266615},
doi = {10.4108/eai.25-10-2016.2266615},
abstract = {Workflows are logical abstraction of processes widely adopted in several contexts such as economy and management sciences (business processes), service engineering (service oriented architecture, Web services, BPEL), software engineering (component based systems, UML, flowcharts) distributed computing (Grid, Cloud, Mapreduce). Design and operation of workflows are critical stages in which problems and issues not manifested by the single block arise from compositions. To deal with such issues, proper techniques and tools should be implemented as support for workflow designers and operators. This paper proposes a solution for the evaluation of workflow performance starting from the components’ ones. Based on the stochastic characterization of the workflow tasks, phase type distributions and stochastic workflow reduction rules, the proposed approach allows to overcome the limits of existing solutions, considering general response time distributions while providing parametric analysis on customer usage profiles and design alternatives. To demonstrate the effectiveness of the proposed solution an example taken from literature is evaluated.},
booktitle = {Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {117–124},
numpages = {8},
keywords = {workflow, usage profile, phase type, performance, non-markovian behaviors, design alternatives},
location = {Taormina, Italy},
series = {VALUETOOLS'16}
}

@article{10.1007/s00034-018-1002-6,
author = {Chen, Chengying and Chen, Liming},
title = {A 79-dB SNR 1.1-mW Fully Integrated Hearing Aid SoC},
year = {2019},
issue_date = {July      2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {7},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-1002-6},
doi = {10.1007/s00034-018-1002-6},
abstract = {For low-power hearing aid device application, a fully integrated optimized hearing aid SoC structure is proposed in this paper. The SoC consists of high-resolution, low-power analog front-end (AFE), time-division-multiplexed power-on-reset circuit, charge pump, digital signal processing (DSP) platform, and Class-D amplifier. A novel peak-statistical algorithm is proposed to track signal amplitude and adjust automatic gain control loop gain precisely. A comparative DWA is applied to break the correlation of in-band tone and sequential selection scheme, which realizes second-order noise shaping and suppresses harmonic effectively. The SoC has been implemented with 0.13 µm CMOS process. By measurement, it shows that the peak signal-to-noise ratio (SNR) of AFE is 82.6 dB and peak SNR of Class-D amplifier is 79.8 dB. Also, three main algorithms of wide dynamic range compression, noise reduction, and feedback cancelation are executed through DSP platform. With 1 V supply voltage, total SoC power is 1.1 mW and core area is 9.3 mm2. Based on our SoC, a hearing aid device prototype is produced that shows its great potential for mass manufacture in the future.},
journal = {Circuits Syst. Signal Process.},
month = jul,
pages = {2893–2909},
numpages = {17},
keywords = {Low power, Hearing aid, DSP, Analog front-end}
}

@article{10.1016/j.jss.2012.04.079,
author = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
title = {Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop},
year = {2012},
issue_date = {December, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.04.079},
doi = {10.1016/j.jss.2012.04.079},
abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2707–2719},
numpages = {13},
keywords = {Self-tuning, Preference, Goal-oriented reasoning, Feedback control theory, Earned business value}
}

@inproceedings{10.1145/3235830.3235834,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Speedup and Energy Analysis of EEG Classification for BCI Tasks on CPU-GPU Clusters},
year = {2018},
isbn = {9781450365314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3235830.3235834},
doi = {10.1145/3235830.3235834},
abstract = {Many data mining applications on bioinformatics and bioengineering require solving problems with different profiles from the point of view of their implicit parallelism. In this context, heterogeneous architectures comprised by interconnected nodes with multiple multi-core microprocessors and accelerators, such as vector processors, Graphics Processing Units (GPUs), or Field-Programmable Gate Arrays would constitute suitable platforms that offer the possibility of not only to accelerate the running time of the applications, but also to optimize the energy consumption. In this paper, we analyze the speedups and energy consumption of a parallel multiobjective approach for feature selection and classification of electroencephalograms in Brain Computing Interface tasks, by considering different implementation alternatives in a heterogeneous CPU-GPU cluster. The procedure is able to take advantage of parallelism through message-passing among the CPU-GPU nodes of the cluster (through shared-memory and thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The experimental results show high code accelerations and high energy-savings: running times between 1.4 and 5.3% of the sequential time and energy consumptions between 5.9 and 11.6% of the energy consumed by the sequential execution.},
booktitle = {Proceedings of the 6th International Workshop on Parallelism in Bioinformatics},
pages = {33–43},
numpages = {11},
keywords = {Parallelism, Heterogeneous Cluster, Energy-aware Computing, EEG Classification, Distributed Programming, BCI Tasks},
location = {Barcelona, Spain},
series = {PBio 2018}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Query processing, Query optimization, Co-processing}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.5555/2337223.2337416,
author = {Perrouin, Gilles and Morin, Brice and Chauvel, Franck and Fleurey, Franck and Klein, Jacques and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards flexible evolution of dynamically adaptive systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1353–1356},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1145/1970353.1970369,
author = {Abouzeid, Fady and Clerc, Sylvain and Firmin, Fabian and Renaudin, Marc and Sas, Tiempo and Sicard, Gilles},
title = {40nm CMOS 0.35V-Optimized Standard Cell Libraries for Ultra-Low Power Applications},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/1970353.1970369},
doi = {10.1145/1970353.1970369},
abstract = {Ultra-low voltage is now a well-known solution for energy constrained applications designed using nanometric process technologies. This work is focused on setting up an automated methodology to enable the design of ultra-low voltage digital circuits exclusively using standard EDA tools. To achieve this goal, a 0.35V energy-delay optimized library was developed. This library, fully compliant with standard library design flow and characterization, was verified through the design and fabrication of a BCH decoder circuit, following a standard front-end to back-end flow. At 0.33V, it performs at 600 kHz with a dynamic energy consumption reduced by a factor 14x from nominal 1.1V. Based on this design, experiments, and preliminary silicon results, two additional libraries were developed in order to enhance future ultra-low voltage circuit performance.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {35},
numpages = {17},
keywords = {ultra low voltage, subthreshold, methodology, low power, logic, library, energy, design, circuit, CMOS, Bose Choudhury Hocquenghem}
}

@inproceedings{10.1145/3386263.3406898,
author = {Fu, Rongliang and Zhang, Zhi-Min and Tang, Guang-Ming and Huang, Junying and Ye, Xiao-Chun and Fan, Dong-Rui and Sun, Ning-Hui},
title = {Design Automation Methodology from RTL to Gate-level Netlist and Schematic for RSFQ Logic Circuits},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406898},
doi = {10.1145/3386263.3406898},
abstract = {The superconducting rapid single flux quantum (RSFQ) logic circuit has the characteristics of high speed and low power consumption, making it an attractive candidate for future supercomputers. However, computer-aided design (CAD) tools for CMOS cannot be directly applied to RSFQ logic due to their distinct properties. For instance, the RSFQ logic gate can work properly when all its fan-ins have the same logic level. This paper presents the design flow from RTL to RSFQ logic netlist and schematic. First, we implement logic synthesis for RSFQ logic circuits. It achieves path balancing while minimizing the number of DFFs. In addition, we propose an automatic schematic generator for the RSFQ logic circuits. It converts the synthesized netlist into its equivalent schematic. A layer assignment algorithm is proposed, which makes all gates layered in the order of the clock arrival time. Experimental results with ISCAS85 and EPFL benchmarks along with some Kogge-Stone adders have shown a 29.2% reduction in the number of DFFs over the breadth-first first search; moreover, 59.57% and 5.3% decrease in the number of layers of the schematic and number of edge crossings over the ELK tool.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {145–150},
numpages = {6},
keywords = {superconducting logic circuits, schematic generation, logic synthesis, layer assignment, design automation, RSFQ},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Process model, Migration methodology, Legacy application, Evaluation framework, Cloud migration, Cloud computing}
}

@inproceedings{10.5555/1939864.1939916,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Dynamic resource reallocation between deployment components},
year = {2010},
isbn = {3642169007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today's software systems are becoming increasingly configurable and designed for deployment on a plethora of architectures, ranging from sequential machines via multicore and distributed architectures to the cloud. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To model and analyze systems without a fixed architecture, the models need to naturally capture and range over relevant deployment scenarios. For this purpose, it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. In this paper, the object-oriented modeling language Creol is extended with a notion of dynamic deployment components with parametric processing resources, such that processor resources may be explicitly reallocated. The approach is compositional in the sense that functional models and reallocation strategies are both expressed in Creol, and functional models can be run alone or in combination with different reallocation strategies. The formal semantics of deployment components is given in rewriting logic, extending the semantics of Creol, and executes on Maude, which allows simulations and test suites to be applied to models which vary in their available resources as well as in their resource reallocation strategies.},
booktitle = {Proceedings of the 12th International Conference on Formal Engineering Methods and Software Engineering},
pages = {646–661},
numpages = {16},
location = {Shanghai, China},
series = {ICFEM'10}
}

@article{10.1016/j.asoc.2016.08.030,
author = {Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
title = {The experimental applications of search-based techniques for model-based testing},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.030},
doi = {10.1016/j.asoc.2016.08.030},
abstract = {Graphical abstractDisplay Omitted HighlightsA systematic review of applications of search-based techniques for model-based testing is provided.Four taxonomies are proposed to classify the applications based on the purpose, problems, solutions and evaluations.The applications are analyzed based on the proposed taxonomies.The development of search-based techniques for model-based testing is discussed.Limitations and potential research directions are summarized. ContextModel-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. ObjectiveThe objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. MethodWe conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. ResultsThe results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. ConclusionThis extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1094–1117},
numpages = {24},
keywords = {Test case generation, Taxonomy, Systematic literature review, Software testing, Search-based techniques, Model-based testing}
}

@article{10.1016/j.jss.2017.09.033,
author = {Badampudi, Deepika and Wnuk, Krzysztof and Wohlin, Claes and Franke, Ulrik and Smite, Darja and Cicchetti, Antonio},
title = {A decision-making process-line for selection of software asset origins and components},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.033},
doi = {10.1016/j.jss.2017.09.033},
abstract = {Presents a process-line for selecting software asset origins and components.Process-line helps decision-makers to build their decisions-making process.The process-line is evaluated through five case studies in three companies.The practitioners did not perceive any activity to be missing in the process-line.A sub-set of activities were followed by the companies without any specific order. Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company.},
journal = {J. Syst. Softw.},
month = jan,
pages = {88–104},
numpages = {17},
keywords = {Decision-making, Component-based software engineering, Case study}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/1949303.1949307,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Validating timed models of deployment components with parametric concurrency},
year = {2010},
isbn = {3642180698},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many software systems today are designed without assuming a fixed underlying architecture, and may be adapted for sequential, multicore, or distributed deployment. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. Models of such systems need to capture and range over relevant deployment scenarios, so it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. This paper proposes an abstract model of deployment components for concurrent objects, extending the Creol modeling language. The deployment components are parametric in the amount of concurrency they provide; i.e., they vary in processing resources. We give a formal semantics of deployment components and characterize equivalence between deployment components which differ in concurrent resources in terms of test suites. Our semantics is executable on Maude, which allows simulations and test suites to be applied to a deployment component with different concurrent resources.},
booktitle = {Proceedings of the 2010 International Conference on Formal Verification of Object-Oriented Software},
pages = {46–60},
numpages = {15},
location = {Paris, France},
series = {FoVeOOS'10}
}

@inproceedings{10.1109/WI-IAT.2009.363,
author = {Li, Mu and Huai, JinPeng and Guo, HuiPeng},
title = {An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.363},
doi = {10.1109/WI-IAT.2009.363},
abstract = {In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {395–402},
numpages = {8},
keywords = {web service selection, prediction, Structural Equation Modeling, QoS},
series = {WI-IAT '09}
}

@inproceedings{10.1145/2988336.2988353,
author = {Issarny, Valerie and Mallet, Vivien and Nguyen, Kinh and Raverdy, Pierre-Guillaume and Rebhi, Fadwa and Ventura, Raphael},
title = {Dos and Don'ts in Mobile Phone Sensing Middleware: Learning from a Large-Scale Experiment},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988353},
doi = {10.1145/2988336.2988353},
abstract = {Mobile phone sensing contributes to changing the way we approach science: massive amount of data is being contributed across places and time, and paves the way for advanced analyses of numerous phenomena at an unprecedented scale. Still, despite the extensive research work on enabling resource-efficient mobile phone sensing with a very-large crowd, key challenges remain. One challenge is facing the introduction of a new heterogeneity dimension in the traditional middleware research landscape. The middleware must deal with the heterogeneity of the contributing crowd in addition to the system's technical heterogeneities. In order to tackle these two heterogeneity dimensions together, we have been conducting a large-scale empirical study in cooperation with the city of Paris. Our experiment revolves around the public release of a mobile app for urban pollution monitoring that builds upon a dedicated mobile crowd-sensing middleware. In this paper, we report on the empirical analysis of the resulting mobile phone sensing efficiency from both technical and social perspectives, in face of a large and highly heterogeneous population of participants. We concentrate on the data originating from the 20 most popular phone models of our user base, which represent contributions from over 2,000 users with 23 million observations collected over 10 months. Following our analysis, we introduce a few recommendations to overcome -technical and crowd- heterogeneities in the implementation of mobile phone sensing applications and supporting middleware.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {17},
numpages = {13},
keywords = {Urban sensing, Sensors heterogeneity, Sensing accuracy, Mobile phone sensing, Crowd-sensing},
location = {Trento, Italy},
series = {Middleware '16}
}

@article{10.1177/1094342004041291,
author = {P\"{u}schel, Markus and Moura, Jos\'{e} M. F. and Singer, Bryan and Xiong, Jianxin and Johnson, Jeremy and Padua, David and Veloso, Manuela and Johnson, Robert W.},
title = {Spiral: A Generator for Platform-Adapted Libraries of Signal Processing Algorithms},
year = {2004},
issue_date = {February  2004},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {18},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342004041291},
doi = {10.1177/1094342004041291},
abstract = {SPIRAL is a generator for libraries of fast software implementations of linear signal processing transforms. These libraries are adapted to the computing platform and can be re-optimized as the hardware is upgraded or replaced. This paper describes the main components of SPIRAL: the mathematical framework that concisely describes signal transforms and their fast algorithms; the formula generator that captures at the algorithmic level the degrees of freedom in expressing a particular signal processing transform; the formula translator that encapsulates the compilation degrees of freedom when translating a specific algorithm into an actual code implementation; and, finally, an intelligent search engine that finds within the large space of alternative formulas and implementations the "best" match to the given computing platform. We present empirical data that demonstrate the high performance of SPIRAL generated code.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {21–45},
numpages = {25},
keywords = {signal transform, signal processing, search, program generation, optimization, domain-specific language, automatic performance tuning, Fourier transform, FFT, DFT}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {IaaS, Feature model, EC2, Cloud migration, Automated analysis}
}

@article{10.1145/3294054,
author = {Choi, Young-Kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {In-Depth Analysis on Microarchitectures of Modern Heterogeneous CPU-FPGA Platforms},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3294054},
doi = {10.1145/3294054},
abstract = {Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain.This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = feb,
articleno = {4},
numpages = {20},
keywords = {Xeon+FPGA, Heterogeneous computing, CPU-FPGA platform, CAPI, AWS F1}
}

@inproceedings{10.1007/978-3-540-30554-5_19,
author = {Spinczyk, Olaf and Schoettner, Michael and Gal, Andreas},
title = {Programming languages and operating systems},
year = {2004},
isbn = {354023988X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30554-5_19},
doi = {10.1007/978-3-540-30554-5_19},
abstract = {This report gives an overview over the First ECOOP Workshop on Programming Languages and Operating Systems (PLOS 2004). It explains the motivation for the workshop and gives a summary of the workshop contributions and discussions during the workshop.},
booktitle = {Proceedings of the 2004 International Conference on Object-Oriented Technology},
pages = {202–213},
numpages = {12},
location = {Oslo, Norway},
series = {ECOOP'04}
}

@inproceedings{10.1145/3412382.3458260,
author = {Xu, Weitao and Li, Zhenjiang and Xue, Wanli and Yu, Xiaotong and Wei, Bo and Wang, Jia and Luo, Chengwen and Li, Wei and Zomaya, Albert Y.},
title = {InaudibleKey: Generic Inaudible Acoustic Signal based Key Agreement Protocol for Mobile Devices},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458260},
doi = {10.1145/3412382.3458260},
abstract = {Secure Device-to-Device (D2D) communication is becoming increasingly important with the ever-growing number of Internet-of-Things (IoT) devices in our daily life. To achieve secure D2D communication, the key agreement between different IoT devices without any prior knowledge is becoming desirable. Although various approaches have been proposed in the literature, they suffer from a number of limitations, such as low key generation rate and short pairing distance. In this paper, we present InaudibleKey, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey exploits the acoustic channel frequency response of two legitimate devices as a common secret to generating keys. InaudibleKey employs several novel technologies to significantly improve its performance. We conduct extensive experiments to evaluate the proposed system in different real environments. Compared to state-of-the-art works, InaudibleKey improves key generation rate by 3 times, extends pairing distance by 3.2 times, and reduces information reconciliation counts by 2.5 times. Security analysis demonstrates that InaudibleKey is resilient to a number of malicious attacks. We also implement InaudibleKey on modern smartphones and resource-limited IoT devices. Results show that it is energy-efficient and can run on both powerful and resource-limited IoT devices without incurring excessive resource consumption.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {106–118},
numpages = {13},
keywords = {Mobile devices, Key generation, Device pairing, Acoustic signal},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@article{10.1016/j.scico.2012.07.019,
author = {Chabridon, Sophie and Conan, Denis and Abid, Zied and Taconet, Chantal},
title = {Building ubiquitous QoC-aware applications through model-driven software engineering},
year = {2013},
issue_date = {October, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {10},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.07.019},
doi = {10.1016/j.scico.2012.07.019},
abstract = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications.},
journal = {Sci. Comput. Program.},
month = oct,
pages = {1912–1929},
numpages = {18},
keywords = {Ubiquitous computing, Quality of context, Pervasive computing, Model-driven software engineering, Domain specific language, Context}
}

@article{10.1016/j.jpdc.2019.03.002,
author = {Zarei Zefreh, Ebrahim and Lotfi, Shahriar and Mohammad Khanli, Leyli and Karimpour, Jaber},
title = {Topology and computational-power aware tile mapping of perfectly nested loops with dependencies on distributed systems},
year = {2019},
issue_date = {Jul 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.03.002},
doi = {10.1016/j.jpdc.2019.03.002},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {14–35},
numpages = {22},
keywords = {Distributed systems, Topology aware tile mapping, Computational-power aware tile mapping, Parallelization, Nested loop}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Survey, Decision making, Software architecture, Sourcing, Component-based software engineering}
}

@inproceedings{10.1007/978-3-642-12261-3_7,
author = {Van Baelen, Stefan and Weigert, Thomas and Ober, Ileana and Espinoza, Huascar and Ober, Iulian},
title = {Model based architecting and construction of embedded systems (ACES-MB 2009)},
year = {2009},
isbn = {3642122604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12261-3_7},
doi = {10.1007/978-3-642-12261-3_7},
abstract = {The second ACES-MB workshop brought together researchers and practitioners interested in model-based software engineering for real-time embedded systems, with a particular focus on the use of models for architecture description and domain-specific design, and for capturing non-functional constraints. Eleven presenters proposed contributions on domain-specific languages for embedded systems, the Architecture Analysis and Design Language (AADL), analysis and formalization, semantics preservation issues, and variability and reconfiguration. In addition, a lively group discussion tackled the issue of combining models from different Domain Specific Modeling Languages (DSMLs). This report presents an overview of the presentations and fruitful discussions that took place during the ACES-MB 2009 workshop.},
booktitle = {Proceedings of the 2009 International Conference on Models in Software Engineering},
pages = {63–67},
numpages = {5},
location = {Denver, CO},
series = {MODELS'09}
}

@article{10.1016/j.sysarc.2015.07.010,
author = {Meier, Matthias and Breddemann, Mark and Spinczyk, Olaf},
title = {Interfacing the hardware API with a feature-based operating system family},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {61},
number = {10},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2015.07.010},
doi = {10.1016/j.sysarc.2015.07.010},
abstract = {Multiprocessor systems on a chip (MPSoCs) are a popular class of course-grained parallel computer architectures, which are very useful, because they support re-use of legacy software components and application-specific tailoring of hardware structures at the same time. Furthermore, model-driven design frameworks for MPSoCs such as Xilinx' EDK or our own LavA-framework facilitate very fast system development. However, in this paper we argue that these design frameworks are not ideal from the development process perspective. Instead, we propose a software-centric approach that is based on the hardware API concept. The API is a representation of hardware components on the software level, which is generated from a hardware meta-model. It allows us to automatically derive a hardware structure based on access patterns in software, revealed by a static code analysis. This trick reduces the number of hardware details the developer needs to deal with and avoids configuration inconsistencies between the hardware and software levels by design. Furthermore, we present how the development process can benefit from the hardware API, when the API is interfaced with a configurable operating system.},
journal = {J. Syst. Archit.},
month = nov,
pages = {531–538},
numpages = {8},
keywords = {Software-centric configuration, Operating system, MPSoC, Hardware representation, HW/SW co-design, FPGA}
}

@article{10.1007/s00034-018-0859-8,
author = {Xu, Jin and Qiao, Yuansong and Fu, Zhizhong and Wen, Quan},
title = {Image Block Compressive Sensing Reconstruction via Group-Based Sparse Representation and Nonlocal Total Variation},
year = {2019},
issue_date = {January   2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {1},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-0859-8},
doi = {10.1007/s00034-018-0859-8},
abstract = {Compressive sensing (CS) has recently drawn considerable attentions in signal and image processing communities as a joint sampling and compression approach. Generally, the image CS reconstruction can be formulated as an optimization problem with a properly chosen regularization function based on image priors. In this paper, we propose an efficient image block compressive sensing (BCS) reconstruction method, which combine the best of group-based sparse representation (GSR) model and nonlocal total variation (NLTV) model to regularize the solution space of the image CS recovery optimization problem. Specifically, the GSR model is utilized to simultaneously enforce the intrinsic local sparsity and the nonlocal self-similarity of natural images, while the NLTV model is explored to characterize the smoothness of natural images on a larger scale than the classical total variation (TV) model. To efficiently solve the proposed joint regularized optimization problem, an algorithm based on the split Bregman iteration is developed. The experimental results demonstrate that the proposed method outperforms current state-of-the-art image BCS reconstruction methods in both objective quality and visual perception.},
journal = {Circuits Syst. Signal Process.},
month = jan,
pages = {304–328},
numpages = {25},
keywords = {Split Bregman iteration, Nonlocal total variation, Joint regularization, Group-based sparse representation, Block compressive sensing}
}

@inproceedings{10.1007/978-3-319-04891-8_10,
author = {Meier, Matthias and Breddemann, Mark and Spinczyk, Olaf},
title = {Hardware APIs: A Software-Centric Approach for Automated Derivation of MPSoC Hardware Structures Based on Static Code Analysis},
year = {2014},
isbn = {9783319048901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-04891-8_10},
doi = {10.1007/978-3-319-04891-8_10},
abstract = {Multiprocessor systems on a chip (MPSoCs) are a popular class of course-grained parallel computer architectures, which are very useful, because they support re-use of legacy software components and application-specific tailoring of hardware structures at the same time. Furthermore, model-driven design frameworks for MPSoCs such as Xilinx' EDK or our own LavA-framework facilitate very fast system development. However, in this paper we argue that these design frameworks are not ideal from the development process perspective. Instead, we propose a software-centric approach that is based on the hardware API concept. The API is a representation of hardware components on the software level, which is generated from a hardware meta-model. It allows us to automatically derive a hardware structure based on access patterns in software, revealed by a static code analysis. This trick reduces the number of hardware details the developer needs to deal with and avoids configuration inconsistencies between the hardware and software levels by design.},
booktitle = {Proceedings of the 27th International Conference on Architecture of Computing Systems  ARCS 2014 - Volume 8350},
pages = {111–122},
numpages = {12}
}

@article{10.1155/2021/5798741,
author = {Kim, Sungwook and Bazzi, Alessandro},
title = {A New Two-Stage Bargaining Game Approach for Intra- and Inter-WBAN Management},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/5798741},
doi = {10.1155/2021/5798741},
abstract = {The Internet of Medical Things (IoMT) is an amalgamation of smart devices to operate the wireless body area network (WBAN) by using networking technologies. To reduce the burden on WBANs, they link to the mobile edge computing (MEC), on which captured medical data can be stored and analyzed. In this paper, we design a new control scheme to effectively share the limited computation and communication resources in the MEC-assisted WBAN (M-W) platform. Based on the bargaining game theory, our proposed scheme explores the mutual benefits of intra- and inter-WBAN interactions. To dynamically adapt the current system conditions, we shape each WBAN’s aspirations to reach a reciprocal consensus for different application services. Utilizing two control factors, we provide a unifying framework for the study of intra- and inter-WBAN bargaining problems to share the limited system resource. Based on the feasibility and real-time effectiveness, the main novelty of the proposed scheme is the ability to achieve a relevant tradeoff between efficiency and fairness through the interactive bargaining process. At last, the experimental results show that the proposed scheme achieves substantial performance improvements to the comparison schemes.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@article{10.1504/IJIPT.2016.079546,
author = {Balakrishnan, Senthil Murugan and Sangaiah, Arun Kumar},
title = {Aspect-oriented middleware framework for resolving service discovery issues in Internet of Things},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2/3},
issn = {1743-8209},
url = {https://doi.org/10.1504/IJIPT.2016.079546},
doi = {10.1504/IJIPT.2016.079546},
abstract = {Internet of Things IoT is a model of future internet and pervasive computing which have its own challenges derived from the internet in terms of scalability, undefined topology and so on. The proposed work aims to resolve the challenges posed by IoT in service discovery functionality. Considering the pervasive and context dependent nature of IoT the planned work bases its development strategy using an aspect oriented software development methodology. The novelty lies in achieving high degree configuration and customisability by selecting subset of middleware functionality depending on the need. The performance is compared with MUSIC pervasive computing middleware and Android built-in configuration. The result reveals 6.5% decrease on average boot up and reconfiguration time for smart phones and 11.6% percentage decrease in Android tablets. In the context of boot up and reconfiguration time the middleware brings out 5% decrease for smart phones and 3% for Android tablets when compared with MUSIC middleware. The middleware shows 6.3% and 4% reduction in execution time of applications on smart phones and tablets when assessed with MUSIC middleware.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {smartphones, service discovery, reconfiguration time, pervasive computing, internet of things, boot up time, aspect-oriented middleware, Spring AOP, IoT, Android tablets}
}

@article{10.1007/s10664-013-9263-y,
author = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and Engstr\"{o}m, Emelie and Regnell, Bj\"{o}rn and Sabaliauskaite, Giedre and Loconsole, Annabella and Gorschek, Tony and Feldt, Robert},
title = {Challenges and practices in aligning requirements with verification and validation: a case study of six companies},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9263-y},
doi = {10.1007/s10664-013-9263-y},
abstract = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1809–1855},
numpages = {47},
keywords = {Verification, Validation, Testing, Requirements engineering, Case study, Alignment}
}

@inproceedings{10.1109/WI.2006.173,
author = {Zhou, Jiehan and Niemela, Eila},
title = {Toward Semantic QoS Aware Web Services: Issues, Related Studies and Experience},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.173},
doi = {10.1109/WI.2006.173},
abstract = {Semantic QoSaware Web services incorporating the emerging Web services in the QoSaware system development are promoting ServiceOriented Software Engineering (SOSE). To identify the steps toward semantic quality of service (QoS)aware Web services, this paper examines previous studies related to semantic QoSaware Web services, including QoSaware Web service architectures, QoS classification, QoS ontology, QoS specification languages, and Web service creation tools. Moreover, a case study is presented to discuss the gaps between our current quality driven software development approach and the semantic QoSaware Web services.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '06}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {sustainability, survey, software architecture, evolution scenario, architectural metric},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2491956.2462163,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A general constraint-centric scheduling framework for spatial architectures},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462163},
doi = {10.1145/2491956.2462163},
abstract = {Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures.Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {495–506},
numpages = {12},
keywords = {spatial architectures, spatial architecture scheduling, integer linear programming},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1145/2532443.2532448,
author = {Zhao, Tianqi and Zhao, Haiyan and Zhang, Wei},
title = {A preliminary study on requirements modeling methods for self-adaptive software systems},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532448},
doi = {10.1145/2532443.2532448},
abstract = {Internetware denotes a kind of complex distributed software system, which executes in an open, uncertain and dynamic environment, and adapts itself to changes in the environment. An important problem related to the development of Internetware applications is how to define their requirements. Traditional requirements modeling methods work well with software applications deployed in predictable environment, but cannot deal with Internetware applications, which have to identify and adapt themselves to the unpredictable situations of their environment. The self-adaptation characteristic of Internetware applications introduces challenges to the effective modeling of the requirements of Internetware applications. In this paper, we carry out a preliminary study on requirements modeling methods for self-adaptive software systems. In particular, we focus on how existing requirements modeling methods address the challenges caused by self-adaptation and what are the advantages and disadvantages of their solutions. By doing this study, we aim to identify the essential capabilities or properties that a requirements modeling method should possess so as to support the requirements modeling of self-adaptive software systems like Internetware.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {3},
numpages = {10},
keywords = {self-adaptive system, requirement verification, requirement representation, requirement modeling method, requirement evolution},
location = {Changsha, China},
series = {Internetware '13}
}

@article{10.1016/j.adhoc.2017.03.010,
author = {Kishk, S. and Almofari, N.H. and Zaki, F.W.},
title = {Distributed resource allocation in D2D communication networks with energy harvesting relays using stable matching},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2017.03.010},
doi = {10.1016/j.adhoc.2017.03.010},
abstract = {Fifth Generation (5G) cellular networks are expected to provide high data rates by using emerging technologies such as multi-tier heterogeneous networks, Device to Device (D2D) communication and densification of small base stations. D2D uses direct transmission between two cellular devices to increase the system throughput. Relays are used to reduce the loss in user data rate when the D2D users are far from each other and the relay nodes are used to serve the cellular users when the channel is not good enough. To get good performance from D2D relay assisted network, suitable, robust and low complexity resource allocation algorithm must be used. In this paper, an algorithm for user association, resource blocks allocation and power control when considering the energy harvesting relays in heterogeneous multi-tier network is presented. This paper introduces a centralized solution using time sharing strategy and a distributed low complexity solution using stable matching theory.},
journal = {Ad Hoc Netw.},
month = jun,
pages = {114–123},
numpages = {10},
keywords = {Stable matching, Resource allocation, D2D relay assisted}
}

@article{10.1007/s10270-017-0592-y,
author = {Ross, Jordan A. and Murashkin, Alexandr and Liang, Jia Hui and Antkiewicz, Micha\l{} and Czarnecki, Krzysztof},
title = {Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0592-y},
doi = {10.1007/s10270-017-0592-y},
abstract = {In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today's engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {739–767},
numpages = {29},
keywords = {Multi-perspective architectures, Multi-level architectures, Early design, E/E architecture, Candidate architectures, Architecture synthesis, Architecture optimization}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {software bloat, program concerns, feature oriented programming},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@article{10.1007/s10664-016-9466-0,
author = {Behnamghader, Pooyan and Le, Duc Minh and Garcia, Joshua and Link, Daniel and Shahbazian, Arman and Medvidovic, Nenad},
title = {A large-scale study of architectural evolution in open-source software systems},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9466-0},
doi = {10.1007/s10664-016-9466-0},
abstract = {From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated, sometimes careless changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by introducing an architecture recovery framework, ARCADE, for conducting large-scale replicable empirical studies of architectural change across different versions of a software system. ARCADE includes two novel architectural change metrics, which are the key to enabling large-scale empirical studies of architectural change. We utilize ARCADE to conduct an empirical study of changes found in software architectures spanning several hundred versions of 23 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during the system's maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1146–1193},
numpages = {48},
keywords = {Software evolution, Software architecture, Open-source software, Architecture recovery, Architectural change}
}

@inproceedings{10.5555/2485288.2485422,
author = {Rahimi, Abbas and Marongiu, Andrea and Burgio, Paolo and Gupta, Rajesh K. and Benini, Luca},
title = {Variation-tolerant OpenMP tasking on tightly-coupled processor clusters},
year = {2013},
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {We present a variation-tolerant tasking technique for tightly-coupled shared memory processor clusters that relies upon modeling advance across the hardware/software interface. This is implemented as an extension to the OpenMP 3.0 tasking programming model. Using the notion of Task-Level Vulnerability (TLV) proposed here, we capture dynamic variations caused by circuit-level variability as a high-level software knowledge. This is accomplished through a variation-aware hardware/software codesign where: (i) Hardware features variability monitors in conjunction with online per-core characterization of TLV metadata; (ii) Software supports a Task-level Errant Instruction Management (TEIM) technique to utilize TLV metadata in the runtime OpenMP task scheduler. This method greatly reduces the number of recovery cycles compared to the baseline scheduler of OpenMP [22], consequently instruction per cycle (IPC) of a 16-core processor cluster is increased up to 1.51\texttimes{} (1.17\texttimes{} on average). We evaluate the effectiveness of our approach with various number of cores (4,8,12,16), and across a wide temperature range(ΔT=90°C).},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {541–546},
numpages = {6},
location = {Grenoble, France},
series = {DATE '13}
}

@inproceedings{10.5555/3320516.3320905,
author = {Stoldt, Johannes and Prell, Bastian and Schlegel, Andreas and Putz, Matthias},
title = {Applications for models of renewable energy sources and energy storages in material flow simulation},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {The increasing reliance on volatile renewable energy sources in the European Union raises questions regarding the future mechanisms of the energy markets. Energy-intensive production industries are particularly expected to take a more active role by shaping their energy demand according to the availability of wind and sun. Hence, they will need to align their production processes with external energy market signals. This paper presents an application example for a Siemens Plant Simulation extension that makes holistic material flow and energy flow studies of factories possible. The so-called eniBRIC class library provides functionalities for investigating the flow of energy between infrastructure and production equipment. Since its latest update, it can also be used to model renewable energy sources as well as energy storages. A case study of the E3-Research Factory showcases the features of eniBRIC and provides an outlook on future research in the field of energy-flexible production.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3287–3298},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.5555/1045658.1045692,
author = {Billig, Andreas and Busse, Susanne and Leicher, Andreas and S\"{u}\ss{}, J\"{o}rn Guy},
title = {Platform independent model transformation based on triple},
year = {2004},
isbn = {3540234284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reuse is an important topic in software engineering as it promises advantages like faster time-to-market and cost reduction. Reuse of models on an abstract level is more beneficial than on the code level, because these models can be mapped into several technologies and can be adapted according to different requirements. Unfortunately, development tools only provide fixed mappings between abstract models described in a language such as UML and source code for a particular technology. These mappings are based on one-to-one relationships between elements of both levels. As a consequence, it is rarely possible to customize mappings according to specific user requirements.We aim to improve model reuse by providing a framework that generates customized mappings according to specified requirements. The framework is able to handle mappings aimed for several component technologies as it is based on an ADL. It is realized in Triple to represent components on different levels of abstraction and to perform the actual transformation. It uses feature models to describe mapping alternatives.},
booktitle = {Proceedings of the 5th ACM/IFIP/USENIX International Conference on Middleware},
pages = {493–511},
numpages = {19},
location = {Toronto, Canada},
series = {Middleware '04}
}

@inproceedings{10.5555/978-3-030-29983-5_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {i–xxii},
location = {Paris, France}
}

@article{10.1007/s10515-017-0215-4,
author = {Boussa\"{\i}d, Ilhem and Siarry, Patrick and Ahmed-Nacer, Mohamed},
title = {A survey on search-based model-driven engineering},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0215-4},
doi = {10.1007/s10515-017-0215-4},
abstract = {Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering. MDE aims to raise the level of abstraction in order to cope with the complexity of software systems, while SBSE involves the application of metaheuristic search techniques to complex software engineering problems, reformulating engineering tasks as optimization problems. The purpose of this paper is to survey the relatively recent research activity lying at the interface between these two fields, an area that has come to be known as search-based model-driven engineering. We begin with an introduction to MDE, the concepts of models, of metamodels and of model transformations. We also give a brief introduction to SBSE and metaheuristics. Then, we survey the current research work centered around the combination of search-based techniques and MDE. The literature survey is accompanied by the presentation of references for further details.},
journal = {Automated Software Engg.},
month = jun,
pages = {233–294},
numpages = {62},
keywords = {Search-based software engineering (SBSE), Model-driven engineering (MDE), Metaheuristics, Metaheuristic}
}

@inproceedings{10.5555/2028067.2028076,
author = {Snow, Kevin Z. and Krishnan, Srinivas and Monrose, Fabian and Provos, Niels},
title = {SHELLOS: enabling fast detection and forensic analysis of code injection attacks},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {The availability of off-the-shelf exploitation toolkits for compromising hosts, coupled with the rapid rate of exploit discovery and disclosure, has made exploit or vulnerability-based detection far less effective than it once was. For instance, the increasing use of metamorphic and polymorphic techniques to deploy code injection attacks continues to confound signature-based detection techniques. The key to detecting these attacks lies in the ability to discover the presence of the injected code (or, shellcode). One promising technique for doing so is to examine data (be that from network streams or buffers of a process) and efficiently execute its content to find what lurks within. Unfortunately, current approaches for achieving this goal are not robust to evasion or scalable, primarily because of their reliance on software-based CPU emulators. In this paper, we argue that the use of software-based emulation techniques are not necessary, and instead propose a new framework that leverages hardware virtualization to better enable the detection of code injection attacks. We also report on our experience using this framework to analyze a corpus of malicious Portable Document Format (PDF) files and network-based attacks.},
booktitle = {Proceedings of the 20th USENIX Conference on Security},
pages = {9},
numpages = {1},
location = {San Francisco, CA},
series = {SEC'11}
}

@article{10.1177/0037549715603480,
author = {Peters, Brady},
title = {Integrating acoustic simulation in architectural design workflows},
year = {2015},
issue_date = {9 2015},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {91},
number = {9},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549715603480},
doi = {10.1177/0037549715603480},
abstract = {Sound is an important part of our experience of buildings. However, architects design largely using visually based techniques and largely for visual phenomena. Aiming to address this problem, the research presented in this paper proposes four digital design workflows that integrate acoustic computer simulation into architectural design. These techniques enable architects to design for both visual and acoustic criteria. The goal is to develop rapid and accessible workflows for architects that allow acoustic performance to be tuned as geometry and materials are modified at the scale of the room, and also at the scale of the surface. The discovery and testing of these techniques takes place within the design of the FabPod, a semi-enclosed meeting room situated within an open-plan working environment. The project builds on previous research investigating the design principles, the acoustic performance, and the fabrication methods of hyperboloid surface geometry. Four design workflows were developed: two of these investigate the acoustic performance of the room and use existing acoustic simulation software, and the other two workflows investigate the acoustic performance of the surface and use custom-written scripts to calculate and visualize sound scattering. This paper presents the background to the study, outlines the digital workflows, describes how they integrate acoustic simulation, and shows some of the data produced by these simulations.},
journal = {Simulation},
month = sep,
pages = {787–808},
numpages = {22},
keywords = {simulation visualization, performance-based design, design workflows, computer-aided design, architectural design, architectural acoustics, acoustic simulation}
}

@article{10.1016/j.jss.2006.08.039,
author = {Kuz, Ihor and Liu, Yan and Gorton, Ian and Heiser, Gernot},
title = {CAmkES: A component model for secure microkernel-based embedded systems},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.08.039},
doi = {10.1016/j.jss.2006.08.039},
abstract = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
journal = {J. Syst. Softw.},
month = may,
pages = {687–699},
numpages = {13},
keywords = {Microkernel, Embedded system, Component architecture}
}

@article{10.1145/2638550,
author = {Wu, Lisa and Polychroniou, Orestis and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Energy Analysis of Hardware and Software Range Partitioning},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/2638550},
doi = {10.1145/2638550},
abstract = {Data partitioning is a critical operation for manipulating large datasets because it subdivides tasks into pieces that are more amenable to efficient processing. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries. This article measures the performance and energy of state-of-the-art software partitioners, and describes and evaluates a hardware range partitioner that further improves efficiency.The software implementation is broken into two phases, allowing separate analysis of the partition function computation and data shuffling costs. Although range partitioning is commonly thought to be more expensive than simpler strategies such as hash partitioning, our measurements indicate that careful data movement and optimization of the partition function can allow it to approach the throughput and energy consumption of hash or radix partitioning.For further acceleration, we describe a hardware range partitioner, or HARP, a streaming framework that offers a seamless execution environment for this and other streaming accelerators, and a detailed analysis of a 32nm physical design that matches the throughput of four to eight software threads while consuming just 6.9% of the area and 4.3% of the power of a Xeon core in the same technology generation.},
journal = {ACM Trans. Comput. Syst.},
month = aug,
articleno = {8},
numpages = {24},
keywords = {streaming data, specialized functional unit, microarchitecture, data partitioning, Accelerator}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {verification, validation, standards, safety, requirements, hazard analysis, assurance, Certification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Video analysis, Variability, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3458864.3467880,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Owlet: enabling spatial information in ubiquitous acoustic devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467880},
doi = {10.1145/3458864.3467880},
abstract = {This paper presents a low-power and miniaturized design for acoustic direction-of-arrival (DoA) estimation and source localization, called Owlet. The required aperture, power consumption, and hardware complexity of the traditional array-based spatial sensing techniques make them unsuitable for small and power-constrained IoT devices. Aiming to overcome these fundamental limitations, Owlet explores acoustic microstructures for extracting spatial information. It uses a carefully designed 3D-printed metamaterial structure that covers the microphone. The structure embeds a direction-specific signature in the recorded sounds. Owlet system learns the directional signatures through a one-time in-lab calibration. The system uses an additional microphone as a reference channel and develops techniques that eliminate environmental variation, making the design robust to noises and multipaths in arbitrary locations of operations. Owlet prototype shows 3.6° median error in DoA estimation and 10cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing. The prototype consumes less than 100th of the energy required by a traditional microphone array to achieve similar DoA estimation accuracy. Owlet opens up possibilities of low-power sensing through 3D-printed passive structures.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {255–268},
numpages = {14},
keywords = {spatial sensing, low-power sensing, acoustic metamaterial, IoT},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}

@inproceedings{10.1145/2602928.2603080,
author = {Lytra, Ioanna and Sobernig, Stefan and Tran, Huy and Zdun, Uwe},
title = {A pattern language for service-based platform integration and adaptation},
year = {2012},
isbn = {9781450329439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602928.2603080},
doi = {10.1145/2602928.2603080},
abstract = {Often software systems accommodate one or more software platforms on top of which various applications are developed and executed. Different application areas, such as enterprise resource planning, mobile devices, telecommunications, and so on, require different and specialized platforms. Many of them offer their services using standardized interface technologies to support integration with the applications built on top of them and with other platforms. The diversity of platform technologies and interfaces, however, renders the integration of multiple platforms challenging. In this paper, we discuss design alternatives for tailoring heterogeneous service platforms by studying high-level and low-level architectural design decisions for integrating and for adapting platforms. We survey and organize existing patterns and design decisions in the literature as a pattern language. With this pattern language, we address the various decision categories and interconnections for the service-based integration and the adaptation of applications developed based on software platforms. We apply this pattern language in an industry case study.},
booktitle = {Proceedings of the 17th European Conference on Pattern Languages of Programs},
articleno = {4},
numpages = {27},
keywords = {service-based platform integration, pattern language, design patterns},
location = {Irsee, Germany},
series = {EuroPLoP '12}
}

@article{10.1007/s10270-015-0498-5,
author = {Rodrigues, Taniro and Delicato, Fl\'{a}via C. and Batista, Thais and Pires, Paulo F. and Pirmez, Luci},
title = {An approach based on the domain perspective to develop WSAN applications},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0498-5},
doi = {10.1007/s10270-015-0498-5},
abstract = {As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {949–977},
numpages = {29},
keywords = {WSAN applications, UML profile, Model-driven architecture, Domain-specific language, Code generation, Architecture, Abstraction}
}

@inproceedings{10.1145/949344.949346,
author = {Thomas, Dave and Barry, Brian M.},
title = {Model driven development: the case for domain oriented programming},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949346},
doi = {10.1145/949344.949346},
abstract = {In this paper, we offer an alternative vision for domain driven development (3D). Our approach is model driven and emphasizes the use of generic and specific domain oriented programming (DOP) languages. DOP uses strong specific languages, which directly incorporate domain abstractions, to allow knowledgeable end users to succinctly express their needs in the form of an application computation. Most domain driven development (3D) approaches and techniques are targeted at professional software engineers and computer scientists. We argue that DOP offers a promising alternative. Specifically we are focused on empowering application developers who have extensive domain knowledge as well as sound foundations in their professions, but may not be formally trained in computer science.We provide a brief survey of DOP experiences, which show that many of the best practices such as patterns, refactoring, and pair programming are naturally and ideally practiced in a Model Driven Development (MDD) setting. We compare and contrast our DOP with other popular approaches, most of which are deeply rooted in the OO community.Finally we highlight challenges and opportunities in the design and implementation of such languages.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {2–7},
numpages = {6},
keywords = {programming by professional end users, model driven development, end user programming, domain specific languages, domain driven development},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1016/j.asoc.2017.02.016,
author = {Oladimeji, Muyiwa Olakanmi and Turkey, Mikdam and Dudley, Sandra},
title = {HACH},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.02.016},
doi = {10.1016/j.asoc.2017.02.016},
abstract = {Graphical abstractDisplay Omitted Wireless sensor networks (WSNs) require energy management protocols to efficiently use the energy supply constraints of battery-powered sensors to prolong its network lifetime. This paper proposes a novel Heuristic Algorithm for Clustering Hierarchy (HACH), which sequentially performs selection of inactive nodes and cluster head nodes at every round. Inactive node selection employs a stochastic sleep scheduling mechanism to determine the selection of nodes that can be put into sleep mode without adversely affecting network coverage. Also, the clustering algorithm uses a novel heuristic crossover operator to combine two different solutions to achieve an improved solution that enhances the distribution of cluster head nodes and coordinates energy consumption in WSNs. The proposed algorithm is evaluated via simulation experiments and compared with some existing algorithms. Our protocol shows improved performance in terms of extended lifetime and maintains favourable performances even under different energy heterogeneity settings.},
journal = {Appl. Soft Comput.},
month = jun,
pages = {452–461},
numpages = {10},
keywords = {Wireless sensor networks, Sleep scheduling, Heuristic crossover, Energy heterogeneity, Coverage, Clustering}
}

@inproceedings{10.1145/2541940.2541962,
author = {Zahedi, Seyed Majid and Lee, Benjamin C.},
title = {REF: resource elasticity fairness with sharing incentives for multiprocessors},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541962},
doi = {10.1145/2541940.2541962},
abstract = {With the democratization of cloud and datacenter computing, users increasingly share large hardware platforms. In this setting, architects encounter two challenges: sharing fairly and sharing multiple resources. Drawing on economic game-theory, we rethink fairness in computer architecture. A fair allocation must provide sharing incentives (SI), envy-freeness (EF), and Pareto efficiency (PE).We show that Cobb-Douglas utility functions are well suited to modeling user preferences for cache capacity and memory bandwidth. And we present an allocation mechanism that uses Cobb-Douglas preferences to determine each user's fair share of the hardware. This mechanism provably guarantees SI, EF, and PE, as well as strategy-proofness in the large (SPL). And it does so with modest performance penalties, less than 10% throughput loss, relative to an unfair mechanism.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–160},
numpages = {16},
keywords = {multiprocessor architectures, game theory, fair sharing, economic mechanisms},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@inbook{10.5555/2167873.2167886,
author = {Kalyanakrishnan, Shivaram and Hester, Todd and Quinlan, Michael and Bentor, Yinon and Stone, Peter},
title = {Three humanoid soccer platforms: comparison and synthesis},
year = {2010},
isbn = {3642118755},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this article, we provide an overview of three humanoid soccer platforms currently in use at RoboCup: 3D simulation, the humanoid Standard Platform League (SPL), and the Webots-based simulator released with the SPL. Although these platforms trace different historical roots, today they share the same robot model, the Aldebaran Nao. Consequently, they face a similar set of challenges, primary among which is the need to develop reliable and robust bipedal locomotion. In this paper, we compare and contrast these platforms, drawing on the experiences of our team, UT Austin Villa, in developing agents for each of them. We identify specific roles for these three platforms in advancing the overarching goals of RoboCup.},
booktitle = {RoboCup 2009: Robot Soccer World Cup XIII},
pages = {140–152},
numpages = {13}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {software measures, feature-oriented software development, feature interactions},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.5555/244522.244552,
author = {Cong, Jason and He, Lei},
title = {An efficient approach to simultaneous transistor and interconnect sizing},
year = {1997},
isbn = {0818675977},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we study the simultaneous transistor and interconnect sizing (STIS) problem. We define a class of optimization problems as CH-posynomial programs and reveal a general dominance property for all CH-posynomial programs. We show that the STIS problems under a number of transistor delay models are CH-posynomial programs and propose an efficient and near-optimal STIS algorithm based on the dominance property. When used to solve the simultaneous driver/buffer and wire sizing problem for real designs, it reduces the maximum delay by up to 16.1%, and more significantly, reduces the power consumption by a factor of 1.63X, when compared with the original designs. When used to solve the transistor sizing problem, it achieves a smooth area-delay trade-off. Moreover, the algorithm optimizes a clock net of 367 drivers/buffers and 59304 /spl mu/m-long wire in 120 seconds, and a 32-bit adder with 1026 transistors in 66 seconds on a SPARC-5 workstation.},
booktitle = {Proceedings of the 1996 IEEE/ACM International Conference on Computer-Aided Design},
pages = {181–186},
numpages = {6},
keywords = {wire sizing problem, transistor sizing, transistor and interconnect sizing, driver/buffer, circuit CAD, STIS, CH-posynomial programs},
location = {San Jose, California, USA},
series = {ICCAD '96}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {Systems engineering, Systematic mapping study, SysML}
}

@inproceedings{10.5555/1129601.1129626,
author = {Chen, Tung-Chieh and Chang, Yao-Wen and Lin, Shyh-Chang},
title = {IMF: interconnect-driven multilevel floorplanning for large-scale building-module designs},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper, a new interconnect-driven multilevel floorplanning, called IMF, to handle large-scale building-module designs. Unlike the traditional multilevel framework that adopts the "V-cycle" framework: bottom-up coarsening followed by top-down uncoarsening, in contrast, IMF works in the "/spl Lambda/-cycle" manner: top-down uncoarsening (partitioning) followed by bottom-up coarsening (merging). The top-down partitioning stage iteratively partitions the floorplan region based on mm-cut bipartitioning with exact net-weight modeling to reduce the number of global interconnections and thus the total wirelength. Then, the bottom-up merging stage iteratively applies fixed-outline floorplanning using simulated annealing for all regions and merges two neighboring regions recursively. We also propose an accelerative fixed-outline floorplanning (AFF) to speed up wirelength minimization under the outline constraint. Experimental results show that IMF consistently obtains the best floorplanning results with the smallest wirelength for large-scale building-module designs, compared with all publicly available floorplanners. In particular, IMF scales very well as the circuit size increases. The /spl Lambda/-cycle multilevel framework outperforms the V-cycle one in the optimization of global circuit effects, such as interconnection and crosstalk optimization, since the /spl Lambda/-cycle framework considers the global configuration first and then processes down to local ones level by level and thus the global effects can be handled at earlier stages. The /spl Lambda/-cycle multilevel framework is general and thus can be readily applied to other problems.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {159–164},
numpages = {6},
location = {San Jose, CA},
series = {ICCAD '05}
}

@inproceedings{10.1145/974044.974052,
author = {Grassi, Vincenzo and Mirandola, Raffaela},
title = {Towards automatic compositional performance analysis of component-based systems},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974052},
doi = {10.1145/974044.974052},
abstract = {To make predictive analysis an effective tool for component-based software development (CBSD), it should be, as much as possible: compositional, to allow the re-use of known information about the properties of existing components, and automatic, to keep the pace with the timeliness and cost-effectiveness promises of CBSD. Towards this end, focusing on the predictive analysis of performance properties, we define a simple language, based on an abstract component model, to describe a component assembly, outlining which information should be included in it to support compositional performance analysis. Moreover, we outline a mapping of the constructs of the proposed language to elements of the RT-UML Profile, to give them a precisely defined "performance semantics", and to get a starting point for the exploitation of proposed UML-based methodologies and algorithms for performance analysis.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {59–63},
numpages = {5},
keywords = {software component, predictive analysis, performance, component specification},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1016/j.ijinfomgt.2015.09.008,
author = {Chang, Victor and Ramachandran, Muthu and Yao, Yulin and Kuo, Yen-Hung and Li, Chung-Sheng},
title = {A resiliency framework for an enterprise cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {1},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2015.09.008},
doi = {10.1016/j.ijinfomgt.2015.09.008},
abstract = {We have presented a resilient framework for an enterprise cloud.We have developed an architecture with four major services to demonstrate resiliency, where the cloud computing adoption framework (CCAF) takes the center role to blend other services.We explain how our work is relevant to business resiliency.We have the support from a large scale survey to ensure that our design and service can meet the large number of user requirements. This paper presents a systematic approach to develop a resilient software system which can be developed as emerging services and analytics for resiliency. While using the resiliency as a good example for enterprise cloud security, all resilient characteristics should be blended together to produce greater impacts. A framework, cloud computing adoption framework (CCAF), is presented in details. CCAF has four major types of emerging services and each one has been explained in details with regard to the individual function and how each one can be integrated. CCAF is an architectural framework that blends software resilience, service components and guidelines together and provides real case studies to produce greater impacts to the organizations adopting cloud computing and security. CCAF provides business alignments and provides agility, efficiency and integration for business competitive edge. In order to validate user requirements and system designs, a large scale survey has been conducted with detailed analysis provided for each major question. We present our discussion and conclude that the use of CCAF framework can illustrate software resilience and security improvement for enterprise security. CCAF framework itself is validated as an emerging service for enterprise cloud computing with analytics showing survey analysis.},
journal = {Int. J. Inf. Manag.},
month = feb,
pages = {155–166},
numpages = {12},
keywords = {Resilient software for Enterprise Cloud, Cloud security and software engineering best practice, Cloud computing Adoption Framework (CCAF), - Software resiliency}
}

@article{10.1145/2658993,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A Scheduling Framework for Spatial Architectures Across Multiple Constraint-Solving Theories},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/2658993},
doi = {10.1145/2658993},
abstract = {Spatial architectures provide energy-efficient computation but require effective scheduling algorithms. Existing heuristic-based approaches offer low compiler/architect productivity, little optimality insight, and low architectural portability.We seek to develop a spatial-scheduling framework by utilizing constraint-solving theories and find that architecture primitives and scheduler responsibilities can be related through five abstractions: computation placement, data routing, event timing, resource utilization, and the optimization objective. We encode these responsibilities as 20 mathematical constraints, using SMT and ILP, and create schedulers for the TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using constraint solving is implementable, is practical, and can outperform specialized schedulers.},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
articleno = {2},
numpages = {30},
keywords = {Satisfiability Modulo Theories, Spatial architectures, integer linear programming, spatial architecture scheduling}
}

@article{10.1016/j.future.2019.01.042,
author = {Dawaliby, Samir and Bradai, Abbas and Pousset, Yannis},
title = {Adaptive dynamic network slicing in LoRa networks},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {98},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.01.042},
doi = {10.1016/j.future.2019.01.042},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {697–707},
numpages = {11},
keywords = {Quality of service (QoS), Resource allocation, Network slicing, LoRa, Wireless networks, Internet of Things (IoT)}
}

@article{10.1145/3351239,
author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
title = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351239},
doi = {10.1145/3351239},
abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {81},
numpages = {24},
keywords = {wearable devices, echo, ear canal, biometric, authentication, Acoustic}
}

@article{10.1016/j.specom.2017.02.002,
author = {Wu, Liang and Xiao, Ke and Wang, Supin and Wan, Mingxi},
title = {Vocal efficiency of electrolaryngeal speech production},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.02.002},
doi = {10.1016/j.specom.2017.02.002},
abstract = {Vocal efficiency of EL speech production was defined and measured.The actual utilization efficiency of the battery energy was no more than 0.1%.The energy transfer process was divided into three successive stages.The non-linear transducer and the neck tissue features were main reasons for power losses. From the perspective of efficiency, this article studied the energy transfer and conversion in the process of electrolaryngeal (EL) speech production. An overall vocal efficiency of EL speech production was defined as the ratio of the acoustic power of the EL speech to the electric power supplied by the battery. The measurements of a commercial EL showed that the actual utilization efficiency of the battery energy was no more than 0.1%. The energy transfer process was divided into three successive stages. The corresponding efficiencies of these stages were defined and estimated to analyze potential power losses and possible impact of two factors (EL cap and vowel) on the vocal efficiency. It was concluded that the non-linear transducer of the EL device and the physiological features of the neck tissue were the main reasons for the high power losses and low vocal efficiency. Furthermore, both EL cap and phonation vowel showed significant effects on the EL vocal efficiency. Thus, improvement of EL linear vibrator and compatible cap will be beneficial to raising the vocal efficiency and improving the EL speech quality.},
journal = {Speech Commun.},
month = may,
pages = {17–24},
numpages = {8},
keywords = {Vocal efficiency, Energy transfer, Electrolaryngeal speech}
}

@inproceedings{10.5555/2362793.2362812,
author = {Borders, Kevin and Springer, Jonathan and Burnside, Matthew},
title = {Chimera: a declarative language for streaming network traffic analysis},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Intrusion detection systems play a vital role in network security. Central to these systems is the language used to express policies. Ideally, this language should be powerful, implementation-agnostic, and cross-platform. Unfortunately, today's popular intrusion detection systems fall short of this goal. Each has their own policy language in which expressing complicated logic requires implementation-specific code. Database systems have adapted SQL to handle streaming data, but have yet to achieve the efficiency and flexibility required for complex intrusion detection tasks.In this paper, we introduce Chimera, a declarative query language for network traffic processing that bridges the gap between powerful intrusion detection systems and a simple, platform-independent SQL syntax. Chimera extends streaming SQL languages to better handle network traffic by adding structured data types, first-class functions, and dynamic window boundaries. We show how these constructs can be applied to real-world scenarios, such as side-jacking detection and DNS feature extraction. Finally, we describe the implementation and evaluation of a compiler that translates Chimera queries into low-level code for the Bro event language.},
booktitle = {Proceedings of the 21st USENIX Conference on Security Symposium},
pages = {19},
numpages = {1},
location = {Bellevue, WA},
series = {Security'12}
}

@inproceedings{10.1007/978-3-642-31762-0_13,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {A formal model of user-defined resources in resource-restricted deployment scenarios},
year = {2011},
isbn = {9783642317613},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31762-0_13},
doi = {10.1007/978-3-642-31762-0_13},
abstract = {Software today is often developed for deployment on varying architectures. In order to model and analyze the consequences of such deployment choices at an early stage in software development, it seems desirable to capture aspects of low-level deployment concerns in high-level models. In this paper, we propose an integration of a generic cost model for resource consumption with deployment components in Timed ABS, an abstract behavioral specification language for executable object-oriented models. The actual cost model may be user-defined and specified by means of annotations in the executable Timed ABS model, and can be used to capture specific resource requirements such as processing capacity or memory usage. Architectural variations are specified by resource-restricted deployment scenarios with different capacities. For this purpose, the models have deployment components which are parametric in their assigned resources. The approach is demonstrated on an example of multimedia processing servers with a user-defined cost model for memory usage. We use our simulation tool to analyze deadline misses for given usage and deployment scenarios.},
booktitle = {Proceedings of the 2011 International Conference on Formal Verification of Object-Oriented Software},
pages = {196–213},
numpages = {18},
location = {Turin, Italy},
series = {FoVeOOS'11}
}

@article{10.1155/2013/683615,
author = {Chattopadhyay, Anupam},
title = {Ingredients of adaptability: a survey of reconfigurable processors},
year = {2013},
issue_date = {January 2013},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2013},
issn = {1065-514X},
url = {https://doi.org/10.1155/2013/683615},
doi = {10.1155/2013/683615},
abstract = {For a design to survive unforeseen physical effects like aging, temperature variation, and/or emergence of new application standards, adaptability needs to be supported. Adaptability, in its complete strength, is present in reconfigurable processors, which makes it an important IP in modern System-on-Chips (SoCs). Reconfigurable processors have risen to prominence as a dominant computing platform across embedded, general-purpose, and high-performance application domains during the last decade. Significant advances have been made in many areas such as, identifying the advantages of reconfigurable platforms, their modeling, implementation flow and finally towards early commercial acceptance. This paper reviews these progresses from various perspectives with particular emphasis on fundamental challenges and their solutions. Empowered with the analysis of past, the future research roadmap is proposed.},
journal = {VLSI Des.},
month = jan,
articleno = {10},
numpages = {1}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@article{10.1016/j.jnca.2017.12.001,
author = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
title = {Distributed data stream processing and edge computing},
year = {2018},
issue_date = {February 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2017.12.001},
doi = {10.1016/j.jnca.2017.12.001},
abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them. HighlightsThe paper surveys state of the art on stream processing engines and mechanisms.The work describes how existing solutions exploit resource elasticity features of cloud computing in stream processing.It presents a gap analysis and future directions on stream processing on heterogeneous environments.},
journal = {J. Netw. Comput. Appl.},
month = feb,
pages = {1–17},
numpages = {17},
keywords = {Stream processing, Resource elasticity, Cloud computing, Big Data}
}

@article{10.1016/j.jss.2013.02.061,
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
title = {An orchestrated survey of methodologies for automated software test case generation},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.02.061},
doi = {10.1016/j.jss.2013.02.061},
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1978–2001},
numpages = {24},
keywords = {Test case generation, Test automation, Symbolic execution, Software testing, Search-based software testing, Orchestrated survey, Model-based testing, Combinatorial testing, Adaptive random testing}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@article{10.1145/383845.383863,
author = {Coady, Yvonne and Kiczales, Gregor and Feeley, Mike and Hutchinson, Norm and Ong, Joon Suan},
title = {Structuring operating system aspects: using AOP to improve OS structure modularity},
year = {2001},
issue_date = {Oct. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/383845.383863},
doi = {10.1145/383845.383863},
journal = {Commun. ACM},
month = oct,
pages = {79–82},
numpages = {4}
}

@inproceedings{10.5555/3290281.3290302,
author = {Kalra, Sumit and T, Prabhakar},
title = {Implementation patterns for multi-tenancy},
year = {2017},
isbn = {9781941652060},
publisher = {The Hillside Group},
address = {USA},
abstract = {Recently multi-tenant applications for SaaS in cloud computing are on rise. These applications increase the degree of resource sharing among tenants with various functional and non-functional requirements. However, often it results in higher design complexity. In this work, we discuss various design patterns to build these applications with efficient tenant management. We divided these patterns in the three categories. The categorization is based on their applicability to design, development, and runtime phases during software development life cycle. These patterns make an application tenant aware and enable multi-tenancy without adding much overhead and complexity in its design.},
booktitle = {Proceedings of the 24th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {16},
keywords = {tenant operation and management, multi-tenant},
location = {Vancouver, British Columbia, Canada},
series = {PLoP '17}
}

@article{10.1145/2339118.2339120,
author = {Abd-El-Malek, Michael and Wachs, Matthew and Cipar, James and Sanghi, Karan and Ganger, Gregory R. and Gibson, Garth A. and Reiter, Michael K.},
title = {File system virtual appliances: Portable file system implementations},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/2339118.2339120},
doi = {10.1145/2339118.2339120},
abstract = {File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.},
journal = {ACM Trans. Storage},
month = sep,
articleno = {9},
numpages = {26},
keywords = {virtual machines, file systems, Operating systems}
}

@article{10.1007/s11235-014-9886-3,
author = {Wan, Liangtian and Han, Guangjie and Rodrigues, Joel J. and Si, Weijian and Feng, Naixing},
title = {An energy efficient DOA estimation algorithm for uncorrelated and coherent signals in virtual MIMO systems},
year = {2015},
issue_date = {May       2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {59},
number = {1},
issn = {1018-4864},
url = {https://doi.org/10.1007/s11235-014-9886-3},
doi = {10.1007/s11235-014-9886-3},
abstract = {The multiple input and multiple output (MIMO) and smart antenna (SA) technique have been widely accepted as promising schemes to improve the spectrum efficiency and coverage of mobile communication systems. The definition of direction-of-arrival (DOA) estimation is that multiple directions of incident signals can be estimated simultaneously by some algorithms using the received data. The conventional DOA estimation of user equipments (UEs) is one by one, which is named as sequential scheme. The Virtual MIMO (VMIMO) scheme is that the base station (BS) estimates the DOAs of UEs in a parallel way, which adopts the SA simultaneously. Obviously, when the power is fixed, the VMIMO scheme is much more energy efficient than the sequential scheme. In VMIMO scheme, a set of UEs are grouped together to simultaneously communicate with the BS on a given resource block. Then the BS using multiple antennas can estimate the 2D-DOA of the UEs in the group simultaneously. Based on VMIMO system, the 2D-DOA estimation algorithm for uncorrelated and coherent signals is proposed in this paper. The special structure of mutual coupling matrix (MCM) of uniform linear array (ULA) is applied to eliminate the effect of mutual coupling. The 2D-DOA of uncorrelated signals can be estimated by DOA-matrix method. The parameter pairing between azimuth and elevation is accomplished. Then these estimations are utilized to get the mutual coupling coefficients. Based on spatial smoothing and DOA matrix method, the 2D-DOA of coherent signals can be estimated. The Cramer---Rao lower bound is derived at last. Simulation results demonstrate the effectiveness and performance of the proposed algorithm.},
journal = {Telecommun. Syst.},
month = may,
pages = {93–110},
numpages = {18},
keywords = {VMIMO system, Mutual coupling, Multipath, DOA estimation}
}

@inproceedings{10.1145/3452021.3458317,
author = {Alur, Rajeev and Hilliard, Phillip and Ives, Zachary G. and Kallas, Konstantinos and Mamouras, Konstantinos and Niksic, Filip and Stanford, Caleb and Tannen, Val and Xue, Anton},
title = {Synchronization Schemas},
year = {2021},
isbn = {9781450383813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452021.3458317},
doi = {10.1145/3452021.3458317},
abstract = {We present a type-theoretic framework for data stream processing for real-time decision making, where the desired computation involves a mix of sequential computation, such as smoothing and detection of peaks and surges, and naturally parallel computation, such as relational operations, key-based partitioning, and map-reduce. Our framework unifies sequential (ordered) and relational (unordered) data models. In particular, we define synchronization schemas as types, and series-parallel streams (SPS) as objects of these types. A synchronization schema imposes a hierarchical structure over relational types that succinctly captures ordering and synchronization requirements among different kinds of data items. Series-parallel streams naturally model objects such as relations, sequences, sequences of relations, sets of streams indexed by key values, time-based and event-based windows, and more complex structures obtained by nesting of these. We introduce series-parallel stream transformers (SPST) as a domain-specific language for modular specification of deterministic transformations over such streams. SPSTs provably specify only monotonic transformations allowing streamability, have a modular structure that can be exploited for correct parallel implementation, and are composable allowing specification of complex queries as a pipeline of transformations.},
booktitle = {Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {1–18},
numpages = {18},
keywords = {synchronization schemas, stream processing, series-parallel partial orders, relations, parallelism, streams, database query languages},
location = {Virtual Event, China},
series = {PODS'21}
}

@article{10.1007/s11276-016-1404-y,
author = {Doosti-Aref, Abdollah and Ebrahimzadeh, Ataollah},
title = {Efficient cooperative multicarrier underwater acoustic communication over the Persian Gulf channel},
year = {2018},
issue_date = {May       2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-016-1404-y},
doi = {10.1007/s11276-016-1404-y},
abstract = {Real-time and reliable transmission with high data rate is one of the most crucial design issues in underwater acoustic (UWA) communications. This paper is part of an operational project in the Persian Gulf which is known as a shallow water channel worldwide. Building on the promising combination of cooperative and multicarrier techniques, we first address the design of an efficient UWA communication system over the Persian Gulf channel. We assume sparse and frequency-selective Rician fading, non-white correlated Gaussian ambient noise, and non-uniform Doppler distortion among subcarriers. Based on the extensive field measurements, we adopt a comprehensive channel model including modified sound speed profile, modified absorption coefficient, reflections from the surface and bottom of the sea, and modified ambient noise model. In our work, carrier frequency is efficiently determined based on the system and environmental parameters. In addition, a simple criterion for Doppler scale calculation is proposed. Moreover, based on the approximate auto-correlation function of the ambient noise, whitening filter is utilized at the receiver. In many applications of underwater acoustic sensors network (UW-ASN), nodes may not be big enough to have more than one antenna. Therefore, to achieve spatial diversity, cooperation between nodes can be a proper alternate method. In another part of this research, toward a proper approach that each node operates under full-duplex mode, we address a cooperative virtually-aided transmission scenario which is called cooperative multiple input single output. Simulation results demonstrate that our proposed models can significantly improve the bit error rate performance of UW-ASN in the Persian Gulf channel.},
journal = {Wirel. Netw.},
month = may,
pages = {1265–1278},
numpages = {14},
keywords = {Underwater acoustic channel, Persian Gulf, OFDM, Cooperative multicarrier transmission, CMISO}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Software customization, Software configuration, Markov decision processes, Genetic algorithms}
}

@article{10.1007/s10270-016-0575-4,
author = {Voelter, Markus and Kolb, Bernd and Szab\'{o}, Tam\'{a}s and Ratiu, Daniel and Deursen, Arie},
title = {Lessons learned from developing mbeddr: a case study in language engineering with MPS},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0575-4},
doi = {10.1007/s10270-016-0575-4},
abstract = {Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations--textual, tabular, symbolic and graphical--and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr's development has spanned 10 person-years so far, and the tool is used in practice and continues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around five research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {585–630},
numpages = {46},
keywords = {Languages, Language workbenches, Language extension, Language engineering, Experimentation, Domain-specific language, Case study}
}

@inproceedings{10.5555/787260.787727,
author = {Turgis, S. and Daga, J. M. and Portal, J. M. and Auvergne, D.},
title = {Internal power modelling and minimization in CMOS inverters},
year = {1997},
isbn = {0818677864},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper an alternative for the internal (short-circuit and overshoot) power dissipation estimation of CMOS structures. Using a first order macro-modelling, we consider submicronic additional effects such as: input slow dependency of short-circuit currents and input-to-output coupling. Considering an equivalent capacitance concept we directly compare the different power components. Validations are presented by comparing simulated values (HSPICE level 6, foundry model 0.7 /spl mu/m) to calculated ones. Application to buffer design enlightens the importance of the internal power component and clearly shows that common sizing alternatives for power and delay minimization can be considered.},
booktitle = {Proceedings of the 1997 European Conference on Design and Test},
pages = {603},
keywords = {short-circuit current, overshoot, minimization, macromodel, internal power modelling, input-to-output coupling, foundry model, equivalent capacitance, buffer design, HSPICE simulation, CMOS logic circuits, CMOS inverter},
series = {EDTC '97}
}

@article{10.1504/IJWMC.2014.063054,
author = {Siala, Fatma and Ghedira, Khaled},
title = {How to select dynamically a QoS-driven composite web service by a multi-agent system using CBR method},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2014.063054},
doi = {10.1504/IJWMC.2014.063054},
abstract = {Service-oriented architecture permits the composition of web services provided with different Quality of Service QoS levels. In a given composition, finding the set of services that optimises some QoS attributes under its constraints is a problem that needs to be solved. Our aim is to propose an intelligent approach to the selection of a Composite Web Service CWS based on QoS. This paper reports the authors' recent research on addressing the issue. An overview on the previously proposed approaches is presented. These approaches correspond to several improvements of an existing multi-agent one, which is well-cited in the specialised literature. Each framework, implemented on JADE Java Agent Development framework, improves another one in terms of CPU time and/or QoS score, to reach a new agent-based and scalable framework. The last framework utilises the agents' ability of negotiation, interaction and cooperation in order to facilitate the selection of composite web services. By using CBR method, the agents can memorise QoS scores and availability. The improvements are related not only to the CPU time but also to the Composite QoS CQoS value, while operating in a dynamic environment and taking into account user preferences.},
journal = {Int. J. Wire. Mob. Comput.},
month = jul,
pages = {327–347},
numpages = {21}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {verified software grand challenge, verification, tools, specification languages, programming languages, program generation, correctness by construction, annotations},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@article{10.1007/s10515-012-0117-4,
author = {N\"{o}hrer, Alexander and Egyed, Alexander},
title = {C2O configurator: a tool for guided decision-making},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0117-4},
doi = {10.1007/s10515-012-0117-4},
abstract = {Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, it is often neither obvious which decisions have the most significant impact nor which decisions might ultimately conflict. Unfortunately, the current state-of-the-art provides little support for dealing with such situations. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times, and living with conflicts may be preferable over forcing the user to fix them right away--particularly because fixing conflicts becomes easier as more is known about a user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order--with and without the presence of inconsistencies. While giving users those freedoms, it still supports and guides them by (i) rearranging the order of questions according to their potential to minimize user input, (ii) providing guidance to avoid follow-on conflicts, and (iii) supporting users in fixing conflicts at a later time.},
journal = {Automated Software Engg.},
month = jun,
pages = {265–296},
numpages = {32}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@inbook{10.5555/2340883.2340909,
author = {Minsky, Naftaly H.},
title = {Decentralized governance of distributed systems via interaction control},
year = {2012},
isbn = {9783642294136},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper introduces an abstract reference model, called &lt;em&gt;interaction control&lt;/em&gt; (IC), for the governance of large and heterogeneous distributed systems. This model goes well beyond conventional access control, along a number of dimensions. In particular, the IC model has the following characteristics: (1) it is inherently decentralized, and thus scalable even for a wide range of stateful policies; (2) it is very general, and not biased toward any particular type of policies; thus providing a significant realization of the age-old principle of &lt;em&gt;separation of policy from mechanism&lt;/em&gt; ; and (3) it enables flexible, composition-free, interoperability between different policies.The IC model, which is an abstraction of a mechanism called law-governed interaction (LGI), has been designed as a minimalist reference model that can be reified into a whole family of potential control mechanisms that may support different types of communication, with different performance requirements and for different application domains.},
booktitle = {Logic Programs, Norms and Action: Essays in Honor of Marek J. Sergot on the Occasion of His 60th Birthday},
pages = {374–400},
numpages = {27}
}

@inproceedings{10.1145/2095536.2095554,
author = {Siala, Fatma and Lajmi, Soufiene and Ghedira, Khaled},
title = {Multi-agent selection of multiple composite web services based on CBR method and driven by QoS},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095554},
doi = {10.1145/2095536.2095554},
abstract = {Many companies aim to use Web services to integrate heterogeneous or remote applications in SOA (Service Oriented Architecture) contexts. Indeed, one of the main assets of service-orientation is a composition to develop higher level services, so-called composite services, by re-using existing services. Since many available Web services provide overlapping or identical functionality, with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. However, for a composition, we can have different combinations and execution paths. Particularly, a composite service can generate different schemes that give various QoS scores.This paper presents a framework which deals with the selection of composite Web services on the base of Multi-Agents negotiation. The objective of these agents is to find out the best Composite QoS (CQoS) based on Web services availability. This scalable framework supports different combinations and execution paths using CBR technique. The proposed Multi-Agents framework is compared to an existing approach in terms of execution time. Experiments have demonstrated that our framework provide reliable results in comparison with the existing approach.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {90–97},
numpages = {8},
keywords = {web service, multi-agent system, execution paths, contract-net protocol, composition, QoS, CBR technique},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inproceedings{10.1145/2345396.2345586,
author = {Natarajan, Jaya Sudha and Sevukamoorthy, Lakshmi},
title = {Auto-clever fuzzy (ACF) based intelligent system for monitoring and controlling the hydrocarbons -air toxics emitted by the vehicle motors},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345586},
doi = {10.1145/2345396.2345586},
abstract = {Numerous methods have been designed for choosing the operating point of an IC Engine in a parallel hybrid electric vehicle. This paper is based on choosing a service point about an optimal operating point, based on the road load, the Battery State of Charge (SOC), and the optimal operating point of an IC Engine. An optimal service point (ideal case) is calculated based on the minimization of a criterion, of which fuel and emissions are challenging parameters. Based on a set of weights, the relative importance of fuel economy and emissions is dynamically chosen, and an optimal torque that can be requested from the IC Engine is calculated. Based on the battery SOC constraints and the road load (driver's request), the actual output torque of the IC Engine is computed. The remaining torque (at that speed) required to meet the road load is provided by a buffer to the Electric Motor (EM). The buffer may produce either positive or negative torque. This process would highly be effective for saving the environment from pollutions caused by motor vehicles.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {1187–1192},
numpages = {6},
keywords = {machine intelligence, fuzzy c- means, SOC},
location = {Chennai, India},
series = {ICACCI '12}
}

@article{10.1109/90.251895,
author = {Partridge, Craig and Pink, Stephen},
title = {A faster UDP},
year = {1993},
issue_date = {Aug. 1993},
publisher = {IEEE Press},
volume = {1},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/90.251895},
doi = {10.1109/90.251895},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {429–440},
numpages = {12}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Tool, Software architecture, Quality attribute, Ontology, Model-driven development, Evaluation}
}

@inproceedings{10.5555/2429759.2430133,
author = {Rabe, Markus and Horvath, Adrienn and Spieckermann, Sven and Fechteler, Till},
title = {An approach for increasing flexibility in green supply chains driven by simulation},
year = {2012},
publisher = {Winter Simulation Conference},
abstract = {Flexibility is a relevant topic in the field of green supply chains (GSC), as disturbances lead to additional transport and storage, frequently aggravated by energy-consuming air conditioning requirements. This paper discusses how simulation can support to establish flexible GSCs with specific focus on decreasing CO2 and energy consumption. For this purpose, the term flexibility is structured into categories, and methodological approaches driven by simulation in supply chains are studied. Flexibility requirements in the context of a GSC are analyzed and potential support derived for increasing this flexibility, gained by a join of simulation techniques, data models and morphological characteristics of flexibility. An approach for systematic flexibility analysis is presented on the grounds of a data mart that represents both internal and external factors influencing GSC scenarios.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {277},
numpages = {12},
location = {Berlin, Germany},
series = {WSC '12}
}

@inproceedings{10.5555/2675983.2676315,
author = {Gutenschwager, Kai and Rabe, Markus and Sari, Mehmet Umut and Fechteler, Till},
title = {A data model for carbon footprint simulation in consumer goods supply chains},
year = {2013},
isbn = {9781479920778},
publisher = {IEEE Press},
abstract = {CO2 efficiency is currently a popular topic in supply chain management. Most approaches are based on the Life Cycle Assessment (LCA) which usually exploits data from a static database. This approach is effective when estimating the carbon footprint of products or groups of products in general. Simulation has been a proper method for metering the effectiveness of logistics systems, and could thus be expected to also support the analysis of CO2 efficiency in supply chains (SC) when combined with an LCA database. However, research shows that this combination does not deliver reliable results when the target of the study is improvement of the logistics in the SC. The paper demonstrates the shortcomings of the LCA-analogous approach and proposes a data model that enables discrete event simulation of SC logistics including its impact on the carbon footprint that is under development in the e-SAVE joint project funded by the European Commission.},
booktitle = {Proceedings of the 2013 Winter Simulation Conference: Simulation: Making Decisions in a Complex World},
pages = {2677–2688},
numpages = {12},
location = {Washington, D.C.},
series = {WSC '13}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/1950413.1950448,
author = {Kenter, Tobias and Plessl, Christian and Platzner, Marco and Kauschke, Michael},
title = {Performance estimation framework for automated exploration of CPU-accelerator architectures},
year = {2011},
isbn = {9781450305549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950413.1950448},
doi = {10.1145/1950413.1950448},
abstract = {In this paper we present a fast and fully automated approach for studying the design space when interfacing reconfigurable accelerators with a CPU. Our challenge is, that a reasonable evaluation of architecture parameters requires a hardware/software partitioning that makes best use of each given architecture configuration. Therefore we developed a framework based on the LLVM infrastructure that performs this partitioning with high-level estimation of the runtime on the target architecture utilizing profiling information and code analysis. By making use of program characteristics also during the partitioning process, we improve previous results for various benchmarks and especially for growing interface latencies between CPU and accelerator.},
booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {177–180},
numpages = {4},
keywords = {performance estimation, llvm, hardware/software partitioning, design space exploration},
location = {Monterey, CA, USA},
series = {FPGA '11}
}

@inproceedings{10.5555/1765571.1765583,
author = {Liu, Shih-Hsi and Bryant, Barrett R. and Auguston, Mikhail and Gray, Jeff and Raje, Rajeev and Tuceryan, Mihran},
title = {A component-based approach for constructing high-confidence distributed real-time and embedded systems},
year = {2005},
isbn = {9783540711551},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In applying Component-Based Software Engineering (CBSE) techniques to the domain of Distributed Real-time and Embedded (DRE) Systems, there are five critical challenges: 1) discovery of relevant components and resources, 2) specification and modeling of components, 3) exploration and elimination of design assembly options, 4) automated generation of heterogeneous component bridges, and 5) validation of context-related embedded systems. To address these challenges, this paper introduces four core techniques to facilitate high-confidence DRE system construction from components: 1) A component and resource discovery technique promotes component searching based on rich and precise descriptions of components and context; 2) A timed colored Petri Net-based modeling toolkit enables design and analysis on DRE systems, as well as reduces unnecessary later work by eliminating infeasible design options; 3) A formal specification language describes all specifications consistently and automatically generates component bridges for seamless system integration; and 4) A grammar-based formalism specifies context behaviors and validates integrated systems using sufficient context-related test cases. The success of these ongoing techniques may not only accelerate the software development pace and reduce unnecessary development cost, but also facilitate high-confidence DRE system construction using different formalisms over the entire software life-cycle.},
booktitle = {Proceedings of the 12th Monterey Conference on Reliable Systems on Unreliable Networked Platforms},
pages = {225–247},
numpages = {23},
location = {Laguna Beach, CA, 2005}
}

@article{10.1504/IJAIP.2009.026765,
author = {Koshizen, Takamasa and Kon, Motohri and Raynolds, Carson and Aihara, Kazuyuki},
title = {Habituation detection with Allen-Cahn boundary generation},
year = {2009},
issue_date = {June 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {1},
number = {4},
issn = {1755-0386},
url = {https://doi.org/10.1504/IJAIP.2009.026765},
doi = {10.1504/IJAIP.2009.026765},
abstract = {We describe a new habituation system that makes use of Partial Differential Equations (PDEs) to deal with time-inconsistent patterns. With respect to our detection system, the Allen-Cahn (AC) equation is used to model a boundary which evolves over time. As a result, the AC could outperform the traditional techniques even real-world dataset. Thus, it leads to a boundary computation for robust detection system.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jun,
pages = {463–487},
numpages = {25},
keywords = {user modelling, time inconsistency, robust detection systems, pattern categorisation, partial differential equations, habituation, boundary growth, PDEs, Allen-Cahn equation}
}

@article{10.5555/1046430.1046440,
author = {Yu, Yang and Prasanna, Viktor K.},
title = {Energy-balanced task allocation for collaborative processing in wireless sensor networks},
year = {2005},
issue_date = {February 2005},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1–2},
issn = {1383-469X},
abstract = {We propose an energy-balanced allocation of a real-time application onto a single-hop cluster of homogeneous sensor nodes connected with multiple wireless channels. An epoch-based application consisting of a set of communicating tasks is considered. Each sensor node is equipped with discrete dynamic voltage scaling (DVS). The time and energy costs of both computation and communication activities are considered. We propose both an Integer Linear Programming (ILP) formulation and a polynomial time 3-phase heuristic. Our simulation results show that for small scale problems (with ≤ 10 tasks), up to 5x lifetime improvement is achieved by the ILP-based approach, compared with the baseline where no DVS is used. Also, the 3-phase heuristic achieves up to 63% of the system lifetime obtained by the ILP-based approach. For large scale problems (with 60-100 tasks), up to 3.5x lifetime improvement can be achieved by the 3-phase heuristic. We also incorporate techniques for exploring the energy-latency tradeoffs of communication activities (such as modulation scaling), which leads to 10x lifetime improvement in our simulations. Simulations were further conducted for two real world problems - LU factorization and Fast Fourier Transformation (FFT). Compared with the baseline where neither DVS nor modulation scaling is used, we observed up to 8x lifetime improvement for the LU factorization algorithm and up to 9x improvement for FFT.},
journal = {Mob. Netw. Appl.},
month = feb,
pages = {115–131},
numpages = {17},
keywords = {single-hop wireless networks, sensor networks, energy saving, ILP}
}

@article{10.1016/j.cor.2012.06.014,
author = {Garroppo, Rosario G. and Giordano, Stefano and Nencioni, Gianfranco and Scutell\`{a}, Maria Grazia},
title = {Mixed Integer Non-Linear Programming models for Green Network Design},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2012.06.014},
doi = {10.1016/j.cor.2012.06.014},
abstract = {The paper focuses on Network Power Management in telecommunication infrastructures. Specifically, the paper describes four energy aware network design problems, with the related mathematical models, for reducing the power consumption of the current and future Internet. Each problem is based on a different characterization and power awareness of the network devices, leading to either Mixed Integer Linear Programming or Mixed Integer Non-Linear Programming models. We have assessed the effectiveness of the proposed approaches under different real core network topology scenarios by evaluating the impact of several network parameters. To the best of our knowledge, this is the first work that deeply investigates the behavior of a pool of diverse Network Power Management approaches, including the first Mixed Integer Non-Linear Programming model for the Power Aware Network Design with Bundled Links.},
journal = {Comput. Oper. Res.},
month = jan,
pages = {273–281},
numpages = {9},
keywords = {Computational analysis, MILP models, MINLP models, Power aware problems, Routing and design}
}

@article{10.1016/j.jpdc.2019.09.005,
author = {Amah, Tekenate E. and Kamat, Maznah and Abu Bakar, Kamalrulnizam and Moreira, Waldir and Oliveira, Antonio and Batista, Marcos A.},
title = {Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.09.005},
doi = {10.1016/j.jpdc.2019.09.005},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {21–55},
numpages = {35},
keywords = {Opportunistic networks, Contact information overhead, Sensed Data Collection, Wireless sensors, Smart City}
}

@inproceedings{10.5555/1411620.1411666,
author = {Seiciu, Petre Lucian and Filipoiu, Ioan Dan and Laurian, Tiberiu},
title = {Theoretical aspects related to the design of a mechatronic system for recovery of the disabled persons},
year = {2008},
isbn = {9789606766770},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Human walking is theoretically explained by two prevailing, but contradictory theories. The first theory called "The Six Determinants Of Gait" (SDG) aims to minimize the energetic cost of locomotion by reducing the vertical displacement of the body center of mass (COM). The second theory called "The Inverted Pendulum Analogy" (IPA) suggests that walking is a movement combination of two pendulums: the inverted pendulum for the stance leg and a direct pendulum for the swing leg. Therefore, results that COM describes a quasi-sinusoid curve. The main evidence against SDG theory is that a flattened COM trajectory increases muscle work and force requirements. The IPA theory analysis shows that it predicts no work or force requirements. These shortcomings may be resolved through a new theory: "The Dynamic Walking Theory" DWT. These theories describe more or less appropriate walking of healthy persons. The paper presents some theoretical aspects to be considered at the design of a mechatronic system for the locomotory disabled persons.},
booktitle = {Proceedings of the 9th WSEAS International Conference on International Conference on Automation and Information},
pages = {230–235},
numpages = {6},
keywords = {locomotory disabled persons, mechatronic system, rehabilitation},
location = {Bucharest, Romania},
series = {ICAI'08}
}

@article{10.1016/j.automatica.2012.06.027,
author = {Zhang, Wen-An and Feng, Gang and Yu, Li},
title = {Multi-rate distributed fusion estimation for sensor networks with packet losses},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {48},
number = {9},
issn = {0005-1098},
url = {https://doi.org/10.1016/j.automatica.2012.06.027},
doi = {10.1016/j.automatica.2012.06.027},
abstract = {This paper presents a distributed fusion estimation method for estimating states of a dynamical process observed by wireless sensor networks (WSNs) with random packet losses. It is assumed that the dynamical process is not changing too rapidly, and a multi-rate scheme by which the sensors estimate states at a faster time scale and exchange information with neighbors at a slower time scale is proposed to reduce communication costs. The estimation is performed by taking into account the random packet losses in two stages. At the first stage, every sensor in the WSN collects measurements from its neighbors to generate a local estimate, then local estimates in the neighbors are further collected at the second stage to form a fused estimate to improve estimation performance and reduce disagreements among local estimates at different sensors. Local optimal linear estimators are designed by using the orthogonal projection principle, and the fusion estimators are designed by using a fusion rule weighted by matrices in the linear minimum variance sense. Simulations of a target tracking system are given to show that the time scale of information exchange among sensors can be slower while still maintaining satisfactory estimation performance by using the developed estimation method.},
journal = {Automatica},
month = sep,
pages = {2016–2028},
numpages = {13},
keywords = {Distributed estimation, Information fusion, Kalman filtering, Packet losses, Wireless sensor networks}
}

@article{10.1016/j.comnet.2012.03.009,
author = {Cuomo, Francesca and Cianfrani, Antonio and Polverini, Marco and Mangione, Daniele},
title = {Network pruning for energy saving in the Internet},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {56},
number = {10},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2012.03.009},
doi = {10.1016/j.comnet.2012.03.009},
abstract = {Many scientific works propose methods of reducing the amount of energy consumed by the Internet. Although the structure of the Internet was not developed with specific attention to energy consumption, there are various components on which it is possible to act. In our work, we analyze the possibility of affecting the topology of the network. We propose a heuristic called Energy Saving based on TOPology control (ESTOP), which identifies poorly used router line cards by leveraging certain topological properties of the graph modeling an Internet Service Provider (ISP). By acting on these line cards - for example, by putting them into sleep mode - we prune the Internet topology and achieve significant energy savings while preserving the primary topological characteristics of the pruned network. Although ESTOP is traffic-unaware, we assess its behavior under real traffic loads, demonstrating that its performance is comparable to the more complex traffic-aware solutions proposed in the literature.},
journal = {Comput. Netw.},
month = jul,
pages = {2355–2367},
numpages = {13},
keywords = {Energy consumption, Graph theory, Green Internet}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@article{10.1145/2159542.2159547,
author = {Milder, Peter and Franchetti, Franz and Hoe, James C. and P\"{u}schel, Markus},
title = {Computer Generation of Hardware for Linear Digital Signal Processing Transforms},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2159542.2159547},
doi = {10.1145/2159542.2159547},
abstract = {Linear signal transforms such as the discrete Fourier transform (DFT) are very widely used in digital signal processing and other domains. Due to high performance or efficiency requirements, these transforms are often implemented in hardware. This implementation is challenging due to the large number of algorithmic options (e.g., fast Fourier transform algorithms or FFTs), the variety of ways that a fixed algorithm can be mapped to a sequential datapath, and the design of the components of this datapath. The best choices depend heavily on the resource budget and the performance goals of the target application. Thus, it is difficult for a designer to determine which set of options will best meet a given set of requirements.In this article we introduce the Spiral hardware generation framework and system for linear transforms. The system takes a problem specification as input as well as directives that define characteristics of the desired datapath. Using a mathematical language to represent and explore transform algorithms and datapath characteristics, the system automatically generates an algorithm, maps it to a datapath, and outputs a synthesizable register transfer level Verilog description suitable for FPGA or ASIC implementation. The quality of the generated designs rivals the best available handwritten IP cores.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = apr,
articleno = {15},
numpages = {33},
keywords = {Digital signal processing transform, discrete Fourier transform, fast Fourier transform, hardware generation, high-level synthesis, linear transform}
}

@inproceedings{10.1145/319151.319161,
author = {Chiueh, Tzi-cker and Venkitachalam, Ganesh and Pradhan, Prashant},
title = {Integrating segmentation and paging protection for safe, efficient and transparent software extensions},
year = {1999},
isbn = {1581131402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319151.319161},
doi = {10.1145/319151.319161},
abstract = {The trend towards extensible software architectures and component-based software development demands safe, efficient, and easy-to-use extension mechanisms to enforce protection boundaries among software modules residing in the same address space. This paper describes the design, implementation, and evaluation of a novel intra-address space protection mechanism called Palladium, which exploits the segmentation and paging hardware in the Intel X86 architecture and efficiently supports safe kernel-level and user-level extensions in a way that is largely transparent to programmers and existing programming tools. Based on the considerations on ease of extension programming and systems implementation complexity, Palladium uses different approaches to support user-level and kernel-level extension mechanisms. To demonstrate the effectiveness of the Palladium architecture, we built a Web server that exploits the user-level extension mechanism to invoke CGI scripts as local function calls in a safe way, and we constructed a compiled network packet filter that exploits the kernel-level extension mechanism to run packet-filtering binaries safely inside the kernel at native speed. The current Palladium prototype implementation demonstrates that a protected procedure call and return costs 142 CPU cycles on a Pentium 200MHz machine running Linux.},
booktitle = {Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles},
pages = {140–153},
numpages = {14},
location = {Charleston, South Carolina, USA},
series = {SOSP '99}
}

@article{10.1561/2500000045,
author = {Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
title = {QED at Large: A Survey of Engineering of Formally Verified Software},
year = {2019},
issue_date = {Sep 2019},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {5},
number = {2–3},
issn = {2325-1107},
url = {https://doi.org/10.1561/2500000045},
doi = {10.1561/2500000045},
abstract = {Development of formal proofs of correctness of programs can
increase actual and perceived reliability and facilitate better understanding
of program specifications and their underlying assumptions.
Tools supporting such development have been available
for over 40 years, but have only recently seen wide practical use.
Projects based on construction of machine-checked formal proofs
are now reaching an unprecedented scale, comparable to large software
projects, which leads to new challenges in proof development
and maintenance. Despite its increasing importance, the field of
proof engineering is seldom considered in its own right; related
theories, techniques, and tools span many fields and venues. This
survey of the literature presents a holistic understanding of proof
engineering for program correctness, covering impact in practice,
foundations, proof automation, proof organization, and practical
proof development.},
journal = {Found. Trends Program. Lang.},
month = sep,
pages = {102–281},
numpages = {183}
}

@inproceedings{10.1145/1287731.1287738,
author = {Ono, Yasuhiro and Lifton, Joshua and Feldmeier, Mark and Paradiso, Joseph A.},
title = {Distributed acoustic conversation shielding: an application of a smart transducer network},
year = {2007},
isbn = {9781595937353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287731.1287738},
doi = {10.1145/1287731.1287738},
abstract = {In this paper, we introduce distributed acoustic conversation shielding, a novel application of a transducer (sensors and speakers) network. This application protects the privacy of spontaneous conversations in a workplace by masking the participants' voices with sound from distributed loudspeakers that adapt to the dynamic location of the conversation vs. that of potential eavesdroppers. We demonstrate how the speakers collaborate with various sensors to produce masking sounds that satisfy the requirements of this application. An index of intelligibility, SNR (Signal-to-Noise Ratio) was used to evaluate the performance of our system. We suggest how the measured SNR can be used to adaptively servo the volume of the masking sounds.},
booktitle = {Proceedings of the First ACM Workshop on Sensor and Actor Networks},
pages = {27–34},
numpages = {8},
keywords = {sound masking, conversation shielding, distributed control, location awareness, sensor network},
location = {Montreal, Quebec, Canada},
series = {SANET '07}
}

@article{10.5555/2773570.2773724,
author = {Lluch-Lafuente, Alberto and Montanari, Ugo},
title = {Quantitative μ-calculus and CTL Based on Constraint Semirings},
year = {2005},
issue_date = {January 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1571-0661},
abstract = {Model checking and temporal logics are boolean. The answer to the model checking question does a system satisfy a property? is either true or false, and properties expressed in temporal logics are defined over boolean propositions. While this classic approach is enough to specify and verify boolean temporal properties, it does not allow to reason about quantitative aspects of systems. Some quantitative extensions of temporal logics has been already proposed, especially in the context of probabilistic systems. They allow to answer questions like with which probability does a system satisfy a property?We present a generalization of two well-known temporal logics: CTL and the μ-calculus. Both extensions are defined over c-semirings, an algebraic structure that captures many problems and that has been proposed as a general framework for soft constraint satisfaction problems (CSP). Basically, a c-semiring consists of a domain, an additive operation and a multiplicative operation, which satisfy some properties. We present the semantics of the extended logics over transition systems, where a formula is interpreted as a mapping from the set of states to the domain of the c-semiring, and show that the usual connection between CTL and μ-calculus does not hold in general. In addition, we reason about the feasibility of computing the logics and illustrate some applications of our framework, including boolean model checking.},
journal = {Electron. Notes Theor. Comput. Sci.},
month = jan,
pages = {37–59},
numpages = {23},
keywords = {Constraint Semirings, Constraints, Quantitative Model Checking, Temporal Logics}
}

@article{10.1016/j.comnet.2009.10.004,
author = {Kant, Krishna},
title = {Data center evolution},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {53},
number = {17},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2009.10.004},
doi = {10.1016/j.comnet.2009.10.004},
abstract = {Data centers form a key part of the infrastructure upon which a variety of information technology services are built. As data centers continue to grow in size and complexity, it is desirable to understand aspects of their design that are worthy of carrying forward, as well as existing or upcoming shortcomings and challenges that would have to be addressed. We envision the data center evolving from owned physical entities to potentially outsourced, virtualized and geographically distributed infrastructures that still attempt to provide the same level of control and isolation that owned infrastructures do. We define a layered model for such data centers and provide a detailed treatment of state of the art and emerging challenges in storage, networking, management and power/thermal aspects.},
journal = {Comput. Netw.},
month = dec,
pages = {2939–2965},
numpages = {27},
keywords = {Data center, Ethernet, InfiniBand, Power management, Solid state storage, Virtualization}
}

@inproceedings{10.1145/224538.224547,
author = {Chiueh, Tzi-cker and Verma, Manish},
title = {A compiler-directed distributed shared memory system},
year = {1995},
isbn = {0897917286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224538.224547},
doi = {10.1145/224538.224547},
booktitle = {Proceedings of the 9th International Conference on Supercomputing},
pages = {77–86},
numpages = {10},
location = {Barcelona, Spain},
series = {ICS '95}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@book{10.5555/1537084,
author = {Shinder, Thomas W. and Shinder, Debra Littlejohn and Dimcev, Adrian F. and Eaton-Lee, James and Jones, Jason and Moffat, Steve},
title = {Dr. Tom Shinder's ISA Server 2006 Migration Guide},
year = {2007},
isbn = {9780080555515},
publisher = {Syngress Publishing},
abstract = {Dr. Tom Shinder's ISA Server 2006 Migration Guide provides a clear, concise, and thorough path to migrate from previous versions of ISA Server to ISA Server 2006. ISA Server 2006 is an incremental upgrade from ISA Server 2004, this book provides all of the tips and tricks to perform a successful migration, rather than rehash all of the features which were rolled out in ISA Server 2004. Also, learn to publish Exchange Server 2007 with ISA 2006 and to build a DMZ. * Highlights key issues for migrating from previous versions of ISA Server to ISA Server 2006. * Learn to Publish Exchange Server 2007 Using ISA Server 2006. * Create a DMZ using ISA Server 2006. * Dr. Tom Shinder's previous two books on configuring ISA Server have sold more than 50,000 units worldwide. * Dr. Tom Shinder is a Microsoft Most Valuable Professional (MVP) for ISA Server and a member of the ISA Server beta testing team. * This book will be the "Featured Product" on the Internet's most popular ISA Server site www.isaserver.org.}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

