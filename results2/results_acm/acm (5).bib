@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {Software product lines, design, quality assurance, requirements engineering, variability management, variability modeling},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106219,
author = {Gregg, Susan P. and Albert, Denise M. and Clements, Paul},
title = {Product Line Engineering on the Right Side of the "V"},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106219},
doi = {10.1145/3106195.3106219},
abstract = {Product line engineering (PLE) is well-known for the savings it brings to organizations. This paper shows how a very large, in-service systems and software product line is achieving PLE-based savings in their verification and validation phase of development. The paper addresses how to achieve the sharing across product variants while the products being tested are evolving over time. Additionally, we will give a pragmatic set of decision criteria to help answer the longstanding issue in PLE-based testing of whether to test on the domain side or the application (product) side of the product derivation process.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {165–174},
numpages = {10},
keywords = {AEGIS Combat System, PLE factory, Product line engineering, bill-of-features, feature modeling, feature profiles, product configurator, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/3385032.3385043,
author = {Bilic, Damir and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter and Causevic, Adnan and Amlinger, Christoffer and Barkah, Dani},
title = {Towards a Model-Driven Product Line Engineering Process: An Industrial Case Study},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385043},
doi = {10.1145/3385032.3385043},
abstract = {Many organizations developing software-intensive systems face challenges with high product complexity and large numbers of variants. In order to effectively maintain and develop these product variants, Product-Line Engineering methods are often considered, while Model-based Systems Engineering practices are commonly utilized to tackle product complexity. In this paper, we report on an industrial case study concerning the ongoing adoption of Product Line Engineering in the Model-based Systems Engineering environment at Volvo Construction Equipment (Volvo CE) in Sweden. In the study, we identify and define a Product Line Engineering process that is aligned with Model-based Systems Engineering activities at the engines control department of Volvo CE. Furthermore, we discuss the implications of the migration from the current development process to a Model-based Product Line Engineering-oriented process. This process, and its implications, are derived by conducting and analyzing interviews with Volvo CE employees, inspecting artifacts and documents, and by means of participant observation. Based on the results of a first system model iteration, we were able to document how Model-based Systems Engineering and variability modeling will affect development activities, work products and stakeholders of the work products.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {9},
numpages = {11},
keywords = {Engine System Development, Model-Based Systems Engineering, Product Line Engineering},
location = {Jabalpur, India},
series = {ISEC '20}
}

@inproceedings{10.1007/978-3-030-64148-1_6,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A Portfolio-Driven Development Model and Its Management Method of Agile Product Line Engineering Applied to Automotive Software Development},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_6},
doi = {10.1007/978-3-030-64148-1_6},
abstract = {In recent automotive systems development, realizing both variability and agility is the key competitiveness to meet the diverse requirements in global markets and rapidly increasing intelligent functions. This article proposes a portfolio-driven development method and its management method of APLE (Agile Product Line Engineering). The proposed method is intended to manage agile evolution of multiple product lines while increasing variability of products. To establish a portfolio management of development resources, it is necessary for an organization to manage multiple product lines on APLE in an entire development. We propose a portfolio-driven development method of three layers on APLE and its management method based on a concept of portfolio management life cycle. We applied the proposed management model and method to the multiple product lines of automotive software systems, and demonstrated an improvement of manageability with better predictability of both productivity and development size. This article contributes to provide an entire development management method for APLE, and its practical experience in the automotive multiple product lines.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {88–105},
numpages = {18},
keywords = {Software product line, Agile software development, Agile product line engineering, Portfolio management, Automotive software development},
location = {Turin, Italy}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {industrial experiences, product line adoption, product line evaluation},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655967,
author = {Assun\c{c}\~{a}o, Wesley Klewerton Guez and Vergilio, Silvia Regina},
title = {Feature location for software product line migration: a mapping study},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655967},
doi = {10.1145/2647908.2655967},
abstract = {Developing software from scratch is a high cost and error-prone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {52–59},
numpages = {8},
keywords = {evolution, reengineering, reuse, software product line},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2362536.2362572,
author = {Leitner, Andrea and Zehetner, Josef and Toeglhofer, Philipp and Watzenig, Daniel},
title = {Requirement identification for variability management in a co-simulation environment},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362572},
doi = {10.1145/2362536.2362572},
abstract = {Co-simulation is a powerful approach to verify a system design and to support concept decisions early in the automotive development process. Due to the heterogeneous nature of the co-simulation framework there is a lot of potential for variability requiring the systematic handling of it.We identified two main scenarios for variability management techniques in a co-simulation environment. Variability management capabilities can be included in the co-simulation tool itself or provide variability mechanisms to configure the co-simulation externally from a software product line. Depending on the context, one or even both scenarios can be applied.This work addresses different types of variability in an independent co-simulation framework (ICOS) and defines requirements for a realization concept.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {269–274},
numpages = {6},
keywords = {co-simulation, software product line engineering, variability management},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Design pattern, Search-based software engineering, Software product line architecture}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {challenges and opportunities, overloaded assets, software product-line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {core assets, higher-order simple predicate logic, quality attributes, relational algebra, type theory},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/1753235.1753247,
author = {Chen, Lianping and Ali Babar, Muhammad and Ali, Nour},
title = {Variability management in software product lines: a systematic review},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Variability Management (VM) in Software Product Line (SPL) is a key activity that usually affects the degree to which a SPL is successful. SPL community has spent huge amount of resources on developing various approaches to dealing with variability related challenges over the last decade. To provide an overview of different aspects of the proposed VM approaches, we carried out a systematic literature review of the papers reporting VM in SPL. This paper presents and discusses the findings from this systematic literature review. The results reveal the chronological backgrounds of various approaches over the history of VM research, and summarize the key issues that drove the evolution of different approaches. This study has also identified several gaps that need to be filled by future efforts in this line of research.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {81–90},
numpages = {10},
keywords = {software product lines, systematic reviews, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.infsof.2012.02.005,
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
title = {Issue-based variability management},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.005},
doi = {10.1016/j.infsof.2012.02.005},
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {933–950},
numpages = {18},
keywords = {Empirical software engineering, Product line engineering, Rationale management, Requirements engineering}
}

@inproceedings{10.5555/2819009.2819208,
author = {Klewerton, Wesley and Assun\c{c}\~{a}o, Guez},
title = {Search-based migration of model variants to software product line architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Software Product Lines (SPLs) are families of related software systems developed for specific market segments or domains. Commonly, SPLs emerge from sets of existing variants when their individual maintenance becomes infeasible. However, current approaches for SPL migration do not support design models, are partially automated, or do not reflect constraints from SPL domains. To tackle these limitations, the goal of this doctoral research plan is to propose an automated approach to the SPL migration process at the design level. This approach consists of three phases: detection, analysis and transformation. It uses as input the class diagrams and lists of features for each system variant, and relies on search-based algorithms to create a product line architecture that best captures the variability present in the variants. Our expected contribution is to support the adoption of SPL practices in companies that face the scenario of migrating variants to SPLs.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {895–898},
numpages = {4},
keywords = {migration, re-engineering, reuse, search-based software engineering, software product line},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1016/j.infsof.2012.06.014,
author = {Andersson, Henric and Herzog, Erik and \"{O}Lvander, Johan},
title = {Experience from model and software reuse in aircraft simulator product line engineering},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.014},
doi = {10.1016/j.infsof.2012.06.014},
abstract = {Context: ''Reuse'' and ''Model Based Development'' are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft. Objective: The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method: The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results: A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics. Conclusion: The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {595–606},
numpages = {12},
keywords = {Configurator, Knowledge Based Engineering, Model Based Development, PDM, SPL, Software Product Line}
}

@inproceedings{10.5555/1753235.1753274,
author = {Pech, Daniel and Knodel, Jens and Carbon, Ralf and Schitter, Clemens and Hein, Dirk},
title = {Variability management in small development organizations: experiences and lessons learned from a case study},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Product line practices promise to reduce development and maintenance efforts, to improve the productivity and to reduce the time to market by systematic reuse of commonalities and variabilities. However, in order to reap the fruits of exploiting those, an upfront investment is required. This paper presents a case study, which analyzes the cost-benefit ratio for one product line discipline -- variability management. Wikon GmbH -- a small German development organization evolving a product line of remote monitoring and controlling devices -- switched from manual, file-based conditional compilation to tool-supported decision models. We discuss experiences made and show that the break-even was reached with the 4th product derivation.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {285–294},
numpages = {10},
keywords = {decision model, evolution, product line engineering, software architecture, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/2701319.2701330,
author = {Rabiser, Rick and Vierhauser, Michael and Gr\"{u}nbacher, Paul},
title = {Variability Management for a Runtime Monitoring Infrastructure},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701330},
doi = {10.1145/2701319.2701330},
abstract = {Many software systems today are systems of systems (SoS), which are difficult to analyze due to their size, complexity, heterogeneity, and variability. For instance, unexpected behavior of SoS is often caused by the complex interactions between the involved systems and their environment at runtime. Monitoring infrastructures (MIs) provide support for engineers and support staff analyzing the behavior of SoS during development and operation. Variability plays an important role in MIs, however, while some approaches exist, managing variability of MIs remains challenging. In this paper, we describe how we applied a variability management approach to support the reconfiguration of a SoS monitoring infrastructure (MI) at runtime. Our approach provides configuration support for setting up the MI to reflect system variability. It also supports runtime reconfiguration of the MI to reflect the different monitoring tasks of users and to support evolution. We motivate our work using the case of monitoring a real-world SoS from the domain of industrial automation and discuss variability-related challenges in four monitoring scenarios. We evaluate the feasibility of our approach by applying it to these scenarios. We also demonstrate that our approach reduces manual reconfiguration effort and helps to reduce the overhead of the MI.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {Software monitoring, large-scale systems, reconfiguration, variability management},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.4018/ijismd.2014070103,
author = {Lotz, Alex and Ingl\'{e}s-Romero, Juan F. and Stampfer, Dennis and Lutz, Matthias and Vicente-Chicote, Cristina and Schlegel, Christian},
title = {Towards a Stepwise Variability Management Process for Complex Systems: A Robotics Perspective},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014070103},
doi = {10.4018/ijismd.2014070103},
abstract = {Complex systems are executed in environments with a huge number of potential situations and contingencies, therefore a mechanism is required to express dynamic variability at design-time that can be efficiently resolved in the application at run-time based on the then available information. We present an approach for dynamic variability modeling and its exploitation at run-time. It supports different developer roles and allows the separation of two different kinds of dynamic variability at design-time: (i) variability related to the system operation, and (ii) variability associated with QoS. The former provides robustness to contingencies, maintaining a high success rate in task fulfillment. The latter focuses on the quality of the application execution (defined in terms of non-functional properties like safety or task efficiency) under changing situations and limited resources. The authors also discuss different alternatives for the run-time integration of the two variability management mechanisms, and show real-world robotic examples to illustrate them.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {55–74},
numpages = {20},
keywords = {Modeling Run-Time Variability, Service Robotics, SmartTCL, VML, Variability Management}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@inproceedings{10.1145/2304676.2304679,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Respecting component architecture to migrate product copies to a software product line},
year = {2012},
isbn = {9781450313483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304676.2304679},
doi = {10.1145/2304676.2304679},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating existing, customised product copies to a product line is still an open issue due to the required comprehension of differences among products and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present how component architecture information can be used to enhance a variabilty reverse engineering process to target this challenge and show the relevance of component architecture in the individual requirements on the resulting SPL. We further provide an illustrating example to show how the concept is applied.},
booktitle = {Proceedings of the 17th International Doctoral Symposium on Components and Architecture},
pages = {7–12},
numpages = {6},
keywords = {component architecture, reverse engineering, software product line},
location = {Bertinoro, Italy},
series = {WCOP '12}
}

@inproceedings{10.1145/2755644.2755647,
author = {Duplyakin, Dmitry and Haney, Matthew and Tufo, Henry},
title = {Architecting a Persistent and Reliable Configuration Management System},
year = {2015},
isbn = {9781450335706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755644.2755647},
doi = {10.1145/2755644.2755647},
abstract = {Streamlined configuration management plays a significant role in modern, complex distributed systems. Via mechanisms that promote consistency, repeatability, and transparency, configuration management systems (CMSes) address complexity and aim to increase the efficiency of administrative procedures, including deployment and failure recovery scenarios. Considering the importance of minimizing disruptions in these systems, we design an architecture that increases persistency and reliability of infrastructure management. We present our architecture in the context of hybrid, cluster-cloud environments and describe our highly available implementation that builds upon the open source CMS called Chef and infrastructure-as-a-service cloud resources from Amazon Web Services. We demonstrate how we enabled a smooth transition from the pre-existing single-server configuration to the proposed highly available management system. We summarize our experience with managing a 20-node Linux cluster using this implementation. Our analysis of utilization and cost of necessary cloud resources indicates that the designed system is a low-cost alternative to acquiring additional physical hardware for hardening cluster management. We also highlight the prototype's security and manageability features that are suitable for larger, production-ready deployments.},
booktitle = {Proceedings of the 6th Workshop on Scientific Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {automated infrastructure management, cloud computing, computing cluster, configuration management system, high availability, hybrid systems},
location = {Portland, Oregon, USA},
series = {ScienceCloud '15}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Quality attributes, Service-based systems, Systematic literature review, Variability}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {benchmark extractor, feature location, feature revision, repository mining, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.scico.2012.02.006,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and Mccomas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara and Krikhaar, Rene and Verhoef, Chris and Montgomery, Lisa P.},
title = {An analysis of unit tests of a flight software product line},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.02.006},
doi = {10.1016/j.scico.2012.02.006},
abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software System (CFS) product line team at the NASA Goddard Space Flight Center (GSFC). The goal of the analysis is to understand, review, and recommend strategies for improving the CFS' existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other software product line (SPL) teams for their unit testing. The results of the analysis show that the core and application modules of the CFS are unit tested in isolation using a stub framework developed by the CFS team. The application developers can unit test their code without waiting for the core modules to be completed, and vice versa. The analysis found that this unit testing approach incorporates many practical and useful solutions such as allowing for unit testing without requiring hardware and special OS features in-the-loop by defining stub implementations of dependent modules. These solutions are worth considering when deciding how to design the testing architecture for a SPL.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2360–2380},
numpages = {21},
keywords = {Flight software, Metrics, Self-testable components, Software architecture, Stub, Unit testing}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@inproceedings{10.1145/2430502.2430516,
author = {Lanceloti, Leandro A. and Maldonado, Jos\'{e} C. and Gimenes, Itana M. S. and Oliveira, Edson A.},
title = {SMartyParser: a XMI parser for UML-based software product line variability models},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430516},
doi = {10.1145/2430502.2430516},
abstract = {Variability management is an important issue for the software-intensive systems domain. Such an issue is essential for the success of software product line (SPL) adoption strategies. Although it is a well-discussed subject in the SPL community, there is a lack of tool support for environments that handle UML-based SPL variabilities, as several variability management approaches take UML as a basis, specially its profiling mechanism. Such environments might handle variabilities for several reasons, such as, evaluating SPLs, defining and applying metrics based on a SPL modeling, and automating the product generation. Therefore, this paper presents the SMartyParser, a parser for processing UML-based SPL models. Such models can be obtained, in the XMI format, from every UML specification-compliant tool. Such a parser provides several services to make it easier the handling of variability data in a particular SPL environment/tool. SMartyParser was built by taking the Open Core framework as a basis for processing XMI files. A parser use example is presented by taking into account the SPL Arcade Game Maker UML models.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {5},
keywords = {UML, XMI, parser, software product line, stereotype, variability management},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/3461002.3473942,
author = {Kahraman, G\"{o}khan and Cleophas, Loek},
title = {Automated derivation of variants in manufacturing systems design},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473942},
doi = {10.1145/3461002.3473942},
abstract = {The Logistics Specification and Analysis Tool (LSAT) is a modelbased engineering tool used for design-space exploration of flexible manufacturing systems. LSAT provides domain specific languages to model a manufacturing system and means to analyze the productivity characteristics of such a system. In LSAT, developers can specify a system and model its deterministic operations as a set of activities. Given a set of activities, it is possible to construct an individual activity sequence that represents one valid system execution, and with minor variations in the specification individual systems can be obtained. To avoid modeling each variant separately, which means cloning and maintaining the common parts, new functionality is needed to deal with the variability of system specifications. In this study, we aim to establish integration between LSAT and product line engineering techniques. Specifically, we provide a realization of a toolchain including variability representation of LSAT realization artifacts and automated variant derivation for the LSAT model variants. Delta modeling, a transformational variability realization mechanism, is employed to model the variability within LSAT realization artifacts. Using the toolchain, we develop an industry-related case for a product line, the so called Extended Twilight System, a Cyber Physical System (CPS) inspired by the CPSs of our industrial partner.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {45–50},
numpages = {6},
keywords = {delta modeling, manufacturing systems, model-based engineering, product lines, variability modeling},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Cyber-Physical Production System, Product-Process-Resource, Variability Modelling},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1842752.1842809,
author = {Helleboogh, Alexander and Avgeriou, Paris and Bouck\'{e}, Nelis and Heymans, Patrick},
title = {Workshop on Variability in Software Product Line Architectures (VARI-ARCH 2010)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842809},
doi = {10.1145/1842752.1842809},
abstract = {The objective of this workshop is to bring together researchers from the software product line community and software architecture community to identify critical challenges and progress the state-of-the-art on variability in software product line architectures.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {309–311},
numpages = {3},
keywords = {assets, concern, model, product line architecture, product lines, software architecture, variability, view, viewpoint},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1007/978-3-642-12107-4_7,
author = {Zschaler, Steffen and S\'{a}nchez, Pablo and Santos, Jo\~{a}o and Alf\'{e}rez, Mauricio and Rashid, Awais and Fuentes, Lidia and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Kulesza, Uir\'{a}},
title = {VML* – a family of languages for variability management in software product lines},
year = {2009},
isbn = {3642121063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12107-4_7},
doi = {10.1007/978-3-642-12107-4_7},
abstract = {Managing variability is a challenging issue in software-product-line engineering. A key part of variability management is the ability to express explicitly the relationship between variability models (expressing the variability in the problem space, for example using feature models) and other artefacts of the product line, for example, requirements models and architecture models. Once these relations have been made explicit, they can be used for a number of purposes, most importantly for product derivation, but also for the generation of trace links or for checking the consistency of a product-line architecture. This paper bootstraps techniques from product-line engineering to produce a family of languages for variability management for easing the creation of new members of the family of languages. We show that developing such language families is feasible and demonstrate the flexibility of our language family by applying it to the development of two variability-management languages.},
booktitle = {Proceedings of the Second International Conference on Software Language Engineering},
pages = {82–102},
numpages = {21},
keywords = {domain-specific languages, family of languages, software product lines, variability management},
location = {Denver, CO},
series = {SLE'09}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {product-line analysis, regression analysis, software configuration management, software evolution, software product lines, software variation, variability management, variability-aware analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {SME, experience report, product line engineering, project management},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/3382025.3414970,
author = {Kr\"{u}ger, Jacob and Mahmood, Wardah and Berger, Thorsten},
title = {Promote-pl: a round-trip engineering process model for adopting and evolving product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414970},
doi = {10.1145/3382025.3414970},
abstract = {Process models for software product-line engineering focus on proactive adoption scenarios---that is, building product-line platforms from scratch. They comprise the two phases domain engineering (building a product-line platform) and application engineering (building individual variants), each of which defines various development activities. Established more than two decades ago, these process models are still the de-facto standard for steering the engineering of platforms and variants. However, observations from industrial and open-source practice indicate that the separation between domain and application engineering, with their respective activities, does not fully reflect reality. For instance, organizations rarely build platforms from scratch, but start with developing individual variants that are re-engineered into a platform when the need arises. Organizations also appear to evolve platforms by evolving individual variants, and they use contemporary development activities aligned with technical advances. Recognizing this discrepancy, we present an updated process model for engineering software product lines. We employ a method for constructing process theories, building on recent literature as well as our experiences with industrial partners to identify development activities and the orders in which these are performed. Based on these activities, we synthesize and discuss the new process model, called promote-pl. Also, we explain its relation to modern software-engineering practices, such as continuous integration, model-driven engineering, or simulation testing. We hope that our work offers contemporary guidance for product-line engineers developing and evolving platforms, and inspires researchers to build novel methods and tools aligned with current practice.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {2},
numpages = {12},
keywords = {process model, round-trip engineering, software reuse},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1176887.1176897,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Defining a strategy to introduce a software product line using existing embedded systems},
year = {2006},
isbn = {1595935428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176887.1176897},
doi = {10.1145/1176887.1176897},
abstract = {Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study.},
booktitle = {Proceedings of the 6th ACM &amp; IEEE International Conference on Embedded Software},
pages = {63–72},
numpages = {10},
keywords = {clone detection and classification, economics, engine control systems, reverse rngineering, software, software product line},
location = {Seoul, Korea},
series = {EMSOFT '06}
}

@article{10.1016/j.infsof.2012.07.010,
author = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
title = {MOD2-SCM: A model-driven product line for software configuration management systems},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.010},
doi = {10.1016/j.infsof.2012.07.010},
abstract = {Context: Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective: Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems. Method: We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results: Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion: The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {630–650},
numpages = {21},
keywords = {Code generation, Executable models, Feature models, Model transformation, Model-driven software engineering, Software configuration management, Software product line engineering}
}

@article{10.1016/j.infsof.2006.11.003,
author = {Niemel\"{a}, Eila and Immonen, Anne},
title = {Capturing quality requirements of product family architecture},
year = {2007},
issue_date = {November, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {11–12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.11.003},
doi = {10.1016/j.infsof.2006.11.003},
abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1107–1120},
numpages = {14},
keywords = {Quality requirement, Software architecture, Software product family, Traceability}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2362536.2362562,
author = {Mohalik, Swarup and Ramesh, S. and Millo, Jean-Vivien and Krishna, Shankara Narayanan and Narwane, Ganesh Khandu},
title = {Tracing SPLs precisely and efficiently},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362562},
doi = {10.1145/2362536.2362562},
abstract = {In a Software Product Line (SPL), the central notion of implementability provides the requisite connection between specifications (feature sets) and their implementations (component sets), leading to the definition of products. While it appears to be a simple extension (to sets) of the trace-ability relation between components and features, it actually involves several subtle issues which are overlooked in the definitions in existing literature. In this paper, we give a precise and formal definition of implementability over a fairly expressive traceability relation to solve these issues. The consequent definition of products in the given SPL naturally entails a set of useful analysis problems that are either refinements of known problems, or are completely novel. We also propose a new approach to solve these analysis problems by encoding them as Quantified Boolean Formula(QBF) and solving them through Quantified Satisfiability (QSAT) solvers. The methodology scales much better than the SAT-based solutions hinted in the literature and is demonstrated through a prototype tool called SPLANE (SPL Analysis Engine), on a couple of fairly large case studies.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {186–195},
numpages = {10},
keywords = {QSAT, feature model, formal methods, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {bill-of-features, feature constraints hierarchical product lines, feature modeling, feature profiles, product baselines, product configurator, product line adoption, product line engineering, product line governance, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {software product lines, textual specification languages, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr\"{u}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/2629395,
author = {Jain, Radhika and Cao, Lan and Mohan, Kannan and Ramesh, Balasubramaniam},
title = {Situated Boundary Spanning: An Empirical Investigation of Requirements Engineering Practices in Product Family Development},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629395},
doi = {10.1145/2629395},
abstract = {Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {29},
keywords = {Boundary spanning, agile development, product family development, requirements engineering}
}

@inproceedings{10.1145/3236405.3236407,
author = {Ghofrani, Javad and Fehlhaber, Anna Lena},
title = {ProductlinRE: online management tool for requirements engineering of software product lines},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236407},
doi = {10.1145/3236405.3236407},
abstract = {The lack of online tools for managing various artifacts of software product lines is problematic, and stands in contradiction to findings about the need to support collaboration. In this paper, we present ProductLinRE, a web application allowing product line engineers to work cooperatively on artifacts of requirements engineering for software product lines. Our proposed online tool allows distributed teamwork, using a tracking mechanism for projects, artifacts and features while tailoring the requirements artifacts according to the selected features.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {17–22},
numpages = {6},
keywords = {online tools, requirements engineering, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {industrial experience, multiple product lines, software reuse},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106202,
author = {Wille, David and Wehling, Kenny and Seidl, Christoph and Pluchator, Martin and Schaefer, Ina},
title = {Variability Mining of Technical Architectures},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106202},
doi = {10.1145/3106195.3106202},
abstract = {Technical architectures (TAs) represent the computing infrastructure of a company with all its hardware and software components. Over the course of time, the number of TAs grows with the companies' requirements and usually a large variety of TAs has to be maintained. Core challenge is the missing information on relations between the existing variants of TAs, which complicates reuse of solutions across systems. However, identifying these relations is an expensive task as architects have to manually analyze each TA individually. Restructuring the existing TAs poses severe risks as often sufficient information is not available (e.g., due to time constraints). To avoid failures in productive systems and resulting loss of profit, companies continue to create new solutions without restructuring existing ones. This increased variability in TAs represents technical debt. In this paper, we adapt the idea of variability mining from the software product line domain and present an efficient and automatic mining algorithm to identify the common and varying parts of TAs by analyzing a potentially arbitrary number of TAs in parallel. Using the identified variability information, architects are capable of analyzing the relations of TAs, identifying reuse potential, and making well-founded maintenance decisions. We show the feasibility and scalability of our approach by applying it to a real-world industrial case study with large sets of TAs.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {39–48},
numpages = {10},
keywords = {enterprise architecture, technical architecture, variability mining},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791080,
author = {Van Landuyt, Dimitri and Walraven, Stefan and Joosen, Wouter},
title = {Variability middleware for multi-tenant SaaS applications: a research roadmap for service lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791080},
doi = {10.1145/2791060.2791080},
abstract = {Software product line engineering (SPLE) and variability enforcement techniques have been applied to run-time adaptive systems for quite some years, also in the context of multi-tenant Software-as-a-Service (SaaS) applications. The focus has been mainly on (1) the pre-deployment phases of the development life cycle and (2) fine-grained (tenant-level), run-time activation of specific variants. However, with upcoming trends such as DevOps and continuous delivery and deployment, operational aspects become increasingly important.In this paper, we present our integrated vision on the positive interplay between SPLE and adaptive middleware for multi-tenant SaaS applications, focusing on the operational aspects of running and maintaining a successful SaaS offering. This vision, called Service Lines, is based on and motivated by our experience and frequent interactions with a number of Belgian SaaS providers.We concretely highlight and motivate a number of operational use cases that require advanced variability support in middleware and have promising added value for the economic feasibility of SaaS offerings. In addition, we provide a gap analysis of what is currently lacking from the perspectives of variability modeling and management techniques and middleware support, and as such sketch a concrete roadmap for continued research in this area.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {211–215},
numpages = {5},
keywords = {models at run time, multi-tenant SaaS, operational support, run-time variability, service lines, variability middleware},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {code metrics, model checking, non-functional properties, sampling, software product lines, static analysis, testing, theorem proving, tool support, type checking},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791089,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Harnessing variability in product-lines of self-adaptive software systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791089},
doi = {10.1145/2791060.2791089},
abstract = {This work studies systematic reuse in the context of self-adaptive software systems. In our work, we realized that managing variability for such platforms is different compared to traditional platforms, primarily due to the run-time variability and system uncertainties. Motivated by the fact that recent trends show that self-adaptation will be used more often in future system generation and that software reuse state-of-practice or research do not provide sufficient support, we have investigated the problems and possibly resolutions in this context. We have analyzed variability for these systems, using a systematic reuse prism, and identified a research gap in variability management. The analysis divides variability handling into four activities: (1) identify variability, (2) constrain variability, (3) implement variability, and (4) manage variability. Based on the findings we envision a reuse framework for the specific domain and present an example framework that addresses some of the identified challenges. We argue that it provides basic support for engineering self-adaptive software systems with systematic reuse. We discuss some important avenues of research for achieving the vision.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {191–200},
numpages = {10},
keywords = {self-adaptive software systems, software reuse, variability analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106213,
author = {Fu\ss{}berger, Nicolas and Zhang, Bo and Becker, Martin},
title = {A Deep Dive into Android's Variability Realizations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106213},
doi = {10.1145/3106195.3106213},
abstract = {The open source Android operation system is widely used in both mobile consumer electronics as well as other industrial devices. It has actually become a variability-intensive system that can be highly customized to support different customers' requirements and hardware environments, which is a good inspiration for both practitioners and researchers. However, it is still unclear where and how variability is realized in its source code repository. In this paper, we conduct a systematic analysis on the variability realization of the Android operation system. The analysis focuses on the usage of different variability realization mechanisms (e.g., Conditional Compilation) in the Android source code and build environment. Finally, the study provides qualitative and quantitative results that help to understand i) what variability-specific artefacts exist in the Android source repository using which variability mechanisms and techniques; ii) how these artefacts express and instantiate variability along the layered Android realization architecture.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {69–78},
numpages = {10},
keywords = {Android, Variability Mechanisms, Variability Realization},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/2648511.2648528,
author = {Barreiros, Jorge and Moreira, Ana},
title = {A cover-based approach for configuration repair},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648528},
doi = {10.1145/2648511.2648528},
abstract = {Feature models are often used to describe variability and commonality in Software Product Lines, specifying admissible configurations of valid products. However, invalid configurations may arise in some scenarios. These include feature model evolution that invalidates pre-existing products or collaborative configuration by multiple stakeholders with conflicting goals, among others. This problem has been acknowledged in the literature and some techniques for configuration repair have already been proposed. However, common optimization criteria such as proximity between original and repaired configurations can result in a significant number of alternative repair possibilities, easily attaining thousands of alternatives for models of practical dimension. Consequently, rather than just efficiently providing an exhaustive list of possibilities, an approach that specifically addresses this issue should be able to offer the user a manageable and comprehensible view of the configuration problems and potential repair options. We offer a novel approach for configuration repair, based on partitioning and cover analysis, with high performance and generating high quality solutions, which allows efficient identification and presentation of multiple competing repairs.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {157–166},
numpages = {10},
keywords = {configuration, configuration diagnosis, configuration repair, feature modeling, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {ClaferMOO, collective adaptive systems, multi-objective optimization, quantitative analysis, quantitative modeling, variability analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-319-23781-7_27,
author = {Bouarar, Selma and Jean, St\'{e}phane and Siegmund, Norbert},
title = {SPL Driven Approach for Variability in Database Design},
year = {2015},
isbn = {9783319237800},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23781-7_27},
doi = {10.1007/978-3-319-23781-7_27},
abstract = {The evolution of computer technology has strongly impacted the database design. No phase was spared: several conceptual formalisms e.g. ER, UML, ontological, various logical models e.g. relational, object, key-value, a wide panoply of physical optimization structures and deployment platforms have been proposed. As a result, the database design process has become more complex involving more tasks and even more actors as database architect or analyst. Getting inspired from software engineering in dealing with variable similar systems, we propose a methodological framework for a variability-aware design of databases, whereby this latter is henceforth devised as a Software Product Line. Doing so guarantees a high reuse, automation, and customizability in generating ready-to-be implemented databases. We also propose a solution to help users make a suitable choice among the wide panoply. Finally, a case study is presented.},
booktitle = {Proceedings of the 5th International Conference on Model and Data Engineering - Volume 9344},
pages = {332–342},
numpages = {11},
keywords = {Database design, Software Product Line, Variability},
location = {Rhodes, Greece},
series = {MEDI 2015}
}

@inproceedings{10.1145/3442391.3442397,
author = {Silva, Leandro Flores da and OliveiraJr, Edson},
title = {SMartyModeling: an Environment for Engineering UML-based Software Product Lines},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442397},
doi = {10.1145/3442391.3442397},
abstract = {Software Product Line (SPL) has been successfully consolidated as an approach for systematic reuse. The adoption of the SPL approach aims at increasing the reuse of requirements and artifacts, thus reusing documents, source code and artifacts and ensuring better quality control to software production in a large-scale. One of the essential activities for SPL management is the modeling of variability. Variability modeling in UML-based SPL has been carried out mostly using the UML Profiling mechanism, in which new stereotypes and tagged values are created for such purpose. The available option in general-purpose UML tools for exporting UML models is through XMI files, standardized by OMG. This option is important to process XMI files in particular environments or tools, for example, managing variabilities, generating product configurations from an SPL, and even collecting metrics, and estimating SPLs. However, different versions, tool restrictions, and different file standards compromise operations involving XMI files. In this scenario, the industry has increasingly required the supporting tools for the SPL approach. However, the current support tools are mainly restricted to the problem space based on feature modeling and present problems with data integration with other tools. Therefore, we developed SMartyModeling, an environment for engineering UML-based SPLs in which variabilities are modeled as stereotypes using any UML compliant profile. This paper presents an overview of SMartyModeling, describing its motivation, main components, and available features.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {5},
keywords = {SMarty, UML, software product line, variability modeling},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s10664-014-9359-z,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: proposing theories from a case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9359-z},
doi = {10.1007/s10664-014-9359-z},
abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1623–1669},
numpages = {47},
keywords = {Case study, Software architecture, Software product line, Variability}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {core asset development, data intensiveness, data oriented approach, enterprise applications, product line architecture, quality attributes, relational database management system},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {case study, core assets, feature modeling, product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362549,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Behavioural modelling and verification of real-time software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362549},
doi = {10.1145/2362536.2362549},
abstract = {In Software Product Line (SPL) engineering, software products are build in families rather than individually. Many critical software are nowadays build as SPLs and most of them obey hard real-time requirements. Formal methods for verifying SPLs are thus crucial and actively studied. The verification problem for SPL is, however, more complicated than for individual systems; the large number of different software products multiplies the complexity of SPL model-checking. Recently, promising model-checking approaches have been developed specifically for SPLs. They leverage the commonality between the products to reduce the verification effort. However, none of them considers real time.In this paper, we combine existing SPL verification methods with established model-checking procedures for real-time systems. We introduce Featured Timed Automata (FTA), a formalism that extends the classical Timed Automata with constructs for modelling variability. We show that FTA model-checking can be achieved through a smart combination of real-time and SPL model checking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {66–75},
numpages = {10},
keywords = {features, model checking, real-time, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019146,
author = {Gerlach, Simon},
title = {Improving efficiency when deriving numerous products from software product lines simultaneously},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019146},
doi = {10.1145/2019136.2019146},
abstract = {In-car infotainment systems must allow for product differentiation and the adaption to the needs of different markets. Product line approaches are applied because large numbers of different product variants need to be developed simultaneously. During development, updated versions of each projected product variant need to be derived from the product line assets repeatedly. Current build tools create each of the numerous product variants one after another. Accordingly, the creation process can take much time. This paper presents an approach to abbreviate this creation process based on the fact that multiple product variants created at once can have parts in common. To benefit from this optimization potential the workflow that creates an individual product variant is subdivided into multiple fragments. Whenever a set of such product variants needs to be created, an optimization algorithm then calculates an individual execution order of the fragments for this set. This order minimizes the total execution time by a systematic reuse of workflow fragment's results for the creation of multiple different product variants.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {9},
numpages = {4},
keywords = {application engineering, automotive, product configuration, product derivation, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {feature model configuration, preferences, social choice},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791072,
author = {Patel, Sachin and Shah, Vipul},
title = {Automated testing of software-as-a-service configurations using a variability language},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791072},
doi = {10.1145/2791060.2791072},
abstract = {The benefits offered by cloud technologies have compelled enterprises to adopt the Software-as-a-Service (SaaS) model for their enterprise software needs. A SaaS has to be configured or customized to suit the specific requirements of every enterprise that subscribes to it. IT service providers have to deal with the problem of testing many such configurations created for different enterprises. The software gets upgraded periodically and the configurations need to be tested on an ongoing basis to ensure business continuity. In order to run the testing organization efficiently, it is imperative that the test cycle is automated. Developing automated test scripts for a large number of configurations is a non-trivial task because differences across them may range from a few user interface changes to business process level changes. We propose an approach that combines the benefits of model driven engineering and variability modeling to address this issue. The approach comprises of the Enterprise Software Test Modeling Language to model the test cases. We use the Common Variability Language to model variability in the test cases and apply model transformations on a base model to generate a test model for each configuration. These models are used to generate automated test scripts for all the configurations. We describe the test modelling language and an experiment which shows that the approach can be used to automatically generate variations in automated test scripts.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {253–262},
numpages = {10},
keywords = {enterprise software testing, model based testing, software-as-a-service, test automation, variability specification},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2364412.2364435,
author = {Saller, Karsten and Oster, Sebastian and Sch\"{u}rr, Andy and Schroeter, Julia and Lochau, Malte},
title = {Reducing feature models to improve runtime adaptivity on resource limited devices},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364435},
doi = {10.1145/2364412.2364435},
abstract = {Mobile devices like smartphones are getting increasingly important in our daily lifes. They are used in various environments and have to dynamically adapt themselves accordingly in order to provide an optimal runtime behavior. Naturally, adapting to continuously changing environmental conditions is a challenging task because mobile devices are always limited in their resources and have to adapt in real-time. In this paper, we introduce an approach that enables resource limited devices to adapt to changing conditions using dynamic software product lines techniques. Therefore, feature models are reduced to a specific hardware context before installing the adaptive mobile application on the device. This reduces the amount of possible configurations that are compatible with the device and, thereby, minimizes the costs and the duration of an adaptation during runtime.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {135–142},
numpages = {8},
keywords = {adaptive systems, context-awareness, dynamic software product lines, feature models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {DSPL, adaptive systems, contexts, feature models, state space reduction},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/1753235.1753238,
author = {White, Jules and Dougherty, Brian and Schmidt, Doulas C. and Benavides, David},
title = {Automated reasoning for multi-step feature model configuration problems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of increasing software reusability. One software reuse approach is to develop a Software Product-line (SPL), which is a reconfigurable software architecture that can be reused across projects. Creating configurations of the SPL that meets arbitrary requirements is hard.Existing research has focused on techniques that produce a configuration of the SPL in a single step. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these CSP configuration problem CSPs can be derived automatically with a constraint solver. Third, we present empirical results demonstrating that our CSP-based technique can solve multi-step configuration problems involving hundreds of features in seconds.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {11–20},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753243,
author = {Savolainen, Juha and Bosch, Jan and Kuusela, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Default values for improved product line management},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Many companies apply software product lines based on explicit variability management. These product lines have existed for more than ten years. While research has progressed during that time and industrial experiences have been extensively reported, there seems to still be a gap between the industrial practice and research with respect to explicit software variability management. In this paper, we explain how commonality is managed in industry; how default values can be used to gain control over expanding scope and near-commonality; and discuss how evolution of default values indicate potential problems in industrial product lines. In addition, we explain what corrective actions can be taken to alleviate the identified problems.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {51–60},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@inproceedings{10.5555/302163.302187,
author = {J\'{e}z\'{e}quel, J.-M.},
title = {Reifying configuration management for object-oriented software},
year = {1998},
isbn = {0818683686},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 20th International Conference on Software Engineering},
pages = {240–249},
numpages = {10},
location = {Kyoto, Japan},
series = {ICSE '98}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {Software product line, multi-objective optimisation, test prioritisation, test selection}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {multi-objective evolutionary algorithms, product line architecture design, search-based software engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {QoS aggregation, feature modeling, non-functional properties, process family, service variability, service-oriented architecture (SOA), software product line (SPL), variability management},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1145/302405.302409,
author = {DeBaud, Jean-Marc and Schmid, Klaus},
title = {A systematic approach to derive the scope of software product lines},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302409},
doi = {10.1145/302405.302409},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {domain engineering, product line scoping, reuse economic models, software product line},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@inproceedings{10.1145/2362536.2362544,
author = {Dietrich, Christian and Tartler, Reinhard and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {A robust approach for variability extraction from the Linux build system},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362544},
doi = {10.1145/2362536.2362544},
abstract = {With more than 11,000 optional and alternative features, the Linux kernel is a highly configurable piece of software. Linux is generally perceived as a textbook example for preprocessor-based product derivation, but more than 65 percent of all features are actually handled by the build system. Hence, variability-aware static analysis tools have to take the build system into account.However, extracting variability information from the build system is difficult due to the declarative and turing-complete make language. Existing approaches based on text processing do not cover this challenges and tend to be tailored to a specific Linux version and architecture. This renders them practically unusable as a basis for variability-aware tool support -- Linux is a moving target!We describe a robust approach for extracting implementation variability from the Linux build system. Instead of extracting the variability information by a text-based analysis of all build scripts, our approach exploits the build system itself to produce this information. As our results show, our approach is robust and works for all versions and architectures from the (git-)history of Linux.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {21–30},
numpages = {10},
keywords = {Linux, VAMOS, build systems, configurability, kbuild, maintenance, static analysis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Feature model, Software product line, Defect, Product line model, Quality}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00013,
author = {Kehrer, Timo and Th\"{u}m, Thomas and Schulthei\ss{}, Alexander and Bittner, Paul Maximilian},
title = {Bridging the gap between clone-and-own and software product lines},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00013},
doi = {10.1109/ICSE-NIER52604.2021.00013},
abstract = {Software is often released in multiple variants to meet all customer requirements. While software product lines address this need by advocating the development of an integrated software platform, practitioners frequently rely on ad-hoc reuse based on a principle which is known as clone-and-own. This practice avoids high up-front investments, as new variants of a software family are created by simply copying and adapting an existing variant, but maintenance costs explode once a critical number of variants is reached. With our research project VariantSync, we aim to bridge the gap between clone-and-own and product lines by combining the minimal overhead and flexibility of clone-and-own with the systematic handling of variability in software product lines. The key idea is to transparently integrate product-line concepts with variant management facilities known from version control systems in order to automatically synchronize a set of evolving variants. We believe that VariantSync has the potential to change the way how practitioners develop multi-variant software systems for which it is hard to foresee which variants will be added in the future.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {21–25},
numpages = {5},
keywords = {clone-and-own, configuration management, feature traceability, software product lines},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@inproceedings{10.1145/1985484.1985490,
author = {Stallinger, Fritz and Neumann, Robert and Schossleitner, Robert and Kriener, Stephan},
title = {Migrating towards evolving software product lines: challenges of an SME in a core customer-driven industrial systems engineering context},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985490},
doi = {10.1145/1985484.1985490},
abstract = {In this paper we identify key challenges a medium-sized software organization is facing in migrating towards Software Product Line Engineering (SPLE). The software engineering context of the company is characterized by a two-fold access to the market - core customer driven product enhancement and product development for a broader, anonymous market - and the embedding of software engineering in multi-disciplinary systems and solutions engineering.Based on a characterization of the business, the software product subject to migration towards SPLE, and the goals and background of the SPLE initiative, seven key challenges with respect to the migration are identified. These challenges relate to process diversity in the face of multiple reuse approaches; the management of requirements and variability; the integration of requirements traceability and variability management; legacy software and discipline vs. software-specific modularization; integration with systems engineering; costing and pricing models; and project vs. product documentation.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {20–24},
numpages = {5},
keywords = {sme, software engineering, software product line, software product migration, systems engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/2110147.2110166,
author = {M\ae{}rsk-M\o{}ller, Hans Martin and J\o{}rgensen, Bo N\o{}rregaard},
title = {Cardinality-dependent variability in orthogonal variability models},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110166},
doi = {10.1145/2110147.2110166},
abstract = {During our work on developing and running a software product line for eco-sustainable greenhouse-production software tools, which currently have three products members we have identified a need for extending the notation of the Orthogonal Variability Model (OVM) to support what we refer to as cardinality range dependencies. The cardinality-range-dependency type enables expressing that the binding of a certain number of variants to a variation point can influence variability in other places in the model. In other words, we acknowledge that variability can be influenced, not necessarily by the specific variants being bound, but by their sheer numbers.This paper contributes with an extension to the meta-model underlying the OVM notation, suggesting a notation for the new type of dependency and shows its applicability. The specific case, which initially required this extension, will work as running example throughout the paper and underline the need for the extension. Finally, the paper evaluates and discusses the general applicability of the proposed notation extension and future perspectives.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–172},
numpages = {8},
keywords = {documentation, orthogonal variability model (OVM), software product line engineering, variability modeling language},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1016/j.infsof.2019.08.007,
author = {Nogueira Teixeira, Eld\^{a}nae and Aleixo, Fellipe Ara\'{u}jo and Am\^{a}ncio, Francisco Dione de Sousa and OliveiraJr, Edson and Kulesza, Uir\'{a} and Werner, Cl\'{a}udia},
title = {Software process line as an approach to support software process reuse: A systematic literature review},
year = {2019},
issue_date = {Dec 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {116},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.007},
doi = {10.1016/j.infsof.2019.08.007},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Systematic review, Software process, Process reuse, Software process line, Process variability management}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Feature location, Marlin, Bitcoin-wallet, Case study, Feature facets, Software product line}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {Domain engineering, conflicting configurations, constraint satisfaction problem, domain-specific language, extended feature model, model transformation chain},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@inproceedings{10.1145/1655925.1656013,
author = {Alsawalqah, Hamad I. and Abotsi, Komi S. and Lee, Dan Hyung},
title = {An automated mechanism for organizing and retrieving core asset artifacts for product derivation in SPL},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656013},
doi = {10.1145/1655925.1656013},
abstract = {Software Product Line, SPL, is a software development strategy in which products are developed from a common set of core assets in a prescribed way with product specific features to satisfy specific market segment [1]. The SPL development process is carried out in two phases: the first phase is about building core assets called domain engineering, which has gained a lot of researchers' attention. The second step is about instantiating the specifics of the products by adding to the common part the specific features that identify the product from the other application engineering. For large and complex domains, it is argued that organizing and retrieving the development of artifacts from the core asset required by the application under development is a way of shortening the application development time, thus reduces the time to market. In this paper, we propose an automation mechanism for organizing the core assets using feature based organization to divide the customized domain feature model based on the application features and their dependencies. When that retrieval step where the artifacts are represented by relations that inherit the dependencies between the features in each division of the feature model, takes place, the final result is a set of development artifacts with their traceability links to be customized based on the application variability model and integrated with the application specific artifacts. To demonstrate our work, we applied this mechanism on a watch, a case study in the digital watch domain.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {480–485},
numpages = {6},
keywords = {digital watch, feature model, ontology, product derivation, software product line},
location = {Seoul, Korea},
series = {ICIS '09}
}

@article{10.1007/s10270-019-00722-3,
author = {Schw\"{a}gerl, Felix and Westfechtel, Bernhard},
title = {Integrated revision and variation control for evolving model-driven software product lines},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00722-3},
doi = {10.1007/s10270-019-00722-3},
abstract = {Software engineering projects are faced with abstraction, which is achieved by software models, historical evolution, which is addressed by revision control, and variability, which is managed with the help of software product line engineering. Addressing these phenomena by separate tools ignores obvious overlaps and therefore fails at exploiting synergies between revision and variation control for models. In this article, we present a conceptual framework for integrated revision and variation control of model-driven software projects. The framework reuses the abstractions of revision graphs and feature models and follows an iterative, revision-control-like approach to software product line engineering called product-based product line development. A single version (i.e., a variant of a selected revision) is made available in a workspace, where the user may apply arbitrary modifications. Based on a user-provided specification of the affected variants, the changes are automatically written back to a transparent repository that relies on an internal multi-version storage. The uniform handling of revisions and variants of models is achieved by transparently mapping version concepts to a semantic base layer, which is defined upon propositional logic. At the heart of the conceptual framework is a dynamic filtered editing model, which allows that the versioned artifacts and the feature model co-evolve. We contribute algorithms for checkout and commit, which satisfy a set of consistency constraints referring to variant specifications in an evolving feature model. This article furthermore addresses the orchestration of collaborative development by distributed replication and the well formedness of text and model artifacts to be checked out into the workspace. The Eclipse-based tool SuperMod demonstrates the feasibility of the conceptual framework. It allows the user to reuse arbitrary editing tools for text-based programming and/or Ecore-based modeling languages. An evaluation based on three case studies investigates the properties of SuperMod with a specific focus on filtered editing. The evaluation demonstrates that the dynamic filtered editing model reduces the cognitive complexity and the amount of user interaction necessary for variation control when compared to unfiltered model-driven approaches to software product line engineering.},
journal = {Softw. Syst. Model.},
month = dec,
pages = {3373–3420},
numpages = {48},
keywords = {Model versioning, Model-driven product lines, Variation control systems, Tool integration, Integrated historical and logical versioning}
}

@article{10.1007/s10664-017-9499-z,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Reengineering legacy applications into software product lines: a systematic mapping},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9499-z},
doi = {10.1007/s10664-017-9499-z},
abstract = {Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2972–3016},
numpages = {45},
keywords = {Evolution, Legacy systems, Product family, Reengineering, Systematic reuse}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Variability, Configuration workflow, Process mining, Process discovery, Clustering}
}

@article{10.1016/j.jss.2007.06.002,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Industrial validation of COVAMOF},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.06.002},
doi = {10.1016/j.jss.2007.06.002},
abstract = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
journal = {J. Syst. Softw.},
month = apr,
pages = {584–600},
numpages = {17},
keywords = {Industrial validation, Product family engineering, Software Variability Management}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Feature model, Multi-step configuration, Software product line}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {BPM, Business process management, PL, Software product line}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Large-scale systems, Multi product lines, Product line engineering, Systematic literature review}
}

@article{10.1007/s42979-021-00899-9,
author = {Altunel, Haluk and Say, Bilge},
title = {Software Product System Model: A Customer-Value Oriented, Adaptable, DevOps-Based Product Model},
year = {2021},
issue_date = {Jan 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {1},
url = {https://doi.org/10.1007/s42979-021-00899-9},
doi = {10.1007/s42979-021-00899-9},
abstract = {DevOps pipelines have brought notable advantages, such as fast and frequent software delivery to software production paradigms, but dynamically dealing with quality attributes desired by the customer employing a DevOps pipeline remains a challenge. This work aims to define the design of a systems&nbsp;thinking inspired model, called Software Product System Model (SPSM), applying a customer-value oriented, holistic approach for implementing quality requirements, and its application and evaluation in a large software house. The main features include dynamic control of quality gates, the parameters of which are driven by customer requirements and feedback from surveys. All of the inputs are collected in a product backlog and fed forward to the quality gates over the DevOps pipeline. SPSM was successfully deployed in a large software house extending a DevOps pipeline with an accompanying improvement of customer-value oriented key performance indicators for projects. In a 2-year-long case study, security and code quality were the main quality attributes, with the metrics on security vulnerabilities and unit test coverage. At the end of the 2020, the DevOps pipeline within SPSM provided a 69.50% decrease of security vulnerabilities of all software products, and a 29.43% increase in unit test coverage for the whole code base for increasing code quality. At the end of 2020, the project completion ratio was measured to be 99.50% and the Schedule Performance Index (SPI) was measured to be 99.78% as the average of 762 projects delivered. The flexibility of SPSM allowed the software house to adapt to changing customer expectations. A checklist is provided for the replicability of the model application.},
journal = {SN Comput. Sci.},
month = nov,
numpages = {11},
keywords = {DevOps, Software Product System Model, Systems&nbsp;thinking, Software attributes, Customer value, Software metrics, Software product management}
}

@article{10.1016/j.infsof.2015.11.004,
author = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Javier Cabrerizo, Francisco and Herrera-Viedma, Enrique},
title = {A bibliometric analysis of 20 years of research on software product lines},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.11.004},
doi = {10.1016/j.infsof.2015.11.004},
abstract = {Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality.Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way.Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations.Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {1–15},
numpages = {15},
keywords = {Bibliometrics, Performance analysis, Science mapping, Software product lines}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {Java, feature-oriented software development, memory footprint, monitoring},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/2382756.2382783,
author = {do Carmo Machado, Ivan and McGregor, John D. and Santana de Almeida, Eduardo},
title = {Strategies for testing products in software product lines},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382783},
doi = {10.1145/2382756.2382783},
abstract = {The software product line engineering strategy enables the achievement of significant improvements in quality through reuse of carefully crafted software assets across multiple products. However, high levels of quality in the software product line assets, which are used to create products, must be accompanied by effective and efficient test strategies for the products in the software product line. The goal of this study is to understand which strategies for testing products in software product lines have been reported in the literature, enabling discussions on the significant issues, and also pointing out further research directions. A systematic literature review was carried out that identified two hundred seventy-three papers, published from the years 1998 and early in 2012. From such a set of papers, a systematic selection resulted in forty-one relevant papers. The analysis of the reported strategies comprised two important aspects: the selection of products for testing, and the actual test of products. The findings showed a range of strategies, dealing with both aspects, but few empirical evaluations of their effectiveness have been performed, which limits the inferences that can be drawn.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {1–8},
numpages = {8},
keywords = {software product lines, software testing, systematic review}
}

@inproceedings{10.1145/1138474.1138485,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Assessing merge potential of existing engine control systems into a product line},
year = {2006},
isbn = {1595934022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138474.1138485},
doi = {10.1145/1138474.1138485},
abstract = {Engine Control Systems (ECS) for automobiles have many variants for many manufactures and several markets. To improve their development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets this ECS business background. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing the strategy into existing products. Thereafter, a strategy will be derived systematically and realize the desired benefits.This paper reports an experience with the up-front investigation performed for Hitachi's ECS. We focus on the approach to plan the migration of the existing family of individual systems into a future product line. The approach assesses potential ways of merging software from existing variants and eventually defines a procedure for performing the migration. To get a high quality strategy, we integrate the approach of software measurement, the expertise of software architects, and reverse engineering techniques.},
booktitle = {Proceedings of the 2006 International Workshop on Software Engineering for Automotive Systems},
pages = {61–67},
numpages = {7},
keywords = {clone detection and classification, engine control systems, reverse engineering, software product line},
location = {Shanghai, China},
series = {SEAS '06}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguarat\~{a} Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Software product lines, Software quality, Software testing, Systematic literature review}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Software reuse, Trends in software reuse, Systematic literature review, Tertiary study}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {Model-based product-line engineering, UML/OCL, consistent configuration, constraint satisfaction techniques, formal specification, product configuration}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {DesignSmell, Antipatterns, Detection tools, Quality models, Systematic mapping study}
}

@article{10.1016/j.jss.2017.05.052,
author = {Bastos, Jonatas Ferreira and da Mota Silveira Neto, Paulo Anselmo and OLeary, Pdraig and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {Software product lines adoption in small organizations},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.052},
doi = {10.1016/j.jss.2017.05.052},
abstract = {Provides a better understanding of SPL adoption in the context of SMEs.A set of empirical studies performed in academic and industry environments.Inputs to establish guidelines for SPL adoption.A discussion of the evidences, with insights to guide future investigations. ContextAn increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. ObjectiveThe aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. MethodThis paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. ResultsThe study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption. ConclusionThis research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption.},
journal = {J. Syst. Softw.},
month = sep,
pages = {112–128},
numpages = {17},
keywords = {Adoption barriers, Case study, Mapping study, Multi-method approach, SPL adoption, Software product lines, Survey}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Feature model, Reverse engineering, Search Based Software Engineering}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Autonomic computing, Distributed and concurrent architecture, Elastic computing, Patterns, Performance, Reliability, Software architecture}
}

@article{10.1145/1363102.1363104,
author = {Mohagheghi, Parastoo and Conradi, Reidar},
title = {An empirical investigation of software reuse benefits in a large telecom product},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1363102.1363104},
doi = {10.1145/1363102.1363104},
abstract = {Background. This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods. Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results. We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions. The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {13},
numpages = {31},
keywords = {Software reuse, fault density, product family, risks, standardization}
}

@article{10.1016/j.compind.2008.03.003,
author = {Luo, Xinggang and Tu, Yiliu and Tang, Jiafu and Kwong, C. K.},
title = {Optimizing customer's selection for configurable product in B2C e-commerce application},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {8},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2008.03.003},
doi = {10.1016/j.compind.2008.03.003},
abstract = {Many companies provide configurable products on Internet to satisfy customers' diversified requirements. Most of business-to-consumer (B2C) e-commerce software systems use tree- or wizard-like approaches to guide customers in configuring a customized product on Internet web pages. However, customers may feel confused while they are selecting components of a product from option lists, since they are usually not familiar with the technical details of these components. A few e-commerce sites use recommendation systems to provide suggested products for customers, but they have to maintain user profiles and have limitations such as new user problem and complexity. Therefore, they may not be suitable for small and medium-sized enterprises. This research proposes a new approach to help customers configure their expected products. By using this approach, once a customer inputs the levels of importance of requirements, total budget of the expected product, the software system can figure out a customized product which maximally meets the customer's expectations, and can also provide the suboptimal solutions for further selections. A mathematical model to formulate this optimization problem is established. A case study is used to demonstrate the feasibility and effectiveness of this approach.},
journal = {Comput. Ind.},
month = oct,
pages = {767–776},
numpages = {10},
keywords = {Product configuration, Product family, Recommendation system, e-Commerce}
}

@inproceedings{10.1145/303008.303063,
author = {Bayer, Joachim and Flege, Oliver and Knauber, Peter and Laqua, Roland and Muthig, Dirk and Schmid, Klaus and Widen, Tanya and DeBaud, Jean-Marc},
title = {PuLSE: a methodology to develop software product lines},
year = {1999},
isbn = {1581131011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/303008.303063},
doi = {10.1145/303008.303063},
booktitle = {Proceedings of the 1999 Symposium on Software Reusability},
pages = {122–131},
numpages = {10},
keywords = {domain engineering, domain-specific software architecture, software product line},
location = {Los Angeles, California, USA},
series = {SSR '99}
}

@article{10.1007/s10515-010-0076-6,
author = {Dhungana, Deepak and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {The DOPLER meta-tool for decision-oriented variability modeling: a multiple case study},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0076-6},
doi = {10.1007/s10515-010-0076-6},
abstract = {The variability of a product line is typically defined in models. However, many existing variability modeling approaches are rigid and don't allow sufficient domain-specific adaptations. We have thus been developing a flexible and extensible approach for defining product line variability models. Its main purposes are to guide stakeholders through product derivation and to automatically generate product configurations. Our approach is supported by the DOPLER (  D ecision-  O riented  P roduct  L ine  E ngineering for effective  R euse) meta-tool that allows modelers to specify the types of reusable assets, their attributes, and dependencies for their specific system and context. The aim of this paper is to investigate the suitability of our approach for different domains. More specifically, we explored two research questions regarding the implementation of variability and the utility of DOPLER for variability modeling in different domains. We conducted a multiple case study consisting of four cases in the domains of industrial automation systems and business software. In each of these case studies we analyzed variability implementation techniques. Experts from our industry partners then developed domain-specific meta-models, tool extensions, and variability models for their product lines using DOPLER. The four cases demonstrate the flexibility of the DOPLER approach and the extensibility and adaptability of the supporting meta tool.},
journal = {Automated Software Engg.},
month = mar,
pages = {77–114},
numpages = {38},
keywords = {Decision models, Meta-tools, Product line engineering}
}

@article{10.1016/j.infsof.2008.04.002,
author = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
title = {Variability assessment in software product families},
year = {2009},
issue_date = {January, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.002},
doi = {10.1016/j.infsof.2008.04.002},
abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {195–218},
numpages = {24},
keywords = {Assessment, Evolution, Software product families, Variability}
}

@inproceedings{10.1145/1456659.1456662,
author = {Chapman, Mark and van der Merwe, Alta},
title = {Contemplating systematic software reuse in a project-centric company},
year = {2008},
isbn = {9781605582863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1456659.1456662},
doi = {10.1145/1456659.1456662},
abstract = {Systematic software reuse is still the most promising strategy for increasing productivity and improving quality in the software industry. Although it is simple in concept, successful software reuse implementation is difficult in practice. A reason put forward for this is the dependence of software reuse on the context in which it is implemented. This paper describes an interpretive case study aimed at investigating the potential for the implementation of systematic software reuse in a project-centric company. The study confirmed the need for systematic software reuse and identified the reuse issues that could present challenges. The study also revealed a number of problems relating to the project-centric structure for which systematic reuse provides potential solutions.},
booktitle = {Proceedings of the 2008 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries: Riding the Wave of Technology},
pages = {16–26},
numpages = {11},
keywords = {action research, ethnography, interpretive case study, project-centric, software product line engineering, software product lines, software reuse, systematic software reuse},
location = {Wilderness, South Africa},
series = {SAICSIT '08}
}

@inproceedings{10.5555/1158337.1158681,
author = {Scheidemann, Kathrin D.},
title = {Optimizing the Selection of Representative Configurations in Verification of Evolving Product Lines of Distributed Embedded Systems},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Electronics and computer science play a more and more prominent role in automotive technology. In the future the prevalence of those new technologies and the customers' demand for individuality will lead to tremendously large configuration spaces of vehicle control systems. To cope with the resulting complexity in verification, new strategies need to be explored. One likely future challenge will be to determine a set of vehicle configurations, such that the successful verification of this small set implies the correctness of the entire product family. This paper presents a method to address this task, based on exploiting communalities in architecture and requirements. We introduce efficient algorithms with provable quality guarantees for the optimization problems of choosing the minimum set of configurations necessary to verify all possible configurations and choosing the best k configurations to maximize the verification coverage of the entire product family. We discuss extensions of our method which allow requirement priorities and the consideration of configuration costs, and present a technique for automatically determining communalities in architecture and requirements which can be exploited by our optimization methods. We demonstrate the effectiveness of our method on an indicator light system product family. In this example a configuration reduction by 60% can be achieved.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {75–84},
numpages = {10},
series = {SPLC '06}
}

@article{10.1016/j.jss.2009.10.011,
author = {Sun, Chang-ai and Rossing, Rowan and Sinnema, Marco and Bulanov, Pavel and Aiello, Marco},
title = {Modeling and managing the variability of Web service-based systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.10.011},
doi = {10.1016/j.jss.2009.10.011},
abstract = {Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context. We present a framework and related tool suite for modeling and managing the variability of Web service-based systems for design and run-time, respectively. It is an extension of the COVAMOF framework for the variability management of software product families, which was developed at the University of Groningen. Among the novelties and advantages of the approach are the full modeling of variability via UML diagrams, the run-time support, and the low involvement of the user. All of which leads to a great deal of automation in the management of all kinds of variability.},
journal = {J. Syst. Softw.},
month = mar,
pages = {502–516},
numpages = {15},
keywords = {Service engineering, Variability management, Variability modeling, Web services}
}

@inproceedings{10.1145/2739011.2739017,
author = {Eloranta, Veli-Pekka and Lepp\"{a}nen, Marko},
title = {Patterns for distributed machine control systems},
year = {2013},
isbn = {9781450334655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739011.2739017},
doi = {10.1145/2739011.2739017},
abstract = {Control systems are getting more and more complex and include growing number of features. The control systems are nowadays software controlled to high degree. All features of the machine, however, are not useful for all customers and customers do not want to pay for the features they do not need. The product, the work machine, needs to be tailored to fit the customer's needs. However, tailoring the product for each customer is not an option as there would be too many control system software versions and configurations of the control system and for example updating the control system software would became impossible. In this paper, we will present three patterns for control systems to help in addressing the aforementioned problems. These patterns generate software architecture that supports varying the control system for different customer needs.},
booktitle = {Proceedings of the 18th European Conference on Pattern Languages of Program},
articleno = {6},
numpages = {15},
keywords = {architecture patterns, configuration management, distributed control systems, pattern language, product management},
location = {Irsee, Germany},
series = {EuroPLoP '13}
}

@article{10.1145/1163514.1178645,
author = {Sinnema, Marco and van der Ven, Jan Salvador and Deelstra, Sybren},
title = {Using variability modeling principles to capture architectural knowledge},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1163514.1178645},
doi = {10.1145/1163514.1178645},
abstract = {In the field of software architectures, there is an emerging awareness of the importance of architectural decisions. In this view, the architecting process is explained as a decision process, while the design and eventually the software system are seen as the result of this decision process. However, the effects of different alternatives on the quality of the system often remain implicit. In the field of software product families, the same issues arise when configuring products. We propose to use the proven expertise from COVAMOF, a framework for managing variability, to solve the issues that arise when relating quality attributes to architectural decisions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {5–es},
numpages = {6},
keywords = {architectural decisions, architectural knowledge, quality attributes}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Autonomic computing, Constraint programming, Dynamic adaptation, Dynamic software product line, Models at runtime, Variability, Verification, Web service composition}
}

@inproceedings{10.1145/2897045.2897047,
author = {da Mota Silveira Neto, Paulo Anselmo and de Santana, Taijara Loiola and de Almeida, Eduardo Santana and Cavalcanti, Yguarata Cerqueira},
title = {RiSE events: a testbed for software product lines experimentation},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897047},
doi = {10.1145/2897045.2897047},
abstract = {Software Product Lines (SPL) demand mature software engineering, planning and reuse, adequate practices of management and development, and also the ability to deal with organizational issues and architectural complexity. Thus, it is important the development of new techniques, tools and methods to deal with SPL complexity required by the variability management. To address this issue, an SPL has been proposed, where the existing variability was implemented by applying conditional compilation. Moreover, no framework was used to develop it, allowing any researcher to use the SPL without losing time learning some framework. In this work, we implemented an SPL test bed containing 34 functional features has 26.457 lines of code, 1493 methods and 496 classes.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {12–13},
numpages = {2},
keywords = {security and availability tacticts, software product lines, test bed, variability},
location = {Austin, Texas},
series = {VACE '16}
}

@article{10.1016/j.future.2019.04.032,
author = {Mousa, Afaf and Bentahar, Jamal and Alam, Omar},
title = {Context-aware composite SaaS using feature model},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.032},
doi = {10.1016/j.future.2019.04.032},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {376–390},
numpages = {15}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@article{10.1016/j.jss.2014.12.041,
author = {Pascual, Gustavo G. and Lopez-Herrejon, Roberto E. and Pinto, M\'{o}nica and Fuentes, Lidia and Egyed, Alexander},
title = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.041},
doi = {10.1016/j.jss.2014.12.041},
abstract = {Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
journal = {J. Syst. Softw.},
month = may,
pages = {392–411},
numpages = {20},
keywords = {DSPL, Dynamic reconfiguration, Evolutionary algorithms}
}

@article{10.1007/s10009-014-0362-x,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Mapping the design-space of textual variability modeling languages: a refined analysis},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0362-x},
doi = {10.1007/s10009-014-0362-x},
abstract = {Variability modeling is a major part of modern product line engineering. Graphical or table-based approaches to variability modeling are focused around abstract models and specialized tools to interact with these models. However, more recently textual variability modeling languages, comparable to some extent to programming languages, were introduced. We consider the recent trend in product line engineering towards textual variability modeling languages as a phenomenon, which deserves deeper analysis. In this article, we report on the results and approach of a literature survey combined with an expert study. In the literature survey, we identified 11 languages, which enable the textual specification of product line variability and which are sufficiently described for an in-depth analysis. We provide a classification scheme, useful to describe the range of capabilities of such languages. Initially, we identified the relevant capabilities of these languages from a literature survey. The result of this has been refined, validated and partially improved by the expert survey. A second recent phenomenon in product line variability modeling is the increasing scale of variability models. Some authors of textual variability modeling languages argue that these languages are more appropriate for large-scale models. As a consequence, we would expect specific capabilities addressing scalability in the languages. Thus, we compare the capabilities of textual variability modeling techniques, if compared to graphical variability modeling approaches and in particular to analyze their specialized capabilities for large-scale models.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {559–584},
numpages = {26},
keywords = {Domain-specific languages, Scalability, Survey, Variability modeling}
}

@article{10.1147/JRD.2010.2050539,
author = {Molloy, C. and Iqbal, M.},
title = {Improving data-center efficiency for a smarter planet},
year = {2010},
issue_date = {July 2010},
publisher = {IBM Corp.},
address = {USA},
volume = {54},
number = {4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2010.2050539},
doi = {10.1147/JRD.2010.2050539},
abstract = {In 2009, IBM launched its Smarter Planet™ initiative, which is based on the paradigm that virtually any physical object, process, or system can be instrumented, interconnected, and infused with intelligence. Because of the increased demand for information technology (IT) to facilitate Smarter Planet solutions, physical data centers have become interconnected, instrumented, and intelligent (i.e., adaptable, scalable, energy efficient, and cost effective). "Green data centers," which are discussed in this paper, are those that make use of facilities and IT integration, resulting in lower energy costs, reduced carbon footprint, and reduced demand for power, space, and cooling resources. This paper reviews energy-efficiency strategies that are being incorporated in eight million square feet of data-center space that IBM operates in support of its customers. These strategies include both a dynamic infrastructure IT initiative and an energy-efficiencies pillar of the IBM New Enterprise Data Center initiative. For example, compared to nonmodular designs, implementation of a modular design to optimize expandable data centers will better match the IT demand with the data-center energy supply. This high-level overview describes the integration of these strategies to create a class of leading-edge IBM data centers and a plan for the optimization of existing data centers.},
journal = {IBM J. Res. Dev.},
month = jul,
pages = {388–395},
numpages = {8}
}

@inproceedings{10.1145/2695664.2695743,
author = {Garcia, Cleiton and Paludo, Marco and Malucelli, Andreia and Reinehr, Sheila},
title = {A software process line for service-oriented applications},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695743},
doi = {10.1145/2695664.2695743},
abstract = {The management of processes and systems is a complex and time-consuming activity for organizations and also an ongoing Information Technology (IT) challenge. Among the different approaches for bringing flexibility to the business processes and systems are Service-Oriented Architecture (SOA) and Business Process Management (BPM). The SOA approach has become popular providing services and interfaces, enabling integration of heterogeneous and distributed platforms and BPM leverages the cycles of improvements, control and evaluation of business processes. BPM and SOA should work together aiming at improving business processes and evolving systems architecture. One main problem to apply BPM and SOA is the lack of established processes and this work proposes a software process line in order to simplify variability control and enable the instantiation of new development process applying BPM and SOA. It also aims at developing an environment to support the proposed software process line in order to automate the process, integrating industrial tools with one specifically developed to perform the transformation of UMA models into BPM notation. The main contribution of this work is the definition of the software process line for engineering service-oriented products. It is highly relevant to software industry since software process lines lacks experiments, practices and tools.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1680–1687},
numpages = {8},
keywords = {BPM, SOA, SPL, software process lines, web-based services},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Domain-specific languages, Language concern, Language reuse}
}

@article{10.1016/j.infsof.2019.106198,
author = {Assun\c{c}\~{a}o, Wesley K.G. and Vergilio, Silvia R. and Lopez-Herrejon, Roberto E.},
title = {Automatic extraction of product line architecture and feature models from UML class diagram variants},
year = {2020},
issue_date = {Jan 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {117},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106198},
doi = {10.1016/j.infsof.2019.106198},
journal = {Inf. Softw. Technol.},
month = jan,
numpages = {19},
keywords = {Model merging, Feature model, SPL architecture, Search-based techniques}
}

@article{10.1007/s11219-014-9258-y,
author = {Galindo, Jos\'{e} A. and Turner, Hamilton and Benavides, David and White, Jules},
title = {Testing variability-intensive systems using automated analysis: an application to Android},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9258-y},
doi = {10.1007/s11219-014-9258-y},
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
journal = {Software Quality Journal},
month = jun,
pages = {365–405},
numpages = {41},
keywords = {Android, Automated analysis, Feature models, Software product lines, Testing}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Component-based software engineering, Sourcing, Software architecture, Decision making, Survey}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Composition, Feature models, Scientific workflows, Software product lines}
}

@inproceedings{10.1007/978-3-030-58920-2_12,
author = {Bressan, Lucas and de Oliveira, Andr\'{e} L. and Campos, Fernanda and Papadopoulos, Yiannis and Parker, David},
title = {An Integrated Approach to Support the Process-Based Certification of Variant-Intensive Systems},
year = {2020},
isbn = {978-3-030-58919-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58920-2_12},
doi = {10.1007/978-3-030-58920-2_12},
abstract = {Component-based approaches and software product lines have been adopted by industry to manage the diversity of configurations on safety-critical software. Safety certification demands compliance with standards. ISO 26262 standard uses the concept of Automotive Safety Integrity Level (ASIL) to allocate safety requirements to components of a system under design. Compliance with standards is demonstrated through achieving those ASILs which can be very expensive when requirements are high. While achieving safety certification of variant-intensive components without being unnecessarily stringent or expensive is desirable for economy, it poses challenges to safety engineering. In this paper, we propose an approach to manage the diversity of safety goals and supporting safety certification of software components. Our approach is built upon the integration among ASIL decomposition, software process modeling, and variability management techniques. The approach supports cost-effective safety certification and the efficient tailoring of process models to components according to their ASILs. We evaluated our approach in the automotive domain. The approach is feasible in supporting the management of the diversity of safety goals, and cost-effective safety certification of software components.},
booktitle = {Model-Based Safety and Assessment: 7th International Symposium, IMBSA 2020, Lisbon, Portugal, September 14–16, 2020, Proceedings},
pages = {179–193},
numpages = {15},
keywords = {Safety certification, Safety critical software, Software development process, Model-based engineering},
location = {Lisbon, Portugal}
}

@article{10.1016/j.jss.2019.110429,
author = {Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
title = {A survey on clone refactoring and tracking},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110429},
doi = {10.1016/j.jss.2019.110429},
journal = {J. Syst. Softw.},
month = jan,
numpages = {27},
keywords = {Code clones, Clone-types, Clone refactoring, Clone tracking}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification}
}

@inproceedings{10.1145/3377024.3377033,
author = {Santos, Edilton Lima dos and Perrouin, Gilles and Schobbens, Pierre-Yves},
title = {STARS: software technology for adaptable and reusable systems PhD research project},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377033},
doi = {10.1145/3377024.3377033},
abstract = {Dynamically Adaptive Systems (DAS) modify their behaviours in response to (sometimes unpredictable) changes in their environment or to the evolution of their own abilities (sensors and actuators). To support adaptation, a reference architecture (such as the MAPE-K model) is paramount. Yet, this is not sufficient as challenges concerning the fine-grained variability management and testability remain. The STARS Ph.D. project aims at developing a variability and context-aware architectural model for DAS that particularly takes into account testability. In particular, we want to adapt tests at runtime in order to assess and prevent the impact of inappropriate adaptations.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {2},
keywords = {MAPE-K loop, self-adapting, software architecture, software test},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.jss.2019.03.027,
author = {Xu, Zhou and Li, Shuai and Luo, Xiapu and Liu, Jin and Zhang, Tao and Tang, Yutian and Xu, Jun and Yuan, Peipei and Keung, Jacky},
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.027},
doi = {10.1016/j.jss.2019.03.027},
journal = {J. Syst. Softw.},
month = aug,
pages = {59–78},
numpages = {20},
keywords = {Cross version defect prediction, Spare modeling, Training subset selection, Weighted extreme learning machine, 00–01, 99-00}
}

@article{10.1007/s10270-017-0594-9,
author = {Famelis, Michalis and Chechik, Marsha},
title = {Managing design-time uncertainty},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0594-9},
doi = {10.1007/s10270-017-0594-9},
abstract = {Managing design-time uncertainty, i.e., uncertainty that developers have about making design decisions, requires creation of "uncertainty-aware" software engineering methodologies. In this paper, we propose a methodological approach for managing uncertainty using partial models. To this end, we identify the stages in the lifecycle of uncertainty-related design decisions and characterize the tasks needed to manage it. We encode this information in the Design-Time Uncertainty Management (DeTUM) model. We then use the DeTUM model to create a coherent, tool-supported methodology centred around partial model management. We demonstrate the effectiveness and feasibility of our methodology through case studies.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1249–1284},
numpages = {36},
keywords = {Design space management, Software design, Software methodology, Software modelling, Uncertainty}
}

@inproceedings{10.1007/978-3-642-34026-0_12,
author = {Asirelli, Patrizia and ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {A compositional framework to derive product line behavioural descriptions},
year = {2012},
isbn = {9783642340253},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34026-0_12},
doi = {10.1007/978-3-642-34026-0_12},
abstract = {Modelling variability in product families has been the subject of extensive study in the literature on Software Product Lines, especially that concerning Feature Modelling. In recent years, we have laid the basis for the study of the application of temporal logics to the formal modelling of behavioural variability in product family definitions. A critical point in this formalization is to give an adequate representation of the elements of the feature model and their relation with the behaviour of the many products that are to be derived from the family. To this aim, we propose a methodology to systematize this step as much as possible, in order to allow the derivation of behavioural models that are general enough to capture the behaviour of all consistent products belonging to the family.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Technologies for Mastering Change - Volume Part I},
pages = {146–161},
numpages = {16},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@inproceedings{10.1145/111062.111068,
author = {Nicklin, Peter J.},
title = {Managing multi-variant software configuration},
year = {1991},
isbn = {0897914295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/111062.111068},
doi = {10.1145/111062.111068},
booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
pages = {53–57},
numpages = {5},
location = {Trondheim, Norway},
series = {SCM '91}
}

@inproceedings{10.5555/1885639.1885651,
author = {Nolan, Andy J. and Abrah\~{a}o, Silvia},
title = {Dealing with cost estimation in software product lines: experiences and future directions},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {After 5 years invested in developing accurate cost estimation tools, Rolls-Royce has learnt about the larger potential of the tools to shape many aspects of the business. A good estimation tool is a "model" of a project and is usually used to estimate cost and schedule, but it can also estimate and validate risks and opportunities. Estimation tools have unified engineering, project and business needs. The presence of good estimation tools has driven higher performance and stability in the business. It was evident we needed this capability to underpin decisions in our new Software Product Line strategy. The objective of this paper is twofold. First, we report the experiences gained in the past on the use of estimation tools. Second, we describe the current efforts and future directions on the development of an estimation tool for Software Product Lines. At the heart of the Product Line estimation tool is a simple representation of the product - represented as the number of Lines Of Code (LOC). The next generation of tool, will need to consider wider aspects of product quality in order to create more accurate estimates and support better decisions about our products.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {121–135},
numpages = {15},
keywords = {cost estimation, industrial experiences, software product lines},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.infsof.2012.06.005,
author = {Hutchesson, Stuart and Mcdermid, John},
title = {Trusted Product Lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.005},
doi = {10.1016/j.infsof.2012.06.005},
abstract = {Context: The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority. Objective: The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation. Method: The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this. Results: The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper. Conclusion: Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {525–540},
numpages = {16},
keywords = {DO-178B/ED-12B, GSN, High-integrity software, Model transformation, SPARK, Software Product Lines}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Evolution, Software product lines, Systematic mapping study}
}

@article{10.1016/j.future.2018.12.025,
author = {Cao, Yang and Lung, Chung-Horng and Ajila, Samuel A. and Li, Xiaolin},
title = {Support mechanisms for cloud configuration using XML filtering techniques: A case study in SaaS},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.12.025},
doi = {10.1016/j.future.2018.12.025},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {52–67},
numpages = {16},
keywords = {Cloud Computing, Software-as-a-Service, Multi-Tenancy, Feature Modeling, XML Filtering, Yfilter}
}

@inproceedings{10.1109/ICSE43902.2021.00029,
author = {Zhang, Yuanliang and He, Haochen and Legunsen, Owolabi and Li, Shanshan and Dong, Wei and Xu, Tianyin},
title = {An Evolutionary Study of Configuration Design and Implementation in Cloud Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00029},
doi = {10.1109/ICSE43902.2021.00029},
abstract = {Many techniques were proposed for detecting software misconfigurations in cloud systems and for diagnosing unintended behavior caused by such misconfigurations. Detection and diagnosis are steps in the right direction: misconfigurations cause many costly failures and severe performance issues. But, we argue that continued focus on detection and diagnosis is symptomatic of a more serious problem: configuration design and implementation are not yet first-class software engineering endeavors in cloud systems. Little is known about how and why developers evolve configuration design and implementation, and the challenges that they face in doing so.This paper presents a source-code level study of the evolution of configuration design and implementation in cloud systems. Our goal is to understand the rationale and developer practices for revising initial configuration design/implementation decisions, especially in response to consequences of misconfigurations. To this end, we studied 1178 configuration-related commits from a 2.5 year version-control history of four large-scale, actively-maintained open-source cloud systems (HDFS, HBase, Spark, and Cassandra). We derive new insights into the software configuration engineering process. Our results motivate new techniques for proactively reducing misconfigurations by improving the configuration design and implementation process in cloud systems. We highlight a number of future research directions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {188–200},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3183399.3183403,
author = {Xu, Tianyin and Marinov, Darko},
title = {Mining container image repositories for software configuration and beyond},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183403},
doi = {10.1145/3183399.3183403},
abstract = {This paper introduces the idea of mining container image repositories for configuration and other deployment information of software systems. Unlike traditional software repositories (e.g., source code repositories and app stores), image repositories encapsulate the entire execution ecosystem for running target software, including its configurations, dependent libraries and components, and OS-level utilities, which contributes to a wealth of data and information. We showcase the opportunities based on concrete software engineering tasks that can benefit from mining image repositories. To facilitate future mining efforts, we summarize the challenges of analyzing image repositories and the approaches that can address these challenges. We hope that this paper will stimulate exciting research agenda of mining this emerging type of software repositories.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {49–52},
numpages = {4},
keywords = {configuration, container, docker, image, software repository},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@article{10.1016/j.jss.2014.01.021,
author = {Walraven, Stefan and Van Landuyt, Dimitri and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
title = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.021},
doi = {10.1016/j.jss.2014.01.021},
abstract = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
journal = {J. Syst. Softw.},
month = may,
pages = {48–62},
numpages = {15},
keywords = {Multi-tenancy, SaaS, Variability}
}

@article{10.4018/IJCSSA.2015070103,
author = {Kolagari, Ramin Tavakoli and Chen, DeJiu and Lanusse, Agnes and Librino, Renato and L\"{o}nn, Henrik and Mahmud, Nidhal and Mraidha, Chokri and Reiser, Mark-Oliver and Torchiaro, Sandra and Tucci-Piergiovanni, Sara and W\"{a}gemann, Tobias and Yakymets, Nataliya},
title = {Model-Based Analysis and Engineering of Automotive Architectures with EAST-ADL: Revisited},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015070103},
doi = {10.4018/IJCSSA.2015070103},
abstract = {Modern cars have turned into complex high-technology products, subject to strict safety and timing requirements, in a short time span. This evolution has translated into development processes that are not as efficient, flexible and agile as they could or should be. This paper presents the main aspects and capabilities of a rich model-based design framework, founded on EAST-ADL. EAST-ADL is an architecture description language specific to the automotive domain and complemented by a methodology compliant with the functional safety standard for the automotive domain ISO26262. The language and the methodology are used to develop an information model in the sense of a conceptual model, providing the engineer the basis for specifying the various aspects of the system. Inconsistencies, redundancies, and partly even missing system description aspects can be found automaticlally by advanced analyses and optimization capabilities to effectively improve development processes of modern cars.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jul,
pages = {25–70},
numpages = {46},
keywords = {AUTOSAR, Automotive Software Development, Dependability, EAST-ADL, Functional Safety, ISO 26262, Model-Based Software Development, Optimization, Software Product Lines, Timing Modelling}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Automated production systems, Automation, Evolution, Software engineering}
}

@inproceedings{10.1145/2568225.2568267,
author = {Salay, Rick and Famelis, Michalis and Rubin, Julia and Di Sandro, Alessio and Chechik, Marsha},
title = {Lifting model transformations to product lines},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568267},
doi = {10.1145/2568225.2568267},
abstract = {Software product lines and model transformations are two techniques used in industry for managing the development of highly complex software. Product line approaches simplify the handling of software variants while model transformations automate software manipulations such as refactoring, optimization, code generation, etc. While these techniques are well understood independently, combining them to get the benefit of both poses a challenge because most model transformations apply to individual models while model-level product lines represent sets of models. In this paper, we address this challenge by providing an approach for automatically ``lifting'' model transformations so that they can be applied to product lines. We illustrate our approach using a case study and evaluate it through a set of experiments.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {117–128},
numpages = {12},
keywords = {Model Driven Engineering, Model Transformations, Software Product Lines},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@inproceedings{10.1145/375212.375269,
author = {Gacek, Critina and Anastasopoules, Michalis},
title = {Implementing product line variabilities},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375269},
doi = {10.1145/375212.375269},
abstract = {Software product lines have numerous members. Thus, a product line infrastructure must cover various systems. This is the significant difference to usual software systems and the reason for additional requirements on the various assets present during software product line engineering. It is imperative that they support the description of the product line as a whole, as well as its instantiation for the derivation of individual products.Literature has already addressed how to create and instantiate generic product line assets, such as domain models and architectures to generate instance specific ones [1, 2, 3], yet little attention has been given on how to actually deal with this genericity at the code level.This paper addresses the issue of handling product line variability at the code level. To this end various implementation approaches are examined with respect to their use in a product line context.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {109–117},
numpages = {9},
keywords = {implementation approaches, implementing variabilities, product line variability, software product lines, traceability},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {cross-project defect prediction, transfer learning, balancing distribution, effort-aware indicator}
}

@inproceedings{10.1145/3468264.3468578,
author = {Oh, Jeho and Y\i{}ld\i{}ran, Necip Faz\i{}l and Braha, Julian and Gazzillo, Paul},
title = {Finding broken Linux configuration specifications by statically analyzing the Kconfig language},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468578},
doi = {10.1145/3468264.3468578},
abstract = {Highly-configurable software underpins much of our computing infrastructure. It enables extensive reuse, but opens the door to broken configuration specifications. The configuration specification language, Kconfig, is designed to prevent invalid configurations of the Linux kernel from being built. However, the astronomical size of the configuration space for Linux makes finding specification bugs difficult by hand or with random testing. In this paper, we introduce a software model checking framework for building Kconfig static analysis tools. We develop a formal semantics of the Kconfig language and implement the semantics in a symbolic evaluator called kclause that models Kconfig behavior as logical formulas. We then design and implement a bug finder, called kismet, that takes kclause models and leverages automated theorem proving to find unmet dependency bugs. kismet is evaluated for its precision, performance, and impact on kernel development for a recent version of Linux, which has over 140,000 lines of Kconfig across 28 architecture-specific specifications. Our evaluation finds 781 bugs (151 when considering sharing among Kconfig specifications) with 100% precision, spending between 37 and 90 minutes for each Kconfig specification, although it misses some bugs due to underapproximation. Compared to random testing, kismet finds substantially more true positive bugs in a fraction of the time.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {893–905},
numpages = {13},
keywords = {Kconfig, formal verification, software configuration, static analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1504/IJKESDP.2013.052716,
author = {Elfaki, Abdelrahman Osman and Fong, Sim Liew and Aik, Kevin Loo Teow and Johar, Md Gapar Md},
title = {Towards detecting redundancy in domain engineering process using first order logic rules},
year = {2013},
issue_date = {March 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {1},
issn = {1755-3210},
url = {https://doi.org/10.1504/IJKESDP.2013.052716},
doi = {10.1504/IJKESDP.2013.052716},
abstract = {Software product line SPL is an emerging methodology for developing software products. SPL consists of two processes: domain-engineering and application-engineering. Successful software product is highly dependent on the validity of a domain engineering process. Therefore, validation is a significant process within the domain-engineering. Anomalies such as dead feature, redundancy, and wrong-cardinality are well-known problems in SPL. In the literature, redundancy did not take the signs of attentions as a dead feature and wrong-cardinality. The maturity of the SPL can be enhanced by detecting and removing the redundancy from the domain engineering. This paper proposes first order logic FOL rules for detecting the redundancy in domain-engineering process. Detecting redundancy in the domain engineering direct is our contribution. Our methodology comprised of three steps: 1 variability is modelled in the form of predicates as a prerequisite; 2 for each type of the redundancy, a general form is formulated to swathe all possible cases; 3 FOL rules are illustrated to implement each possibility based on deducing the results from predefined cases. As a result, all forms of redundancies in the domain-engineering process are amorphous. Finally, experiments are conducted to attest the scalability of our method.},
journal = {Int. J. Knowl. Eng. Soft Data Paradigm.},
month = mar,
pages = {1–20},
numpages = {20}
}

@inproceedings{10.1145/3084226.3084253,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {Evaluating Software Architecture Evaluation Methods: An Internal Replication},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084253},
doi = {10.1145/3084226.3084253},
abstract = {Context: The size and complexity of software systems along with the demand for ensuring quality requirements have fostered the interest in software architecture evaluation methods. Although several empirical studies have been reported, the actual body of knowledge is still insufficient. To address this concern, we presented a family of four controlled experiments that compares a recently proposed method, the Quality-Driven Architecture Derivation and Improvement (QuaDAI) method against the well-known Architecture Tradeoff Analysis Method (ATAM).Objective: To provide further evidence on the efficiency, effectiveness, and perceived satisfaction of participants using these two software architecture evaluation methods. We report the results of a differentiated internal replication study.Method: The same materials used in the baseline experiments were employed in this replication but the participants were sixteen practitioners. In addition, we used a simpler design to reduce the treatments' application sequences.Results: The participants obtained architectures with better quality when applying QuaDAI, and they found this method to be more useful and likely to be used than ATAM, but no difference in terms of efficiency and perceived ease of use were found.Conclusions: The results are in line with the baseline experiments and support the hypothesis that QuaDAI achieve better results than ATAM when performing architectural evaluations; however, further work is need to improve the methods usability.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {144–153},
numpages = {10},
keywords = {Experiment Replication, Software Architecture Evaluation},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.5555/2666064.2666076,
author = {Hu, Jie and Yang, Ye and Wang, Qing and Ruhe, Guenther and Wang, Haitao},
title = {Value-based portfolio scoping: an industrial case study},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Customization is considered as a promising way for better satisfying diversity of customer needs. In organizations short of resources, it is a frequent challenge to get balance between development and customization workload in order to ensure product success as well as customer satisfaction. In this paper, we proposed a value-based product portfolio scoping approach to determine optimal product scale for planning software product line adoption. The approach blends existing methods in domain analysis, requirements clustering, and valuation theory. An industrial case study is presented to demonstrate the application of the approach and its effectiveness.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {45–48},
numpages = {4},
keywords = {cost benefit, customization, product line, product portfolio, requirements analysis, scoping},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {MAPE-K, autonomic elements, context, goals, off-line training, on-line, variability, variants, variation-points},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1016/j.infsof.2016.12.004,
author = {Vale, Tassio and de Almeida, Eduardo Santana and Alves, Vander and Kulesza, Uir and Niu, Nan and de Lima, Ricardo},
title = {Software product lines traceability},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {84},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.12.004},
doi = {10.1016/j.infsof.2016.12.004},
abstract = {This paper analyzes 62 SPL traceability studies dating from 2001 to 2015.We discuss goals, strategies, domains, research intensity and challenges of the area.This work identifies gaps in research which should be addressed.It identifies common practices and areas for further improvements as well. Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for SPL traceability.Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of SPL traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research.Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review.Results: This work provides a structured understanding of SPL traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous SPL traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of SPL traceability in practice.Conclusions: This work concludes that SPL traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for SPL and single-system development.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {1–18},
numpages = {18},
keywords = {Software and systems traceability, Software product lines, Software reuse, Systematic mapping study}
}

@article{10.1016/j.jvlc.2013.08.001,
author = {Anjorin, Anthony and Saller, Karsten and Reimund, Ingo and Oster, Sebastian and Zorcic, Ivan and Sch\"{u}rr, Andy},
title = {Model-driven rapid prototyping with programmed graph transformations},
year = {2013},
issue_date = {December, 2013},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {24},
number = {6},
issn = {1045-926X},
url = {https://doi.org/10.1016/j.jvlc.2013.08.001},
doi = {10.1016/j.jvlc.2013.08.001},
abstract = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations. Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems.},
journal = {J. Vis. Lang. Comput.},
month = dec,
pages = {441–462},
numpages = {22},
keywords = {Metamodelling, Model-driven testing, Programmed graph transformations, Rapid prototyping, Software product lines}
}

@article{10.3233/KES-170356,
author = {Alidra, Abdelghani and Kimour, Mohamed Tahar},
title = {Adapting large pervasive and context-aware systems. A new evolutionary-based approach},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-170356},
doi = {10.3233/KES-170356},
abstract = {In order to enable ``anywhere, anytime'' computing, pervasive
systems must autonomously adapt at runtime. The use of dynamic software
product lines has emerged as a promising paradigm where well established
variability management techniques are leveraged at runtime to describe
evolution strategies and adaptation scenarios in terms of combinations of
features. In order to identify the optimal target configuration of the
system under certain circumstances, most existing approaches generate the
set of valid combinations of features and return the best one. Obviously,
while such approaches are well suited to small systems with a reduced number
of configurations, they fail in the case of large modern pervasive systems
because the generation/evaluation of all valid combinations is very costly
in terms of resources and time consumption. In the present article, we
introduce a new scalable, evolutionary-based approach to runtime adaptation
of pervasive systems. To this end, we define the concept of transitive
dependency between features and we exploit it to fasten the generation of
the optimal configuration of the system. We evaluate the scalability of our
proposal by reporting experimental results that show that our genetic
algorithm converges in up to 90% less time than the one from the
literature while preserving the exploration capabilities and solutions
quality. Finally, we illustrate our proposal on the smart homes use case.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {103–121},
numpages = {19},
keywords = {Context-awareness, pervasive systems, online adaptation, dynamic software product lines, features transitive dependencies, genetic algorithms}
}

@inproceedings{10.5555/1885639.1885676,
author = {Mannion, Mike and Savolainen, Juha},
title = {Aligning business and technical strategies for software product lines},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A successful software product line strategy has business goals, a business strategy, a target market and a technical strategy that is aligned with the business goals and the target market. A common challenge in a number of organizations is for business and engineering units to understand what business and technical strategy alignment actually means in practice and to maintain that alignment as business goals and target markets evolve. If they are misaligned, then at best significant development inefficiencies occur, and at worst there is loss of market share. This paper explains different business and technical strategies, describes commonly used engineering techniques to manage commonality and variability and their deployment under different strategies.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {406–419},
numpages = {14},
keywords = {business alignment, business strategy, feature modeling, product lines, software architecture},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1007/s11042-019-7478-7,
author = {Liu, Yuling},
title = {RETRACTED ARTICLE: Research on multimedia play mode and image optimization based on compensation factor adaptive model},
year = {2020},
issue_date = {Apr 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {13–14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7478-7},
doi = {10.1007/s11042-019-7478-7},
abstract = {When traditional multimedia network video image is compressed and transmitted to compensate, because of the different loss of video image features in the acquisition process, the error of compressed transmission compensation is large and the efficiency is low. Firstly, the NLMS algorithm and the improved NLMS algorithm are analyzed. To solve the problem that the compensation factor in the algorithm is too large due to the severe network shake, the NLMS algorithm is further improved by adaptively adjusting the compensation factor coefficient with the change of the network and the prediction error. The research shows that the multimedia playback mode and the image optimization system software structure based on the compensation factor adaptive model realize the dynamic adaptation of the system to various network conditions and the image optimization function during video playback by adopting the bandwidth adaptive strategy and method.. The conclusion shows that in the multimedia network video image compression transmission, the improved compensation method has higher performance in multimedia network video image optimization and real-time compression transmission compensation, which has certain advantages compared with the traditional compensation method.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {9315–9330},
numpages = {16},
keywords = {Compensation factor self-adaptation, Multimedia playback, Image optimization}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {evolutionary algorithms, genetic improvement, genetic programming, software product lines, variability},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2641483.2641542,
author = {Alrashoud, Mubarak and Ahmed, Lubaid and Abhari, Abdolreza},
title = {Binary Linear Programming-based Release Planning for Multi-tenant Business SaaS},
year = {2014},
isbn = {9781450327121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641483.2641542},
doi = {10.1145/2641483.2641542},
abstract = {In multi-tenant Software as a Service (SaaS) business software, the degree of tenants' satisfaction is a significant indicator of the success of the SaaS system. Tenants' satisfaction can be achieved by continuously fulfilling their evolving needs. Usually, SaaS providers frequently deliver new releases of the application. Each release contains new or enhanced features. However, SaaS providers have limited resources, which makes it difficult to them to incorporate all of the tenants' requests in the next release. Therefore, some requirements shall be postponed to later releases. In order to achieve the highest possible level of tenets' satisfaction, SaaS providers shall include the most common requirements in the next release, which guarantee the satisfaction of highest possible number of tenants with less effort. Additionally, tenants' priorities and preferences about the requirements must be considered. Besides maximizing tenants' satisfaction, it is crucial to meet different types of constraints such as resource, technical, and contractual constraints. This paper identifies the factors that govern the release planning process for multi-tenant business software, which are contractual constraints, commonality of requirements, tenants' preferences and decision weights, risk, technical constraints. The first two factors are suggested by this paper, while the remaining factors are inherited from the traditional release planning process. Moreover, this paper proposes a framework that deals with the uniqueness of the release planning process in multi-tenant SaaS system. In this framework, Binary Linear Programming (BLP) is employed to optimize the selection process of the requirements that will be implemented in the next release. An experiments section is provided to illustrate the degree of satisfaction that can be achieved using the proposed framework.},
booktitle = {Proceedings of the 2014 International C* Conference on Computer Science &amp; Software Engineering},
articleno = {15},
numpages = {8},
keywords = {SaaS development, SaaS requirements engineering, Software release planning, applications of binary linear programming, requirements prioritization},
location = {Montreal, QC, Canada},
series = {C3S2E '14}
}

@inproceedings{10.1145/3106237.3106283,
author = {Gazzillo, Paul},
title = {Kmax: finding all configurations of Kbuild makefiles statically},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106283},
doi = {10.1145/3106237.3106283},
abstract = {Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles. We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {279–290},
numpages = {12},
keywords = {Configuration, Kbuild, Kmax, Makefiles, Static Analysis, Variability},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1109/ASE.2015.47,
author = {Li, Yi and Rubin, Julia and Chechik, Marsha},
title = {Semantic slicing of software version histories},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.47},
doi = {10.1109/ASE.2015.47},
abstract = {Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, "inheriting" additional, unwanted functionality.In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {686–696},
numpages = {11},
keywords = {dependency, software changes, version history},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3141848.3141850,
author = {Weckesser, Markus and Lochau, Malte and Ries, Michael and Sch\"{u}rr, Andy},
title = {Towards complete consistency checks of Clafer models},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141850},
doi = {10.1145/3141848.3141850},
abstract = {Clafer is a general purpose modeling language that combines UML-like class and meta-modeling with feature-oriented variability modeling and first-order logic constraints. The considerable expressiveness of Clafer makes automated reasoning about properties like model consistency (i.e., finding a valid model instance) very challenging. In particular, multiplicity annotations and recursive model structures yield a potentially unbounded number of model instances resulting in an infinite search space. Existing approaches for consistency checking encode Clafer models into finite constraint-satisfaction problems by either manually or heuristically, setting bounds for the search space. Hence, if no valid model instance has been found, it is unknown whether the model is inconsistent, or whether the bounds have been chosen too tight. In this paper, we characterize a restricted sub-language of Clafer with complex inheritance relations that is crucial for facilitating sound and complete model-consistency checking. To this end, we present the idea of a novel technique for automated search-space restriction, by flattening Clafer models and encoding them as Integer Linear Programs (ILP). Our evaluation shows very promising results of our approach in terms of runtime efficiency for both flattening of complex inheritance hierarchies as well as sound and complete consistency checking.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {11–20},
numpages = {10},
keywords = {Automated Validation, Clafer, Integer Linear Programming},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@inproceedings{10.5555/648114.748920,
author = {Hallsteinsen, Svein O. and Bass, Leonard J.},
title = {Platform &amp; Quality Solutions},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {164–168},
numpages = {5},
series = {PFE '01}
}

@article{10.1016/j.compind.2015.04.004,
author = {Kristianto, Yohanes and Helo, Petri and Jiao, Roger Jianxin},
title = {A system level product configurator for engineer-to-order supply chains},
year = {2015},
issue_date = {September 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {72},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2015.04.004},
doi = {10.1016/j.compind.2015.04.004},
abstract = {System level configuration is a common need for engineering businesses.Engineer to order processes and incomplete configurations present challenges.A prototype system for system level configuration management is presented. Supply chains in construction, infrastructure building, ship building, factory design and conveyor systems are operating in an engineer-to-order type of environment. Companies in these project-based businesses have special requirements for product configuration. Products have configuration dependencies with each other and there are system level configuration dependencies between several products. Incomplete product configuration items that are subject to change or require engineering work prior to production can occur. This paper introduces the requirements for system level configuration and proposes a prototype solution for ship projects and engine-room related supply chains.},
journal = {Comput. Ind.},
month = sep,
pages = {82–91},
numpages = {10},
keywords = {Engineer-to-order, Product configurator, Systems}
}

@inproceedings{10.1145/2897045.2897051,
author = {Perrouin, Gilles and Acher, Mathieu and Davril, Jean-Marc and Legay, Axel and Heymans, Patrick},
title = {A complexity tale: web configurators},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897051},
doi = {10.1145/2897045.2897051},
abstract = {Online configurators are basically everywhere. From physical goods (cars, clothes) to services (cloud solutions, insurances, etc.) such configurators have pervaded many areas of everyday life, in order to provide the customers products tailored to their needs. Being sometimes the only interfaces between product suppliers and consumers, much care has been devoted to the HCI aspects of configurators, aiming at offering an enjoyable buying experience. However, at the backend, the management of numerous and complex configuration options results from ad-hoc process rather than a systematic variability-aware engineering approach. We present our experience in analysing web configurators and formalising configuration options in terms of feature models or product configuration matrices. We also consider behavioural issues and perspectives on their architectural design.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {28–31},
numpages = {4},
keywords = {complexity, variability, web configurator},
location = {Austin, Texas},
series = {VACE '16}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@inproceedings{10.1145/581339.581415,
author = {Schmid, Klaus},
title = {A comprehensive product line scoping approach and its validation},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581415},
doi = {10.1145/581339.581415},
abstract = {Product Line Engineering is a recent approach to software development that specifically aims at exploiting commonalities and systematic variabilities among functionally overlapping systems in terms of large scale reuse. Taking full advantage of this potential requires adequate planning and management of the reuse approach as otherwise huge economic benefits will be missed due to an inappropriate alignment of the reuse infrastructure.Key in product line planning is the scoping activity, which aims at focussing the reuse investment where it pays. Scoping actually happens on several levels in the process: during the domain analysis step (analysis of product line requirements) a focusing needs to happen just like during the decision of what to implement for reuse. The latter decision has also important ramifications for the development of an appropriate reference architecture as it provides the reusability requirements for this step.In this paper, we describe an integrated approach that has been developed, improved, and validated over the last few years. The approach fully covers the scoping activities of domain scoping and reuse infrastructure scoping and was validated in several industrial case studies.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {593–603},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/2528265.2528270,
author = {Erwig, Martin and Walkingshaw, Eric and Chen, Sheng},
title = {An abstract representation of variational graphs},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528270},
doi = {10.1145/2528265.2528270},
abstract = {In the context of software product lines, there is often a need to represent graphs containing variability. For example, extending traditional modeling techniques or program analyses to variational software requires a corresponding notion of variational graphs. In this paper, we introduce a general model of variational graphs and a theoretical framework for discussing variational graph algorithms. Specifically, we present an abstract syntax based on tagging for succinctly representing variational graphs and other data types relevant to variational graph algorithms, such as variational sets and paths. We demonstrate how (non-variational) graph algorithms can be generalized to operate on variational graphs, to accept variational inputs, and produce variational outputs. Finally, we discuss a filtering operation on variational graphs and how this interacts with variational graph algorithms.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {25–32},
numpages = {8},
keywords = {choice calculus, variational algorithms, variational data structures},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1109/MODELS-C.2019.00077,
author = {Kaur, Navpreet and Famelis, Michalis},
title = {Towards reasoning about product lines with design choices},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00077},
doi = {10.1109/MODELS-C.2019.00077},
abstract = {While designing changes to Software Product Lines (SPLs), engineers may need to consider many alternative SPL designs. In the absence of enough information to pick an appropriate SPL design, they face design-time uncertainty about how to make the appropriate design choices. The combination of the two dimensions (variability and design choices) leads to Software Product Lines with Design Choices (SPLDCs). We propose Tyson, an Alloy-based domain-specific language for modelling SPLDCs and reasoning about their structural properties. We illustrate the applicability and feasibility of Tyson with a worked example, showing the kind of nuanced feedback necessary for meaningful analysis of SPLs with design choices.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {483–492},
numpages = {10},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {aspect-oriented modeling, case study, comparison criteria, object-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@article{10.1007/s11277-015-2410-6,
author = {Li, Chunlin and Li, Layuan},
title = {Efficient Market Strategy Based Optimal Scheduling in Hybrid Cloud Environments},
year = {2015},
issue_date = {July      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {83},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-015-2410-6},
doi = {10.1007/s11277-015-2410-6},
abstract = {Hybrid cloud is a combination of a private cloud combined with the use of public cloud services where one or several touch points exist between the environments. Depending on utilization, data center cost and the costs of the cloud provider, an efficient scheduling policy has to decide whether or not moving from private cloud to public cloud is profitable. The paper proposes a market based hybrid cloud optimal scheduling optimization in hybrid cloud. The hybrid cloud marketplace is a virtual place where one or more public cloud providers and private cloud users meet to negotiate simultaneously. The scheduling optimization is conducted by hybrid cloud local scheduling and hybrid cloud global scheduling. For the global scheduling, the hybrid cloud system implements the allocation of public cloud resources to the private cloud application groups; the private cloud application group coordinates the deployments of all private cloud applications that consume the allocation of public cloud resources. For the local scheduling, the private cloud local level adjusts the cloud resource usages to optimize the utility of single private cloud application. In the simulations, compared with other related algorithm, our proposed market based hybrid cloud optimal scheduling algorithms achieve the better performance in terms of QoS satisfaction rate and allocation efficiency.},
journal = {Wirel. Pers. Commun.},
month = jul,
pages = {581–602},
numpages = {22},
keywords = {Global scheduling, Hybrid cloud, Local scheduling, Market based scheduling}
}

@article{10.1145/3284971.3284975,
author = {Lazreg, Sami and Collet, Philippe and Mosser, S\'{e}bastien},
title = {Functional feasibility analysis of variability-intensive data flow-oriented applications over highly-configurable platforms},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284975},
doi = {10.1145/3284971.3284975},
abstract = {Data-flow oriented embedded systems, such as automotive systems used to render HMI (e.g., instrument clusters, infotainments), are increasingly built from highly variable specifications while targeting different constrained hardware platforms configurable in a fine-grained way. These variabilities at two different levels lead to a huge number of possible embedded system solutions, which functional feasibility is extremely complex and tedious to predetermine. In this paper, we propose a tooled approach that capture high level specifications as variable dataflows, and targeted platforms as variable component models. Dataflows can then be mapped onto platforms to express a specification of such variability-intensive systems. The proposed solution transforms this specification into structural and behavioral variability models and reuses automated reasoning techniques to explore and assess the functional feasibility of all variants in a single run. We also report on the validation of the proposed approach. A qualitative evaluation has been conducted on an industrial case study of automotive instrument cluster, while a quantitative one is reported on large generated datasets.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {behavioral product lines model checking, embedded system design engineering, feature model, variability modeling}
}

@inproceedings{10.1145/1233901.1233908,
author = {Navarro, Luis Daniel Benavides and Schwanninger, Christa and Sobotzik, Robert and S\"{u}dholt, Mario},
title = {ATOLL: aspect-oriented toll system},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233908},
doi = {10.1145/1233901.1233908},
abstract = {Product line development places emphasis on quality attributes like understandability, maintainability, reusability and variability. Better modularization techniques like aspect-oriented programming are supposed to improve these attributes.In the context of an industrial case study in the domain of infrastructure software for toll systems from Siemens AG, Germany, we have investigated how OO designs can be enhanced using AO techniques. We have explored, in particular, how sequential crosscutting concerns can be modularized using AspectJ and how distributed ones can be modularized using AWED, a system that features aspects with explicit distribution. Concretely, we show how sequential and distributed aspects improve the implementation of the charge calculation functionality that is central to real-world tolling systems.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {7–es},
keywords = {aspect-oriented software development, software product lines},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Component-based software development, Component-based software engineering, Software component, Systematic mapping study}
}

@inproceedings{10.1145/2245276.2245370,
author = {Parra, Carlos and Romero, Daniel and Mosser, S\'{e}bastien and Rouvoy, Romain and Duchien, Laurence and Seinturier, Lionel},
title = {Using constraint-based optimization and variability to support continuous self-adaptation},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2245370},
doi = {10.1145/2245276.2245370},
abstract = {Self-adaptation is one of the upcoming paradigms that accurately tackles nowadays systems complexity. In this context, Dynamic Software Product Lines model the intrinsic variability of a family of systems, and dynamically support their reconfiguration according to updated context. However, when several configurations are available for the same context, making a decision about the right one is a hard challenge: further dimensions such as QoS are needed to enrich the decision making process. In this paper, we propose to combine variability with Constraint-Satisfaction Problem techniques to face this challenge. The approach is illustrated and validated with a context-driven system used to support the control of a home through mobile devices.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {486–491},
numpages = {6},
location = {Trento, Italy},
series = {SAC '12}
}

@inbook{10.5555/1985668.1985673,
author = {Kazhamiakin, Raman and Benbernou, Salima and Baresi, Luciano and Plebani, Pierluigi and Uhlig, Maike and Barais, Olivier},
title = {Adaptation of service-based systems},
year = {2010},
isbn = {3642175988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Service Research Challenges and Solutions for the Future Internet: S-Cube - towards Engineering, Managing and Adapting Service-Based Systems},
pages = {117–156},
numpages = {40}
}

@inproceedings{10.1145/355045.355048,
author = {Hollingsworth, Joseph E. and Blankenship, Lori and Weide, Bruce W.},
title = {Experience report: using RESOLVE/C++ for commercial software},
year = {2000},
isbn = {1581132050},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/355045.355048},
doi = {10.1145/355045.355048},
abstract = {Academic research sometimes suffers from the “ivory tower” problem: ideas that sound good in theory do not necessarily work well in practice. An example of research that potentially could impact practice over the next few years is a novel set of component-based software engineering design principles, known as the RESOLVE discipline. This discipline has been taught to students for several years [23], and previous papers (e.g., [24]) have reported on student-sized software projects constructed using it. Here, we report on a substantial commercial product family that was engineered using the same principles — an application that we designed, built, and continue to maintain for profit, not as part of a research project. We discuss the impact of adhering to a very prescriptive set of design principles and explain our experience with the resulting applications. Lessons learned should benefit others who might be considering adopting such a component-based software engineering discipline in the future.},
booktitle = {Proceedings of the 8th ACM SIGSOFT International Symposium on Foundations of Software Engineering: Twenty-First Century Applications},
pages = {11–19},
numpages = {9},
keywords = {component-based software engineering, design discipline, design-by-contract, generic, software reuse, swapping, template},
location = {San Diego, California, USA},
series = {SIGSOFT '00/FSE-8}
}

@article{10.3233/JID210027,
author = {Salinesi, Camille and Achtaich, Asmaa and Souissi, Nissrine and Mazo, Ra\'{u}l and Roudies, Ounsa and Villota, Angela},
title = {State-Constraint Transition: A Language for the Formal Specification of Dynamic Cyber-System Requirements},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1092-0617},
url = {https://doi.org/10.3233/JID210027},
doi = {10.3233/JID210027},
abstract = {Existing formal languages for the specification of self-adaptive cyber-physical systems focus on re-configuring the system-to-be depending on its current context, to satisfy the user’s requirements, that is by dynamically composing the software’s structure and behavior. While these approaches specify context-sensitive requirements, they rarely consider their run-time dynamic and scalable nature. The State-Constraint Transition (SCT) modeling language, introduced in this paper, provides an answer to the problems linked to the specification of dynamic requirements by introducing the concept of configuration states, in which requirements are translated into constraints. The expressiveness of existing approaches is thus extended, combining the ease of use of well-established notations, notably those based on characteristics, and those based on Finite-state Machines (FSM), with the computational power and expressiveness of the constraint programming approach. The paper briefly presents the results of the preliminary evaluation, which assesses the expressiveness, scalability, and domain independence of the SCT language.},
journal = {J. Integr. Des. Process Sci.},
month = jan,
pages = {80–99},
numpages = {20},
keywords = {Cyber-physical systems, self-adaptive systems, software product lines, requirements engineering, state-machines, constraint programming, languages}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1016/j.jss.2016.02.026,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.026},
doi = {10.1016/j.jss.2016.02.026},
abstract = {We propose feature-driven crossover to improve feature modularization in PLA design.We compare the performance of a search algorithm with and without that crossover.Empirical results show the feature-driven crossover provides benefits to PLA design.Such crossover allows achieving better solutions and greater diversity of solutions.Such crossover operator also contributes to improve basic design principles. The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized PLA design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in PLA design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential PLA designs), with higher feature-based cohesion, and less feature scattering and tangling.},
journal = {J. Syst. Softw.},
month = nov,
pages = {126–143},
numpages = {18},
keywords = {Crossover operator, Empirical study, Multi-objective genetic algorithm, Product line architecture design}
}

@article{10.1016/j.jss.2014.11.036,
author = {Magdaleno, Andr\'{e}a Magalh\~{a}es and de Oliveira Barros, Marcio and Werner, Cl\'{a}udia Maria Lima and de Araujo, Renata Mendes and Batista, Carlos Freud Alves},
title = {Collaboration optimization in software process composition},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.11.036},
doi = {10.1016/j.jss.2014.11.036},
abstract = {Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions.Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components.Findings: The main findings indicate that GA is more effective (best results regarding the fitness function) than HC and RS in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These SBSE results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition.Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts.Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development.Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in SBSE to the field of software process which has not been properly explored by former research. This work describes an optimization approach (using heuristic search algorithms) to maximize collaboration in software process composition.It was evaluated through an experimental study conducted in the context of a large oil company in Brazil.Genetic algorithm (GA) is more effective (best results regarding the fitness function) than hill climbing (HC) and random search (RS) in the search of solutions for large instances.All algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context.Feedback given by specialist indicates satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition.Brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area.},
journal = {J. Syst. Softw.},
month = may,
pages = {452–466},
numpages = {15},
keywords = {Collaboration, SBSE, Software process}
}

@article{10.1016/j.infsof.2015.08.007,
author = {Sep\'{u}lveda, Samuel and Cravero, Ania and Cachero, Cristina},
title = {Requirements modeling languages for software product lines},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.08.007},
doi = {10.1016/j.infsof.2015.08.007},
abstract = {There is a concern for generating proposals with higher levels of expressiveness.There is not a strong relationship between the proposals and SPL development process.There is a need for better ways to validate the modeling proposals.The proposals have a low level of empirical validation and adoption in industry.The level of maturity, expressive power and tool support of the proposals is low. Display Omitted Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs.Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption.Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013.Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry.Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–36},
numpages = {21},
keywords = {Modeling languages, Requirements engineering, Software product lines, Systematic literature review}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {Software product lines, Model learning, Family model, T-wise sampling}
}

@article{10.1145/505532.505551,
author = {Butler, Greg},
title = {Generative techniques for product lines},
year = {2001},
issue_date = {November 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/505532.505551},
doi = {10.1145/505532.505551},
abstract = {A software product line leverages the knowledge of one or more domains in order to achieve short time-to-market, cost savings, and high quality software. The highest level of reuse comes by using domain-specific languages or visual builders to describe a member of the product line, and to generate the member from the description. Generative techniques can help us to capture the configuration knowledge for a product line and use it to generate concrete family members. This workshop focuses on technical issues of product lines, rather than economic issues.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {74–76},
numpages = {3}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Automotive software engineering, Automotive systems, Embedded systems, Literature survey, Software-intensive systems, Systematic mapping study}
}

@article{10.1016/j.infsof.2017.02.002,
author = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Genana N. and Carvalho, Hervaldo},
title = {Building reliable and maintainable Dynamic Software Product Lines},
year = {2017},
issue_date = {June 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {86},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.02.002},
doi = {10.1016/j.infsof.2017.02.002},
abstract = {Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN).Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain.Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist.Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30ms, notification and adaptation plan update time over the network around 5s, and space consumption around 5 MB.Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patientthus providing early detection of serious health issues and the possibility of proactive treatmentand a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {54–70},
numpages = {17},
keywords = {Adaptiveness, Body Sensor Network, Context-awareness, Dynamic Software Product Lines, Maintainability, Reliability}
}

@inproceedings{10.1145/1985394.1985397,
author = {Sengupta, Bikram and Roychoudhury, Abhik},
title = {Engineering multi-tenant software-as-a-service systems},
year = {2011},
isbn = {9781450305914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985394.1985397},
doi = {10.1145/1985394.1985397},
abstract = {Increasingly, Software-as-a-Service (SaaS) is becoming a dominant mechanism for the consumption of software by end users. From a vendor's perspective, the benefits of SaaS arise from leveraging economies of scale, by serving a large number of customers ("tenants") through a shared instance of a centrally hosted software service. Consequently, a SaaS provider would, in general, try to drive commonality amongst the requirements of different tenants, and at best, offer a fixed set of customization options. However, many tenants would also come with custom requirements, which may be a pre-requisite for them to adopt the SaaS system. These requirements should then be addressed by evolving the SaaS system in a controlled manner, while still supporting the needs of existing tenants. This need to balance tenant variability and commonality, and to optimize on development and testing effort, can make the evolution of multitenant SaaS systems an interesting engineering challenge; this has strong economic undertones as well, given the "pay-per-use" subscription model of SaaS, and the cost of incremental development and maintenance to cater to new tenant needs. In this paper, we outline a set of research issues in the design, testing and maintenance of multi-tenant SaaS systems, and highlight some of the interesting optimization questions that arise in the process. Presenting specific technical solutions is beyond the scope of this paper - instead, our goal is to help shape a research agenda for multi-tenant SaaS that can provide stimulus for further investigation into this area by the software and service engineering research community.},
booktitle = {Proceedings of the 3rd International Workshop on Principles of Engineering Service-Oriented Systems},
pages = {15–21},
numpages = {7},
keywords = {cloud computing, multi-tenancy, refinement, semantics, software-as-a-service, testing},
location = {Waikiki, Honolulu, HI, USA},
series = {PESOS '11}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00012,
author = {Nuryyev, Batyr and Nadi, Sarah and Bhuiyan, Nazim Uddin and Banderali, Leonardo},
title = {Challenges of implementing software variability in eclipse OMR: an interview study},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00012},
doi = {10.1109/ICSE-SEIP52600.2021.00012},
abstract = {Software variability is the ability of a software system to be customized or configured for a particular context. In this paper, we discuss our experience investigating software variability implementation challenges in practice. Eclipse OMR, developed by IBM, is a set of highly configurable C++ components for building language runtimes; it supports multiple programming languages and target architectures. We conduct an interview study with 6 Eclipse OMR developers and identify 8 challenges incurred by the existing variability implementation, and 3 constraints that need to be taken into account for any reengineering effort. We discuss these challenges and investigate the literature and existing open-source systems for potential solutions. We contribute a solution for one of the challenges, namely adding variability to enumerations and arrays. We also share our experiences and lessons learned working with a large-scale highly configurable industry project. For example, we found that the "latest and greatest" research solutions may not always be favoured by developers due to small practical considerations such as build dependencies, or even C++ version constraints.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {31–40},
numpages = {10},
keywords = {eclipse OMR, language runtimes, software variability, variability implementation},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/2600821.2600825,
author = {Nikitina, Natalja and Kajko-Mattsson, Mira},
title = {Guiding the adoption of software development methods},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600825},
doi = {10.1145/2600821.2600825},
abstract = {Literature shows that as many as 82% of the organizations that adopt agile methods experience problems in their agile adoptions. Despite this, very few reports have provided guidelines for how to conduct software method adoption. This paper suggests a process model of software method adoption and lists contextual factors for guiding the deployment of software development methods. The adoption model and the contextual factors have been evaluated in six industrial method adoption projects and they have proven to be useful for guiding organizations in their software method adoption efforts.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {109–118},
numpages = {10},
keywords = {Process change, SPI, agile adoption, agile method, deployment, method adoption, method engineering, process model},
location = {Nanjing, China},
series = {ICSSP '14}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A retrospective analysis of SAC requirements: engineering track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {26–41},
numpages = {16},
keywords = {SAC, relevance, requirements engineering, retrospective, scoping study, symposium on applied computing, systematic mapping study, trends}
}

@article{10.1016/j.jss.2007.10.030,
author = {Trinidad, P. and Benavides, D. and Dur\'{a}n, A. and Ruiz-Cort\'{e}s, A. and Toro, M.},
title = {Automated error analysis for the agilization of feature modeling},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.030},
doi = {10.1016/j.jss.2007.10.030},
abstract = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter's theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work.},
journal = {J. Syst. Softw.},
month = jun,
pages = {883–896},
numpages = {14},
keywords = {Agile methods, Constraint programming, Error analysis, Feature models, Theory of diagnosis}
}

@inproceedings{10.1145/1842752.1842777,
author = {Dhungana, Deepak and Groher, Iris and Schludermann, Elisabeth and Biffl, Stefan},
title = {Software ecosystems vs. natural ecosystems: learning from the ingenious mind of nature},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842777},
doi = {10.1145/1842752.1842777},
abstract = {The use of the term ecosystem in the context of extensible software platforms and third-party developers or user communities has made us ponder about the similarities between software ecosystems and natural ecosystems. We therefore compare software ecosystems and natural ecosystems to present an agenda for further research by analyzing some key characteristics of both types of ecosystems. We discuss the regulatory factors and mechanisms existing in nature, and then deduce key challenges that need to be dealt with, in order to achieve healthy operation of software ecosystems.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {96–102},
numpages = {7},
keywords = {ecosystem, nature, software},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1007/11531142_8,
author = {Lopez-Herrejon, Roberto E. and Batory, Don and Cook, William},
title = {Evaluating support for features in advanced modularization technologies},
year = {2005},
isbn = {354027992X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11531142_8},
doi = {10.1007/11531142_8},
abstract = {A software product-line is a family of related programs. Each program is defined by a unique combination of features, where a feature is an increment in program functionality. Modularizing features is difficult, as feature-specific code often cuts across class boundaries. New modularization technologies have been proposed in recent years, but their support for feature modules has not been thoroughly examined. In this paper, we propose a variant of the expression problem as a canonical problem in product-line design. The problem reveals a set of technology-independent properties that feature modules should exhibit. We use these properties to evaluate five technologies: AspectJ, Hyper/J, Jiazzi, Scala, and AHEAD. The results suggest an abstract model of feature composition that is technology-independent and that relates compositional reasoning with algebraic reasoning.},
booktitle = {Proceedings of the 19th European Conference on Object-Oriented Programming},
pages = {169–194},
numpages = {26},
location = {Glasgow, UK},
series = {ECOOP'05}
}

@inproceedings{10.1145/2745802.2745831,
author = {Mahmood, Sajjad and Anwer, Sajid and Niazi, Mahmood and Alshayeb, Mohammad and Richardson, Ita},
title = {Identifying the factors that influence task allocation in global software development: preliminary results},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745831},
doi = {10.1145/2745802.2745831},
abstract = {Over the last decade, an increasing number of organizations have started software development in a globally distributed environment. One of the major challenges is that many organizations endorse the process of global software development without testing their management readiness for the globally distributed development activity. This includes work distribution through task allocation in the globally distributed development environment. The objective of this research paper is to identify factors that influence task allocation in global software development through carrying out a systematic literature review. We used customized search terms, derived from our research question, to identify literature on work distribution and task allocation in a global context. We identified criteria such as site technical expertise, time zone difference, resource cost, task dependency, vendor reliability, task size and vendor maturity level as key task allocation factors in globally distributed software projects. Based on the systematic literature review results, we suggest that there is a need to develop work distribution strategies and standards through global task allocation to help software development organizations in achieving the true potential of global software development at lower development costs and shorter time-to-market.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {31},
numpages = {6},
keywords = {global software development, systematic literature review, task allocation, work distribution},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.1007/s10515-010-0075-7,
author = {Liu, Jing and Basu, Samik and Lutz, Robyn R.},
title = {Compositional model checking of software product lines using variation point obligations},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0075-7},
doi = {10.1007/s10515-010-0075-7},
abstract = {This paper introduces a technique for incremental and compositional model checking that allows efficient reuse of model-checking results associated with the features in a product line. As the use of product lines has increased, so has the need to verify the models used to construct the products in the product line. However, this effort is currently hampered by the difficulty of composing model-checking results for the features in a way that allows reuse for subsequent products. The contributions of this paper are to remove restrictions on how the features can be sequentially composed, to describe how to generate obligations such that all sequentially composed systems can be verified, and to show how to compositionally model check the product in the product line by reusing the variation-point obligations. The paper develops the technique and its implementation in the context of a medical-device product line.},
journal = {Automated Software Engg.},
month = mar,
pages = {39–76},
numpages = {38},
keywords = {Compositional model checking, Feature, Software product lines, Variation point}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Delaying of Decisions, Model Interfaces, Model Reuse, Model-Driven Engineering, Reuse Hierarchies},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/2039239.2039242,
author = {Tartler, Reinhard and Lohmann, Daniel and Dietrich, Christian and Egger, Christoph and Sincero, Julio},
title = {Configuration coverage in the analysis of large-scale system software},
year = {2011},
isbn = {9781450309790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2039239.2039242},
doi = {10.1145/2039239.2039242},
abstract = {System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.In order to find as many bugs as possible, usually a "full configuration" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version.},
booktitle = {Proceedings of the 6th Workshop on Programming Languages and Operating Systems},
articleno = {2},
numpages = {5},
location = {Cascais, Portugal},
series = {PLOS '11}
}

@inproceedings{10.5555/1892801.1892811,
author = {Dolstra, Eelco},
title = {Integrating software construction and software deployment},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Classically, software deployment is a process consisting of building the software, packaging it for distribution, and installing it at the target site. This approach has two problems. First, a package must be annotated with dependency information and other meta-data. This to some extent overlaps with component dependencies used in the build process. Second, the same source system can often be built into an often very large number of variants. The distributor must decide which element(s) of the variant space will be packaged, reducing the flexibility for the receiver of the package. In this paper we show how building and deployment can be integrated into a single formalism. We describe a build manager called Maak that can handle deployment through a sufficiently general module system. Through the sharing of generated files, a source distribution transparently turns into a binary distribution, removing the dichotomy between these two modes of deployment. In addition, the creation and deployment of variants becomes easy through the use of a simple functional language as the build formalism.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {102–117},
numpages = {16},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@article{10.1007/s11219-011-9160-9,
author = {Perrouin, Gilles and Oster, Sebastian and Sen, Sagar and Klein, Jacques and Baudry, Benoit and Traon, Yves},
title = {Pairwise testing for software product lines: comparison of two approaches},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9160-9},
doi = {10.1007/s11219-011-9160-9},
abstract = {Software Product Lines (SPL) are difficult to validate due to combinatorics induced by variability, which in turn leads to combinatorial explosion of the number of derivable products. Exhaustive testing in such a large products space is hardly feasible. Hence, one possible option is to test SPLs by generating test configurations that cover all possible t feature interactions (t-wise). It dramatically reduces the number of test products while ensuring reasonable SPL coverage. In this paper, we report our experience on applying t-wise techniques for SPL with two independent toolsets developed by the authors. One focuses on generality and splits the generation problem according to strategies. The other emphasizes providing efficient generation. To evaluate the respective merits of the approaches, measures such as the number of generated test configurations and the similarity between them are provided. By applying these measures, we were able to derive useful insights for pairwise and t-wise testing of product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {605–643},
numpages = {39},
keywords = {Alloy, Model-based engineering and testing, Software product lines, Test generation, t-wise and pairwise}
}

@article{10.5555/1850683.1850691,
author = {Viswanathan, M. and Shaikh, H. and Sailer, A. and Song, Y. and Fang, X. and Wu, Y. H. and Zou, Z. L. and Reddy, K. P. and Deshmukh, A. and Gupta, M. and Krishnamurthy, B. and Sethi, M. and Viswanathan, B. and Gulla, J. G. and Matar, F.},
title = {ERMIS: designing, developing, and delivering a remote managed infrastructure services solution},
year = {2009},
issue_date = {November 2009},
publisher = {IBM Corp.},
address = {USA},
volume = {53},
number = {6},
issn = {0018-8646},
abstract = {Remote management of IT (information technology) infrastructures, as a service, has received significant attention due to the numerous benefits it offers. In this paper, we discuss our work on taking a previously India-only remote IT infrastructure management service offering to a global audience. We first provide an overview of this IBM offering, called Express™ Remote Managed Infrastructure Services (ERMIS), and then focus on two specific aspects, service catalog and problem determination and resolution (PDR). In service catalog-based remote management, customers browse the catalog and place orders as they would using an online store. Our service catalog entries contain workflows to automate the deployment of requested applications or the configuration change of customer resources. High-level customer requirements are translated into capacity sizing and configuration parameters, which are then used to provision the final solution by making use of ITIL® (Information Technology Infrastructure Library) release and configuration management processes. PDR involves detecting, locating, and fixing anomalies. Probe-based techniques are used for targeting the problem location, while knowledge management assists with problem analysis, diagnosis, and classification. ERMIS architectural decisions have been driven by emerging-business requirements related to small and mediumsized businesses.},
journal = {IBM J. Res. Dev.},
month = nov,
pages = {871–888},
numpages = {18}
}

@inproceedings{10.1145/1083063.1083075,
author = {Lapouchnian, Alexei and Liaskos, Sotirios and Mylopoulos, John and Yu, Yijun},
title = {Towards requirements-driven autonomic systems design},
year = {2005},
isbn = {1595930396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083063.1083075},
doi = {10.1145/1083063.1083075},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying and analyzing alternative ways of how the main objectives of the system can be achieved and designing a system that supports all of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and sketches a possible architecture for autonomic systems that can be built using the this approach.},
booktitle = {Proceedings of the 2005 Workshop on Design and Evolution of Autonomic Application Software},
pages = {1–7},
numpages = {7},
keywords = {autonomic computing software customization, goal-oriented requirements engineering, self-management, software variability},
location = {St. Louis, Missouri},
series = {DEAS '05}
}

@article{10.1007/s11219-016-9341-7,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire and Zander, Justyna},
title = {Automatic generation of test system instances for configurable cyber-physical systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9341-7},
doi = {10.1007/s11219-016-9341-7},
abstract = {Cyber-physical systems (CPSs) are ubiquitous systems that integrate digital technologies with physical processes. These systems are becoming configurable to respond to the different needs that users demand. As a consequence, their variability is increasing, and they can be configured in many system variants. To ensure a systematic test execution of CPSs, a test system must be elaborated encapsulating several sources such as test cases or test oracles. Manually building a test system for each configuration is a non-systematic, time-consuming, and error-prone process. To overcome these problems, we designed a test system for testing CPSs and we analyzed the variability that it needed to test different configurations. Based on this analysis, we propose a methodology supported by a tool named ASTERYSCO that automatically generates simulation-based test system instances to test individual configurations of CPSs. To evaluate the proposed methodology, we selected different configurations of a configurable Unmanned Aerial Vehicle, and measured the time required to generate their test systems. On average, around 119 s were needed by our tool to generate the test system for 38 configurations. In addition, we compared the process of generating test system instances between the method we propose and a manual approach. Based on this comparison, we believe that the proposed tool allows a systematic method of generating test system instances. We believe that our approach permits an important step toward the full automation of testing in the field of configurable CPSs.},
journal = {Software Quality Journal},
month = sep,
pages = {1041–1083},
numpages = {43},
keywords = {Configurable cyber-physical systems, Test automation, Test system generation}
}

@inproceedings{10.1145/1966445.1966451,
author = {Tartler, Reinhard and Lohmann, Daniel and Sincero, Julio and Schr\"{o}der-Preikschat, Wolfgang},
title = {Feature consistency in compile-time-configurable system software: facing the linux 10,000 feature problem},
year = {2011},
isbn = {9781450306348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966445.1966451},
doi = {10.1145/1966445.1966451},
abstract = {Much system software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. A good example is the Linux kernel, which provides more than 10,000 configurable features, growing rapidly.From the maintenance point of view, compile-time configurability imposes big challenges. The configuration model (the selectable features and their constraints as presented to the user) and the configurability that is actually implemented in the code have to be kept in sync, which, if performed manually, is a tedious and error-prone task. In the case of Linux, this has led to numerous defects in the source code, many of which are actual bugs.We suggest an approach to automatically check for configurability-related implementation defects in large-scale configurable system software. The configurability is extracted from its various implementation sources and examined for inconsistencies, which manifest in seemingly conditional code that is in fact unconditional. We evaluate our approach with the latest version of Linux, for which our tool detects 1,776 configurability defects, which manifest as dead/superfluous source code and bugs. Our findings have led to numerous source-code improvements and bug fixes in Linux: 123 patches (49 merged) fix 364 defects, 147 of which have been confirmed by the corresponding Linux developers and 20 as fixing a new bug.},
booktitle = {Proceedings of the Sixth Conference on Computer Systems},
pages = {47–60},
numpages = {14},
keywords = {configurability, linux, maintenance, static analysis, vamos},
location = {Salzburg, Austria},
series = {EuroSys '11}
}

@inproceedings{10.1007/978-3-319-35122-3_9,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Alam, Omar and Sch\"{o}ttle, Matthias and Belloir, Nicolas and Collet, Philippe and Combemale, Benoit and Deantoni, Julien and Klein, Jacques and Rumpe, Bernhard},
title = {VCU: The Three Dimensions of Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_9},
doi = {10.1007/978-3-319-35122-3_9},
abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces VCU. A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {122–137},
numpages = {16},
keywords = {Concern-oriented reuse, Configuration, Customization, Extension, Interfaces, Reuse, Usage, Variability},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1109/ASE.2009.66,
author = {Wang, Yiqiao and Mylopoulos, John},
title = {Self-Repair through Reconfiguration: A Requirements Engineering Approach},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.66},
doi = {10.1109/ASE.2009.66},
abstract = {High variability software systems can deliver their functionalities in multiple ways by reconfiguring their components. High variability has become important because of current trends towards software systems that come in product families, offer high levels of personalization, and fit well within a service-oriented architecture. The purpose of our research is to propose a framework that exploits such variability to allow a software system to self-repair in cases of failure. We propose an autonomic architecture that consists of monitoring, diagnosis, reconfiguration and execution components. This architecture uses requirements models as a basis for monitoring, diagnosis, and reconfiguration. We illustrate our proposal with a medium-sized publicly available case study (an Automated Teller Machine (ATM) simulation), and evaluate its performance through a series of experiments. Our experimental results demonstrate that it is feasible to scale our approach to software systems with medium-size requirements.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {257–268},
numpages = {12},
keywords = {Adaptive systems, Autonomic computing, Requirement Monitoring and Diagnosis, Self-reconfiguration},
series = {ASE '09}
}

@article{10.1016/j.cie.2007.05.004,
author = {Hvam, Lars and Ladeby, Klaes},
title = {An approach for the development of visual configuration systems},
year = {2007},
issue_date = {October, 2007},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {53},
number = {3},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2007.05.004},
doi = {10.1016/j.cie.2007.05.004},
abstract = {How can a visual configuration system be developed to support the specification process in companies that manufacture customer tailored products? This article focuses on how visual configuration systems can be developed. The approach for developing visual configuration systems has been developed by Centre for Product Modelling (CPM) at The Technical University of Denmark. The approach is based on experiences from a visualization project in co-operation between CPM and the global provider of power protection American Power Conversion (APC). The visual configuration system was developed in 2001-2002 and has during its operation since the beginning of 2003 delivered promising results. The lead-time for approval of quotations has been reduced and the percentages of first-time-completed and correct configuration sessions are increased thanks to an increased user-interaction, caused by the visualization of the product in the visual configuration system.},
journal = {Comput. Ind. Eng.},
month = oct,
pages = {401–419},
numpages = {19},
keywords = {Product modeling, Visual configuration, Visual configuration systems}
}

@article{10.1007/s10664-015-9410-8,
author = {Unterkalmsteiner, Michael and Gorschek, Tony and Feldt, Robert and Lavesson, Niklas},
title = {Large-scale information retrieval in software engineering - an experience report from industrial application},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9410-8},
doi = {10.1007/s10664-015-9410-8},
abstract = {Software Engineering activities are information intensive. Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks, such as establishing and maintaining traceability links, fault identification, and software maintenance. We describe an engineering task, test case selection, and illustrate our problem analysis and solution discovery process. The objective of the study is to gain an understanding of to what extent IR techniques (one potential solution) can be applied to test case selection and provide decision support in a large-scale, industrial setting. We analyze, in the context of the studied company, how test case selection is performed and design a series of experiments evaluating the performance of different IR techniques. Each experiment provides lessons learned from implementation, execution, and results, feeding to its successor. The three experiments led to the following observations: 1) there is a lack of research on scalable parameter optimization of IR techniques for software engineering problems; 2) scaling IR techniques to industry data is challenging, in particular for latent semantic analysis; 3) the IR context poses constraints on the empirical evaluation of IR techniques, requiring more research on developing valid statistical approaches. We believe that our experiences in conducting a series of IR experiments with industry grade data are valuable for peer researchers so that they can avoid the pitfalls that we have encountered. Furthermore, we identified challenges that need to be addressed in order to bridge the gap between laboratory IR experiments and real applications of IR in the industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2324–2365},
numpages = {42},
keywords = {Data mining, Experiment, Information retrieval, Test case selection}
}

@inproceedings{10.1145/2593743.2593746,
author = {Sierszecki, Krzysztof and Steffens, Michaela and Fogdal, Thomas and Savolainen, Juha and Mikkonen, Tommi},
title = {Towards green power electronics: software controllers and domain knowledge},
year = {2014},
isbn = {9781450328449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593743.2593746},
doi = {10.1145/2593743.2593746},
abstract = {One of the key challenges of green software is that various aspects have an impact to the overall energy consumption over the lifetime of a system operated by software. In particular, in the field of industrial applications, where embedded devices cooperate with many IT systems to make the industrial processes more efficient, to reduce waste or raw materials, and to save the environment, the concept of green software becomes unclear. In this paper, we address the green aspects of software in different phases – software construction, software execution, and software control in both inside an individual component and as a part of a complete industrial application. Furthermore, we demonstrate that the insight into system knowledge, not aspects related to software per se, is the key to create truly green software. Consequently, when considering truly software green, the focus is to be placed on the system level savings for embedded systems at the highest possible level where domain knowledge can be taken into account, not on software development or execution.},
booktitle = {Proceedings of the 3rd International Workshop on Green and Sustainable Software},
pages = {17–22},
numpages = {6},
keywords = {Green software, embedded control systems, green systems, software development, software product lines, variable speed drives},
location = {Hyderabad, India},
series = {GREENS 2014}
}

@inproceedings{10.1109/FOSE.2007.14,
author = {France, Robert and Rumpe, Bernhard},
title = {Model-driven Development of Complex Software: A Research Roadmap},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.14},
doi = {10.1109/FOSE.2007.14},
abstract = {The term Model-Driven Engineering (MDE) is typically used to describe software development approaches in which abstract models of software systems are created and systematically transformed to concrete implementations. In this paper we give an overview of current research in MDE and discuss some of the major challenges that must be tackled in order to realize the MDE vision of software development. We argue that full realizations of the MDE vision may not be possible in the near to medium-term primarily because of the wicked problems involved. On the other hand, attempting to realize the vision will provide insights that can be used to significantly reduce the gap between evolving software complexity and the technologies used to manage complexity.},
booktitle = {2007 Future of Software Engineering},
pages = {37–54},
numpages = {18},
series = {FOSE '07}
}

@article{10.1007/s00165-017-0441-3,
author = {Str\"{u}ber, D. and Rubin, J. and Arendt, T. and Chechik, M. and Taentzer, G. and Pl\"{o}ger, J.},
title = {Variability-based model transformation: formal foundation and application},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0441-3},
doi = {10.1007/s00165-017-0441-3},
abstract = {Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {133–162},
numpages = {30},
keywords = {Model transformation, Graph transformation, Variability, Category theory}
}

@article{10.1016/j.infsof.2017.01.008,
author = {Lavoie, Thierry and Mrineau, Mathieu and Merlo, Ettore and Potvin, Pascal},
title = {A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.01.008},
doi = {10.1016/j.infsof.2017.01.008},
abstract = {Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects.Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions.Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images.Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study.Conclusion: About 24% of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9% and 15.3% of clone fragments for a total of 98.2%. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {32–45},
numpages = {14},
keywords = {Clone detection, Telecommunications software, Test}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {configuration, feature model recovery, feature modules, product line, variability-aware modularity},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@inproceedings{10.1145/2856636.2856649,
author = {Sripada, Sai Krishna and Reddy, Y. Raghu and Khandelwal, Shivam},
title = {Architecting an extensible framework for Gamifying Software Engineering concepts},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2856649},
doi = {10.1145/2856636.2856649},
abstract = {Software engineering activities like code reviews, change management, knowledge management, issue tracking, etc. tend to be heavily process oriented. Gamification of such activities by composing the core activities with game design elements like badges and points can increase developers' interest in performing such activities. While there are various frameworks/applications that assist in gamification, extending the frameworks to add any/all desired game design elements has not been adequately addressed. In this paper, we propose an extensible architectural framework for gamification of software engineering activities where in the game design elements are modeled as services. We create an example instance of our framework by building a prototype for code review activity and note the challenges of designing such an extensible architectural framework. The example instance uses python's Flask micro framework and has five game design elements implemented as services, and exposed using restful APIs.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {119–130},
numpages = {12},
keywords = {Architecture, Code review, Game Design Elements, Gamification, REST API, Services, Web Hooks},
location = {Goa, India},
series = {ISEC '16}
}

@article{10.1007/s10664-018-9670-1,
author = {Ali, Nauman Bin and Engstr\"{o}m, Emelie and Taromirad, Masoumeh and Mousavi, Mohammad Reza and Minhas, Nasir Mehmood and Helgesson, Daniel and Kunze, Sebastian and Varshosaz, Mahsa},
title = {On the search for industry-relevant regression testing research},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9670-1},
doi = {10.1007/s10664-018-9670-1},
abstract = {Regression testing is a means to assure that a change in the software, or its execution environment, does not introduce new defects. It involves the expensive undertaking of rerunning test cases. Several techniques have been proposed to reduce the number of test cases to execute in regression testing, however, there is no research on how to assess industrial relevance and applicability of such techniques. We conducted a systematic literature review with the following two goals: firstly, to enable researchers to design and present regression testing research with a focus on industrial relevance and applicability and secondly, to facilitate the industrial adoption of such research by addressing the attributes of concern from the practitioners' perspective. Using a reference-based search approach, we identified 1068 papers on regression testing. We then reduced the scope to only include papers with explicit discussions about relevance and applicability (i.e. mainly studies involving industrial stakeholders). Uniquely in this literature review, practitioners were consulted at several steps to increase the likelihood of achieving our aim of identifying factors important for relevance and applicability. We have summarised the results of these consultations and an analysis of the literature in three taxonomies, which capture aspects of industrial-relevance regarding the regression testing techniques. Based on these taxonomies, we mapped 38 papers reporting the evaluation of 26 regression testing techniques in industrial settings.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2020–2055},
numpages = {36},
keywords = {Industrial relevance, Recommendations, Regression testing, Systematic literature review, Taxonomy}
}

@article{10.1134/S0005117909030187,
author = {Levin, M. Sh.},
title = {Combinatorial optimization in system configuration design},
year = {2009},
issue_date = {Mar 2009},
publisher = {Plenum Press},
address = {USA},
volume = {70},
number = {3},
issn = {0005-1179},
url = {https://doi.org/10.1134/S0005117909030187},
doi = {10.1134/S0005117909030187},
abstract = {The significance of systems configurations has been increased in many applied domains (e.g., software, hardware, manufacturing systems, communication systems, supply chain systems, solving strategies, modular planning, material engineering and combinatorial chemistry). Generally, the following kinds of systems under configuration (reconfiguration) can be examined: (a) initial data/infomation (or row materials); (b) processing systems (machines, computers, algorithms); (c) resultant decisions (e.g., products, plans); (d) applied support systems (e.g., network systems, services); (e) requirements; and (f) standards. In the paper several system configuration problems are investigated: (i) searching for (selection of) a set (structure) of system components, (ii) searching for a set of compatible system components, (iii) allocation of system components, (iv) reconfiguration of a system as redesign of the system structure, (v) multi-stage design and redesign of system configuration, (vi) design or redesign of the system configuration for multi-product systems, and (vii) design of system hierarchy. Combinatorial optimization models (including multicriteria statements) are under examination: multiple choice problem, allocation problem, graph coloring problems, morphological clique problem (with compatibility of system components), multipartite clique and their modifications, spanning trees problems.},
journal = {Autom. Remote Control},
month = mar,
pages = {519–561},
numpages = {43}
}

@inproceedings{10.1145/2491411.2491446,
author = {Rubin, Julia and Chechik, Marsha},
title = {N-way model merging},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491446},
doi = {10.1145/2491411.2491446},
abstract = {Model merging is widely recognized as an essential step in a variety of software development activities. During the process of combining a set of related products into a product line or consolidating model views of multiple stakeholders, we need to merge multiple input models into one; yet, most of the existing approaches are applicable to merging only two models. In this paper, we define the n-way merge problem. We show that it can be reduced to the known and widely studied NP-hard problem of weighted set packing. Yet, the approximation solutions for that problem do not scale for real-sized software models. We thus evaluate alternative approaches of merging models that incrementally process input models in small subsets and propose our own algorithm that considerably improves precision over such approaches without sacrificing performance.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {301–311},
numpages = {11},
keywords = {Model merging, combining multiple models, weighted set packing},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.5555/3242181.3242285,
author = {Yilmaz, Levent and Chakladar, Sritika and Doud, Kyle and Smith, Alice E. and Teran-Somohano, Alejandro and Oundefineduzt\"{u}z\"{u}n, Halit and \c{C}am, Sema and Dayiba\c{s}, Orcun and G\"{o}r\"{u}r, Bilge K.},
title = {Models as self-aware cognitive agents and adaptive mediators for model-driven science},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {There are often concerns about the reliability of simulation results due to improper design of experiments, limited support in the execution and analysis of experiments, and lack of integrated computational frameworks for model learning through simulation experiments. Such issues result in flawed analysis as well as misdirected human and computational effort. We put forward a methodological basis, which aims to (1) explore the utility of viewing models as adaptive agents that mediate among domain theories, data, requirements, principles, and analogies, (2) underline the role of cognitive assistance for model discovery, experimentation, and evidence evaluation so as to differentiate between competing models and to attain a balance between model exploration and exploitation, and (3) examine strategies for explanatory justification of model assumptions via cognitive models that explicate coherence judgments.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {98},
numpages = {12},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.1007/s10009-012-0254-x,
author = {J\"{o}rges, Sven and Lamprecht, Anna-Lena and Margaria, Tiziana and Schaefer, Ina and Steffen, Bernhard},
title = {A constraint-based variability modeling framework},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0254-x},
doi = {10.1007/s10009-012-0254-x},
abstract = {Constraint-based variability modeling is a flexible, declarative approach to managing solution-space variability. Product variants are defined in a top-down manner by successively restricting the admissible combinations of product artifacts until a specific product variant is determined. In this paper, we illustrate the range of constraint-based variability modeling by discussing two of its extreme flavors: constraint-guarded variability modeling and constraint-driven variability modeling. The former applies model checking to establish the global consistency of product variants which are built by manual specification of variations points, whereas the latter uses synthesis technology to fully automatically generate product variants that satisfy all given constraints. Each flavor is illustrated by means of a concrete case study.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {511–530},
numpages = {20},
keywords = {Constraint-based variability modeling, Software product lines, Variability}
}

@inproceedings{10.1145/2866614.2866619,
author = {Schnabel, Thomas and Weckesser, Markus and Kluge, Roland and Lochau, Malte and Sch\"{u}rr, Andy},
title = {CardyGAn: Tool Support for Cardinality-based Feature Models},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866619},
doi = {10.1145/2866614.2866619},
abstract = {Cardinality-based feature models (CFM) constitute a crucial and non-trivial extension to FODA feature models in terms of UML-like feature multiplicities and corresponding cardinality constraints. CFM allow for specifying configuration choices of software systems incorporating multiple instances (copies) of features, e.g., for tailoring customer-specific and even potentially unrestricted application resources. Nevertheless, the improved expressiveness of CFM compared to FODA feature models complicates configuration semantics, including sub-tree cloning and potentially unbounded configuration spaces. As a consequence, entirely novel anomalies might arise such as dead cardinality intervals, false unboundedness, and cardinality gaps, which are not properly treated by recent feature-modeling tools. In this paper, we present comprehensive tool support for assisting specification, validation, and configuration of CFM. Our tool CARDYGAN, therefore, incorporates capabilities for CFM editing, automated CFM validation including anomaly detection based on a combination of ILP and SMT solvers, as well as a CFM configuration engine based on ALLOY.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {33–40},
numpages = {8},
keywords = {Automated Validation, Extended Feature Models},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@article{10.5555/2873826.2874006,
author = {Hervieu, Aymeric and Marijan, Dusica and Gotlieb, Arnaud and Baudry, Benoit},
title = {Practical minimization of pairwise-covering test configurations using constraint programming},
year = {2016},
issue_date = {March 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {71},
number = {C},
issn = {0950-5849},
abstract = {Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system's feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice.Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible.Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN.Results: PACOGEN was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79% of 224 feature models, PACOGEN generated up to 60% fewer test configurations than the competitor tools. We further evaluated PACOGEN in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60% fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by PACOGEN decreased the time required by test engineers in manual test configuration by 85%, increasing the feature-pairs coverage at the same time.Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {129–146},
numpages = {18},
keywords = {Constraint programming, Highly-configurable software systems, Variability testing}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.1109/SDSOA.2007.5,
author = {Dan, Asit and Johnson, Robert and Arsanjani, Ali},
title = {Information as a Service: Modeling and Realization},
year = {2007},
isbn = {0769529607},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SDSOA.2007.5},
doi = {10.1109/SDSOA.2007.5},
abstract = {Service Oriented Architecture is enabling a new approach to the design and assembly of service-based solutions and environments, which promises a more fundamental alignment of business and I/T organizations in the enterprise, greater agility of applications by the exploitation of loose coupling, and opportunities for effective reuse and governance of business and I/T activities. Current methods and tools that support SOA development activities, however, have focused primarily on supporting business process and business logic. We discuss the application of SOA principles to enable the utilization of Information as a Service (IaaS), the benefits in unifying information and process service approaches to SOA, and key research challenges in modeling and realization for IaaS in the context of SOA development.},
booktitle = {Proceedings of the International Workshop on Systems Development in SOA Environments},
pages = {2},
series = {SDSOA '07}
}

@article{10.1007/s42979-020-00323-8,
author = {Shafik, Wasswa and Matinkhah, S. Mojtaba and Ghasemzadeh, Mohammad},
title = {Theoretical Understanding of Deep Learning in UAV Biomedical Engineering Technologies Analysis},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00323-8},
doi = {10.1007/s42979-020-00323-8},
abstract = {The unmanned aerial vehicles (UAVs) emerged into a promising research trend within the recurrent year where current and future networks are to use enhanced connectivity in these digital immigrations in different fields like medical, communication, search, and rescue operations among others. The current technologies are using fixed base stations to operate on-site and off-site in the fixed position with its associated problems like poor connectivity. This opens gates for the UAVs technology to be used as a mobile alternative to increase accessibility with a fifth-generation (5G) connectivity that focuses on increased availability and connectivity. There has been less usage of wireless technologies in the medical field. This paper first presents a study on deep learning to medical field application in general, and provides detailed steps that are involved in the multi-armed bandit approach in solving UAV biomedical engineering technologies devices and medical exploration to exploitation dilemma. The paper further presents a detailed description of the bandit network applicability to achieve close optimal medical engineered devices’ performance and efficiency. The simulated results depicted that a multi-armed bandit problem approach can be applied in optimizing the performance of any medical networked device issue compared to the Thompson sampling, Bayesian algorithm, and ε-greedy algorithm. The results obtained further illustrated the optimized utilization of biomedical engineering technologies systems achieving thus close optimal performance on the average period through deep learning of realistic medical situations.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {13},
keywords = {Deep learning, Biomedical technology, Unmanned aerial vehicles}
}

@inproceedings{10.1145/2593929.2593930,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Pasquale, Liliana and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {User-centric adaptation of multi-tenant services: preference-based analysis for service reconfiguration},
year = {2014},
isbn = {9781450328647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593929.2593930},
doi = {10.1145/2593929.2593930},
abstract = {Multi-tenancy is a key pillar of cloud services. It allows different tenants to share computing resources transparently and, at the same time, guarantees substantial cost savings for the providers. However, from a user perspective, one of the major drawbacks of multi-tenancy is lack of configurability. Depending on the isolation degree, the same service instance and even the same service configuration may be shared among multiple tenants (i.e. shared multi-tenant service). Moreover tenants usually have different - and in most of the cases - conflicting configuration preferences. To overcome this limitation, this paper introduces a novel approach to support user-centric adaptation in shared multi-tenant services. The adaptation objective aims to maximise tenants’ satisfaction, even when tenants and their preferences change during the service life-time. This paper describes how to engineer the activities of the MAPE loop to support user-centric adaptation, and focuses on the analysis of tenants’ preferences. In particular, we use a game theoretic analysis to identify a service configuration that maximises tenants’ preferences satisfaction. We illustrate and motivate our approach by utilising a multi-tenant desktop scenario. Obtained experimental results demonstrate the feasibility of the proposed analysis.},
booktitle = {Proceedings of the 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {65–74},
numpages = {10},
keywords = {Adaptive systems, cloud, game theory, multi-tenancy},
location = {Hyderabad, India},
series = {SEAMS 2014}
}

@inproceedings{10.5555/3140065.3140086,
author = {Yilmaz, Levent},
title = {Models as autonomous adaptive mediators and cognitive instruments for systems science},
year = {2017},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Although model development is an incremental and iterative process, it often prematurely convergences to a single authoritative model. On the other hand, the scientific method demands coupling of the generation of hypotheses with their evaluation via various cognitive strategies to facilitate competition among plausible explanations. This process results in a theory, which is represented by a cluster of models that jointly produce robust solutions. The multiplicity of models is imposed not only by the complexity of systems, but also by the need to constrain abstractions to a limited number of variables. Motivated by these observations, we propose a methodological foundation that views systems science from the lens of cognitive models of scientific discovery, allowing the management of ensemble of models while supporting autonomous mediation of theory with data.},
booktitle = {Proceedings of the Summer Simulation Multi-Conference},
articleno = {21},
numpages = {12},
keywords = {autonomy, cognitive system, computational discovery, model-driven science},
location = {Bellevue, Washington},
series = {SummerSim '17}
}

@inproceedings{10.5555/3291291.3291314,
author = {Wehling, Kenny and Wille, David and Seidl, Christoph and Schaefer, Ina},
title = {Reducing variability of technically related software systems in large-scale IT landscapes},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {The number of software systems in a company typically grows with the business requirements. Therefore, IT landscapes in large companies can consist of hundreds or thousands of different software systems. As the evolution of such large-scale landscapes is often uncoordinated, they commonly comprise different groups of related software systems using a common core technology (e.g., Java Web-Application) implemented by a variety of architectural components (e.g., different application servers or databases). This leads to increased costs and higher effort for maintaining and evolving these software systems and the entire IT landscape. To alleviate these problems, the variability of such technically related software systems has to be reduced. For this purpose, experts have to assess and evaluate restructuring potentials in order to take appropriate restructuring decisions. As a manual analysis requires high effort and is not feasible for large-scale IT landscapes, experts face a major challenge. To overcome this challenge, we introduce a novel approach to automatically support experts in taking reasonable restructuring decisions. By providing automated methods for assessing, evaluating and simulating restructuring potentials, experts are capable of reducing the variability of related software systems in large-scale IT landscapes. We show suitability of our approach by expert interviews and an industrial case study with architectures of real-world software systems.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {224–235},
numpages = {12},
keywords = {restructuring, software systems, technology architecture, variability},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1016/j.artmed.2016.05.004,
title = {An ensemble method for extracting adverse drug events from social media},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2016.05.004},
doi = {10.1016/j.artmed.2016.05.004},
abstract = {We propose a relation extraction system to distinguish between adverse drug events (ADEs) and non-ADEs on social media.We develop a feature-based method, investigate the effectiveness of feature selection, and analyze the contributions of different features.We investigate whether kernel-based methods can effectively extract ADEs from social media.We propose several classifier ensembles to further enhance ADE extraction capabilities. ObjectiveBecause adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media. Methods and materialsWe develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter). ResultsWhen investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines. ConclusionsOur experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {62–76},
numpages = {15}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {AHP method, Comparative preferences, Conditional preferences, Lexicographic order, S-AHP method, Well-formed preferences}
}

@article{10.1016/j.rcim.2019.05.011,
author = {Zheng, Chen and Qin, Xiansheng and Eynard, Beno\^{\i}t and Li, Jing and Bai, Jing and Zhang, Yicha and Gomes, Samuel},
title = {Interface model-based configuration design of mechatronic systems for industrial manufacturing applications},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {59},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2019.05.011},
doi = {10.1016/j.rcim.2019.05.011},
journal = {Robot. Comput.-Integr. Manuf.},
month = oct,
pages = {373–384},
numpages = {12},
keywords = {Mechatronic systems, Industrial manufacturing, Configuration design, Interface model}
}

@article{10.14778/3415478.3415549,
author = {Hirsch, Vitali and Reimann, Peter and Mitschang, Bernhard},
title = {Exploiting domain knowledge to address multi-class imbalance and a heterogeneous feature space in classification tasks for manufacturing data},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415549},
doi = {10.14778/3415478.3415549},
abstract = {Classification techniques are increasingly adopted for quality control in manufacturing, e.g., to help domain experts identify the cause of quality issues of defective products. However, real-world data often imply a set of analytical challenges, which lead to a reduced classification performance. Major challenges are a high degree of multi-class imbalance within data and a heterogeneous feature space that arises from the variety of underlying products. This paper considers such a challenging use case in the area of End-of-Line testing, i.e., the final functional test of complex products. Existing solutions to classification or data pre-processing only address individual analytical challenges in isolation. We propose a novel classification system that explicitly addresses both challenges of multi-class imbalance and a heterogeneous feature space together. As main contribution, this system exploits domain knowledge to systematically prepare the training data. Based on an experimental evaluation on real-world data, we show that our classification system outperforms any other classification technique in terms of accuracy. Furthermore, we can reduce the amount of rework required to solve a quality issue of a product.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3258–3271},
numpages = {14}
}

@article{10.1007/s10796-016-9678-2,
author = {Cognini, Riccardo and Corradini, Flavio and Gnesi, Stefania and Polini, Andrea and Re, Barbara},
title = {Business process flexibility - a systematic literature review with a software systems perspective},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-016-9678-2},
doi = {10.1007/s10796-016-9678-2},
abstract = {Business Process flexibility supports organizations in changing their everyday work activities to remain competitive. Since much research has been done on this topic a better awareness on the current state of knowledge is needed. This paper reports the results of a systematic literature review to develop a map on Business Process flexibility with a special focus on software systems related aspects. It covers a spectrum of the state of the art from academic point of view. It includes 164 research works from the main computer science digital libraries. After an introduction into the topic the applied methodology is described. The output of the paper is in the form of schemes and reflections. Starting from the needs for Business Process flexibility, its impact on Business Process life-cycle is introduced. Successively instruments used to express and to support Business Process flexibility are presented together with related validation scenarios. In this paper we also highlight possible future research lines needing further investigations. In particular we identified room for future works in the area of languages for modeling flexibility, on-the-fly verification solutions, adaptation of Business Process running instances, and techniques for evolution recognition.},
journal = {Information Systems Frontiers},
month = apr,
pages = {343–371},
numpages = {29},
keywords = {Adaptability, Business process, Business process life-cycle, Business process management, Cases study, Evolution, Flexibility, Languages, Looseness, Mechanisms, Process aware information systems, Systematic literature review, Variability}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/2094091.2094095,
author = {Tartler, Reinhard and Lohmann, Daniel and Dietrich, Christian and Egger, Christoph and Sincero, Julio},
title = {Configuration coverage in the analysis of large-scale system software},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/2094091.2094095},
doi = {10.1145/2094091.2094095},
abstract = {System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.In order to find as many bugs as possible, usually a "full configuration" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jan,
pages = {10–14},
numpages = {5}
}

@article{10.1007/s10270-011-0219-7,
author = {Mohagheghi, Parastoo and Gilani, Wasif and Stefanescu, Alin and Fernandez, Miguel A. and Nordmoen, Bj\o{}rn and Fritzsche, Mathias},
title = {Where does model-driven engineering help? Experiences from three industrial cases},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0219-7},
doi = {10.1007/s10270-011-0219-7},
abstract = {There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the benefits are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-specific models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumption of models for analysis, such as performance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be useful and cost-efficient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identified several benefits of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identified as crucial for a wider industrial adoption of MDE.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {619–639},
numpages = {21},
keywords = {Complex systems, Domain-specific language, Eclipse, Experience report, Model-driven engineering, Simulation}
}

@article{10.1145/2339118.2339120,
author = {Abd-El-Malek, Michael and Wachs, Matthew and Cipar, James and Sanghi, Karan and Ganger, Gregory R. and Gibson, Garth A. and Reiter, Michael K.},
title = {File system virtual appliances: Portable file system implementations},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/2339118.2339120},
doi = {10.1145/2339118.2339120},
abstract = {File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.},
journal = {ACM Trans. Storage},
month = sep,
articleno = {9},
numpages = {26},
keywords = {Operating systems, file systems, virtual machines}
}

@inproceedings{10.1145/3238147.3238201,
author = {Mukelabai, Mukelabai and Ne\v{s}i\'{c}, Damir and Maro, Salome and Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp},
title = {Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238201},
doi = {10.1145/3238147.3238201},
abstract = {Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {155–166},
numpages = {12},
keywords = {Analysis, Highly Configurable Systems, Product Lines},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@article{10.1145/2063239.2063245,
author = {Erwig, Martin and Walkingshaw, Eric},
title = {The Choice Calculus: A Representation for Software Variation},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2063239.2063245},
doi = {10.1145/2063239.2063245},
abstract = {Many areas of computer science are concerned with some form of variation in software---from managing changes to software over time to supporting families of related artifacts. We present the choice calculus, a fundamental representation for software variation that can serve as a common language of discourse for variation research, filling a role similar to the lambda calculus in programming language research. We also develop an associated theory of software variation, including sound transformations of variation artifacts, the definition of strategic normal forms, and a design theory for variation structures, which will support the development of better algorithms and tools.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {6},
numpages = {27},
keywords = {Variation, representation}
}

@inproceedings{10.1145/1629716.1629725,
author = {Barreiros, Jorge and Moreira, Ana},
title = {A model-based representation of configuration knowledge},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629725},
doi = {10.1145/1629716.1629725},
abstract = {Implementation of feature-oriented systems is typically made by creating an admissible configuration, according to a specified feature diagram, that dictates what artifacts are to be composed to create the desired solution. These artefacts are typically grouped according to the feature they concern. However, some artefacts may be related not to a specific feature, but to a combination of them. Also, multiple alternate implementations of a single feature may exist, and the preferred one may be dependent on the specific configuration that is being composed. We propose a graphic model to represent configuration knowledge that is able to address such concerns.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {43–48},
numpages = {6},
keywords = {feature dependency, feature interaction, feature modeling, knowledge representation modeling},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.jss.2017.05.051,
author = {Vogel-Heuser, Birgit and Fischer, Juliane and Feldmann, Stefan and Ulewicz, Sebastian and Rsch, Susanne},
title = {Modularity and architecture of PLC-based software for automated production Systems},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.051},
doi = {10.1016/j.jss.2017.05.051},
abstract = {Overview of the state of the art in industrial software engineering of aPS using PLCs.Benchmark process to evaluate the maturity of aPS application software.SWMAT4aPS consists of a self-assessment questionnaire and an expert analysis.3 Maturity measures: modularity, test/quality assurance and start-up/operation/maintenance.Weaknesses of aPS software related to mechanics and automation hardware in comparison to pure software identified. Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.},
journal = {J. Syst. Softw.},
month = sep,
pages = {35–62},
numpages = {28},
keywords = {Automated production systems, Control software, Factory automation, Maturity, Modularity, Programmable logic controller}
}

@inproceedings{10.1145/2701319.2701334,
author = {M\"{a}tzler, Emanuel and Wally, Bernhard and Mazak, Alexandra},
title = {A Common Home for Features and Requirements: Retrofitting the House of Quality with Feature Models},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701334},
doi = {10.1145/2701319.2701334},
abstract = {Quality function deployment (QFD) is a method for quality assurance developed for application in production processes. One prominent tool for implementing QFD is the House of Quality (HoQ), whose basic design principles have been left unchanged for the last decades. Modern concepts for handling product variability, most notably feature models, represent intuitive means for a refurbished roof construction of the HoQ, and thus more expressiveness in the definition of functional requirements.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {75–79},
numpages = {5},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.4018/ijsse.2015010103,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Baumgartner, Philip and Loinig, Johannes and Steger, Christian and Kreiner, Christian},
title = {Balancing Product and Process Assurance for Evolving Security Systems},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/ijsse.2015010103},
doi = {10.4018/ijsse.2015010103},
abstract = {At present, security-related engineering usually requires a big up-front design BUFD regarding security requirements and security design. In addition to the BUFD, at the end of the development, a security evaluation process can take up to several months. In today's volatile markets customers want to be able to influence the software design during the development process. Agile processes have proven to support these demands. Nevertheless, there is a clash between traditional security design and evaluation processes. In this paper, the authors propose an agile security evaluation method for the Common Criteria standard. This method is complemented by an implementation of a change detection analysis for model-based security requirements. This system facilitates the agile security evaluation process to a high degree. However, the application of the proposed evaluation method is limited by several constraints. The authors discuss these constraints and show how traditional certification schemes could be extended to better support modern industrial software development processes.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {47–75},
numpages = {29},
keywords = {Agile Development, Common Criteria, Model Evolution, Model-Based Software Development, Security, Traceability}
}

@article{10.1145/3447580,
author = {Russo, Daniel and Stol, Klaas-Jan},
title = {PLS-SEM for Software Engineering Research: An Introduction and Survey},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447580},
doi = {10.1145/3447580},
abstract = {Software Engineering (SE) researchers are increasingly paying attention to organizational and human factors. Rather than focusing only on variables that can be directly measured, such as lines of code, SE research studies now also consider unobservable variables, such as organizational culture and trust. To measure such latent variables, SE scholars have adopted Partial Least Squares Structural Equation Modeling (PLS-SEM), which is one member of the larger SEM family of statistical analysis techniques. As the SE field is facing the introduction of new methods such as PLS-SEM, a key issue is that not much is known about how to evaluate such studies. To help SE researchers learn about PLS-SEM, we draw on the latest methodological literature on PLS-SEM to synthesize an introduction. Further, we conducted a survey of PLS-SEM studies in the SE literature and evaluated those based on recommended guidelines.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {78},
numpages = {38},
keywords = {Partial least squares, critical review, research methodology, structural equation modeling}
}

@inproceedings{10.1007/978-3-540-31984-9_12,
author = {Hammouda, Imed and Hautam\"{a}ki, Juha and Pussinen, Mika and Koskimies, Kai},
title = {Managing variability using heterogeneous feature variation patterns},
year = {2005},
isbn = {354025420X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-31984-9_12},
doi = {10.1007/978-3-540-31984-9_12},
abstract = {Feature-driven variability is viewed as an instance of multi-dimensional separation of concerns. We argue that feature variation concerns can be presented as pattern-like entities – called feature variation patterns – cross-cutting heterogeneous artifacts. We show that a feature variation pattern, covering a wide range of artifact types from a feature model to implementation, can be used to manage feature-driven variability in a software development process. A prototype tool environment has been developed to demonstrate the idea, supporting the specification and use of heterogeneous feature variation patterns. We illustrate the idea with a small example taken from J2EE, and further study the practical applicability of the approach in an industrial product-line.},
booktitle = {Proceedings of the 8th International Conference, Held as Part of the Joint European Conference on Theory and Practice of Software Conference on Fundamental Approaches to Software Engineering},
pages = {145–159},
numpages = {15},
location = {Edinburgh, UK},
series = {FASE'05}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {Automated analysis, Cloud migration, EC2, Feature model, IaaS}
}

@inproceedings{10.1145/2459118.2459136,
author = {Banerjee, Ansuman},
title = {A formal model for multi-tenant software-as-a-service in cloud computing},
year = {2012},
isbn = {9781450314404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2459118.2459136},
doi = {10.1145/2459118.2459136},
abstract = {A multi-tenant software as a service (SaaS) system has to meet the needs of several tenant organizations, which connect to the system to utilize its services. To leverage economies of scale through re-use, a SaaS vendor would, in general, like to drive commonality amongst the requirements across tenants. However, many tenants will also come with some custom requirements that may be a pre-requisite for them to adopt the SaaS system. These requirements then need to be addressed by evolving the SaaS system in a controlled manner, while still supporting the requirements of existing tenants. In this paper, we study the challenges associated with engineering multi-tenant SaaS systems and develop a framework to help evolve and validate such systems in a systematic manner. We adopt an intuitive formal model of services. We show that the proposed formalism is easily amenable to tenant requirement analysis and provides a systematic way to support multiple tenant on-boarding and diverse service management.},
booktitle = {Proceedings of the 5th ACM COMPUTE Conference: Intelligent &amp; Scalable System Technologies},
articleno = {18},
numpages = {6},
location = {Pune, India},
series = {COMPUTE '12}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@article{10.1016/j.aei.2014.05.003,
author = {Zheng, Chen and Bricogne, Matthieu and Le Duigou, Julien and Eynard, Beno\'{\i}t},
title = {Survey on mechatronic engineering: A focus on design methods and product models},
year = {2014},
issue_date = {August, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {28},
number = {3},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2014.05.003},
doi = {10.1016/j.aei.2014.05.003},
abstract = {According to the principles of concurrent engineering and integrated design, engineers intend to develop a mechatronic system with a high level integration (functional and physical integrations) based on a well-organised design method. As a result, two main categories of issues have been pointed out: the process-based problems and the design data-related problems. Several approaches to overcome these issues have been put forward. To solve process-based problems, a dynamic perspective is generally used to present how collaboration can be improved during the mechatronic design. For design data-related problems, solutions generally come from product models and how to structure and store the data thanks to the functionality of data and documents management of Product Lifecycle Management systems. To be able to assess design methods and product models, some criteria are proposed in the paper and used to evaluate their added value on integrated design of mechatronic system. After this assessment, main outcomes which focus on the combination of design method and product model for improving the design of mechatronic system are finally discussed.},
journal = {Adv. Eng. Inform.},
month = aug,
pages = {241–257},
numpages = {17},
keywords = {Design methods, Integrated design, Mechatronic engineering, Product Lifecycle Management, Product models, Systems engineering}
}

@article{10.1145/54132.54135,
author = {Calabaugh, Jerry},
title = {Software configuration—an NP-complete problem},
year = {1988},
issue_date = {Summer 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/54132.54135},
doi = {10.1145/54132.54135},
abstract = {Configuration Control File (CCF) production is very complex, thousands of code packages, data blocks and parameter values must be linked under many constraints including:*Common data and code less than 8192 bytes*Maximum of 5 registers per task*All systems data must have common capabilitiesNP-complete problems are commonly known as knapsack or bin packing problems. They have no known algorithm which solves them in a time period bounded by a polynominal function of the number of inputs. Rules-of-thumb, or heuristics are the only practical approach to their solution. CCF segmentation to meet constraints discussed above is an example of Expert System technology applied to a classic NP-complete problem.Heuristics developed with traditional data processing techniques initially performed satisfactorily. However, as program development proceeded, Central Processor Unit (CPU) time for (CCF) production became a concern, both from a commitment of CPU resources and lost productivity. Traditional techniques failed to improve the heuristics and the project began to slip. Projected time to produce the CCF for a fully developed program was totally unacceptable, and jeopardized the project.Clearly another approach was required. Because existing hueristics were based on a concept of rules, research indicated an expert system using rules and a knowledge based approach had the highest probability of success.The paper emphasizes the development process of a knowledge based system from the perspective of the responsible project manager. The methodology is also applicable to common business problems.},
journal = {SIGMIS Database},
month = aug,
pages = {29–34},
numpages = {6}
}

@article{10.1007/s10664-009-9120-1,
author = {Hackbarth, Randy L. and Mockus, Audris and Palframan, John Douglas and Weiss, David M.},
title = {Assessing the state of software in a large enterprise},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9120-1},
doi = {10.1007/s10664-009-9120-1},
abstract = {To be relevant to the goals of an enterprise, an industrial software engineering research organization must identify problems of interest to, and find solutions that have an impact on, the software development organizations within the company. Using a systematic measurement program both to identify the problems and assess the impact of solutions is key to satisfying this need. Avaya has had such a program in place for about seven years. Every year we produce an annual report known as the State of Software in Avaya that describes software development trends throughout the company and that contains prioritized recommendations for improving Avaya's software development capabilities. We start by identifying the goals of the enterprise and use the goal-question-metric approach to identify the measures to compute. The result is insight into the enterprise's problems in software development, recommendations for improving the development process, and problems that require research to solve. We will illustrate the process with examples from the Software Technology Research Department in Avaya Labs whose purpose is to improve the state of software development and know it. "Know it" means that improvement should be subjectively evident and objectively quantifiable. "Know it" also means that one must be skilled at identifying the data sources, performing the appropriate analyses to answer the questions of interest, and validating that the data are accurate and appropriate for the purpose. Examples will include how and why we developed a measure of software quality that appeals to customers, how and why we are studying the effectiveness of distributed software development, and how and why we are helping development organizations to adopt iterative development methods. We will also discuss how we keep the company and the department apprised of the current strengths and weaknesses of software development in Avaya through the publication of the annual State of Software in Avaya Report. Our purpose is both to provide a model for assessment that others may emulate, based on seven years of experience, and to spotlight analyses and conclusions that we feel are common to software development today.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {219–249},
numpages = {31},
keywords = {Agile Development, Code growth, Distributed and offshore development, Iterative development, R&amp;D assessment, R&amp;D domain expertise, R&amp;D productivity, Software assessments, Software development, Software engineering, Software metrics, Software practices, Software process improvement, Software quality}
}

@inproceedings{10.1145/3357384.3358163,
author = {Huang, Chao and Shi, Baoxu and Zhang, Xuchao and Wu, Xian and Chawla, Nitesh V.},
title = {Similarity-Aware Network Embedding with Self-Paced Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358163},
doi = {10.1145/3357384.3358163},
abstract = {Network embedding, which aims to learn low-dimensional vector representations for nodes in a network, has shown promising performance for many real-world applications, such as node classification and clustering. While various embedding methods have been developed for network data, they are limited in their assumption that nodes are correlated with their neighboring nodes with the same similarity degree. As such, these methods can be suboptimal for embedding network data. In this paper, we propose a new method named SANE, short for Similarity-Aware Network Embedding, to learn node representations by explicitly considering different similarity degrees between connected nodes in a network. In particular, we develop a new framework based on self-paced learning by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved node similarities) in network representation learning. To justify our proposed model, we perform experiments on two real-world network data. Experiments results show that SNAE outperforms state-of-the-art embedding models on the tasks of node classification and node clustering.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2113–2116},
numpages = {4},
keywords = {deep neural network, network embedding, self-paced learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/645341.650259,
author = {Raghavan, Gopalakrishna},
title = {Improving Software Quality in Product Families through Systematic Reengineering},
year = {2002},
isbn = {3540437495},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software quality is a very subjective attribute and is a complex mixture of several factors. There is no universal definition or a unique metric to quantify software quality. It is usually measured by analyzing various factors that are significant to the domain or application. It is evident that the end user of a product realizes substantial benefits due to improved software quality. Therefore many software industries strive hard to improve the quality of their product by investing in quality control and quality assurance activities like inspections, reviews, testing and audits. However, many software companies do not endeavor into reengineering activities to reap quality improvements. Product families that share legacy components, which have a lot of common features, could be reengineered in a systematic manner to consolidate knowledge and produce common components that can accommodate future applications. A significant by-product of this systematic reengineering activity would be an improved software quality. This paper presents a systematic reengineering approach and also identifies different quality factors that could be improved during this process. The proposed reengineering technique was used at Nokia Research Center to reengineer existing mobile systems in an efficient manner so that more applications and operating modes could be supported.},
booktitle = {Proceedings of the 7th International Conference on Software Quality},
pages = {90–99},
numpages = {10},
series = {ECSQ '02}
}

@article{10.1016/j.aei.2016.10.001,
author = {Qin, Feiwei and Gao, Shuming and Yang, Xiaoling and Li, Ming and Bai, Jing},
title = {An ontology-based semantic retrieval approach for heterogeneous 3D CAD models},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {4},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2016.10.001},
doi = {10.1016/j.aei.2016.10.001},
abstract = {Many types of heterogeneous CAD models created by previous product development activities are stored in enterprise databases. Searching reusable CAD models for engineer design requirements is still very difficult. In this paper, we propose an ontology-based semantic retrieval approach for heterogeneous 3D CAD models by using semantic web theory and information retrieval theory. By exploiting layered feature ontology and ontology mapping, a uniform description for heterogeneous CAD models is generated as semantic descriptor. Meanwhile, to improve the representative ability, several heuristic rules are constructed with the aid of SWRL language and ontology reasoning is acted on the semantic descriptors, which makes our retrieval system achieve better performance. Furthermore, multi-mode similarity measurement is carried out to meet engineers diversified requirements. We demonstrate the feasibility and effectiveness of our approach through experiments on heterogeneous 3D CAD model library.},
journal = {Adv. Eng. Inform.},
month = oct,
pages = {751–768},
numpages = {18},
keywords = {Heterogeneous, Model retrieval, Ontology, Semantic-based retrieval}
}

@book{10.5555/2974976,
author = {Douglass, Bruce Powel},
title = {Agile Systems Engineering},
year = {2015},
isbn = {9780128023495},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Agile Systems Engineering presents a vision of systems engineering where precise specification of requirements, structure, and behavior meet larger concerns as such as safety, security, reliability, and performance in an agile engineering context. World-renown author and speaker Dr. Bruce Powel Douglass incorporates agile methods and model-based systems engineering (MBSE) to define the properties of entire systems while avoiding errors that can occur when using traditional textual specifications. Dr. Douglass covers the lifecycle of systems development, including requirements, analysis, design, and the handoff to specific engineering disciplines. Throughout, Dr. Douglass couples agile methods with SysML and MBSE to arm system engineers with the conceptual and methodological tools they need to avoid specification defects and improve system quality while simultaneously reducing the effort and cost of systems engineering. Identifies how the concepts and techniques of agile methods can be effectively applied in systems engineering context Shows how to perform model-based functional analysis and tie these analyses back to system requirements and stakeholder needs, and forward to system architecture and interface definition Provides a means by which the quality and correctness of systems engineering data can be assured (before the entire system is built!) Explains agile system architectural specification and allocation of functionality to system components Details how to transition engineering specification data to downstream engineers with no loss of fidelity Includes detailed examples from across industries taken through their stages, including the "Waldo" industrial exoskeleton as a complex system Table of Contents What is Model-Based Systems Engineering What are Agile Methods and Why Should I Care The importance of Agile methods Agile Stakeholder Requirements Engineering Agile Systems Requirements Definition and Analysis System Architectural Analysis and Trade Studies Agile Systems Architectural Design The Handoff to Downstream Engineering Appendix A. T-Wrecks Stakeholder Requirements Appendix B. T-Wrecks System Requirements}
}

@article{10.4018/jsse.2010102004,
author = {Nhlabatsi, Armstrong and Nuseibeh, Bashar and Yu, Yijun},
title = {Security Requirements Engineering for Evolving Software Systems: A Survey},
year = {2010},
issue_date = {January 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010102004},
doi = {10.4018/jsse.2010102004},
abstract = {Long-lived software systems often undergo evolution over an extended period. Evolution of these systems is inevitable as they need to continue to satisfy changing business needs, new regulations and standards, and introduction of novel technologies. Such evolution may involve changes that add, remove, or modify features; or that migrate the system from one operating platform to another. These changes may result in requirements that were satisfied in a previous release of a system not being satisfied in subsequent versions. When evolutionary changes violate security requirements, a system may be left vulnerable to attacks. In this article we review current approaches to security requirements engineering and conclude that they lack explicit support for managing the effects of software evolution. We then suggest that a cross fertilization of the areas of software evolution and security engineering would address the problem of maintaining compliance to security requirements of software systems as they evolve.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {54–73},
numpages = {20},
keywords = {Entailment Relation, Security Requirements Engineering, Software Evolution}
}

@inproceedings{10.1007/978-3-030-02131-3_26,
author = {Wilson, Magnus and Wnuk, Krzysztof},
title = {Business Modeling and Flexibility in Software-Intensive Product Development - A Systematic Literature Review},
year = {2018},
isbn = {978-3-030-02130-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-02131-3_26},
doi = {10.1007/978-3-030-02131-3_26},
abstract = {Continuously achieving and maintaining competitive advantage is the critical survival factor for software-intensive product development companies undergoing digitalization transformation. These companies remain uncertain if investments in business modeling is sufficient to cope with rapidly changing business models, technology, and customer demands. We conducted a Systematic Literature Review using the snowballing methodology to explore the effects of business modeling on business flexibility and variability in the realization. Our results confirm a research gap regarding translating desired strategic flexibility into business options that can efficiently and effectively be implemented using software-based variability in the realization. We conclude that more research is needed consolidating business model innovation, experimentation, and operationalization. Building on theories for learning and knowledge creation, we propose a framework for describing change and analyzing strategic, tactical and operational choices in business model experimentation.},
booktitle = {Challenges and Opportunities in the Digital Era: 17th IFIP WG 6.11 Conference on e-Business, e-Services, and e-Society, I3E 2018, Kuwait City, Kuwait, October 30 – November 1, 2018, Proceedings},
pages = {292–304},
numpages = {13},
location = {Kuwait City, Kuwait}
}

@article{10.1016/j.jss.2016.12.016,
author = {Landwehr, Carl and Ludewig, Jochen and Meersman, Robert and Parnas, David Lorge and Shoval, Peretz and Wand, Yair and Weiss, David and Weyuker, Elaine},
title = {Software Systems Engineering programmes a capability approach},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {125},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.12.016},
doi = {10.1016/j.jss.2016.12.016},
abstract = {Discusses undergraduate programmes that prepare graduates for a career building software intensive systems.Presents detailed description of capabilities that graduates should have acquired.Derived from historical discussions of the field.Explains differences between Science programmes and Engineering programmes.Presents a broad set of specialized programs. This paper discusses third-level educational programmes that are intended to prepare their graduates for a career building systems in which software plays a major role. Such programmes are modelled on traditional Engineering programmes but have been tailored to applications that depend heavily on software. Rather than describe knowledge that should be taught, we describe capabilities that students should acquire in these programmes. The paper begins with some historical observations about the software development field.},
journal = {J. Syst. Softw.},
month = mar,
pages = {354–364},
numpages = {11},
keywords = {Education, Engineering, Information systems, Software design, Software development, Software documentation, Software education}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.1007/s10845-013-0736-z,
author = {Tsai, Chieh-Yuan and Chen, Chih-Jung and Lo, Yu-Ting},
title = {A cost-based module mining method for the assemble-to-order strategy},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {6},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-013-0736-z},
doi = {10.1007/s10845-013-0736-z},
abstract = {The assemble-to-order (ATO) strategy is one of the most popular operations management approaches to achieve mass customized products while maintaining lower costs. In the ATO system, manufacturers keep inventory at the component and module level, and postpone product differentiation until the final stage of production. However, most research on modularity assumes that modules are already known in advance. In fact, in the ATO system, the determination of which components should be pre-assembled as modules mainly depends on the types and volumes of products ordered by customers. That is, module composition and volume should be derived dynamically from the product database based on updated customer orders. To bridge this gap, a two-stage cost-based module mining method for the assemble-to-order strategy is proposed. The first stage determines which sets of components can be formed (pre-assembled) as modules based on a list of customer orders. In the second stage, a cost-based selection approach is developed to evaluate the total cost of each module implementation project generated from the set of feasible modules. The module implementation project with the lowest cost is thus found and suggested to production managers.},
journal = {J. Intell. Manuf.},
month = dec,
pages = {1377–1392},
numpages = {16},
keywords = {Assemble-to-order, Cost analysis, Data mining, Modularity, Module mining}
}

@article{10.1016/j.is.2015.06.003,
author = {Alam, Khubaib Amjad and Ahmad, Rodina and Akhunzada, Adnan and Nasir, Mohd Hairul Nizam Md and Khan, Samee U.},
title = {Impact analysis and change propagation in service-oriented enterprises},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {54},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2015.06.003},
doi = {10.1016/j.is.2015.06.003},
abstract = {ContextThe adoption of Service-oriented Architecture (SOA) and Business Process Management (BPM) is fairly recent. The major concern is now shifting towards the maintenance and evolution of service-based business information systems. Moreover, these systems are highly dynamic and frequent changes are anticipated across multiple levels of abstraction. Impact analysis and change propagation are identified as potential research areas in this regard. ObjectiveThe aim of this study is to systematically review extant research on impact analysis and propagation in the BPM and SOA domains. Identifying, categorizing and synthesizing relevant solutions are the main study objectives. MethodThrough careful review and screening, we identified 60 studies relevant to 4 research questions. Two classification schemes served to comprehend and analyze the anatomy of existing solutions. BPM is considered at the business level for business operations and processes, while SOA is considered at the service level as deployment architecture. We focused on both horizontal and vertical impacts of changes across multiple abstraction layers. ResultsImpact analysis solutions were mainly divided into dependency analysis, traceability analysis and history mining. Dependency analysis is the most frequently adopted technique followed by traceability analysis. Further categorization of dependency analysis indicates that graph-based techniques are extensively used, followed by formal dependency modeling. While considering hierarchical coverage, inter-process and inter-service change analyses have received considerable attention from the research community, whereas bottom-up analysis has been the most neglected research area. The majority of change propagation solutions are top-down and semi-automated. ConclusionsThis study concludes with new insight suggestions for future research. Although, the evolution of service-based systems is becoming of grave concern, existing solutions in this field are less mature. Studies on hierarchical change impact are scarce. Complex relationships of services with business processes and semantic dependencies are poorly understood and require more attention from the research community.},
journal = {Inf. Syst.},
month = dec,
pages = {43–73},
numpages = {31},
keywords = {BPM, CIA, Change propagation, Dependency analysis, MSR, SOA, SOC, Semantic annotation, Systematic literature review (SLR), Web service}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@article{10.5555/2594638.2594646,
author = {Tiihonen, Juha and Heiskala, Mikko and Anderson, Andreas and Soininen, Timo},
title = {WeCoTin --A practical logic-based sales configurator},
year = {2013},
issue_date = {January 2013},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {1},
issn = {0921-7126},
abstract = {Configurable products can realize the ideal of mass-customization by satisfying individual customer requirements efficiently. IT support provided by configurators enables adapting such products for individual customers efficiently and without errors. Few of numerous configurators have been evaluated with respect to modeling efficacy and performance on several product domains, and few evaluation methods exist. Applying the Design Science method, we describe and evaluate a novel configurator called WeCoTin. WeCoTin is based on a high-level object oriented modeling conceptualization and corresponding modeling language with clear formal semantics. WeCoTin consists of a semi-visual Modeling Tool and a web-based Configuration Tool. It applies an inference engine that follows the logic-based answer set programming paradigm. A way to characterize configuration models is proposed and applied to characterize over 20 real-world configuration models, and to evaluate utility of modeling mechanisms. Furthermore, performance is evaluated with real-world products using a developed method, and found adequate.},
journal = {AI Commun.},
month = jan,
pages = {99–131},
numpages = {33},
keywords = {Configuration Knowledge Characterization, Evaluation Of Configurators, Integrated Development Environment, Knowledge-Based Configuration}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@book{10.5555/2505465,
author = {Schmidt, Richard},
title = {Software Engineering: Architecture-driven Software Development},
year = {2013},
isbn = {0124077684},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Engineering: Architecture-driven Software Development is the first comprehensive guide to the underlying skills embodied in the IEEE's Software Engineering Body of Knowledge (SWEBOK) standard. Standards expert Richard Schmidt explains the traditional software engineering practices recognized for developing projects for government or corporate systems. Software engineering education often lacks standardization, with many institutions focusing on implementation rather than design as it impacts product architecture. Many graduates join the workforce with incomplete skills, leading to software projects that either fail outright or run woefully over budget and behind schedule. Additionally, software engineers need to understand system engineering and architecture-the hardware and peripherals their programs will run on. This issue will only grow in importance as more programs leverage parallel computing, requiring an understanding of the parallel capabilities of processors and hardware. This book gives both software developers and system engineers key insights into how their skillsets support and complement each other. With a focus on these key knowledge areas, Software Engineering offers a set of best practices that can be applied to any industry or domain involved in developing software products. A thorough, integrated compilation on the engineering of software products, addressing the majority of the standard knowledge areas and topics Offers best practices focused on those key skills common to many industries and domains that develop software Learn how software engineering relates to systems engineering for better communication with other engineering professionals within a project environment}
}

@article{10.1007/s11257-015-9159-1,
author = {Dim, Eyal and Kuflik, Tsvi and Reinhartz-Berger, Iris},
title = {When user modeling intersects software engineering: the info-bead user modeling approach},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-015-9159-1},
doi = {10.1007/s11257-015-9159-1},
abstract = {User models (UMs) allow systems to provide personalized services to their users. Nowadays, UMs are developed ad-hoc, as part of specific applications, thus requiring repetitive development efforts. In this paper, we propose the info-bead user modeling approach, which is based on ideas taken from software engineering in general and component-based software development in particular. The basic standalone unit, the info-bead, represents a single user attribute within time-tagged information-items. An info-bead encapsulates an inference process that uses data received from sensors or other info-beads and yields an information-item value. Having standard interfaces, info-beads can be linked, thus creating info-pendants. Both info-beads and info-pendants can be assembled as needed into complex and abstract user models (UMs) and group models (GMs). The goal of the suggested approach is to ease the modeling process and to allow reuse of info beads developed for one UM in other UMs that need the same information. In order to assess the reusability and collaboration capabilities of the info-bead user modeling approach, we developed a prototype tool that enables UM designers, who are not necessarily software developers, to easily select and integrate info-beads for constructing UMs and GMs. We further demonstrated the use of the approach in a museum environment, for modeling of assistive technology ontology and for user modeling in various specific domains. Finally, we analyzed and assessed the characteristics of the approach with respect to existing generic user modeling criteria.},
journal = {User Modeling and User-Adapted Interaction},
month = aug,
pages = {189–229},
numpages = {41},
keywords = {Component-based user model, Group model, Info-bead, Info-pendant, User model, User model reusability, User modeling software engineering, User modeling tool}
}

@article{10.5555/1275840.1275845,
author = {Gu, Peihua},
title = {Adaptable Design With Flexible Interface Systems},
year = {2004},
issue_date = {August 2004},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {3},
issn = {1092-0617},
abstract = {The manufacturing industry is one of the major driving forces for economic success for all industrialized nations. As new technologies are rapidly evolving, the world economy has been undergoing some major changes. The impact of these changes on product manufacturing industry includes more globalized competition, shorter market life cycles, more stringent environmental regulations, more educated customers and so forth. Therefore, products must meet various requirements such as shorter delivery time, customization, environment-friendly, in addition to the traditional requirements of functionality, cost and quality. These requirements cannot be met by simply using more advanced manufacturing technologies only. Manufacturers must produce products according to design. More than 75% of the total product costs as well as all product features are determined at the design stage. Therefore, significant improvement on product quality, features, costs and other life cycle performance can be achieved by improving design. This paper reports a new design method, namely adaptable design, to address those conflict requirements. An adaptable design method has been developed, consisting of four main phases: product modelling, platform design, bus system design and design evaluation and re-design. Examples are provided to illustrate the adaptable design and interface systems. In addition, a review of recent developments in design research is provided. Future research directions and possible topics for research are also discussed for both fundamental as well as applied work in engineering design.},
journal = {J. Integr. Des. Process Sci.},
month = aug,
pages = {61–74},
numpages = {14},
keywords = {adaptable design, design theory and methodology, flexible interfaces, life cycle engineering, mechanical buses, product design}
}

@inbook{10.5555/2206963.2207001,
author = {da Silva, Alberto Rodrigues},
title = {Tools exhibits},
year = {2004},
isbn = {3540250816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Live demonstrations of cutting-edge systems were an important and exciting part of the UML2004 conference. The tool exhibits session provided an excellent opportunity where participants analysed and viewed relevant UML and MDA related tools in action and discussed these systems with their creators or distributors. The tool exhibits session took place during the main conference, from October 13 to 15, and included the following live demos: (1) “seCAKE: A complete CASE tool with reuse support”, by dTinf; (2) “Making UML diagrams accessible for visually impaired programmers”, by FNB; (3) “TAU Generation2”, by Telelogic; (4) “IBM Rational Rose XDE Products”, by Sinfic; and (5) “BridgePoint Development Suite”, by Mentor Graphics. The tool exhibit contributions are described in this paper in the form of an extended summary. We briefly describe the related products according the data provided by their respective creators or distributors.},
booktitle = {UML Modeling Languages and Applications},
pages = {281–291},
numpages = {11}
}

@inproceedings{10.1007/978-3-540-31797-5_34,
author = {da Silva, Alberto Rodrigues},
title = {Tools exhibits},
year = {2004},
isbn = {3540250816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-31797-5_34},
doi = {10.1007/978-3-540-31797-5_34},
abstract = {Live demonstrations of cutting-edge systems were an important and exciting part of the UML2004 conference. The tool exhibits session provided an excellent opportunity where participants analysed and viewed relevant UML and MDA related tools in action and discussed these systems with their creators or distributors. The tool exhibits session took place during the main conference, from October 13 to 15, and included the following live demos: (1) “seCAKE: A complete CASE tool with reuse support”, by dTinf; (2) “Making UML diagrams accessible for visually impaired programmers”, by FNB; (3) “TAU Generation2”, by Telelogic; (4) “IBM Rational Rose XDE Products”, by Sinfic; and (5) “BridgePoint Development Suite”, by Mentor Graphics. The tool exhibit contributions are described in this paper in the form of an extended summary. We briefly describe the related products according the data provided by their respective creators or distributors.},
booktitle = {Proceedings of the 2004 International Conference on UML Modeling Languages and Applications},
pages = {281–291},
numpages = {11},
location = {Lisbon, Portugal},
series = {UML'04}
}

@article{10.1016/j.comnet.2009.10.004,
author = {Kant, Krishna},
title = {Data center evolution},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {53},
number = {17},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2009.10.004},
doi = {10.1016/j.comnet.2009.10.004},
abstract = {Data centers form a key part of the infrastructure upon which a variety of information technology services are built. As data centers continue to grow in size and complexity, it is desirable to understand aspects of their design that are worthy of carrying forward, as well as existing or upcoming shortcomings and challenges that would have to be addressed. We envision the data center evolving from owned physical entities to potentially outsourced, virtualized and geographically distributed infrastructures that still attempt to provide the same level of control and isolation that owned infrastructures do. We define a layered model for such data centers and provide a detailed treatment of state of the art and emerging challenges in storage, networking, management and power/thermal aspects.},
journal = {Comput. Netw.},
month = dec,
pages = {2939–2965},
numpages = {27},
keywords = {Data center, Ethernet, InfiniBand, Power management, Solid state storage, Virtualization}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network, 00-01, 99-00}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1016/j.infsof.2012.12.003,
author = {Moros, Bego\~{n}a and Toval, Ambrosio and Rosique, Francisca and S\'{a}nchez, Pedro},
title = {Transforming and tracing reused requirements models to home automation models},
year = {2013},
issue_date = {June, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.12.003},
doi = {10.1016/j.infsof.2012.12.003},
abstract = {Context: Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project's success. Despite the importance of RE, MDSD approaches commonly leave textual requirements specifications to one side. Objective: Our aim is to integrate textual requirements specifications into the MDSD approach by using the MDSD techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained. Method: The approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code. Results: The approach has been evaluated through its application to Home Automation (HA) systems. The HA Requirement Specification Language denominated as HAREL is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the HA Domain Specific Language. Conclusions: An anonymous online survey has been conducted to evaluate the degree of acceptance by both HA application developers and MDSD practitioners. The main conclusion is that 66.7% of the HA experts polled strongly agree that the automatic transformation of the requirements models to HA models improves the quality of the HA models. Moreover, 58.3% of the HA participants strongly agree with the usefulness of the traceability matrix which links requirements to HA functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {941–965},
numpages = {25},
keywords = {Home automation models, Model driven software development, Models transformation, Requirements metamodel, Requirements reuse, Requirements traceability}
}

@inproceedings{10.5555/2337223.2337260,
author = {Li, Jingyue and Ernst, Michael D.},
title = {CBCD: cloned buggy code detector},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Developers often copy, or clone, code in order to reuse or modify functionality. When they do so, they also clone any bugs in the original code. Or, different developers may independently make the same mistake. As one example of a bug, multiple products in a product line may use a component in a similar wrong way. This paper makes two contributions. First, it presents an empirical study of cloned buggy code. In a large industrial product line, about 4% of the bugs are duplicated across more than one product or file. In three open source projects (the Linux kernel, the Git version control system, and the PostgreSQL database) we found 282, 33, and 33 duplicated bugs, respectively. Second, this paper presents a tool, CBCD, that searches for code that is semantically identical to given buggy code. CBCD tests graph isomorphism over the Program Dependency Graph (PDG) representation and uses four optimizations. We evaluated CBCD by searching for known clones of buggy code segments in the three projects and compared the results with text-based, token-based, and AST-based code clone detectors, namely Simian, CCFinder, Deckard, and CloneDR. The evaluation shows that CBCD is fast when searching for possible clones of the buggy code in a large system, and it is more precise for this purpose than the other code clone detectors.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {310–320},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.5555/1275731.1275736,
author = {Mittasch, Christian and Weise, Thomas and Hesselmann, Margret},
title = {Decentralized control structures for distributed workflow applications},
year = {2000},
issue_date = {December 2000},
publisher = {IOS Press},
address = {NLD},
volume = {7},
number = {4},
issn = {1069-2509},
abstract = {This contribution provides an overview about decentralized control in Workflow Management Systems (WfMS) and introduces BPAFrame (Framework for Business Process Automation), a CORBA-based decentralized framework for development and runtime-support of workflows. Several possibilities of configuration and deployment of such an approach are discussed. Performance analysis on the BPAFrame prototype gives an opportunity to prove the influence of configuration and the impact of distribution of server processes. It will be shown that decentralized and generic components provide for current requirements on software systems like WfMS a convenient future adoption and integration into existing software infrastructures.},
journal = {Integr. Comput.-Aided Eng.},
month = dec,
pages = {327–341},
numpages = {15}
}

@article{10.1145/3280988,
author = {Razzaq, Abdul and Wasala, Asanka and Exton, Chris and Buckley, Jim},
title = {The State of Empirical Evaluation in Static Feature Location},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280988},
doi = {10.1145/3280988},
abstract = {Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {2},
numpages = {58},
keywords = {Feature location, bug location, concept location, requirement traceability}
}

@proceedings{10.1145/2984043,
title = {SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1007/11767718_30,
author = {Regnell, Bj\"{o}rn and Olsson, Hans O. and Mossberg, Staffan},
title = {Assessing requirements compliance scenarios in system platform subcontracting},
year = {2006},
isbn = {3540346821},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767718_30},
doi = {10.1007/11767718_30},
abstract = {In the mobile industry, system platforms are offered to device developers to enable rapid product development while sharing expensive technology development investments. This paper presents a framework for assessment of requirements engineering collaboration related to statements-of-compliance negotiation in platform subcontracting. The framework includes a classification of platform compliance scenarios and results from analysis of interviews with engineers at two collaborating companies, a device vendor and a platform vendor. Case study findings particular to the compliance scenarios of the framework are provided. The purpose of the framework is to provide a basis for process improvement in collaborative requirements engineering.},
booktitle = {Proceedings of the 7th International Conference on Product-Focused Software Process Improvement},
pages = {362–376},
numpages = {15},
location = {Amsterdam, The Netherlands},
series = {PROFES'06}
}

@inproceedings{10.1007/978-3-030-27544-0_11,
author = {A\c{s}\i{}k, Okan and G\"{o}rer, Binnur and Ak\i{}n, H. Levent},
title = {End-to-End Deep Imitation Learning: Robot Soccer Case Study},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_11},
doi = {10.1007/978-3-030-27544-0_11},
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {137–149},
numpages = {13},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/2384616.2384673,
author = {K\"{a}stner, Christian and Ostermann, Klaus and Erdweg, Sebastian},
title = {A variability-aware module system},
year = {2012},
isbn = {9781450315616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384616.2384673},
doi = {10.1145/2384616.2384673},
abstract = {Module systems enable a divide and conquer strategy to software development. To implement compile-time variability in software product lines, modules can be composed in different combinations. However, this way, variability dictates a dominant decomposition. As an alternative, we introduce a variability-aware module system that supports compile-time variability inside a module and its interface. So, each module can be considered a product line that can be type checked in isolation. Variability can crosscut multiple modules. The module system breaks with the antimodular tradition of a global variability model in product-line development and provides a path toward software ecosystems and product lines of product lines developed in an open fashion. We discuss the design and implementation of such a module system on a core calculus and provide an implementation for C as part of the TypeChef project. Our implementation supports variability inside modules from #ifdef preprocessor directives and variable linking at the composition level. With our implementation, we type check all configurations of all modules of the open source product line Busybox with 811~compile-time options, perform linker check of all configurations, and report found type and linker errors -- without resorting to a brute-force strategy.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {773–792},
numpages = {20},
keywords = {#ifdef, C, composition, conditional compilation, linker, module system, preprocessor, software product lines, variability},
location = {Tucson, Arizona, USA},
series = {OOPSLA '12}
}

@article{10.1145/3084225,
author = {Storer, Tim},
title = {Bridging the Chasm: A Survey of Software Engineering Practice in Scientific Programming},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3084225},
doi = {10.1145/3084225},
abstract = {The use of software is pervasive in all fields of science. Associated software development efforts may be very large, long lived, and complex, requiring the commitment of significant resources. However, several authors have argued that the “gap” or “chasm” between software engineering and scientific programming is a serious risk to the production of reliable scientific results, as demonstrated in a number of case studies. This article reviews the research that addresses the gap, exploring how both software engineering and research practice may need to evolve to accommodate the use of software in science.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {47},
numpages = {32},
keywords = {Software engineering, scientific programming}
}

@book{10.5555/1534292,
author = {Williams, David E.},
title = {Virtualization with Xen(tm): Including XenEnterprise, XenServer, and XenExpress: Including XenEnterprise, XenServer, and XenExpress},
year = {2007},
isbn = {9780080553931},
publisher = {Syngress Publishing},
abstract = {Complete Coverage of Xen, Including Version 3.2 Virtualization with Xen is the first book to demonstrate to readers how to install, administer, and maintain a virtual infrastructure based on XenSource's latest release, Xen 3.2. It discusses best practices for setting up a Xen environment correctly the first time, maximizing the utilization of server assets while taking advantage of the fastest and most secure enterprise-grade paravirtualization architecture. It covers both basic and advanced topics, such as planning and installation, physical-to-virtual migrations, virtual machine provisioning, resource management, and monitoring and troubleshooting guests and Xen hosts. * Explore Xen's Virtualization Model Find a complete overview of the architecture model as well of all products: Xen 3.0 , Xen Express, XenServer, and Xen Enterprise. * Deploy Xen Understand the system requirements, learn installation methods, and see how to install Xen on a free Linux distribution. * Master the Administrator Console Learn how to use the command-line tools and the remote Java-based consoler that manages the configuration and operations of XenServer hosts and VMs. * Manage Xen with Third-Party Tools Use products like openQRM, Enomalism, and Project ConVirt to manage the VMM. * Deploy a Virtual Machine in Xen Learn about workload planning and installing modified guests, unmodified guests, and Windows guests. * Explore Advanced Xen Concepts Build a Xen Cluster, complete a XenVM migration, and discover XenVM backup and recovery solutions. * See the Future of Virtualization See the unofficial Xen road map and what virtual infrastructure holds for tomorrow's data center. * See Other Virtualization Technologies and How They Compare with Xen Take a look at the different types of server virtualization, other virtual machine software available, and how they compare with Xen. *Complete with a demonstration version of Xen 3.2 on CD-ROM *Xen has the lead in the open-source community; now distributed as a standard kernel package for Novell's SLES 10 and Red Hat's RHEL 5 and Fedora Core 6 Linux distributions *Covers installation, administration, management, monitoring, and deployment planning and strategies}
}

@article{10.1145/309844.310071,
author = {Carey, Mike and Seligman, Len},
title = {NSF workshop on industrial/academic cooperation in database systems},
year = {1999},
issue_date = {March 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/309844.310071},
doi = {10.1145/309844.310071},
journal = {SIGMOD Rec.},
month = mar,
pages = {115–130},
numpages = {16}
}

@article{10.1016/j.knosys.2011.06.019,
author = {Al-Ashaab, Ahmed and Molyneaux, M. and Doultsinou, A. and Brunner, B. and Mart\'{\i}Nez, E. and Moliner, F. and Santamar\'{\i}A, V. and Tanjore, D. and Ewers, P. and Knight, G.},
title = {Knowledge-based environment to support product design validation},
year = {2012},
issue_date = {February, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {26},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2011.06.019},
doi = {10.1016/j.knosys.2011.06.019},
abstract = {A knowledge-based environment to support product design validation of refresh projects (projects with minor changes in comparison with previous ones) has been developed. This was achieved by capturing the relation between the design change of refresh projects with their required physical tests and the historical data of previous projects. The refresh projects constitute the majority, around 60%, of the total number of projects in the collaborating company. The knowledge-based environment framework (Knowledge-Based Environment to Support Product Design Validation - KBE-ProVal) has been implemented on the Product Lifecycle Management Platform, called Teamcenter. The KBE-ProVal development was based on the standardisation of the existing related documents while maintaining the traceability of the decision making process. This implementation will avoid repeating unnecessary and costly physical product tests, thus reducing time and costs for these refresh projects.},
journal = {Know.-Based Syst.},
month = feb,
pages = {48–60},
numpages = {13},
keywords = {Knowledge based environment, PLM, Product design validation, Product development, Teamcenter}
}

@article{10.1007/s10664-015-9401-9,
author = {Jonsson, Leif and Borg, Markus and Broman, David and Sandahl, Kristian and Eldh, Sigrid and Runeson, Per},
title = {Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9401-9},
doi = {10.1007/s10664-015-9401-9},
abstract = {Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1533–1578},
numpages = {46},
keywords = {Bug assignment, Bug reports, Classification, Ensemble learning, Industrial scale; Large scale, Machine learning}
}

@article{10.1007/s10270-019-00752-x,
author = {Anjorin, Anthony and Buchmann, Thomas and Westfechtel, Bernhard and Diskin, Zinovy and Ko, Hsiang-Shang and Eramo, Romina and Hinkel, Georg and Samimi-Dehkordi, Leila and Z\"{u}ndorf, Albert},
title = {Benchmarking bidirectional transformations: theory, implementation, application, and assessment},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00752-x},
doi = {10.1007/s10270-019-00752-x},
abstract = {Bidirectional transformations (bx) are relevant for a wide range of application domains. While bx problems may be solved with unidirectional languages and tools, maintaining separate implementations of forward and backward synchronizers with mutually consistent behavior can be difficult, laborious, and error-prone. To address the challenges involved in handling bx problems, dedicated languages and tools for bx have been developed. Due to their heterogeneity, however, the numerous and diverse approaches to bx are difficult to compare, with the consequence that fundamental differences and similarities are not yet well understood. This motivates the need for suitable benchmarks that facilitate the comparison of bx approaches. This paper provides a comprehensive treatment of benchmarking bx, covering theory, implementation, application, and assessment. At the level of theory, we introduce a conceptual framework that defines and classifies architectures of bx tools. At the level of implementation, we describe Benchmarx, an infrastructure for benchmarking bx tools which is based on the conceptual framework. At the level of application, we report on a wide variety of solutions to the well-known Families-to-Persons benchmark, which were developed and compared with the help of Benchmarx. At the level of assessment, we reflect on the usefulness of the Benchmarx approach to benchmarking bx, based on the experiences gained from the Families-to-Persons benchmark.},
journal = {Softw. Syst. Model.},
month = may,
pages = {647–691},
numpages = {45},
keywords = {Bidirectional transformation, Benchmark, Model synchronization, Framework}
}

@article{10.1016/j.eswa.2005.06.026,
author = {Tseng, Hwai-En and Chang, Chien-Chen and Chang, Shu-Hsuan},
title = {Applying case-based reasoning for product configuration in mass customization environments},
year = {2005},
issue_date = {November, 2005},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {29},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2005.06.026},
doi = {10.1016/j.eswa.2005.06.026},
abstract = {Product variation and customization is a trend in current market-oriented manufacturing environment. Companies produce products in order to satisfy customer's needs. In the customization environment, the R&amp;D sector in an enterprise should be able to offer differentiation in product selection after they take the order. Such product differentiation should meet the requirement of cost and manufacturing procedure. In the light of this, how to generate an accurate bill of material (BOM) that meets the customer's needs and gets ready for the production is an important issue in the intensely competitive market. The purpose of this study is to reduce effectively the time and cost of design under the premise to manufacture an accurate new product. In this study, the Case-Based Reasoning (CBR) algorithm was used to construct the new BOM. Retrieving previous cases that resemble the current problem can save a lot of time in figuring out the problem and offer a correct direction for designers. When solving a new problem, CBR technique can quickly help generate a right BOM that fits the present situation.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {913–925},
numpages = {13},
keywords = {Bill of material, Case-based reasoning, Feature tree, Mass-customization, Product configuration}
}

@article{10.1007/s10916-018-1003-9,
author = {Lan, Kun and Wang, Dan-Tong and Fong, Simon and Liu, Lian-Sheng and Wong, Kelvin K. and Dey, Nilanjan},
title = {A Survey of Data Mining and Deep Learning in Bioinformatics},
year = {2018},
issue_date = {August    2018},
publisher = {Plenum Press},
address = {USA},
volume = {42},
number = {8},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-018-1003-9},
doi = {10.1007/s10916-018-1003-9},
abstract = {The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.},
journal = {J. Med. Syst.},
month = aug,
pages = {1–20},
numpages = {20},
keywords = {Bioinformatics, Biomedicine, Data mining, Deep learning, Machine learning}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@article{10.1016/j.compind.2008.06.005,
author = {Janardanan, V. K. and Adithan, M. and Radhakrishnan, P.},
title = {Collaborative product structure management for assembly modeling},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {8},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2008.06.005},
doi = {10.1016/j.compind.2008.06.005},
abstract = {The paper aims at presenting a Web-based product structure manager that enables designers to hold virtual meetings via Internet and that allows collaborative viewing, modification of product structure and annotation of the 3D model. This Web-based product structure manager provides a shared platform for the designers. Two concerns of designers of large systems are addressed in this application. First, heterogeneous application systems are used by designers and second, designers are geographically dispersed. In this context, STEP standard was found superior for data exchange among diverse CAD systems and the product structure manager has been developed as a ''Web-based'' application to achieve collaboration among designers. A case study has been carried out with this developed architecture for assembly modeling of a launch vehicle system called launcher hold and release system and its outcome is discussed in detail in this paper.},
journal = {Comput. Ind.},
month = oct,
pages = {820–832},
numpages = {13},
keywords = {Collaborative product structure management, Computer-aided design, Product data management, Product structure, Product structure management, Standard for exchange of product model data}
}

@article{10.1145/2790303,
author = {Garc\'{\i}a-gal\'{a}n, Jes\'{u}s and Pasquale, Liliana and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {User-Centric Adaptation Analysis of Multi-Tenant Services},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/2790303},
doi = {10.1145/2790303},
abstract = {Multi-tenancy is a key pillar of cloud services. It allows different users to share computing and virtual resources transparently, meanwhile guaranteeing substantial cost savings. Due to the tradeoff between scalability and customization, one of the major drawbacks of multi-tenancy is limited configurability. Since users may often have conflicting configuration preferences, offering the best user experience is an open challenge for service providers. In addition, the users, their preferences, and the operational environment may change during the service operation, thus jeopardizing the satisfaction of user preferences. In this article, we present an approach to support user-centric adaptation of multi-tenant services. We describe how to engineer the activities of the Monitoring, Analysis, Planning, Execution (MAPE) loop to support user-centric adaptation, and we focus on adaptation analysis. Our analysis computes a service configuration that optimizes user satisfaction, complies with infrastructural constraints, and minimizes reconfiguration obtrusiveness when user- or service-related changes take place. To support our analysis, we model multi-tenant services and user preferences by using feature and preference models, respectively. We illustrate our approach by utilizing different cases of virtual desktops. Our results demonstrate the effectiveness of the analysis in improving user preferences satisfaction in negligible time.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jan,
articleno = {24},
numpages = {26},
keywords = {User systems, human information processing, multi-tenant services}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.1016/j.dss.2006.02.005,
author = {Mohan, Kannan and Jain, Radhika and Ramesh, Balasubramaniam},
title = {Knowledge networking to support medical new product development},
year = {2007},
issue_date = {August, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {4},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2006.02.005},
doi = {10.1016/j.dss.2006.02.005},
abstract = {New product development (NPD) in the pharmaceutical industry is very knowledge intensive. Knowledge generated and used during medical NPD processes is fragmented and distributed across various phases and artifacts. Many challenges in medical NPD can be addressed by the integration of this fragmented knowledge. We propose the creation and use of knowledge networks to address these challenges. Based on a case study conducted in a leading pharmaceutical company, we have developed a knowledge framework that represents knowledge fragments that need to be integrated to support medical NPD. We have also developed a prototype system that supports knowledge integration using knowledge networks. We illustrate the capabilities of the system through scenarios drawn from the case study. Qualitative validation of our approach is also presented.},
journal = {Decis. Support Syst.},
month = aug,
pages = {1255–1273},
numpages = {19},
keywords = {Healthcare, Knowledge integration, Knowledge networks, New product development, Pharmaceutical knowledge management}
}

@book{10.5555/2597852,
author = {Tam, Kenneth and Hoz Salvador, Mart\'{\i}n H. and McAlpine, Ken and Basile, Rick and Matsugu, Bruce and More, Josh},
title = {UTM Security with Fortinet: Mastering FortiOS},
year = {2012},
isbn = {9781597499774},
publisher = {Syngress Publishing},
abstract = {Traditionally, network security (firewalls to block unauthorized users, Intrusion Prevention Systems (IPS) to keep attackers out, Web filters to avoid misuse of Internet browsing, and antivirus software to block malicious programs) required separate boxes with increased cost and complexity. Unified Threat Management (UTM) makes network security less complex, cheaper, and more effective by consolidating all these components. This book explains the advantages of using UTM and how it works, presents best practices on deployment, and is a hands-on, step-by-step guide to deploying Fortinets FortiGate in the enterprise. Provides tips, tricks, and proven suggestions and guidelines to set up FortiGate implementations Presents topics that are not covered (or are not covered in detail) by Fortinets documentation Discusses hands-on troubleshooting techniques at both the project deployment level and technical implementation area Table of Contents Foreword Introduction Part I: General Introduction Chapter 1: Introduction to Unified Threat Management (UTM) Chapter 2: FortiGate Hardware Platform Overview Chapter 3: FortiOS Introduction Part II: UTM Technologies Explained Chapter 4: Connectivity and Networking Technologies Chapter 5: Base Network Security Chapter 6: Application Security Chapter 7: Extended UTM Functionality Chapter 8: Analyzing Your Security Information with FortiAnalyzer Chapter 9: Managing Your Security Configurations with FortiManager Part III: Implementing a Security (UTM) Project Chapter 10: Designing a Security Solution Chapter 11: Security on Distributed EnterprisesRetail (UTM Goes Shopping) Chapter 12: Security on Financial Institutions (UTM Goes to the Bank) Appendix A: Troubleshooting the Project Appendix B: Troubleshooting Technically}
}

@inproceedings{10.1145/1882362.1882369,
author = {Batory, Don},
title = {Thoughts on automated software design and synthesis},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882369},
doi = {10.1145/1882362.1882369},
abstract = {I summarize some personal observations on the topic of automated software design and synthesis that I accumulated over twenty years. They are intended to alert researchers to pitfalls that may be encountered and to identify goals for future efforts in advancing software engineering education and research.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {29–32},
numpages = {4},
keywords = {automated software synthesis},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@article{10.1016/j.infsof.2015.02.001,
author = {de Magalh\~{a}es, Cleyton V.C. and da Silva, Fabio Q.B. and Santos, Ronnie E.S. and Suassuna, Marcos},
title = {Investigations about replication of empirical studies in software engineering},
year = {2015},
issue_date = {August 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {64},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.02.001},
doi = {10.1016/j.infsof.2015.02.001},
abstract = {ContextTwo recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012). ObjectiveIn this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research. MethodWe applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996-2012 complemented by manual and automatic search procedures that collected articles published in 2013. ResultsWe analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012. ConclusionsReplication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {76–101},
numpages = {26},
keywords = {Empirical studies, Experiments, Mapping study, Replications, Software engineering, Systematic literature review}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@article{10.1147/SJ.2008.5386516,
author = {Lumpp, Th. and Schneider, J. and Holtz, J, and Mueller, M. and Lenz, N. and Biazetti, A. and Petersen, D.},
title = {From high availability and disaster recovery to business continuity solutions},
year = {2008},
issue_date = {October 2008},
publisher = {IBM Corp.},
address = {USA},
volume = {47},
number = {4},
issn = {0018-8670},
url = {https://doi.org/10.1147/SJ.2008.5386516},
doi = {10.1147/SJ.2008.5386516},
abstract = {We first provide an overview of the state-of-the-art architectures for continuous availability, briefly covering such traditional concepts as high-availability (HA) clustering on distributed platforms and on the mainframe. We explain how HA can be achieved in environments based on Sun Microsystems J2EE™, which differ from classical clustering approaches, and we discuss how disaster recovery (DR) has become an extension of HA. The paper then presents aspects of service management, including the use and orchestration of process-based (ITIL®) systems management tasks within DR scenarios, where the key challenge is to ensure the right level of redundancy in the integration and service-oriented management of heterogeneous information technology landscapes.},
journal = {IBM Syst. J.},
month = oct,
pages = {605–619},
numpages = {15}
}

@article{10.1016/j.jss.2016.03.067,
author = {Bauer, Veronika and Vetro', Antonio},
title = {Comparing reuse practices in two large software-producing companies},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.067},
doi = {10.1016/j.jss.2016.03.067},
abstract = {We compare two empirical investigations on software reuse at two large companies.We analyzed 108 survey responses and 35 h of interviews with 30 participants.Homogeneous, coherent settings produce clear benefits in development and maintenance.We identify coherence between culture and approach as important reuse success factor.Systematic reuse in heterogeneous contexts requires structured decision support. ContextReuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice__ __ ObjectiveWe propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. MethodWe compare and interpret the study results with a focus on reuse practices, effects, and context. ResultsBoth companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access.Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize.Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations. ConclusionTechnological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging.},
journal = {J. Syst. Softw.},
month = jul,
pages = {545–582},
numpages = {38},
keywords = {Empirical, Software engineering, Software reuse, Survey research, Technology transfer}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@techreport{10.5555/974958,
author = {Sun Microsystems Laboratories Staff},
title = {Fiscal 1996 Project Portfolio Report},
year = {1996},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {A summary of the significant accomplishments of Sun Microsystems Laboratories for the Fiscal Year ending June 30, 1996.}
}

@techreport{10.5555/1698220,
author = {Treichel, Jeanie and Chiu, Katie and Wu, Christopher and Wang, Jeanne},
title = {Sun labs: the second fifty technical reports},
year = {2009},
publisher = {Sun Microsystems, Inc.},
address = {USA},
abstract = {Since the establishment of Sun Labs in 1991, Sun Labs researchers have been publishing technical reports on the technologies developed in their projects. The importance of these reports cannot be underestimated. This commemorative issue spans the years 1995-2001 and is a compendium of abstracts of the second fifty Technical Reports, as well as bios of the authors.}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@article{10.1016/j.scico.2015.09.005,
author = {Gonz\'{a}lez-Torres, Antonio and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n-S\'{a}nchez, Roberto and Colomo-Palacios, Ricardo},
title = {Knowledge discovery in software teams by means of evolutionary visual software analytics},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2015.09.005},
doi = {10.1016/j.scico.2015.09.005},
abstract = {The day-to-day management of human resources that occurs during the development and maintenance process of software systems is a responsibility of project leads and managers, who usually perform such a task empirically. Moreover, rotation and distributed software development affect the establishment of long-term relationships between project managers and software projects, as well as between project managers and companies. It is also common for project leads and managers to face decision-making on human resources without the necessary prior knowledge. In this context, the application of visual analytics to software evolution supports software project leads and managers using analysis methods and a shared knowledge space for decision-making by means of visualization and interaction techniques. This approach offers the possibility of determining which programmer has led a project or contributed more to the development and maintenance of a software system in terms of revisions. Moreover, this approach helps to elucidate both the software items1 that have been changed in common by a group of programmers and who has changed what software items. With this information, software project leads and managers can make decisions regarding task assignment to developers and staff substitutions due to unexpected situations or staff turnover. Consequently, this research is aimed at supporting software practitioners in tasks related to human resources management through the application of Visual Analytics to Software Evolution. Maleku supports software teams through the application of Visual Analytics.New Visualizations are focused mainly in practitioners contributions.Implemented visualizations require fewer reading and processing time.},
journal = {Sci. Comput. Program.},
month = jun,
pages = {55–74},
numpages = {20},
keywords = {Evolutionary Visual Software Analytics, Software Evolution, Software Evolution Visualization, Visual Analytics, Visual Software Analytics}
}

@article{10.1145/329155.329160,
author = {Conn, Richard},
title = {Notes from the 12th Conference on Software Engineering Education and Training (CSEET) and 13th SIGCSE Technical Symposium on Computer Science Education (CSE)},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/329155.329160},
doi = {10.1145/329155.329160},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {30–32},
numpages = {3}
}

@inproceedings{10.1145/1083190.1083191,
author = {Dinkel, Michael and Baumgarten, Uwe},
title = {Modeling nonfunctional requirements: a basis for dynamic systems management},
year = {2005},
isbn = {1595931287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083190.1083191},
doi = {10.1145/1083190.1083191},
abstract = {The management of dynamic systems is an upcoming challenge for software engineers in automotive and other embedded systems. The complexity of current automotive computing systems is already difficult to handle for car makers and the expected growth in the area of electronic devices in vehicles will even intensify this situation. This paper presents a model based approach for enabling automatic configuration of distributed component oriented systems. Nonfunctional requirements and capabilities of software components and platforms are explicitly modeled and provide for well-founded statements whether a component is able to execute on a certain platform or not. With application models and platform models the validity of a configuration is defined in this paper. The models even allow reconfigurations based on information regarding the actual system context like user behavior, backend or environmental sensor information.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Automotive Systems},
pages = {1–8},
numpages = {8},
keywords = {NFR, application model, capability, component, configuration, nonfunctional requirement, platform model, software management, system management},
location = {St. Louis, Missouri},
series = {SEAS '05}
}

@inproceedings{10.5555/645882.672390,
author = {Jolley, Truman M. and Kasik, David J. and Kimball, Conrad E.},
title = {Governance Polarities of Internal Product Lines},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Tension occurs when multiple organizations develop and deliver their own product lines to a single user community. We apply polarity management to the governance of shared architecture, products and processes for the delivery and management of tens of product lines. These product lines contain hundreds of applications used by thousands of engineers in Boeing Commercial Airplanes. This paper focuses on the use of polarity management to construct extensible governance bodies and processes for the second phase of product line expansion. We define polarity to be "two principles which are both true but conflict." Polarities are often mistaken to be problems to be solved; however, polarities are held, not solved. Polarity management of the product line infrastructure, a complex customer-supplier network, identifies primary organizational tensions that require management; poorly held polarities cause chaos and failure.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {284–298},
numpages = {15},
series = {SPLC 2}
}

@article{10.1007/s10922-009-9119-3,
author = {Strassner, John and Souza, Jos\'{e} Neuman and Meer, Sven and Davy, Steven and Barrett, Keara and Raymer, David and Samudrala, Srini},
title = {The Design of a New Policy Model to Support Ontology-Driven Reasoning for Autonomic Networking},
year = {2009},
issue_date = {June      2009},
publisher = {Plenum Press},
address = {USA},
volume = {17},
number = {1–2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-009-9119-3},
doi = {10.1007/s10922-009-9119-3},
abstract = {The purpose of autonomic networking is to manage the business and technical complexity of networked components and systems. However, the lack of a common lingua franca makes it impossible to use vendor-specific network management data to ascertain the state of the network at any given time. Furthermore, the tools used to analyze management data, which include information and data models, ontologies, machine learning algorithms, and policy languages, are all different, and hence require different data in different formats. This paper describes a new version of the Directory Enabled Networks next generation (DEN-ng) policy model, which is part of the FOCALE autonomic network architecture. This new policy model has been built using three guiding principles: (1) the policy model is rooted in information models, so that it can govern managed entities, (2) the model is expressly constructed to facilitate the generation of ontologies, so that reasoning about policies constructed from the model may be done, and (3) the model is expressly constructed so that a policy language can be developed from it.},
journal = {J. Netw. Syst. Manage.},
month = jun,
pages = {5–32},
numpages = {28},
keywords = {Autonomic networking, FOCALE autonomic architecture, Next generation services, Ontology-based management, Policy management, Semantic reasoning}
}

@inproceedings{10.1145/2493288.2493291,
author = {G\"{u}nther, Sebastian and Cleenewerck, Thomas},
title = {Design principles for internal domain-specific languages: a pattern catalog illustrated by Ruby},
year = {2010},
isbn = {9781450301077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493288.2493291},
doi = {10.1145/2493288.2493291},
abstract = {Some programming languages, especially dynamic programming languages, offer suitable mechanisms for the construction of internal domain-specific languages (DSL). Internal DSLs inherit the facilities of their host language such as the availability of libraries, frameworks, tool support, and other DSLs. When developing an internal DSL, there are two challenges. First, to cope with the host language's syntactic and semantic restrictions. Careful and thoughtful extensions and modifications of the host language are crucial to overcome these restrictions without reverting to poor language design. Second, to support several design principles that are genuine for a DSL. Although there is an extensive body of knowledge about DSL design principles and desirable quality properties, it remains difficult to apply them, or to reason about whether a particular DSL exhibits specific desirable principles.Our objective is to put the two perspectives together. We research the most important design principles of a DSL and show how different patterns can be used to support these principles. This allows us to produce an extensive pattern catalog which is the foundation of principled approach for designing internal DSLs. The patterns can be used to assess the design quality of a DSL and structure its implementation. In particular, we show a complex DSL example illustrating each principle and the corresponding patterns. While we stick to Ruby for the explanation and application of the patterns, we also name known uses in Python, Scala, and Smalltalk. Patterns are explained with their context, problem, forces, solution, and consequences. We also explain the patterns with the classical structure of intent, motivation, forces, implementation and their consequences. Finally we reflect upon the pattern utilization by discussing their strengths and weaknesses.},
booktitle = {Proceedings of the 17th Conference on Pattern Languages of Programs},
articleno = {3},
numpages = {35},
keywords = {design principles, domain-specific languages, dynamic programming languages},
location = {Reno, Nevada, USA},
series = {PLOP '10}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@book{10.5555/2568170,
author = {Johnson, Leighton},
title = {Computer Incident Response and Forensics Team Management: Conducting a Successful Incident Response},
year = {2013},
isbn = {159749996X},
publisher = {Syngress Publishing},
edition = {1st},
abstract = {Computer Incident Response and Forensics Team Management provides security professionals with a complete handbook of computer incident response from the perspective of forensics team management. This unique approach teaches readers the concepts and principles they need to conduct a successful incident response investigation, ensuring that proven policies and procedures are established and followed by all team members. Leighton R. Johnson III describes the processes within an incident response event and shows the crucial importance of skillful forensics team management, including when and where the transition to forensics investigation should occur during an incident response event. The book also provides discussions of key incident response components. Provides readers with a complete handbook on computer incident response from the perspective of forensics team managementIdentify the key steps to completing a successful computer incident response investigation Defines the qualities necessary to become a successful forensics investigation team member, as well as the interpersonal relationship skills necessary for successful incident response and forensics investigation teams}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/181610.181620,
author = {Poulin, Jeff and Tracz, Will},
title = {WISR'93: 6th annual workshop on software reuse: summary and working group reports},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/181610.181620},
doi = {10.1145/181610.181620},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {55–71},
numpages = {17}
}

@book{10.5555/2597865,
author = {Loshin, David},
title = {Business Intelligence: The Savvy Manager's Guide},
year = {2012},
isbn = {9780123858900},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Following the footsteps of the first edition, the second edition of Business Intelligence is a full overview of what comprises business intelligence. It is intended to provide an introduction to the concepts to uncomplicate the learning process when implementing a business intelligence program. Over a relatively long lifetime (7 years), the current edition of book has received numerous accolades from across the industry for its straightforward introduction to both business and technical aspects of business intelligence. As an author, David Loshin has a distinct ability to translate challenging topics into a framework that is easily digestible by managers, business analysts, and technologists alike. In addition, his material has developed a following (such as the recent Master Data Management book) among practitioners and key figures in the industry (both analysts and vendors) and that magnifies our ability to convey the value of this book. Guides managers through developing, administering, or simply understanding business intelligence technology Keeps pace with the changes in best practices, tools, methods and processes used to transform an organizations data into actionable knowledge Contains a handy, quick-reference to technologies and terminology. Table of Contents 1. Business Intelligence - An Introduction 2. Value Drivers 3. Planning for Success 4. Developing a Business Intelligence Roadmap 5. The Business Intelligence Environment 6. Business Models and Information Flow 7. Data Requirements Analysis 8. Data Warehouses and the Technical BI Architecture 9. Business Metadata 10. Data Profiling 11. Business Rules 12. Data Quality 13. Data Integration 14. High Performance BI 15. Alternate Information Contexts 16. Location Intelligence and Spatial analysis 17. Knowledge Discovery, Data Mining, and Analytics 18. Using Publicly Available Data 19. Knowledge Delivery 20. New and Emerging Techniques 21. Quick Reference}
}

@book{10.5555/1207340,
author = {Contos, Brian T. and Hunt, Steve and Derodeff, Colby},
title = {Physical and Logical Security Convergence: Powered By Enterprise Security Management: Powered By Enterprise Security Management},
year = {2007},
isbn = {1597491225},
publisher = {Syngress Publishing},
abstract = {Government and companies have already invested hundreds of millions of dollars in the convergence of physical and logical security solutions, but there are no books on the topic. This book begins with an overall explanation of information security, physical security, and why approaching these two different types of security in one way (called convergence) is so critical in today's changing security landscape. It then details enterprise security management as it relates to incident detection and incident management. This is followed by detailed examples of implementation, taking the reader through cases addressing various physical security technologies such as: video surveillance, HVAC, RFID, access controls, biometrics, and more. *This topic is picking up momentum every day with every new computer exploit, announcement of a malicious insider, or issues related to terrorists, organized crime, and nation-state threats *The author has over a decade of real-world security and management expertise developed in some of the most sensitive and mission-critical environments in the world *Enterprise Security Management (ESM) is deployed in tens of thousands of organizations worldwide}
}

@techreport{10.1145/2594148,
title = {Computing Curricula 1991: Report of the ACM/IEEE-CS Joint Curriculum Task Force},
year = {1991},
isbn = {089793817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This report contains curricular recommendations for baccalaureate programs in the discipline of computing, which includes programs with the titles "computer science," "computer engineering," "computer science and engineering," and other similar titles. These recommendations provide a uniform basis for curriculum design across all segments of the educational community---schools and colleges of engineering, arts and sciences, and liberal arts. This report is the first comprehensive undergraduate curriculum report to be endorsed by the two major professional societies in the computing discipline---the Association for Computing Machinery and the Computer Society of the IEEE.}
}

